[
    {
        "title": "Music Similarity Measures: What&apos;s the use?",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.3727057",
        "url": "https://doi.org/10.5281/zenodo.3727057",
        "ee": "https://zenodo.org/records/3727057/files/Thesis Adriana Suarez Final V.pdf",
        "abstract": "This thesis work conduits research toward the estimation of relevance judgments for the task of Audio Music Similarity in the context of MIREX. It is intended to improve and support the evaluation experiments run for this task from the point of view of efficiency, studying different regression models and methods with the aim of reducing the cost of the annotation process. Therefore, by doing better estimations of relevance judgments and using all the tools at hand (research, literature, technology) the time used by people performing this task can be utilized in others activities.",
        "zenodo_id": 3727057,
        "dblp_key": "conf/ismir/AucouturierP02",
        "keywords": [
            "Audio Music Similarity",
            "MIREX",
            "Relevance Judgments",
            "Regression Models",
            "Annotation Process",
            "Evaluation Experiments",
            "Efficiency",
            "Research",
            "Technology",
            "Other Activities"
        ],
        "content": "1  REDUCING BIAS IN THE PROBABILISTIC EVALUATION OF AUDIO MUSIC SIMILARITY   Adriana M. Suárez I.   A thesis submitted for the degree of Master in Sound and Music Computing September 2015, Barcelona, Spain    Advisor:  PhD Julian Urbano  Co- Advisor: PhD Emilia Gómez   Department of Information and Communication Technologies  Universitat Pompeu Fabra     \n\t\n\t2                       \t\n\t3  To my beloved brother Julio, who was my biggest supporter                         \t\n\t4                                  \t\n\t5 Acknowledgments   This work would not have been possible without the help, the support and contribution of many people.  First, and foremost, I thank God for the strength and blessings in completing this work. I will always be thankful for the chance, power and wisdom provided during this master course.  Second, I would like to thank my supervisor Dr. Julian Urbano for his valuable and generous guidance, advice and encouragement through this study.  He is a committed researcher and a master programmer in R who has always been willing to transmit all he knows without hesitation. Under his supervision, I learned a lot of statistic and programming tricks in R.  More importantly, I instilled the spirit of challenge and confidence of going for research of topics considered “different” and still maintain the conviction until the end.  Third, I must express my gratitude to all the crew from the Music and Sound Technology group.  To their head, Dr. Xavier Serra, for the opportunity he provided me to take part of this master. Also, to my co-supervisor, Ph. D Emilia Gómez, the teachers and classmates whom I had shared this chance.  It was a great pleasure to study in a multicultural environment. All the knowledge they transmitted is truly appreciated.  Fourth, I cannot forget the valuable support of my international friends that I had the opportunity to meet in my stay in Barcelona.  I appreciate their encouragement throughout the ups and downs of my study, and their presence and help throughout my stay away from home. Lastly, but by no means the least, I am extremely grateful to my family to whom this thesis is dedicated.  I will be always helpful for their immeasurable love, care and support, which they have always given and it has always been helpful; even \t\n\t6 from distance.  Grateful for understanding this journey that made a significant change in my career, in which I bet a lot but also I know I gained even more, both as a person and as a professional.  Also grateful for comprehending my physical absence in meaningful family times.  Moltes gràcies. Thank you a lot. Muchas gracias.                             \t\n\t7              “Thank you for the music, the songs I'm singing Thanks for all the joy they're bringing Who can live without it, I ask in all honesty What would life be? Without a song or a dance what are we? So I say thank you for the music For giving it to me”   ABBA             \t\n\t8  Abstract  This thesis work conduits research toward the estimation of relevance judgments for the task of Audio Music Similarity in the context of MIREX. It is intended to improve and support the evaluation experiments run for this task from the point of view of efficiency, studying different regression models and methods with the aim of reducing the cost of the annotation process. Therefore, by doing better estimations of relevance judgments and using all the tools at hand (research, literature, technology) the time used by people performing this task can be utilized in others activities.     Resumen   Este trabajo de tesis consiste en  una investigación acerca de la estimación de juicios de relevancia para la tarea de Audio Music Similarity en el contexto de MIREX.  Fue creado para mejorar y apoyar los experimentos de evaluación realizados desde el punto de vista de la eficiencia, estudiando diferentes modelos y métodos probabilísticos con el objeto de reducir el costo del proceso de anotaciones.   Realizando mejores estimaciones de los juicios de relevancia y usando las herramientas disponibles (investigación, estado de la técnica, tecnología) el tiempo usado por las personas que realizan esta tarea actualmente, podría ser mejor utilizado en la ejecución de otras actividades.   \t\n\t9                            \t\n\t10  Index   Abstract .................................................................................................................... 8 Index ....................................................................................................................... 10 Figure List ............................................................................................................... 12 Table List ................................................................................................................ 13 Chapter 1 ................................................................................................................ 15 INTRODUCTION .................................................................................................... 15 1.1 Information Retrieval ........................................................................................ 16 1.2. Information Retrieval Evaluation ..................................................................... 17 1.2.1 Early Work in Text Information Retrieval Evaluation ............................................. 17 1.2.2 Early Work in Music Information Retrieval Evaluation ........................................... 19 1.3. Audio Music Similarity ..................................................................................... 22 1.4 Importance of Evaluation in Music Information Retrieval and Motivation ......... 22 Chapter 2 ................................................................................................................ 24 STATE OF THE ART ............................................................................................. 24 2.1 MIREX Evaluation Process .............................................................................. 24 2.1.1 The Cranfield Paradigm ......................................................................................... 24 2.1.2. MIREX Evaluation in Audio Music Similarity ........................................................ 25 2.2 Validity, Reliability and Effectiveness ............................................................... 26 Chapter 3 ................................................................................................................ 35 IMPROVING THE ESTIMATION OF RELEVANCE ............................................... 35 1. Using others configurations of Ordinal Logistic Regression models .......................... 35 2. Implementing others regression models in order to obtain better results ................... 37 3. Improving model’s attributes ....................................................................................... 38 4. Implementing new attributes ....................................................................................... 40 4.1 Cluster of Genres .................................................................................................. 40 4.2 Using the distances’ media of similarity between genres ...................................... 41 4.3    Using metadata to deal with artist information ....................................................... 44 Conclusions ............................................................................................................ 46 Future work ............................................................................................................ 48 References ............................................................................................................. 49  \t\n\t11                            \t\n\t12  Figure List   Fig 1. Timeline of evaluation in text ir (top) and music ir (bottom). ......................... 21 Fig 2. Estimated vs. Actual absolute effectiveness scores in mirex ....................... 34                      \t\n\t13  Table List   Table 1. Output-based features .............................................................................. 31\tTable 2. Judgment-based features  ....................................................................... 31\tTable 3. Features for the two models ..................................................................... 32\tTable 4. Likehood-ratio Chi-squared  statistic ........................................................ 32\tTable 5. Implementation of rms and MASS packages for Ordinal Logistic Regression in R. . ................................................................................................... 36\tTable 6. Implementation of Logit and Probit regression. ........................................ 37\tTable 7. Implementation of Multinomial Linear Regression). ................................. 38\tTable 8. Implementation of backward elimination of predictors for Mout. ................ 39\tTable 9. Proposed clustering of genres of MIREX’s data. ...................................... 40\tTable 10. Implementation of a new attribute into a Logistic Regression Model. .... 41\tTable 11. Media of distances between genres of queries and songs. ................... 42\tTable 12. Implementation of distance as attributes into a Logistic Regression Model.. .................................................................................................................... 43\tTable 13. Implementation of the attribute artist similarity into a Logistic Regression Model. ..................................................................................................................... 45\t          \t\n\t14                               \t\n\t15 Chapter 1 INTRODUCTION   Information retrieval (IR) is the field concerned with representing, searching, and manipulating large collections of electronic text and other human-language data (Buettcher, Cormack and Clarke, 2010). A user has information need and use an IR system in order to retrieve relevant information from a document collection.  IR systems are boundless and even essential nowadays since they facilitate daily life of people supporting activities in business, entertainment, education, medical services, and so on. Web services engines like Google, Yahoo, among others are the most popular web IR services for their great capacity of converging information from different sources. Music IR systems like Shazam, implementing music identification technology are quite popular and useful today.  These systems have been used prior the invention of computers. Before 1940’s intelligence and commercial retrieve systems where already implemented and just until the appearance of the first computer-based systems, mechanical and electro- mechanical devices performed the retrieve functions. With the generalization of the computers, IR technics grew up as the increase of storage and processor speed allowed managing bigger datasets (Sanderson, M., & Croft, W. B, 2012).  IR has been widely used through the story from several fields: text and cross- language, image and multimedia, speech and music (Manning, C. D., Raghavan, P., & Schütze, H, 2008). In the case of music, Music Information Retrieval (MIR) is concerned on the extraction and inference of meaningful features of music, it’s indexing and the development of different search and retrieval schemes (Downie, J. S, 2003), (Orio, N., 2006), (Schedl, M., Gómez E., & Urbano J., 2014). It started with the analysis of symbolic representations of songs (mostly MIDI scores); with the evolution of computing systems during the early 2000’s, signal processing was \t\n\t16 also included permitting the extraction of features directly from the audio. (Manning, C. D., Raghavan, P., & Schütze, H, 2008). These features are pitch, temporal, harmonic, timbral, editorial, and textual and bibliographic facets.  \t 1.1 Information Retrieval \tInformation retrieval (IR) is the field concerned with representing, searching, and manipulating large collections of electronic text and other human-language data (Buettcher, Cormack and Clarke, 2010). A user has information need and use an IR system in order to retrieve relevant information from a document collection.  IR systems are boundless and even essential nowadays since they facilitate daily life of people supporting activities in business, entertainment, education, medical services, and so on. Web services engines like Google, Yahoo, among others are the most popular web IR services for their great capacity of converging information from different sources. Music IR systems like Shazam, implementing music identification technology are quite popular and useful today.  These systems have been used prior the invention of computers. Before 1940’s intelligence and commercial retrieve systems where already implemented and just until the appearance of the first computer-based systems, mechanical and electro- mechanical devices performed the retrieve functions. With the generalization of the computers, IR technics grew up as the increase of storage and processor speed allowed managing bigger datasets (Sanderson, M., & Croft, W. B, 2012).  IR has been widely used through the story from several fields: text and cross- language, image and multimedia, speech and music (Manning, C. D., Raghavan, P., & Schütze, H, 2008). In the case of music, Music Information Retrieval (MIR) is concerned on the extraction and inference of meaningful features of music, it’s indexing and the development of different search and retrieval schemes (Downie, J. S, 2003). It started with the analysis of symbolic representations of songs \t\n\t17 (mostly MIDI scores); with the evolution of computing systems during the early 2000’s, signal processing was also included permitting the extraction of features directly from the audio. (Manning, C. D., Raghavan, P., & Schütze, H, 2008). These features are pitch, temporal, harmonic, timbral, editorial, and textual and bibliographic facets.   1.2. Information Retrieval Evaluation  \tEvaluation has come to play a critical role in information retrieval research (Downie, 2002) as it allows measuring how successfully an information retrieval system meets the goal of assessing users to fulfill their information needs. The IR community has paid a lot of attention to the topic, implementing evaluation standards and experimental rigor on investigations, which have been effective in moving the field forward. Music Information Retrieval initially followed the evaluation practices of text; however, not enough research has been done to properly know when this approach can be fully applied or not because music, unlike text has, a complex nature.   1.2.1 Early Work in Text Information Retrieval Evaluation   Evaluation in Text Information Retrieval has been the focus of a lot of research:  - The Cranfield Project 2 (1962-1966) was an experiment accomplished by Cyril Cleverdon (Cleverdon, 1991) and considered as the basis that shaped the form that IR evaluation will take for the next years. In this project, experiments were conducted in order to test and compare different search strategies in a controlled laboratory environment (test collection).  - The MEDLARS (Medical Literature Analysis and Retrieval System) Demand \t\n\t18 Search Service (1966-1967) was one of the early operational computer-based retrieval systems. It considered the evaluation of a complete system from a user perspective, taking into consideration the user requirements (Lancaster, 1968).  - The SMART project (1961-1995) (System for the Mechanical Analysis and Retrieval of Text) was created both as a retrieval tool and as a vehicle for evaluating the effectiveness of a large variety of automatic search and analysis techniques, where the main evaluation viewpoint taken was the user (Kent, Lancour, Daily, 1980).  - TREC 2 (Text Retrieval Conference) started (1992) as an annual venue to support research within the information retrieval community by providing the necessary infrastructure for large-scale evaluation of text retrieval methodologies.  - NTCIR (National Institute of Informatics- Test beds and Community for Information access Research) (1999) provided almost the same infrastructure than TREC but for Asian languages.  - CLEF 2 (Conference and Labs of the Evaluation Forum) (2000) was created to promote research, innovation, and development of information access systems with an emphasis on multilingual and multimodal.  - INEX (Evaluation of XML retrieval) (2002), which focuses on structured information.    \t1 http://trec.nist.gov/overview.html  2 http://www.clef-initiative.eu/web/clef-initiative/home  \t\t\n\t19 1.2.2 Early Work in Music Information Retrieval Evaluation   Some initiatives towards the development of Music Information Retrieval evaluation frameworks took place. The organization of the first International Symposium on Music Information Retrieval (ISMIR) in 2000, with the intention of bringing together the MIR research community into one location to treat among other topics, the creation of formal evaluation standards for MIR (Downie, 2000) was one of them. As a consequence, some workshops on the creation of standardized test collections, tasks and metrics for music digital library (MDL) and Music Information Retrieval (MIR) Evaluation, were placed in July 2002 at the ACM/IEEE Joint Conference on Digital Libraries. The outcome of these workshops was the recognition by the Music IR community’s of the creation of a periodic evaluation forum for Music Information Retrieval systems. The story of MIR evaluation has been shaped since then:  - During the 5th edition of the ISMIR in 2004, placed in Barcelona, Spain, an Audio Description Contest (ADC) 3 was accomplished. It proposed some tasks in order to define evaluation and statistical methods to compare systems.  - In 2005, the MIREX 4  (Music Information Retrieval Evaluation eXchange) run for the first time as the community-based framework for the formal evaluation of Music Information Retrieval (MIR) systems and algorithms. MIREX is coordinated and managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana- Champaign.  - MusiClef, which run from 2011-2013 covered multimodal music tagging (Orio, Liem, Peeters, & Schedl, 2012) and focus evaluation on professional application scenarios. \t3 http://ismir2004.ismir.net/ISMIR_Contest.html  4 http://www.music-ir.org/mirex/wiki/  \t\t\n\t20 - The Million Song Dataset Challenge (MSD, 2012) was created to overcome music dataset sharing limitations (Bertin-Mahieux, Ellis, Whitman, & Lamere, 2011). With this approach, researchers could grant access to a number of features but not to the algorithm that performs the process, neither to the audio.  - Quaero-Eval, inspired by NIST and MIREX evaluations, since 2012 focuses on audio and music processing. In this venue the tasks are agreed first with the participants and then a common repository is shared. The algorithms are run in a test sets with evaluation frameworks by an independent body that does not participate in the evaluation process.  - MediaEval 5, 2010, is a initiative focuses in multimodal approaches involving human and social aspects of multimedia e.g., speech recognition, multimedia content analysis, music and audio analysis, user-contributed information (tags, tweets), etc.   Fig 1 graphically encompasses both text and music evaluation initiatives.  \n\t5 http://www.multimediaeval.org/about/  \t\t\n\t21  Fig 1. Timeline of Evaluation in Text IR (top) and Music IR (bottom).              \n\t\n\t22 1.3. Audio Music Similarity   Audio Music Similarity (AMS) deals with the challenge of discovering similar songs. It is generally used in MIR task such as music recommendation, playlist generation or plagiarism detection. AMS is one of the most important tasks in MIR and has participated in MIREX since 2006, evaluating so far 85 systems. Furthermore, the same document collection with 7,000 audio documents has been used since 2007.  In the context of MIREX this task resembles the Text IR scenario: for a given audio clip (the query), a system returns a list of songs from a corpus (candidate songs), sorted by their musical similarity to the query.   1.4 Importance of Evaluation in Music Information Retrieval and Motivation   The Roadmap for Music Information Research, created for the expansion of the context of research from the perspectives of technological advances, stated as one of the main challenges: “vi) promoting best practice evaluation methodology, defining meaningful evaluation methodologies and targeting long-term sustainability of MIR (Serra, Magas, Benetos, Chudy, Dixon, Flexer, Widmer, 2013).  In spite of all initiatives created to widen the scope of evaluation, MIR community is still concern on the way that systems are evaluated because current evaluation practices do not fully allow them to improve as much as they wish (Peeters, Urbano, & Jones, 2012). Furthermore, research by (J Urbano, Schedl, & Serra, 2013), demonstrated that evaluation in ISMIR comprised only 6% of research.  MIREX has been a significant venue to convey the study and establishment of MIR evaluation frameworks; although it was created mirroring TREC methodologies, \t\n\t23 eventually the Music IR community has realized that not everything from text applies to music. Also their evolution in time have been different; in text for instance, research in evaluation has produced an environment of continuous improvement, which has not been the case in Music IR. It seems that MIR community does not seem to pay as much attention as evaluation as it should.  Particularly, in the case of Audio Music Similarity, few studies about the influence of this TREC-like approach have been done.  The purpose of this thesis is to improve the evaluation process in Audio Music Similarity task in MIREX, studied from the perspective of efficiency with emphasis in the reduction of annotation cost.  The approach to follow is twofold: first, study the literature of low cost evaluation in Audio Music Similarity. Second, study models and methods in order to propose a new or improve the existing framework to estimate relevance judgments in Audio Music Similarity.           \t\n\t24 Chapter 2 STATE OF THE ART   2.1 MIREX Evaluation Process   2.1.1 The Cranfield Paradigm   MIREX provides an evaluation framework for MIR researches to compare, contrast and discuss the result of their algorithms and techniques in the same way than TREC has done it to the text Information retrieval community (Downie et al., 2014). In general, MIREX and TREC use test collection with evaluation measures in order to assess effectiveness of their systems. Test collection are a resource used to test and compare search strategies in a laboratory environment. They are composed by:  1. Collection of documents of significant size.   2. Tasks and/or queries to be performed on the test collections; and,   3. Relevance judgments (qrels) compose of a list of document/pair describing the relevance of documents to topics.  Test collection along with evaluation measures stipulates a simulation of users in a real searching environment. They are generally used by researchers for instance to asses retrieval systems in isolation helping finding failures inside their applications and comparing effectiveness among them.  In order to asses the performance of systems, both TREC and MIREX follow the Cranfield’s paradigm which is a test bed consisting in a set of documents D, a set of Information need statements or queries Q and a set of relevance judgments R \t\n\t25 that is compiled by human assessors H, which tell what documents should be retrieved for which query (ground truth). In Music Information Retrieval one of the task that emulate this behavior is Audio Music Similarity: for a given audio clip (the query), an AMS system returns a list of music pieces (documents) considered to be similar to it.   2.1.2. MIREX Evaluation in Audio Music Similarity   For the evaluation of system’s effectiveness in the task of Audio Music Similarity in MIREX, relevance judgments and effectiveness measures are utilized. The relevance judgments in this context are scores given to each query-candidate, representing their similarity. In a real scenario, the task of collecting these judgments takes several days or weeks (J Urbano & Schedl, 2013)  In general terms, the evaluation process in MIREX runs as follows:  1. ~50  queries 6 Q are selected randomly and deliver to the participants.   2. The participant systems retrieved a ranked list with the 107 most similar pieces of music from a music collection D. These music pieces are 30-  second audio clips of music material.   3. All the results are consolidated and evaluated this time using subjective  judgments (ground truth) by human assessor using a software tool called  “Evaluatron 6000” (E6K).   4. After listening to each query-candidate pair, graders were asked to rate the  degree of similarity of the candidate audio excerpt to the query in two ways:  a) By selecting one of the three BROAD categories of \t6 In past editions of MIREX 100 queries were used  7 In past editions of MIREX, 5  similar musical pieces were retrieved   \t\n\t26 similarity: Not Similar (NS), Somewhat Similar (SS), and Very Similar (VS); and,  b) By assigning a FINE8 score between 0.0 (Least similar) and 10.0 (Most  similar).  In the case of effectiveness measures, the one reported to assess effectiveness in Audio Music Similarity is CG@10 (Average Gain after 10 audio documents retrieved) (Downie, Ehmann, Bay, Jones, 2010). For an arbitrary system A:  \t\t\t\t𝐶𝐺@𝑘 = %&\t\t ∑𝐺(&()%       (1) Where 𝐺(\tis the gain of the i-th document (song) retrieved - the similarity scored assigned- by graders, using FINE or BROAD scale. After the process of judging is done, the mean score of the gains obtained for every executed query ranks the systems. In order to minimize random effects the Friedman test is run with the Average Gain score of every system, with the Tukey’s HSD to correct the experiment-wide Type I error rate. The result of this evaluation is a scale- dependent pairwise comparisons between systems, telling which one is better for the current set of queries Q.   2.2 Validity, Reliability and Efficiency  Validity, reliability and efficiency are crucial aspects of testing. All IR evaluation experiments need to be guided considering them. This thesis work will be focus from the point of view of efficiency (Urbano, Schedl & Serra, 2013).  Validity is the extent to which the experiment actually determines what the experimenter wishes to determine (Tague-Sutcliffe, 1992). For example, are the \t8 In past editions of MIREX this value was between 0 and 100  \t\t\n\t27 selected variables really representatives of the experiment? Or in an evaluation experiment, is system A better than system B?  Reliability is the extent to which the experimental results can be replicated. (Tague- Sutcliffe, 1992). Thus, if an experiment is replicated, will we obtain similar results? There is a close relationship between validity and reliability. For example, if with one sample system A performs better than system B, but with a different sample is the opposite case, our results then cannot be repeatable; hence they will be unreliable.  Efficiency is the extent to which an experiment is effective (valid and reliable) (Tague-Sutcliffe, 1992). For instance, if in an evaluation experiment the ground truth annotation process is inaccurate, the validity of the result can be affected. On the other hand, if this ground truth is not efficient enough (as stated before, this process can be tedious and expensive) the reliability of the results may be impacted as well. Therefore, evaluation experiments must find a balance between validity and reliability and the efficient cost of the annotation process. In this context, do exist others experiments related to low cost annotations process to obtain valid and reliable results?  For this reason, searching in the present literature in response to the latter question is a must.  For example, some studies presents that judgments are affected by many characteristics of retrieved records and users, and also by situational factors (Harter, 1996). Therefore, some research shows that crowdsourcing is a viable alternative for the creation of relevance judgment; however because of the diversity in the backgrounds of participants, some control methods need to be established (Alonso, Rose, & Stewart, 2008).  Another approach to this matter is to decrease the necessary number of judgments. For example, in the pooling method, a set of top d ranked documents \t\n\t28 returned by participating systems is selected to create the pool of documents that need to be judged (Spärk Jones & van Rijsbergen, 1975). Next, all the duplicates documents into the pool are eliminated (considered non-relevant) and the remaining ones are evaluated by assessors. TREC was the first event that used these partial relevance judgments. This technique has its drawbacks, for example, the existence of defective systems could affect the pooling methods and assessors can evaluate thousands of irrelevant items.  Some research is focus in how evaluate systems with incomplete judgments and still be confident with the results of the experiments. The idea is to use random variables to represent relevance judgments; the estimation of these values though, can have some degree of error and uncertainty, but also, for most documents they work pretty well.  Let 𝐺( being a Random Variable representing the relevance level assigned to document d. It presents a multinomial distribution and depends of the scale used by human assessors.  The expectation and variances can be defined as random variables as well:  𝐸 [𝐺(]= ∑(𝐺(\t\t=𝑙).\t\t𝑙0\t12\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tVar\t[𝐺(]\t=\t∑(\t𝐺(\t=𝑙)\t.\t\t𝑙8\t–012\t𝐸[𝐺(]8\t\t\t\t\t\t\t\t\t     (2) Every time a human assessor makes an annotation 𝐺( then 𝐸\t[𝐺(] ← 𝑔(\tand 𝑉𝑎𝑟 [𝐺(]= 0; it means there is no uncertainty of 𝐺(.  Research about incomplete judgments can be described as follows:  • (Buckley and Voorhees, 2004) investigated about evaluation measures robust enough to cater for incomplete judgments; this research introduced the need of a proper evaluation measure for large collections bpref, which calculated system’s scores having into account top non-relevant judgments \t\n\t29 rejected by the traditional pooling method.  •  (Carterette, Allan, & Sitaraman, 2006) conduit an investigation about Minimal Test collections for retrieval evaluation which has lead into an algorithm that in minimal time evaluate retrieval systems with high degree of confidence and using a minimal number of judgments.  • (Aslam, Kanoulas  & Yilmaz, 2006), in Estimating Average Precision with Incomplete and Imperfect Information, proposed three different evaluation measures that were more robust to both incomplete and imperfect relevance judgments, in terms or predicting the ranking of systems and the value of average precision. • (Aslam, Yilmaz, 2007) have shown that giving the average precision of a minimal fraction of judge documents using a small number of relevance judgments, the relevance of the remaining unjudged documents can be inferred.   • (Carterette, 2007) studied Robust Test Collections for Retrieval Evaluation, where a model able to achieve reusability with very small sets of relevance judgments per topic was presented.   • (Carterette & Allan, 2007) proposed the use of inter-document similarity, in which document similarity is the key to evaluate retrieval systems with more accurate and robust results, using 99% less relevance judgments than TREC conferences.   • (Aslam, Yilmaz, 2008)  implemented A Simple and Efficient Sampling Method for Estimating AP (Average Precision) and NDCG  (Normalized discounted cumulative gain) were they extended inferred AP using two methods:  in the first one, they used confidence intervals to compare and rank systems according to their quality measured by AP.  In the second method, they selected documents to be judged through a stratified random sampling strategy.    \t\n\t30 As stated before, research in text information retrieval has been meaningful for the creation of continuous improvement in evaluation techniques. In music, this topic has received about half of the attention but still the little research conducted so far, has been significant. For example, in order to create large datasets and reduce the number of annotations needed, low-cost evaluation alternatives have been explored. For instance, (J. Urbano & Schedl, 2013) applied Minimal Test Collection (MTC) algorithms to the evaluation of the Audio Music Similarity task in MIREX, which reduced the annotation cost to less than 5%. Therefore, the researches investigated how to compare systems when incompletes judgments are available and still be confident about the results.  The idea is to model probabilistically the relevance judgments provided by human  assessors using the same concept of random variables. Then, they created models to estimate these relevance judgments as accurately as possible and obtain good estimates of systems effectiveness even with few available judgments.  Let 𝐺( being a Random Variable representing the relevance level assigned to document d. If the scale is Fine, 𝐺( can take one of three values and if this is Broad, it can take one of 11 values.  To estimate the relevance of a document with (2) the P (𝐺(=𝑙)  needs to be known for each relevance level of L (the possible value giving by a human annotator using the scale L). It means, the distribution of 𝐺( has to be calculated. The followed approach was the estimation of the relevance of each document individually, creating two models fitted with features about every query-document.  These features are:  i) Output-based: used when there are no judgments available; represents aspects of the system outputs. (See Table 1). For an arbitrary document d (song) and query q (looking for similarity among songs).   \t\n\t31  Feature Description   % of systems that retrieved d for q. If many systems return d,  fSYS It’s expected that d is more similar to q.      OV Degree of overlap between systems      aRANK Average rank in which systems retrieved d for q. Documents at the top are expected to be more similar to q      sGEN Whether the musical genre of d is the same as q      fGEN % of all documents retrieved for q that belong to the same musical genre than d does      fART % of documents retrieved for q that belong to the same artist as d does      Table 1. Output-based features   ii) Judgment-based features: Utilizes known judgments (See Table 2)  Feature Description     aSYS Average relevance of documents retrieved by the system      aDOC Average relevance of all the other documents retrieved for q      aGEN Average relevance of all the documents retrieved for q that belong to the same genre as d does     aART Average relevance of all the documents retrieved for q performed by the same artist as d      aART % of documents retrieved for q that belong to the same artist as d does      aSYS Average relevance of documents retrieved by the system      aDOC Average relevance of all the other documents retrieved for q   Table 2. Judgment-based features    These models were created and fitted with data from the task of AMS in MIREX \t\n\t32 2007,2009,2010 and 2011. Only those features that improve the model were selected. R2 (coefficient of determination) was used to measure the variability of the predicted outputs, where a value of 1 means a perfect fit of the data by the model. Table 3 introduces these results.  Feature Description  R2 Broad  R2 Fine         Mjud fTEAM, OV, aSYS, aART 0.9156 0.9002 MOut fTEAM, OV, sGEN, fART, fGEN 0.3627 0.3439  Table 3. Features for the two models  Table 3 shows that Mjud presents good estimates. However, the estimation of 𝐺(\thas to be calculated after judging some documents to obtain aSYS and aART. For this reason MOut was created in order to estimate 𝐺(\teven when there is no available judgments.  As expected, the latter model performed worst than the former. Table 4 presents statistics of all features for each model. Models for year Y are fitted to exclude all judgments for that year.                     \n\t\n\t33   Table 4. Likehood-ratio Chi-squared (under the name of All) statistic of all features for each model, with R2 scores, RMSE (Rooted Mean Squared Error) between predicted and actual scores, and average variance of estimates for Mout and Mjud. Adapted from (Julián Urbano, 2013)  Table 4 presents results that show that:  1) In the case of MOut the best results come from features fART, fGEN and sGEN, in other words, from data related to artist and genre confirming that they are good features to estimate similarity between two music excerpts (Flexer and Schnitzer 2010). For Mjud the best results are originated by aART demonstrating that if two songs from two artists are similar, other songs from them tend to be similar as well. This case represents the decision of MIREX to filter out songs that share the same artist than the query because they are likely to be similar.  2) RMSE and Average Variance demonstrate how well these models estimate relevance judgments. For a better comparison across scales, they were normalized between 0 and 1, resulting in Broad = {0; 0.5; 1} and Fine = {0.05; 0.15; ... 0.95}. It can be noticed that Fine scale makes better estimation of relevant judgments.  3) Although Mjud performs better than Mout, this one can still be used because its estimation’s error can be compared to the differences expected when human assessors performs relevance judgments.  Then, after creating the probabilistic estimation of relevance judgments using random variables, effectiveness scores used to rank systems according to their performance in the evaluation of AMS, needed to be predicted using random variables as well. Therefore, three possible scenarios to use according to the evaluation needs were set. In the implementation of this scenarios data from MIREX 2007, 2009, 2010 and 2011 was used. The results demonstrated that:  i) In the first scenario, when there are not relevance judgments available, \t\n\t34 Mout can be used and the order of systems is estimated with an average accuracy of 92% and with an average confidence in the rankings of 94%. ii) In the second scenario, when the goal is estimate system’s differences, it showed that just using 2% of the judgments (estimating the other 98%) the differences could be correctly estimate in 93% of the cases.  iii) In the third scenario, when the focus is the estimation of absolute scores, just with 25% of the relevance judgments they can estimate with an error of +-0.05. In this last scenario, effectiveness in the ranking of systems is highly overestimated. One approach to correct this issue was the use of a threshold of variance as a practical correction factor to use in the stopping condition. As a consequence, the error was reduced but at the expense of making several judgments, (between 15% and 35%). Fig 2 present this situation:  \nFig 2. Estimated vs. actual absolute effectiveness scores in MIREX 2007, 2009, 2010 and 2011 when judging documents until expected error is +-0.05 with an uncorrected (left) or corrected (right) stopping condition. Adapted from (Urbano, 2013)  The objective of this thesis project is to improve the existing probabilistic framework in order to get better estimates of relevance, which is intended to improve the predictions of the ranking of systems, reducing the amount of needed \n\t\n\t35 judgments observed in Figure 2.  Chapter 3  IMPROVING THE ESTIMATION OF RELEVANCE   Estimating relevance judgments will reduce the annotation cost yet achieving better predictions of effectiveness measures of systems. After reviewing the literature and the available models for prediction, several approaches have been considered to obtain better estimations:  1. Using others configurations of Ordinal Logistic Regression models.  2. Implementing others regression models.   3. Improving model’s attributes.   4. Implementing new attributes for models.  Each approach and its corresponding results would be described as follows; data from past edition of MIREX was used:  1. Using others configurations of Ordinal Logistic Regression models  From the literature, (Urbano, 2013), (Carterette, B., Jones, R., 2007)\tused the regression framework with ordinal logistic regression as the main approach to predict relevance since it takes into account the order of relevance level. Using the \t\n\t36 statistical language R 9 , two distinct configurations of ordinal models were tried inside the aforementioned framework: packages rms and MASS. The results of this implementation are depicted in Table 5. \t    ORDINAL LOGISTIC REGRESSION  Packet Model Scale R2 Orig. R2 % RMSE Orig. RMSE % Var Orig.    Var % rms Mout Broad 0,3630 0,3627 0,08% 0,3254 0,3254 0,00% 0,1054 0,1054 0,00% Fine 0,3430 0,3439 -0,26% 0,2833 0,2412 17,45% 0,0178 0,0178 0,00% Mjud Broad 0,9160 0,9156 0,04% 0,1375 0,1376 -0,07% 0,0177 0,0178 -0,84% Fine 0,9060 0,9000 0,67% 0,0900 0,0922 -2,39% 0,0069 0,0069 0,00% MASS Mout Broad This packet does not show R 0,3258 0,3254 0,12% 0,1054 0,1054 0,00% Fine 0,2408 0,2412 -0,17% 0,0177 0,0178 -0,56% Mjud Broad 0,1375 0,1376 -0,07% 0,0177 0,0178 -0,56% Fine 0,0900 0,0922 -2,39% 0,0069 0,0069 0,00%  Table 5. Implementation of rms and MASS packages for Ordinal Logistic Regression in R. Columns Orig. R2, Orig. RMSE and Orig. Var. represent the values obtained from (Urbano, 2013). MASS package does not show the value of the coefficient of determination, R2 .   Table 5 presents that using these configurations of ordinal models the improvement in the results were minimum. For example, proving rms for Mout the coefficient of determination R2 just increased in a 0,08% for the Broad scale and decrease in -0,26% in the case of Fine. For RMSE in Broad, no improvement was achieved and for Fine, the error increased 17%. Respect to the variance, any scale presented an improvement.  For Mjud the results were minimum: For R2 it got 0,04% for Broad and 0,60% for Fine scales; for RMSE and variance, a minimum improvement was achieved. Using MASS package the results enhanced in a minimal amount as well. Hence, using other configurations of Ordinal Logistic Regression did not achieve significant improvements for the prediction of relevance.  \t9 http://www.r-project.org  \t\t\n\t37   2. Implementing others regression models in order to obtain better results  The reviewed literature considered that linear regression was not an appropriate approach because the predicted relevance could be outside the [0,nL−1] limits (Urbano, 2013). However if the results can be truncated inside the possible values of Broad and Fine scales, this issue can be addressed. To prove this hypothesis models from the Generalized Linear Models, which can represent categorical, binary and other response types were tested: linear, probit and logit regressions. For probit and logit, the estimated relevance values need to be first mapped inside the range [0-1] and in order to interpret the results, these values need to be transformed back to the original scales; Table 6 presents the results of the evaluation using these models:  REGRESSION  Model Model Fitted Scale RMSE Orig. RMSE % Var Orig. Last % Logit  Regression Mout Broad 0,3720 0,3254 -12,53% 0,0031 0,1054 -97,06% Fine 0,2416 0,2412 -0,17% 0,0150 0,0177 -15,25% Mjud Broad 0,2997 0,1376 -54,09% 0,0148 0,0177 -16,38% Fine 0,0961 0,0922 -4,06% 0,0148 0,0069 114,49% Probit  Regression Mout Broad 0,3715 0,3254 -12,41% 0,0009 0,1054 -99,15% Fine 0,2415 0,2412 -0,12% 0,0006 0,0177 -96,61% Mjud Broad 0,2950 0,1376 -53,36% 0,0028 0,0177 -84,18% Fine 0,0952 0,0922 -3,15% 0,0004 0,0069 -94,20%  Table 6. Implementation of Logit and Probit regression using VGAM package in R. Columns Orig. RMSE and Orig. Var. represents the values obtained from (Urbano, 2013). This package does not present R2  Logit and Probit regression did not improve the prediction of relevance as it is demonstrated by RMSE and Variance results.   \t\n\t38 In the case of Multiple Linear Regression, the predicted values sometimes do not fall in a range within [0-2] for Broad or [0-100] for Fine scale; in all these cases they need to be truncated inside the corresponding scale in order to obtain the correct mapping with the estimates values. Table 7 presents these results:    MULTINOMIAL REGRESSION  Model Model Fitted Scale R2 Orig. R2  % Multiple Linear Regression Mout Broad 0,3320 0,3627 -1,11% Fine 0,3557 0,3439 0,41% Mjud Broad 0,8824 0,9156 -3,04% Fine 0,9114 0,9002 1,01%  Table 7. Implementation of Multinomial Linear Regression Columns Ant. R2, Ant. represents the values obtained from (Urbano, 2013).   For the case of Mout and Mjud using the Fine scale, an improvement in the   coefficient of determination R2,\tof 0,4% and 1% was achieved. The rest of predictions did not get any improvement.   3. Improving model’s attributes   To improve the prediction power of independent variables some techniques can be applied. For example, implementing a selection method, which is intended to choose the best subset of predictors (Faraway, 2004). For both models Mout and Mjud, backward elimination approach was applied. This method start testing the interaction of all predictors (features, attributes) and then removes the predictors with the less or the highest value of some parameter, depending of the model (higher p-value, R2, lowest deviance or AIC, etc.). In this case, rms packet in R was used with ordinal logistic regression, starting with the interaction of all the \t\n\t39 predictors; therefore, in order to decrease the number of permutations, the selection of variables made by (Urbano, 2013) was followed for Mout (fSYS, OV, sGEN, fGEN, fART). Using the Deviance as an indicator of quality of good or bad fit for the model, the results are presented in Table 8. It shows that even thought the best fit was achieved by the Interaction number 1 and 5, the latter can be selected since it does not used as many parameters as the former. Furthermore, if this interaction is compared with the research from Urbano, the result is almost the same, so this last configuration can be chosen since is less complex than the one presented in interaction 5. Similar results were obtained for Mjud.  MODELS TRIALS Model Predictors Deviance AIC 1  fSYS * OV + fSYS * sGEN + fSYS * fGEN + fSYS  *  fART + OV * sGEN + OV * fGEN + OV * fART +  sGEN * fGEN + sGEN * fART + fGEN * fART 36.342 36.376 2 fSYS + OV + sGEN + fGEN + fART 37.203 37.217 3b fSYS+OV 44.532 44.540 3c fSYS+sGEN 40.483 40.491 3d fSYS+fGEN 38.477 38.485 3e fSYS+fART 43.346 43.354 3j OV+sGEN 40.693 40.701 3k OV+fGEN  38.416 38.424 3l OV+fART 43.534 43.542 3m sGEN+fGEN 38.049 38.057 3n sGEN+fART 39.426 39.434 3o fGEN+fART  38.031 38.039 4a fGEN 38.384 38.394 4b sGEN 40.424 40.434 4c fSYS 44.536 44.542 4e OV 44.910 44.916 4f fART 43.536 47.685 5 fSYS*fGEN + OV*fGEN + sGEN*fGEN + fGEN*fART 36.401 36.423 Urbano, 2013   fSYS*OV + fART + sGEN*fGEN 36.837 36.855        Table 8. Implementation of backward elimination of predictors for Mout.   \t\n\t40  4. Implementing new attributes  4.1  Cluster of Genres  Another considered approach was the use of a new independent variable called Cluster; it was intended to improve the results of relevance’s predictions by clustering genres according to subjective criteria of similarity. The genres used in MIREX are (10):  Baroque, Blues, Classical, Country, Edance, Jazz, Metal, RapHiphop, Rock-and- roll, Romantic. After listening to several songs of each genre from the provided MIREX dataset, the proposed clustering of genres is described in Table 9:   Genres  Cluster      Baroque-Classical-Romantic  Cluster 1: Classical      RapHiphop - Edance  Cluster 2: Electronic      Blues-Rockandroll-Country  Cluster 3: Romantic      Jazz  Cluster 4: Jazz      Metal  Cluster 5: Metal   Table 9. Proposed clustering of genres of MIREX’s data.  Adding this new attribute to the model using a dichotomous binary variable where the value of 1 was assigned if the query had the same genre as the document or 0 otherwise, the results were depicted in Table 10:   \t\n\t41  Model Fitted Scale R2 Genre Clustering  Original R2  % Difference Mout Broad 0,4030 0,3620 11,3% Fine 0,3840 0,3430 12,0% Mjud Broad 0,9150 0,9156 -0,07% Fine 0,9050 0,9000 0,6%      Model Fitted Scale RMSE Genre Clustering  Original RMSE % Difference Mout Broad 0,3170 0,3254 -2,6% Fine 0,2480 0,2412 2,8% Mjud Broad 0,1370 0,1376 -0,4% Fine 0,0900 0,0922 -2,4%      Model Fitted Scale Var. Genre Clustering  Original Var. % Difference Mout Broad 0,1000 0,1054 -5,1% Fine 0,0630 0,0569 10,7% Mjud Broad 0,0170 0,0179 -4,8% Fine 0,0070 0,0069 1,4%  Table 10. Implementation of the new attribute Cluster into a Logistic Regression Model. Columns Original R2, Original RMSE and Original Var. represent the values obtained from (Urbano, 2013).   Table 10 presents that using the new attribute Cluster for Mout  the results were improved for the Broad scale in an 11% and in a 12% for Fine. In the case of Mjud there were not improvements.   4.2  Using the distances’ media of similarity between genres   Therefore, adding new attributes in order to improve the prediction of relevance was a good choice to obtain better results. For this reason, another attribute formed using the media of the similarity’s distances between the genre of the query and the genre of the document (song) called Distance was implemented; with this new feature, one is expected to get better results. Table 11 presents the \t\n\t42 aforementioned distances between genres and Table 12 introduces results using this new attribute.  genreq genred Distance Similarity Jazz Jazz 62.918 Metal Metal 61.092 Classical Classical 58.618 Electronic Electronic 56.681 Romantic Romantic 48.745 Romantic Jazz 37.284 Metal Romantic 37.084 Romantic Metal 36.849 Jazz Romantic 36.288 Metal Electronic 28.107 Electronic Metal 23.881 Electronic Romantic 19.956 Jazz Electronic 17.156 Classical Jazz 16.740 Electronic Jazz 15.982 Jazz Classical 15.919 Romantic Electronic 13.430 Metal Jazz 13.000  Table 11. Media of distances between genres of queries and songs.      R2 with distances (similarity)     Model Fitted Scale R2 no genre clustering R2 with genre clustering Original R2  % between highest and original Mout Broad 0,4670 0,4340 0,3620 29% Fine 0,4490 0,4210 0,3430 31% Mjud Broad 0,9150 0,9150 0,9156 0% Fine 0,9050 0,9050 0,9000 1%   \t\n\t43     RMSE  with distances (similarity)     Model Fitted Scale RMSE no genre clustering RMSE with genre Clustering Original RMSE % Between highest and original Mout Broad 0,3030 0,3110 0,3254 -7% Fine 0,2210 0,2270 0,2412 -8% Mjud Broad 0,1370 0,1370 0,1376 0% Fine 0,0900 0,0900 0,0922 -2%      Variance with distances (similarity)     Model Fitted Scale Var no genre clustering Var with genre clustering Original Var % Between highest and original Mout Broad 0,0910 0,0970 0,1054 -14% Fine 0,0500 0,0530 0,0563 -11% Mjud Broad 0,0170 0,0170 0,0178 -4% Fine 0,0070 0,0070 0,0069 1%  Table 12. Implementation of distance as attribute into a Logistic Regression Model. Columns Original R2, Original RMSE and Original Var. represent the values obtained from (Urbano, 2013).  Columns R2, RMSE and Var. with genre clustering represents values obtained when the cluster of genres of Table 9 was performed.  Columns R2, RMSE and Var. with no genre clustering presents the values without this clustering, using the genres proposed from the original data.  While trying different features’ interactions using the new attribute Distance, one experiment was conducted where no clustering of genres was implemented and the original classification of genres from MIREX was used instead.  It leaded to get even better results:  a significant improvement in R2 of 29% for the Broad scale and of a 31% for the Fine scale for Mout.   As Table 12 presents, using attribute Distance, with clustering of genres the results are still meaningful.  Also RMSE and Variance values were improved. These results proved that adding new attributes as independent variables is an optimal path to take for improving the estimation of relevance.  \t\n\t44 4.3. Using metadata to deal with artist information  After obtaining good results when treating information related to genres, something similar was performed using information from artist. With the restriction that neither the query, nor the document can belong to the same artist, any similarity’s media measurement can be calculated from the data from MIREX; for this reason, the use of an external source was necessary.  The idea is to contrast the information provided from MIREX’s metadata along with information from a music Internet database for instance, in order to look for similarity between artists.  The provided metadata file contained information of track artist, album artist and genre.  Then several steps were followed:  1. The selected Internet database was Echo Nest10 because it allows the access to billion of music data points from media and mobile companies like (MTV, BBC, MOG, Pocket Hipster, etc.).  Its API 11 provides methods to return a wide range of data from many artists; for the particular case, similarity information.   2. Then, setting the API and coding with Phyton12 to compare information from the metadata file versus those artists that exist in Echo Nest, a dataset with a list of artist similarities was obtained.  3. Subsequent, this information was integrated with the data provided from MIREX and using a new attribute called Similarity, new interactions of features for the regression model was tested.    The obtained results did not improve the existing scores.  One reason for this can be that there is no enough overlapping between Echo Nest and MIREX databases in order to obtain meaningful values for artist’s similarity. \t10 http://the.echonest.com 11 http://developer.echonest.com/docs/v4/artist.html#similar 12 https://www.python.org\t\t\n\t45 Table 13 presents these outcomes.   Any improvement neither for  R2, nor for RMSE or Variance were achieved.  Therefore, the best scores gained so far were gotten using distance similarity without clustering between genres.                                                       ARTIST SIMILARITY Model  Fitted Scale R2  R2 no  Genre Clus. % RMSE RMSE no  Genre Clus. % Var  Var RMSE no  Genre Clus. % Mout Broad 0,4350 0,4670 -6,85% 0,311 0,3030 2,64% 0,0968 0,0910 6,37% Fine 0,4230 0,4490 -5,79% 0,2272 0,2210 2,81% 0,0535 0,0500 7,00% Mjud Broad 0,9150 0,9150 0,00% 0,1379 0,1370 0,66% 0,0178 0,0170 4,71% Fine 0,9050 0,9050 0,00% 0,0902 0,0900 0,22% 0,0070 0,0070 0,00%  Table 13. Implementation of the attribute artist similarity into a Logistic Regression Model.                 \t\n\t46 CONCLUSIONS   This thesis work studies models and methods in order to improve the framework to estimate relevance judgments of Audio Music Similarity in the context of MIREX, from the point of view of efficiency.  After reviewing the literature and existing models for predictions (Mout, that predict gain scores when no judgments are available and Mjud that improves the predictions when judgments are available), several approaches were considered in order to obtain better results.  First, others configurations of Ordinal Logistic Regression models were considered.  Two packages or statistical programming language R were used.  The results did not achieve significant improvement for the prediction of relevance. Second, the implementation of others regression models were performed: probit, logit and linear regressions.  In all these cases the estimated values needed to be first mapped inside an specific range and then transformed back to the Broad and Fine scale in order to compare.  A slight improvement were achieved in R2 for both models, using Fine scale, 0,4 % for Mout and 1% for Mjud. Third, improving model’s attributes with the implementation of backward elimination was applied.   After testing many interactions of attributes, a simplest configuration was selected; however, because it was similar and slight complex than the one obtained by (Urbano, 2013), this last one was considered instead. The fourth approach was implementing new attributes for models.  Several experiments were conducted: i) Clustering subjectively the existing genres from the data of MIREX. Adding a new attribute Cluster to the logistic regression model, the predictions improved for Mout  model and the Broad scale in an 11% and in the Fine scale, in a 12%. ii) Using distance’s media of similarity between genres as a new attribute, the results improved better than before; performing this trial ignoring the clustering from i), the results gained an \t\n\t47 improvement in R2 of 29% for the Broad scale and of a 31% for the Fine scale for Mout model.  Also RMSE and Variance values were inferior. .  i) and ii) proved that adding new attributes as independent variables is an optimal path to take for improving the estimation of relevance.   For this reason, the last experiment performed, using this time information from artist was iii) Using metadata to deal with artist information.  Because artist information has restrictions inside MIREX contest, it cannot be calculated with normal statistical procedures as genre; an external source had to be cast-off in order to get similarity measures.  Echo Nest music Internet Database was chosen to get a similarity database of those artists belonging to a metadata file provided from MIREX.  Unfortunately, the results did not improve existing scores.    Overall, the results of this dissertation’s experiments indicate that attributes obtained from information such as the outcome of systems or metadata, conduits to improve the prediction of relevance.  The best results are obtained for Mout over Mjud, which in most cases resembles the real scenario, when no judgments or just a minimum amount of them are available to make predictions. \t\t\t\t\t\t\t\t\t\t\t\t\t\n\t48 FUTURE WORK    There are some lines of research arising from this work that can be pursued.  First, in order to decrease overfitting, the models can be trained with different amount of information.  It will permit a better fit since at the present time some features are calculated using all the judgments, when in real life, this scenario is not always present.  Second, it suggested to keep on improving the prediction of relevance, studying the role of attributes originated from artist or genre information, using others music services and sources of information.  Conclusively, with this dissertation, the author expects to encourage research in methods of low- cost evaluation not just for Audio Music Similarity, but also for the other task in Music Information Retrieval.                 \t\n\t49 REFERENCES    1. Aslam, J. a, Yilmaz, E., (2007). Inferring Document Relevance from Incomplete Information 633–642.    2. Aslam, J. a, Yilmaz, E., (2007). Estimating Average Precision with Incomplete and Imperfect Information, in ACM International Conference on Information and Knowledge Management, 2006, pp. 102–111.  3. Aslam, J. a, Yilmaz, E., (2008). A Simple and Efficient Sampling Method for Estimating AP and NDCG, in International ACM SIGIR Conference on Research and Development in Information Retrieval, 2008, pp. 603–610.  4. Bertin-Mahieux, T., Ellis, D. P. W., Whitman, B., & Lamere, P. (2011). The Million-Song Dataset. Ismir, (Ismir), 591–596. Doi: 10.1145/2187980.2188222.   5. Buckley, C., Voorhees, E. M. (2004). Retrieval evaluation with incomplete information. Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 25–32. Doi: 10.1145/1008992.1009000.  6. Buettcher S., Cormack G. V. and Clarke, C. L. (2010). Information Retrieval: Implementing and Evaluating Search Engines. The MIT Press.   7. Cleverdon, C. W. (1991). The Significance of the Cranfield Tests on Index Languages. In International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3-12.    8. Carterette, B. (2007). Robust Test Collections for Retrieval Evaluation. Evaluation, 55–62. Doi: 10.1145/1277741.1277754 .  9. Carterette, B., Allan, J., & Sitaraman, R. (2006). Minimal test collections for retrieval evaluation. Proceedings of the 29th Annual International ACM SIGIRConference on Research and Development in Information Retrieval - SIGIR ’06, 268. Doi: 10.1145/1148170.1148219   10. Carterette, B., Allan, J. (2007). Semiautomatic evaluation of retrieval systems using document similarities. Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management - CIKM ’07, 873. Doi: 10.1145/1321440.1321564   \t\n\t50 11. Carterette, B., Jones, R. (2007). Evaluating Search Engines by Modeling the Relationship between Relevance and Clicks, in Annual Conference on Neural Information Processing Systems.  12. Downie, J. S. (2002). Interim Report on Establishing MIR/MDL Evaluation Frameworks: Commentary on Consensus Building. ISMIR Panel on Music Information Retrieval Evaluation Frameworks, 43–44.    13. Downie, J.S., Ehmann, A.F., Bay, M. & Jones, M.C.: The Music Information Retrieval Evaluation eXchange: Some Observations and Insights. In: W.R. Zbigniew, A.A. Wieczorkowska (eds.) Advances in Music Information Re-trieval, pp. 93{115. Springer (2010)    14. Downie, J. S. (2003). Music information retrieval. Annual review of information science and technology, 37(1), 295-340.    15. Downie, J. S., Hu, X., Lee, J., Ha, C. K., Cunningham, S. J., & Yun, H. (2014). 15th International Society for Music Information Retrieval Conference (ISMIR 2014). Ten years of Reflections, challenges and opportunities. (Ismir), 657–662.    16. Flexer, A., Schnitzer, D. (2010). Effects of Album and Artist Filters in Audio Similarity Computed for Very Large Music Databases. Computer Music Journal, 34(3), 20–28. Doi: 10.1162/COMJ_a_000028. Cited on pages 104 and 106.    17. Harter, S. P. (1996). Variations in relevance assessments and the measurement of retrieval effectiveness. Journal of the American Society for Information Science, 47(1), 37–49. Doi:10.1002/(SICI)1097-4571(199601)47:1<37::AID-ASI4>3.0.CO;2- 3    18. Faraway, J. (2004). Linear Models with R. Chapman and Hall/CRC.    19. Kent, A., Lancour H., Daily E. Encyclopedia of Library and Information Science: Volume 28 The Smart System to Standards for Libraries. CRC Press (1980). ISBN 9780824720285 - CAT# DK2544. 512 pages. Paperback. Series: Library and Information Science Encyclopedia.   20. Lancaster, F. (1968). Evaluation of the MEDLARS Demand Search Service.             Technical report, U.S. Department of Health, Education, and Welfare.   21. Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to information retrieval (Vol. 1, p. 496). Cambridge: Cambridge university press.   \t\n\t51  22. Moghadasi, S. I., Ravana, S. D., & Raman, S. N. (2013). Low-cost evaluation techniques for information retrieval systems: A review. Journal of Informetrics, 7(2), 301–312. Doi: 10.1016/j.joi.2012.12.001.    23. Orio, N. (2006). Music Retrieval: A Tutorial and Review.  Found. Trends Inf. Retr., vol. 1, no. 1, pp. 1–90, 2006.  24. Orio, N., Liem, C. C. S., Peeters, G., & Schedl, M. (2012). MusiClef: Multimodal music tagging task. Lecture Notes in Computer Science (including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 7488 LNCS, 36–41. Doi: 10.1007/978-3-642-33247-0_5.    25. Peeters, G., Urbano, J. & Jones, G.J. (2012). Notes from the ISMIR 2012 late- breaking session on evaluation in music information retrieval. In International society for music information retrieval conference.    26. Sanderson, M., & Croft, W. B. (2012). The history of information retrieval research. Proceedings of the IEEE, 100(SPL CONTENT), 1444–1451. Doi: 10.1109/JPROC.2012.2189916    27. Schedl, M., Gómez, E. & Urbano J. (2013) Music Information Retrieval: Recent Developments and Applications.  Found. Trends Inf. Retr., vol. 8, no. 2–3, pp Serra, X., Magas, M., Benetos, E., Chudy, M., Dixon, S., Flexer, A., ... Widmer, G. Roadmap for Music Information ReSearch. Retrieved from http://www.ofai.at/research/impml/projects/MIRES_Roadmap_ver_1.0.0.pdf.   28. Spärk Jones, K., van Rijsbergen, C. J. (1975). Report on the need for and provision of an ‘ideal’ information retrieval test collection (British library research and development report no. 5266). Cambridge: Computer Laboratory, University of Cambridge. (p. 43)   29. Tague-Sutcliffe, J. (1992). The pragmatics of information retrieval experimentation, revisited. Information Processing and Management, 28(4), 467–490. Doi: 10.1016/0306-4573(92) 90005-K   30. Urbano, J., Universidad Carlos III de Madrid Tesis Doctoral Evaluation in Audio Music Similarity. (2013).    31. Urbano, J., Schedl, M. (2013). Minimal test collections for low-cost evaluation of audio music similarity and retrieval systems. International Journal of Multimedia Information.... Retrieved from http://link.springer.com/article/10.1007/s13735-012- 0030-4  .    \t\n\t52 32. Urbano, J., Schedl, M., & Serra, X. (2013). Evaluation in music information retrieval, (November 2012). Retrieved from http://link.springer.com/article 10.1007/s10844-013-0249-4. Cited on page 3."
    },
    {
        "title": "Forming a Corpus of Voice Queries for Music Information Retrieval.",
        "author": [
            "David Bainbridge 0001",
            "John R. McPherson"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417301",
        "url": "https://doi.org/10.5281/zenodo.1417301",
        "ee": "https://zenodo.org/records/1417301/files/BainbridgeM02.pdf",
        "abstract": "The use of audio queries for searching multimedia content has in- creased rapidly with the rise of music information retrieval; there are now many Internet-accessible systems that take audio queries as input. However, testing the robustness of such a system can be prob- lematic, as there is currently no standard test-bed of queries and mu- sic files available. A corpus of audio queries would aid researchers in the development of both audio signal processing techniques and audio query systems. Such a corpus would also be essential for mak- ing empirical comparisons between different systems and methods. We propose a pilot study that will field test a procedure for collecting audio queries. The lessons learned in the pilot study will guide us in refining the collection methodology, and we will make a final set of queries freely available to MIR researchers. The participants for this pilot study will be attendees of the ISMIR 2002 Conference.",
        "zenodo_id": 1417301,
        "dblp_key": "conf/ismir/BainbridgeM02",
        "keywords": [
            "audio queries",
            "multimedia content",
            "music information retrieval",
            "Internet-accessible systems",
            "robustness testing",
            "standard test-bed",
            "corpus of audio queries",
            "audio signal processing techniques",
            "audio query systems",
            "empirical comparisons"
        ],
        "content": "FormingaCorpusofVoiceQueriesforMusicInformation Retrieval:APilotStudy\nForming aCorpusofVoiceQueriesforMusicInformation\nRetrieval:APilotStudy\nDavidBainbridge\nDepartmentofComputer\nScience\nUniversityofWaikato\nHamilton\nNewZealand\ndavidb@cs.waikato.ac.nzJohnR.McPherson\nDepartmentofComputer\nScience\nUniversityofWaikato\nHamilton\nNewZealand\njrm21@cs.waikato.ac.nzSallyJoCunningham\nDepartmentofComputer\nScience\nUniversityofWaikato\nHamilton\nNewZealand\nsallyjo@cs .waikato.ac.nz\nABSTRA CT\nTheuseofaudioqueriesforsearching multimedia contenthasin-\ncreasedrapidlywiththeriseofmusicinformation retrieval;there\narenowmanyInternet-accessible systemsthattakeaudioqueriesas\ninput.However,testingtherobustnessofsuchasystemcanbeprob-\nlematic,asthereiscurrently nostandardtest-bedofqueriesandmu-\nsicﬁlesavailable.Acorpusofaudioquerieswouldaidresearchers\ninthedevelopment ofbothaudiosignalprocessing techniques and\naudioquerysystems. Suchacorpuswouldalsobeessentialformak-\ningempirical comparisons betweendifferentsystemsandmethods.\nWeproposeapilotstudythatwillﬁeldtestaprocedure forcollecting\naudioqueries.Thelessonslearnedinthepilotstudywillguideus\ninreﬁningthecollection methodology ,andwewillmakeaﬁnalset\nofqueriesfreelyavailabletoMIRresearchers. Theparticipants for\nthispilotstudywillbeattendees oftheISMIR2002Conference.\n1.BACKGROUND\nAtthe2001International Symposium onMusicInformation Re-\ntrieval,theneedfor“standardised MIRtestcollections” [1]was\ndiscussed. Toourknowledge,therearenosetsofqueryrecordings\ncurrently availabletothecommunity atlarge.Weproposelaying\nthegroundworkforthecreationofacorpusofaudioqueriesthatis\ndesigned withtheneedsofmusicinformation retrievalresearchers\nandpractitioners inmind.Becausetheintendeduseoftherecord-\ningswillbeformally declaredtoparticipants, thereshouldnotbe\nprivacyconcerns associated withthereleaseofthecorpus.\n2.INTRODUCTION\nAnincreasing numberofmusicinformation retrievalpractitioners\nusehumanvoiceinputintheirresearch; applications includequery-\nby-demonstration systemssuchasThemeﬁnder [2],MusArt[3]and\nMeldex[4],andaudiotranscription services.\nVariousfactorsmustbeaccounted forwhenprocessing rawaudio;\noftenthereisatrade-offbetweenthelevelofprocessing neededand\ntheburdenplacedonthepersonperforming aquery.Thetimbre,\nsuchaswhetherthequeryhasbeensung,hummed, orwhistled,\nisnearlyalwaysimportant. Differentsystemsexpectqueriesin\nacertainstyle,whileindividualusers’preferences mayaffectthe\nqualityoftheaudioquery.\nAnothermajorfactorisnoteseparation. Humanspeechandsong\nisoftenlegato,witheachnotetrailingoffandblending intothe\nfollowingnote.Itiseasiertoprocessaudiothathasadiscernible\nsilencebetweeneachnote,andsomesystemsrequirethisofthe\nsinger(forexample,[4]).\nWerecognize thattherearemanydifferenttypesofretrievaltasks,\neachrequiring adifferentcorpusoftestqueries.Inthispilotstudy\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.c\n\u0000\n2002IRCAM-CentrePompidouwewillexaminetheproblemofdevelopingtestqueriesforknown-\nitemsearches—thatis,tasksinwhichthedesiredresultisapiece\nofmusicthatisknowntobeinthecollection andwhichtheuser\nwishestoretrieve.Othermusicretrievaltasks—suchaslocat-\ningsimilarmusicalitems,orautomatic genreclassiﬁcation —will\nrequiredifferenttypesoftestqueries,andmayrequireadifferent\nprocessfordevelopingatestcorpus.\nGiventhattargettask,thecorpusoftestqueriesmustbespeciﬁc\ntoaparticular test-bedofmusicdocuments. Wewilldesignaset\nofqueriesforatest-bedofpop/rock recordings, whereeachquery\nwillbebasedaroundtheretrievalofasinglerecording. Eachquery\nshouldbesufﬁcientlydifﬁcultinthattheremustexistatleastone\notherrecording thatisrelatively’similar’tothetargetsong,where\nsimilarity maybemeasured inseveraldimensions, suchaskey,\nrhythm,melodiccontour,orsharednotesequences.\nIntextretrievalevaluations itissufﬁcienttohaveasingleversion\nofeachtestquery,sincethequeryitselfremainsthesamenomatter\nwhotypesitin.ThequerierplaysamorecentralroleinMIRtasks,\nhowever,aseachindividualmaybeexpectedtovocalizethequery\ndifferently.AnMIRquerycorpus,then,shouldcontainseveralin-\nstantiations ofeachquery,asexpressedbydifferentindividuals.We\nfurtherbelievethatthecorpusshouldalsoincludeseveralversions\nofaquerybyasingleindividual—forexample,ahummed ver-\nsion,asungversion,andsoforth.Acorpusthatcontainsqueriesin\nmultipleformatswould:\u0001allowimplementors totestsystemswiththesub-setofqueries\nthattheyhaveallowedfor(suchas“hummed queries”),\u0001allowcomparison betweensystemsthatrequiredifferentqual-\nitiesintheaudioinput,and\u0001allowtestingtoseetheeffectinisolation thateachofthe\nabovefactorshasonretrievalperformance.\nAsecondary purposeofcollecting asetofqueriesistogainfurther\ninsightintothevariationthatwecanexpectwhendifferentusers\nattempttoposeaquerytoanMIRsystem.Forexample,anear-\nlier,small-scale studyofhowpeople‘natively’prefertogenerate\nqueriesindicates thatsungquerieswilloftendriftinpitchunless\nthequeryisverybrief,andidentiﬁed atendencyforparticipants to\naddordropconsecutivesyllables havingthesamepitch[5].Sub-\nstantiveevidenceofthetypesanddegreesofqueryvariationthatan\nMIRsystemmaybeexposedtowouldbeinvaluableforresearchers\nexploringtechniques forimprovingsearchprecision.\n3.METHODOLOGY\nWeoutlineamethodology forourpilotstudy,whichwillsolicit\nparticipants fromtheattendees ofISMIR2002.Participation in\nthepilotstudy,andindeedfortheensuingdevelopment ofquery\ncorpora, willbeonavoluntary basis.Noidentifying datawill\nbemadeavailableabouttheparticipants, andthequerycollectionFormingaCorpusofVoiceQueriesforMusicInformation Retrieval:APilotStudy\nprocesswillcomplywiththeguidelines oftheUniversityofWaikato\nEthicsCommittee.\nGiventhecommercially sensitivenatureofmusiccollections, copy-\nrightissuesmustbeconsidered intheconstruction ofamusicquery\ncorpus.Article10oftheBerneConvention1971allowsfor“fair\npractice” ofcopyrightedmaterial,althougheachparticipating coun-\ntrydeﬁnestheextentpermissible initsjurisdiction. Manyjuris-\ndictionshaveexceptions intheircopyrightlawsthatallowtheuse\nofportionsofacopyrightedworkforvariouspurposes, including\nscholarly work.\nAtthispoint,wearedesigning aprocessforcollecting acorpusof\nqueries,withtheunderstanding thateachdifferentMIRtest-bedwill\nrequireitsownsetsofmusicdocuments, queries,andretrievaltasks.\nIntheﬁrstinstance, wewillbuildasetoftestqueriesforalargeset\nofpop/rock recordings. Thisgenrewasselectedforthepilotstudy\nfortworeasons.Firstly,severalcollections ofpop/rock recordings\ncurrently existandarealreadyseeinguseinMIRexperiments, and\nsoaquerycorpuscouldbeimmediately usefultomorethanone\nresearchgroup.Secondly,thetunesthatwillformthebasisof\nthetestqueriesaremorelikelytobefamiliartoparticipants than\nother,moreesotericgenresheldinotheravailablemusicdocument\ncollections (forexample,folksongsinavarietyoflanguages), andso\ntheparticipants shouldrequirerelativelylittlecoaching orassistance\ninformingtheirqueries.\nThecorpuswillbecreatedintwoparts.Theaimoftheﬁrstpart\nistofocusontranscription issuesandallowtheseparation ofvar-\niousfactors,suchasquerymethod(whistle, hum)orstyle(legato\nordetached). Subjects willbegivenashortthemefromasong\n(thequery),inbothaudioandnotatedformats,andwillbeasked\ntovocalizethequeryinanumberofprescribed methods—forex-\nample,whistled, hummed, orsungwithtasyllables, ortheirnatural\npreferred method—aswellasagivenarticulation (thatis,staccato\norlegato).Eachsubjectwillperformseveralqueries,andorder\nofquerypresentation willberandomised tominimise testingbias.\nParticipants willbeallowedtodeclinetoperformqueriesforapar-\nticularsongiftheyfeelthattheydonotknowthepieceadequately ,\nandanothersongwillbeoffered.\nThetestcorpus,then,willbebasedaroundasetofqueries,where\neachquerywillbeinstantiated byseveralsubjectsandinmorethan\noneformatpersubject. Eachinstanceinthecorpuswillconsist\nofanindication ofthequerythatitrepresents, anaudiosample,\nmetadata characterising therelevantattributesoftheindividualwho\nhasprovidedthesample,metadata describing thequerytype,and\nmetadata foridentifying theaudiocontent.\nTheaudiocontentwillbeofcompactdiscquality—thatis,recorded\natasamplerateofaround44kilohertz. Asymbolic representation\n(suchasGUIDOformat)oftheaudioquerywouldprovideanide-\nalisedtranscription, allowingimplementors totesttheeffectsofthe\naudio-to-symbol translation processseparately ,ifrequired.\nMetadata describing therelevantcharacteristics ofsubjectswillbe\ngathered throughashortquestionnaire tocollectdemographic in-\nformation suchasthesubject’smusicalbackground, agebracket,\ngender,etc(forexamples, see[6]).Query-speciﬁc metadata will\nincludethevoicequerytype(whether thisqueryinstantiation is\nwhistled, hummed, sung,etc.),andwhetherthenotesareperformed\nlegatoor“detached”.Metadata fordescribing theaudiocontent,suchassongtitleand\nauthor,isneededforidentifying matcheswithaquerydatabase.\nOthermetadata —suchassonggenre,andwhetherornotthequery\nisamainthemeofthesong—couldalsobeuseful,although these\nqualitiesaremoresubjective.\nThesecondpartofthecorpuswillinvolvesubjectschoosing their\nownqueryforwell-knownsongs.Bothanaudiorecording anda\nsymbolic transcription willbemade,sothattranscription effectscan\nbeindependently accounted for.Thetranscription willbemadein\nconjunction withtheexperimenter ,ifassistance isrequired. The\nparticipants mayperformthequeriesintheirownchoiceofstyle.\nWerecognise thatthequeriesgathered duringthispilotstudyare\nunlikelytoformacomplete, unbiased corpusforourselected\ntest-bedofmusicdocuments —forexample,therangeofmu-\nsicalabilityshownbysubjectsmaynotadequately represent the\nqueriesgenerated bytypicalusersofquery-by-demonstration sys-\ntems.ThispilotstudywillallowtheMIRcommunity toex-\nploretherequirements foraneffectiveaudioquerycorpusand\ntoexaminethepractical issuesinvolvedincreatingsuchacor-\npus.Theresultsofthisstudywillbemadeavailableonlineat\u0002http://www.cs.waikato.ac.nz/music \u0003.\n4.CONCLUSION\nWehaveoutlinedaprocedure forcollecting aﬂexiblecorpusof\nmusicqueries,wheretheprocedure isdesigned speciﬁcally tomeet\nsomeoftheneedsofaudioqueryresearchers. However,wean-\nticipatethatmodiﬁcations andadditions maybenecessary inthe\nfuture.Thedesigndiscussed hereispartofalearningprocessas\nourcommunity discoversexactlywhatisrequiredtomakesucha\ncorpussuccessful, andwhatscopecanbereasonably covered.\n5.REFERENCES\n[1]MatthewJ.Dovey.Resolution ontheneedtocreatestandard-\nizedmirtestcollections, tasks,andmetrics.ISMIR2001.\nmusic- ir.org/mirbib2/resolution ,October2001.\n[2]AndreasKornst¨adt.Themeﬁnder: Aweb-based melodicsearch\ntool.Computing inMusicology,11:231–236, 1998.\n[3]WilliamP.Brimingham andRogerB.Dannenber getal.Musart:\nMusicRetrievalviaAuralQueries.InProceedings oftheSec-\nondAnnualInternational Symposium onMusicInformation Re-\ntrieval,Bloomington, Indiana,USA,October2001.\n[4]RodgerJ.McNab,LloydA.Smith,DavidBainbridge, andIanH.\nWitten.TheNewZealandDigitalLibraryMELody inDEX.D-\nLibMagazine,May1997.\n[5]RodgerJ.McNab,LloydA.Smith,IanH.Witten,ClaireL.\nHenderson, andSallyJoCunningham. Towardsthedigitalmu-\nsiclibrary:tuneretrievalfromacousticinput.InProceedings\nofDigitalLibraries'96,pages11–18,Bethesda (MD,USA),\nMarch1996.ACM.\n[6]J.StephenDownie.Creatingtheidealfull-textmusicdatabase:\nUserassessment survey.Technicalreport,Graduate Schoolof\nLibraryandInformation Science, UniversityofWesternOn-\ntario,London,Ontario,Canada,1993."
    },
    {
        "title": "Beating Babel - Identification, Metadata and Rights.",
        "author": [
            "Chris Barlas"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.2530104",
        "url": "https://doi.org/10.5281/zenodo.2530104",
        "ee": "https://zenodo.org/records/2530104/files/metadata_sharing_rights_obligations.pptx",
        "abstract": "As metadata and data converge, particularly where evolving standards such as DDI4 can simultaneously be the data, the code that generated it and its metadata; it is apposite to revisit questions of intellectual copyright, licencing and sharing in a way that a metadata in the form of catalogue record has hitherto not.\n\nDeveloping and maintaining such granularity of metadata can be an expensive proposition. There are costs associated with editing and publishing data and metadata.\n\nWhist most institutions have a data policy, very few have a metadata policy and those that do, seem to be aligning on a policy that advocates or requires a Creative Commons Zero (CC0) licence. The perceived advantages of having such a policy are that it provides a clear and easy-to-understand guidance for users especially where metadata is shared across borders.\n\nThere are obligations which could reasonably be requested of those to whom metadata is shared. If it is being re-shared, is it up-to-date with the original source, has it been altered and is that transparent to a user, have derived products attributed the original source?\n\nThis session will seek to identify the main challenges that a step change in the nature of metadata means for providers and recipients of shared metadata and the subsequent technical challenges this may engender.\n\nIntroduction to panel with Scott Hofer (University of Victoria), Jeremy Iverson (Colectica), Jon Johnson (CLOSER, IOE (UCL Institute of Education, University College London)), Mari Kleemola (FSD - Finnish Social Science Data Archive), chaired by Knut Wenzig (DIW)\n\n",
        "zenodo_id": 2530104,
        "dblp_key": "conf/ismir/Barlas02",
        "keywords": [
            "metadata",
            "data",
            "intellectual copyright",
            "licensing",
            "sharing",
            "Creative Commons Zero (CC0)",
            "data policy",
            "metadata policy",
            "granularity of metadata",
            "costs associated"
        ]
    },
    {
        "title": "Super-convenience for Non-musicans: Querying MP3 and the Semantic Web.",
        "author": [
            "Stephan Baumann 0001",
            "Andreas Klüter"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417231",
        "url": "https://doi.org/10.5281/zenodo.1417231",
        "ee": "https://zenodo.org/records/1417231/files/BaumannK02.pdf",
        "abstract": "Digital music distribution, the success of MP3 and the actual activities concerning the semantic web of music require for convenient music information retrieval. In this paper we will give an overview about the concepts behind our “super-convenience” approach for MIR. By using natural language as input for human- oriented queries to large-scale music collections we were able to address the needs of non-musicians. The entire system is applicable for future semantic web services, existing music web- sites and mobile devices. Beside the framework we present a novel idea to incorporate the processing of lyrics based on standard information retrieval methods, i.e the vector space model.",
        "zenodo_id": 1417231,
        "dblp_key": "conf/ismir/BaumannK02",
        "keywords": [
            "Digital music distribution",
            "MP3 success",
            "semantic web of music",
            "music information retrieval",
            "convenient music info retrieval",
            "human-oriented queries",
            "large-scale music collections",
            "non-musicians needs",
            "semantic web services",
            "novel idea for lyrics processing"
        ],
        "content": "Super-Convenience for Non-musicians : Querying MP3 and the Semantic Web\nSuper-Convenience for Non-musicians:\nQuerying MP3 and the Semantic Web\nStephan Baumann\nGerman Research Center for AI (DFKI)\nErwin Schrödinger Str.\n67608 Kaiserslautern\n+49-631-205-3447\nStephan.Baumann@dfki.deAndreas Klüter\nsonicson GmbH\nLuxemburger Str. 3\n67657 Kaiserslautern\n+49-631-303-2800\nAndreas.Klueter@sonicson.de\nABSTRACT\nDigital music distribution, the success of MP3 and the actual\nactivities concerning the semantic web of music require forconvenient music information retrieval. In this paper we will givean overview about the concepts behind our “super-convenience”approach for MIR. By using natural language as input for human-oriented queries to large-scale music collections we were able toaddress the needs of non-musicians. The entire system isapplicable for future semantic web services, existing music web-sites and mobile devices. Beside the framework we present a novelidea to incorporate the processing of lyrics based on standardinformation retrieval methods, i.e the vector space model.\n1. INTRODUCTION\nThe digital distribution of mus ic is one of the most attracting and\nchallenging topics for musicians and computer scientists thesedays. In despite of the ongoing legal debates we find a lot ofpotential for convenient man-machine-interfaces to music on thetechnical side. Our long-term goal is the provision of a systemarchitecture giving as much flexibility as needed to build powerfulapplications as customized instances of such an approach.\nOur goal is to reach a maximum of convenient usability and a\nminimum amount of manual indexing of underlying large-scaledata, we subsume this as super-convenience : (1) Human-oriented\ninterface paradigm, (2) uniform feature handling and automaticmetadata generation, (3) retrieval and recommendations.\nOur overall approach is targeted to hybrid processing ranging\nfrom pure surface structure recognition to symbolic inferencesamong the concepts of the ontologies. As a unique novelty wepresent the seamless incorporation of lyrics in this approach inorder to get – in the upper end - insight experiences about theperception of moods. We focus on naïve listeners or non-musicians in order to provide applications for the masses.\n2. ONTOLOGICAL BACKBONE\nThe semantic web is on its way to enter the masses. Real killerapplications may be convenient music information retrievalsystems for naïve listeners. These contributions can be seen in thetradition of established standards such as MPEG-7. Indeed,authors report about su ccessful transformations of MPEG-7 to the\nRDF (S) standard used for the semantic web [1]. Furthermore thecollaborative effects of a broad user base can be used to makerecommendations or computing the similarity between musicaltracks. KANDEM is such an approach as described by the groupat MIT media lab [2]. Answering real life questions of non-musicians requires real life knowledge in the music domain to beused within the MIR. For this purpose we modelled an ontology\nabout the domain of music. In our application scenario we notedas terms the concepts of required know-how in the music domain.The relations consist of several types; is-a and part-of relations areused quite often. Is-a-relations are used to indicate specializationsof concepts (e.g. acid jazz is-a jazz ) while part-of-relations denote\nrequired parts (e.g. track part-of compilation ). The aspect of\nsharing knowledge about conceptualizations with others is themost relevant aspect when building ontologies. In such a waydifferent agents can share access to the semantic web of music.These activities are still in their infancies and the problems of thestatus quo are described thoroughly in a recent publication of\nPachet [3]. At present our ontology is able to handle multipleinheritances for the concepts of tracks, albums and artists whooutperforms standard subsumption hierarchies as found on manyMP3-sites. Further concepts are the musical properties, which arelinked to the automatic audio processing (i.e. loudness, tempo,\ntimbre ). As a novelty we introduced a semantic link\ncontains_lyrics , which is grounded by the ASCII-text in our\ndocument database.\n3. MUSIC DATABASE\nThe MIR system accesses the musical data from an underlying\ndatabase. In our first prototype we ripped a private CD collectionto MP3 format at 128kbps. The scope of this dataset is about 1000tracks covering 60 artists and approx. 50 different genres. Theadministrative information about artist, title, and album has beengathered by usage of the CDDB. Unfortunately data quality wasinsufficient for automatic processing. While the inconsistencies inartist, title and volume tags could be removed; the genreinformation remained useless for automatic processing. Thereforethe genre tags have been set manually. For the experiments athand about 500 lyrics have been added as plain ASCII text.\n4. AUDIO ANALYSIS AND NLP\nThe automatic audio analysis recognizes properties such asloud/quiet, fast/slow and MP3 subband features for thedetermination of similarity. For the extraction we used theapproaches of Pfeiffer [4]. Natural Language Processing (NLP)approaches lie between the two extremes of key word processing\n(= disregard for word relations and context) and complete\nunderstanding . Both are not applicable for pragmatic processing\nof natural language music queries. The approach of example-\nbased processing with partial abstraction is especially suited for\nmusic search requests (limited domain, high speed requirements)and offers an optimum trade-off between processing speed andgood-natured r eaction to off-scope requests. Our query interface\nin front of the NLP component is not confused by typing errors.Additionally, the system is able to connect artist names, whichsound similar to each other, i.e. it is still able to pr oduce results\nwhen there is phonetic similarity (such as e.g. „fil collins“ vs.„phil collins“). Many general-purpose sequence distance methodshave been investigated in the past. The phonetic fuzzy match used\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.\n© 2002 IRCAM – Centre PompidouSuper-Convenience for Non-musicians : Querying MP3 and the Semantic Web\nby our system is based on former work at the German Research\nCenter for Artificial Intelligence on this subject. Currently thephonetic fuzzy match is online for a large-scale music informationsystem acting on 50.000 different artists accessible via awebservice at www.sonicson.com. An evaluation of r ecall and\nprecision will be possible by logfile analysis in the future.\n5. EXPLORING LYRICS\nWe started some experiments concerning the similarity of lyricsand the implications for the perception of music similarity. Herewe use state-of-the-art document retrieval and classificationapproaches, which have been recently commercialized andsuccessfully adopted to real-world problems. We used both, anAPI to a commercial tool as well as the text classificationworkbench and its submodules developed at our institute. Weused the Protégé 2000 tool for convenient design of ontologies.The top-level concept lyrics is broken down into a taxonomy oftypical topics covered by mainstream music. In the future suchhandcrafted topic ontology may be supported by semi-automaticontology learning through document clustering approaches. Forthe current experiments we focused first on the “subsymbolic”level of lyrics. State-of-the-art document retrieval andclassification approaches are still missing an in-depth ontologicalsupport. Nevertheless the basic techniques have a long-standingtradition in information retrieval and could be applied to thedomain of lyrics. These tools allow for different functionalities. Aquery in the boolean retrieval model consists of a booleancombination of tests on the occurrence of specific words. Forinstance, the query (hate or love) and girls tests whether a\ndocument contains one of the words hate orlove as well as the\nword girls. To go beyond the boolean retrieval, additional\nfunctionality, which we integrated, is based on the vector spacemodel (VSM). In this model, lyrics as well as queries arerepresented as vectors. The dimension of the vectors indicatespecific terms, the value of a vectors component indicates thenumber of times the respective term occurs in the lyrics/query tobe represented. Defining a similarity measure between vectorsdoes standard document retrieval based on queries in the VSM.The most frequently used measure here is the cosine-measure,which computes the angle between two vectors. Having a vectorrepresenting the query, the documents corresponding to the mostsimilar document vectors are returned as answer documents. Inthis way we realize the computation of similarity among lyrics.Since queries and lyrics in the VSM are represented as vectors,also the similarity between vectors representing just lyrics can becomputed. Roughly spoken, those lyrics, which share manyimportant words, will have a high similarity. Computing the mostrelevant terms can perform a kind of summarization. As a furtherfunctionality the similarity between terms is computable allowingfor automated term expansion and mapping to the taxonomy oftopics in the music ontology. The lyrics collection contains 500documents. While the querying for terms or topics is easy toperform, the more challenging approach is to examine termsimilarities or even document similarities. For the latter we showsome typical results as stereotypes for the most common resultcases of the approach in the following. For simplification wereduced the presentation on the 5 most-relevant terms of a givenreference song and the top 3 similar songs by applying standardmetrics of the vector space model.\n\u001f Song 193: Phil Collins - One More Night\nMost-relevant terms: forever wait night cos, Similar: P. Collins – You\nCant Hurry Love, P.Collins - Inside Out, P.Collins - This must be Love\n\u001f Reference Song 297: Cat Stevens - Father And SonMost-relevant terms: fault decision marry son settle, Similar:\nP.Collins - We're Sons Of Our Fathers, Sheryl Crow - No One Said ItWould Be Easy, George Michael - Father Figure\n\u001f Reference Song 112: Lucy pearl - Dance tonight\nMost-relevant terms: toast spend tonight dance money , Similar : Lucy\nPearl - you (feat. snoop dogg and Q-tipp), Phil Collins - Please ComeOut Tonight, Madonna - Into the groove.\n\u001f Reference Song 56: Fanta4 - Das Kind Vor Dem Euch ...\nMost-relevant terms: wollten euch sehn entsetzt selben , Similar:\nFanta4 - Auf Der Flucht, Freundeskreis - Mit Dir, Fanta4– Populär\n\u001f Reference Song 145: Madonna - Paradise\nFeatures: remains pas encore fois moi, Similar : Zero Hits\n6. DISCUSSION AND FUTURE WORK\nNon-musicians may query the musical database by remembering\nparts of the lyrics. In a recent evaluation with 100 naïve listenerswe found this class of queries being essentially often used in anon-restricted user interface. The integrated approach can handlethese queries. Some artists seem to cope with an overall theme ona complete album or even for a set of albums. Similarity metricsfor term frequencies deliver appropriate results for thesephenomena (see example 193). Some topics can be found acrossgenre-boundaries (see example 297), which is indeed the intentionfor topic-based queries neglecting musical genres. Other topics aremore often represented in specific genres (see example 112).Dancy music often talks about dancing, parties, good vibes.\nSpecific vocabularies are typical for some very specific styles, e.g.German hip-hop (see example 56). This is a first impression,which has to be evaluated thoroughly in the future. Large corporawith multi-lingual entities are obviously necessary to cope withlyrics in different languages. Our initial corpus has been too smallto cope with languages being different from English or German(see example 145). We still see a lot of potential in this kind ofwork if combined with the theory of affective computing .W e\ncould use lyrics and IR techniques to create automaticallymeaningful terms and topics. The emotional perception of such atopic ( war vs.peace ) may be coupled with the emotional\nperception of the audio surface structure ( minor vs. major ). In\nsuch a way the concept of moods [5] could be provided\nautomatically for end-user queries. We presented the concept ofsuper-convenience in this work for the first time. Our framework\ncould be established by using cross-fertilization from differentresearch disciplines, mainly in the area of AI. NLP, IR andOntologies are the most prominent ones which have beenincorporated in this work to get close to our initial goal.\n7. REFERENCES\n[1] Hunter J., Adding Multimedia to the Semantic Web -Building an MPEG-7 Ontology, in Int. SWWS, Stanford,July 30 - August 1, 2001\n[2] Whitman B., KANDEM: Community Metadata for InformedMusic Retrieval, MIT MediaLab, May 2002, Website<URL: http://web.media.mit.edu/~bwhitman/kandem/>\n[3] Pachet F., A Taxonomy of Musical Genre, Proc. of ISMIR2001, Paris, France, 2001\n[4] Pfeiffer S., Vincent T., Formalisation of MPEG-1compressed domain audio features, Technical ReportNumber 01/196, CSIRO, Australia, 2001\n[5] Huron D., Perception and Musical Applications in MusicInformation Retrieval, in Proc. of the ISMIR 2000,Plymouth, Massachusetts, 2000"
    },
    {
        "title": "Usability of Musical Digital Libraries: a Multimodal Analysis.",
        "author": [
            "Ann Blandford",
            "Hanna Stelmaszewska"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417171",
        "url": "https://doi.org/10.5281/zenodo.1417171",
        "ee": "https://zenodo.org/records/1417171/files/BlandfordS02.pdf",
        "abstract": "There has been substantial research on technical aspects of musical digital libraries, but comparatively little on usability aspects. We have evaluated four web-accessible music libraries, focusing particularly on features that are particular to music libraries, such as music retrieval mechanisms. Although the original focus of the work was on how modalities are combined within the interactions with such libraries, that was not where the main difficulties were found. Libraries were generally well designed for use of different modalities. The main challenges identified relate to the details of melody matching and to simplifying the choices of file format. These issues are discussed in detail.",
        "zenodo_id": 1417171,
        "dblp_key": "conf/ismir/BlandfordS02",
        "keywords": [
            "music digital libraries",
            "usability aspects",
            "web-accessible music libraries",
            "music retrieval mechanisms",
            "modalities combined",
            "libraries well designed",
            "details of melody matching",
            "simplifying file format choices",
            "issues discussed",
            "main challenges identified"
        ],
        "content": "Usability of Musical Digital Libraries: a Multimodal Analysis\nUsability of Musical Digital Libraries: a Multimodal\nAnalysis\nAnn Blandford\nUCL Interaction Centre (UCLIC)\nUniversity College London\n26 Bedford Way, London, WC1H 0AB, U.K.\n+44 20 7679 7557\nA.Blandford@ucl.ac.ukHanna Stelmaszewska\nUCL Interaction Centre (UCLIC)\nUniversity College London\n26 Bedford Way, London, WC1H 0AB, U.K.\n+44 20 7679 7557\nH.Stelmaszewska@ucl.ac.uk\nABSTRACT\nThere has been substantial research on technical aspects of\nmusical digital libraries, but comparatively little on usability\naspects. We have evaluated four web-accessible music libraries,\nfocusing particularly on features that are particular to music\nlibraries, such as music retrieval mechanisms. Although the\noriginal focus of the work was on how modalities are combined\nwithin the interactions with such libraries, that was not where\nthe main difficulties were found. Libraries were generally well\ndesigned for use of different modalities. The main challenges\nidentified relate to the details of melody matching and to\nsimplifying the choices of file format. These issues are\ndiscussed in detail.\n1. INTRODUCTION\nAs digital libraries become more widely available, particularly\nvia the Internet, and as the capability of that network to\ntransmit large volumes of data within reasonable times\nincreases, so more collections of music are becoming available\nthrough web-based digital libraries. These music collections\nare stored in, and can be retrieved in, various formats, and can\nalso be accessed by various mechanisms. To be genuinely\nuseful, such collections need to be easily accessed by web\nusers across the globe, with differing levels of musical and\ninformation retrieval expertise. There is very little literature on\nusability issues for such systems; one of the few examples we\nhave found in an evaluation of a Digital Music Library (DML)\nproject conducted by Indiana University [10]. The usability\nstudies conducted for that project examined users’\nperformance on pre-determined tasks and gathered their\nreactions to an early prototype of the DML interface; the study\nfocused on general usability issues and user satisfaction,\nrather than issues that are particular to music libraries. In\ncontrast, Cunningham [9] discusses how potential users of\nmusic information retrieval systems might be identified, and\ntheir needs ascertained. The work reported here takes a different\napproach: it investigates usability issues that pertain\nspecifically to existing digital libraries containing music\ncollections, including music stored in various representational\nformats. The primary focus is on usability issues relating to\nthe modalities employed in the interaction between user and\nsystem. As others (e.g. [14]) have noted, there is a ‘medium\nmismatch problem’ when documents and queries are expressed\nin different media; our work investigates this problem from a\nusability perspective, considering both the use of different\nmedia and also mismatches within one medium.\nThe method employed in this work has been to apply a novel\ntheory-based usability evaluation technique, EvaluatingMultimodal Usability (EMU: [11]), to four different web-\naccessible music digital libraries. These four libraries have\nbeen chosen to provide a reasonably broad representation of\nthe capabilities of existing libraries. They include different\nretrieval mechanisms, from the user simply typing the title of a\ntarget tune to the user entering a sound file that represents the\ntarget melody or entering a representation of the melody using\na text-based tune contour notation (described below). They\nalso include different media for the retrieved tune: musical\nscore, “ABC” notation, lyrics, or various formats of sound file.\nIn one case, the retrieved tunes are accompanied by video clips.\nThese different retrieval mechanisms and media formats are\ndescribed in more detail below.\n1.1 Modality: A definition\nThe focus is on the use of different modalities within the\ninteraction between user and digital library. There exist\nvarious definitions of a ‘modality’ (e.g. [1, 3, 4, 8, 17]). Here,\nwe take the definition derived by Hyde [11] as a “temporally\nbased instance of information perceived by a particular\nsensory channel”. This definition encompasses three\ndimensions:\n• Time, which may be discrete, continuous or dynamic;\n• Information form, which may be lexical, symbolic or\nconcrete; and\n• Sensory channel, which may be acoustic, visual or haptic.\nHere, ‘discrete’ means that the information is communicated\nover a very short period of time, as a discrete event, rather than\nrepeating the same information over an extended period of\ntime (‘continuous’) or varying the information content over\ntime (‘dynamic’).\nThe information may be communicated in words, or other\nlinguistic form (‘lexical’) or may be non-verbal, in which case\nit is classed as either ‘symbolic’ (meaning that the\nrepresentation has some meaning to the user over and above\nbeing a picture, sound or sensation) or ‘concrete’ (simply\n‘being’ – for example, the sight and sound of rain falling).\nInformation is communicated through sensory channels that\ncorrespond to people hearing (‘acoustic’), seeing (‘visual’) and\nfeeling (‘haptic’).\nThe various means of representing music can be classified\naccording to this modality definition. For example, a\ntraditional musical score would be classified as\nvisual–symbolic–continuous, while the heard melody would\nbe acoustic–concrete–dynamic and the lyrics (displayed on the\nscreen) would be visual–lexical–continuous. Each main\nmodality may also have dependent modalities: for example,\nthe volume or dynamics of a melody may impart additional\ninformation, which would add additional acoustic–symbolic–\nmodalities to the basic sound. In addition, modalities may be\nused together; for example, in music videos the auditory andPermission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit orcommercial advantage and that copies bear this notice and thefull citation on the first page.© 2002 IRCAM – Centre PompidouUsability of Musical Digital Libraries: a Multimodal Analysis\nvisual modalities complement each other, and in some cases\nlyrics are displayed as the corresponding melody is played.\nIn interaction, user and computer system both express (output)\nmodalities and receive (input) modalities; one source of\ndifficulties will be incompatibilities between the two – for\nexample, a user singing to a computer system that has no\nmeans of receiving auditory input.\nThe EMU evaluation technique involves defining user goals\nand tasks while working with a library and then stepping\nthrough the task identifying what input and output modalities\nare involved in each step, and checking for any mismatches or\nclashes. In practice, for any new system, this first involves an\nexploratory study in order to identify what user goals the\nsystem supports, and what the corresponding task structures\nare.\nHyde’s [11] modality definition and her approach to assessing\nmultimodal usability were used to structure the work reported\nhere.\n1.2 The collections\nIn order to investigate a range of the usability issues posed by\nweb-accessible music collections, four collections that make\nuse of different representational formats and retrieval\nmechanisms were selected for study. Two of the collections are\navailable in the New Zealand Digital Library (NZDL) because\nwe are collaborating with the developers at the University of\nWaikato [12, 2], so that usability evaluations are informing\nongoing development work. Another two collections were\nidentified as providing interesting contrasts to further explore\nmultimodal usability issues. Each of these four collections is\ndescribed below. These descriptions are based on the systems\nas they were in December 2001, and do not reflect any changes\nmade more recently.\n1.2.1  The NZDL Music Library\nThe NZDL Music Library1 consists of several separate\ncollections that have been developed at different times, and\nwith different research objectives. All are based on the same\nretrieval mechanisms, of which the key features are as follows.\n• Both search and browse facilities are available.\n• Browsing is performed by tune title only.\n• Searching can be performed by text entry of tune title,\nor by defining a melody file that contains the melody\nto be matched. Both ‘and’ and ‘or’ searches are\nsupported.\n• Any melody file has to be created ‘outside’ the NZDL\nsystem; a variety of sound formats are supported.\nMelody files are typically created by singing a tune\nclearly, using any available recording software. The\nmelody files used in this study were created using an\nAIFF recorder on a Macintosh computer.\n• When using melody files, the user can play back:\no ‘what you sang’ (the user’s original sound\nrecording);\no ‘what I heard’ (the sound file created by the\nmelody indexing system, Meldex, after\nprocessing the input file, which is then\ncompared against files in the NZDL music\ncollection); and\no any retrieved melodies.\n• Tunes can be retrieved as melodies (played back\nusing a browser plug-in such as QuickTime on the\n                                                                        1  accessed via http://www.nzdl.org/Macintosh) or as scores. In some collections,\nadditional information about tunes can also be\nretrieved, but we do not consider that further here.\n1.2.2  JC’s ABC Tunefinder\nJC’s ABC Tunefinder2 supports retrieval of music as ‘ABC’\nnotation, standard musical scores and MIDI (sound) files. The\nsite contains an index of files from over 200 separate online\nABC collections, and the various output formats are generated\nautomatically from the ABC files. Consequently, this site\nsupports searching but not browsing. The user can specify a\ntune by text entry of either the title or a specification of the\nsound contour. The latter is generated by typing ‘u’ if a note is\nhigher than the previous one, ‘d’ if it is lower, and ‘s’ if it is\nthe same (so, for example, the opening bars of ‘Three Blind\nMice’ would be written as ‘ddudd’) [16].\nThe ABC notation is an established text-based notation that is\nboth human and machine readable, and that expresses the main\nfeatures of a tune as it might otherwise appear in standard\nmusic notation3.\nThe site provides other facilities, such as a capability to\nconvert ABC files into other formats, that we do not consider\nfurther here.\n1.2.3  The Folk Music Collection\nThe Folk Music Collection4 consists of early (prior to 1927)\nfolk music. Users can search by text only; that text may be in\nthe title, lyrics or other information about the song. Songs can\nalso be browsed by various categories – for example, by\ncountry of origin or by type. The user can retrieve lyrics,\ntogether with any additional information about the song that\nhas been made available, and can also download a midi file of\nthe tune, which will play using the relevant browser plug-in.\nOn most pages in this system, an appropriate tune is played as\nbackground music whenever the page is displayed.\n1.2.4  The NZDL Music Videos Collection\nThe NZDL music videos collection5 consists of short pop and\nrock video clips, with accompanying sound tracks. Tunes are\nretrieved by title or artist, and are played back as audiovisual\nclips using an appropriate browser plug-in, such as QuickTime\non the Macintosh. The videos collection can also be browsed\nby title or artist. (In each category, items are ordered\nalphabetically; within artist, items by the same artist are\ngrouped together).\n2. AIMS AND METHOD\nThe aim of the work reported here is, as noted above, to\ndevelop an understanding of the usability issues that apply to\ncollections of musical information, and particularly to\nretrieval of musical documents from a digital library. The four\ncollections described above were selected as being\nrepresentative of the current state of the art, in terms of what is\npublicly available for music retrieval.\nFor each collection, analysis proceeded as follows:\n1) A familiarisation phase: the collection was studied\ninformally, to identify key features and interaction\npossibilities, and to define representative tasks to be\nused for the EMU analysis.\n2) For the representative tasks defined in step 1, a full\nEMU analysis was conducted.\n                                                                        2  http://trillian.mit.edu/~jc/music/abc/FindTune.html\n3  for more details see http://www.gre.ac.uk/~c.walshaw/abc/\n4  http://www.contemplator.com/folk.html\n5  accessed via http://www.nzdl.org/Usability of Musical Digital Libraries: a Multimodal Analysis\n3) In addition to the EMU analysis, which focuses on\nmismatches and clashes, other usability difficulties,\nrelating specifically to the type of data being\nretrieved and the retrieval mechanisms made\navailable, were noted. Although these were not\nidentified explicitly through the EMU notation, the\nuse of EMU provided a structure for the analysis.\nMore specifically, the need to understand the systems\nwell enough to conduct the EMU analysis meant that\nthorough exploratory walkthroughs were conducted,\nthrough which a range of usability challenges were\nidentified.\nThe results of these two kinds of analysis are summarised\nbelow. Extended EMU analyses for all collections are presented\nby Blandford and Stelmaszewska [5].\n3. RESULTS\nThe detailed EMU analyses were conducted for canonical tasks\n– that is, tasks in which the user makes no errors and that are\nachievable. Since each collection supports different user goals,\nwhich correspondingly different task structures, a different\ntask was used for each collection. The EMU analyses per se\nidentified very few usability difficulties, the main one being a\nphysical clash in the Folk Music collection, which we describe\nbriefly here. As noted above, on many pages, the relevant tune\nis played as soon as the page has loaded in the user’s browser\nwindow. However, the user is also presented with an option to\ndownload the tune. In some browsers, if the user does so, the\ntune will start playing a second time, ‘over’ the first version.\nAlthough the computer system is capable of transmitting two\nstreams of audio data, the user is not able to separate them into\ntwo separate streams, and therefore hears a cacophony of\nsound. Whether or not this happens depends on which browser\nis being used.\nThe fact that this was the only substantive usability problem\nrelating to modalities that was found, and that this situation is\neasily avoided by a user once it has been discovered, indicates\nthat, in term of modalities employed in the interaction, all four\ncollections analysed were usable. The discussion that follows\nconsiders broader issues involved in the specification,\nmatching and presentation of musical information.\nWe structure the discussion of results by the kinds of activity\ninvolved in retrieval of a tune: browsing; text-based\nsearching; tune matching; how retrieved scores, melodies and\nother information is presented to the user; and how the\ncollections interface with other systems.\n3.1 Browsing\nThe two NZDL collections and the Folk Music collection\nsupport full browsing, and all collections support browsing\nwithin a search results set. In the case of the NZDL Music\nCollection, browsing is by title only, which supports user\nfamiliarisation with the overall contents of the collection. In\nthe case of the NZDL Music Videos Collection, browsing can\nbe by title or artist. Within the Folk Music Collection,\nbrowsing is by genre.\nAll four collections support browsing within search results,\nbut present results in different orders. In the case of the NZDL\ncollections, the order is by calculated quality of the match,\nwith the best match first. In the case of the Folk Music\nCollection, search results can be browsed in alphabetical order\nof file name (although filenames ending in ‘.html’ are listed\nbefore ones ending in ‘.htm’), which does not guarantee that\ntunes are listed in an order that might seem natural to the user.\nJC’s ABC Tunefinder also lists results alphabetically. In all\ncases except the music videos, the fact that tunes are collected\nfrom disparate sources means that there may be multiple copies\nof the same tune, with the same or slightly different titles.For a music collection, an alternative browsing technique\nmight be auditory. However, auditory browsing is difficult to\nimplement effectively due to the dynamic nature of auditory\ninformation, which does not fit well with the user-controlled\nactivity of skimming results quickly, which is much more\neasily achieved for continuous information. In the context of\nthe Internet, where sound files are stored remotely and\ndownload times (even for small sound files) may be\nsignificant, implementing auditory browsing would be very\ndifficult. Therefore, it is hardly surprising that browsing has\nbeen limited to test only in existing music libraries.\n3.2 Text-based searching\nText-based searching by title matching posed only minor\ndifficulties. In some collections, there is ambiguity about what\ntext is being matched; for instance, in the NZDL Music\nCollection, the user is invited to enter ‘text’, but the only text\nthat will be matched against is that from the titles of tunes.\nConversely, in the Folk Collection, text is matched against any\ntext in the file, which may be title, lyrics or other information.\nIn most cases, the user is given insufficient information about\nwhat is being matched, and no opportunity to define the scope\nof the matching. This is likely to be a consequence of the ways\nthese kinds of collections have been amassed, since most have\na structure that was not designed for the current purpose. The\nexception is the NZDL Music Videos collection, where users\ncan select to search by title, artist or both.\nThe NZDL Music Collection search engine can be set to ‘case\nsensitive’ or ‘case insensitive’, but defaults to ‘case sensitive’.\nSince some collections have tunes indexed by titles in upper\ncase and some in title case, it is difficult for the user to\nanticipate which case is appropriate; it is also not immediately\nclear to the user that matching is case sensitive. This is one\nexample (more are discussed in the following section) of a\nsituation where the matching algorithm may not return the\nexpected or intended results.\nDue to the way folk tunes are archived, with titles being passed\non by word of mouth, there can be variations in the ways tune\ntitles are spelt, resulting in erratic retrieval of tunes. This is\nreported (Chambers, personal communication) to cause\ndifficulties sometimes, but we did not experience any\nparticular problems with text based searching.\nWith these provisos, text-based searching is cognitively\nundemanding and generally successful. The user enters lexical\nitems that can be easily matched against corresponding\ndatabase items.\n3.3 Tune matching\nTune matching presents much greater challenges to systems\ndevelopers and users alike. While it offers great promise as a\nmeans of retrieving information (such as title and score) about\ntunes for which only the melody is known, there are\nsubstantial technical and usability difficulties to be overcome.\nOnly two of the four collections analysed support tune-based\nretrieval: the NZDL Music Collection and JC’s ABC Tune\nFinder. We start by considering difficulties with the ‘contour’\nfeature of the ABC Tune Finder.\nAs discussed above, ‘contour’ search allows the user to enter\nletters (u, d and s) representing the direction in which the notes\ngo (i.e. up, down and same). The contour does not allow the\nuser to identify how much higher or lower each note is than the\npreceding one, or to represent note durations. If trying to\nretrieve a tune that is in their head, the user has to sing the tune\nto themselves, noting whether each note is higher, lower or the\nsame as the previous one. For ‘ABC novices’ this is a\ncognitively demanding task as it involves converting from\nrelative pitch (an acoustic, concrete, dynamic modality) to aUsability of Musical Digital Libraries: a Multimodal Analysis\ntextual representation on each (discrete) note-change in a\ndynamic melody (visual, lexical, discrete modality).\nFurthermore, there are features of the ABC tune finder that\nmight put a user off track. The first is that the system matches\nthe user’s entry against only the first 16 (or fewer) notes of the\nstored tune (i.e. 15 intervals); if the user enters more intervals\nthat this then the system returns no matches, but with no\nexplanation: the more precise the user tries to make the results\nset, the greater the chances of them getting no results at all. The\nsecond is that the system uses the ABC notation as an\nintermediate representation, so that heard notes that are a semi-\ntone apart may be represented as the ‘same’ (e.g. C and C#), and\na single heard note that is internally represented as two or more\nslurred notes is also represented as a ‘same’ interval. A third\ndifficulty is that notes that are not part of the main melody line\nmay be included in the file. These phenomena are illustrated in\nFigures 1 and 2. Figure 1 shows the opening bars of\n“Yesterday”, which includes a D-flat and D-natural in the\nsecond bar, which are on the same note of the stave: this\ninterval sounds ‘up’ but has to be represented as ‘same’. A\nsecond difficulty for individuals ‘singing to themselves’ is\nthat the three notes at the end of the first bar and beginning of\nsecond are part of the accompaniment, and do not have lyrics\nattached, so someone singing the tune based on the lyrics\nwould miss out these three notes (and the corresponding\nintervals), and hence the tune would not match. The contour of\nthe song ‘Yesterday’ is represented as ‘dssuduuusuudds’, but\nthe user might reasonably enter ‘dsuuuuuudds’ for the same\nmusical phrase; the latter returns ‘no matches’.\nFigure 1: Extract from the score of “Yesterday”\nThe difficulty with slurred notes is illustrated by “Layla”, as\nshown in Figure 2. This has a contour representation of\n‘usuddduuudduddu’. Here, notes (e.g. second and third of first\nbar) are slurred, so that the later of the two notes is not\nsounded separately. Again, the interval between these notes\nhas to be represented as ‘same’ even though the second is not\n(audibly) a separate note from the first. In future, the\ndevelopers are experimenting with removing ‘same’ from the\nmatching, to assess whether more satisfactory results are\nreturned.\nFigure 2: Extract from the score of “Layla”\nFor the user who already has the musical score and is simply\ntrying to access the ABC notation or a MIDI file, these\nproblems are surmountable, but the user who is ‘playing by\near’ is likely to experience tune retrieval as a very ‘hit or miss’\naffair.\nIn summary, although conceptually simple, the ABC Tune\nFinding technique presents usability difficulties for many of\nthe intended user population because the underlying datarepresentation is precise, but does not necessarily match the\nuser’s internal representation of the tune accurately.\nThe NZDL Music Collection approach of ‘singing to your\ncomputer’ is, conceptually, even simpler than that of\ndescribing contours textually. However, it suffers from the\nsame tension between the precision of the database matching\nand the accuracy (relative to the user’s “song in the head”).\nEven though the matching is approximate [13], it can appear\ntoo precise for the user. Here, we consider only contour\nmatching within the collection (where the system derives a\ncontour from the sung melody for matching purposes).\nBecause the files in the database have been gathered in various\nways, the quality is variable: the files in one collection (‘Fake\nBook’) were collected by applying optical musical recognition\nto printed music, while those in another (‘MIDIMax’) were\ngathered by trawling the web. Consequently, some melodies in\nthe database have missing notes or the occasional inaccurate\nnote, so that contour matching fails if the user sings the\nmelody correctly. Preference settings (e.g. metronome beats)\nmay also affect the way a melody is interpreted (the difference\nbetween ‘what you sang’ and ‘what I heard’) in ways that are\ndifficult for non-expert musicians to anticipate. The user is\ngiven feedback, both visual and auditory, on how their input\nwas processed, as shown in Figure 3. In principle this gives\nappropriate feedback, at least to the user who can read the\nstandard music notation. One case where this can break down\nfor new users occurs when they sing indistinctly, so that the\ninterpretation mechanism fails to identify any notes, resulting\nin a display that shows just the musical stave with no notes\nshowing. To the new user, this can look like decoration rather\nthan feedback. The difficulty of poor database entries is\nillustrated in Figure 3 by the absence of ‘AULD LANG SYNE’\n(non-Christmas version), which is in the database, but was not\nmatched because of missing notes.\nFigure 3: Search results, including feedback to user on ‘what\nI heard’\nIn summary, in both contour-matching systems, there are user\ndifficulties caused by differences between a user’s mental\nrepresentation of a tune, which will typically include\napproximate pitch intervals and note durations in the melody,\nand the precise but abstract representation of the melody in the\ndatabase, which abstracts over pitch intervals (leaving just a\ncontour) and durations (leaving just temporal sequence).\nAdditional user difficulties lie in the means of translating the\ntune in the head into an external representation that can be\nentered into the computer system (whether that be an ABC\ncontour or a sung melody) and then checking that the external\nrepresentation is correct.Usability of Musical Digital Libraries: a Multimodal Analysis\nFigure 4: Search results for contour search, including the various output formats in JC’s ABC Tune Finder\n3.4 Representation of output possibilities\nThe developers of music collections are faced with a real\nchallenge in determining appropriate output formats. There are\nboth multiple modalities (the musical score, the sound file,\nand other textual representations such as ABC notation, lyrics\nor other information) and also multiple formats for each\nmodality – typically, that have been developed for different\nplatforms or different user populations.\nThe outputs available for JC’s ABC Tune Finder are shown in\nFigure 4. In summary, there is one option to ‘GET’ a whole file\n(rather than just one tune representation), two textual options\n(‘TXT’ and “ABC’) that return the ABC notation, five graphicaloptions that return the musical score, in formats from PS to\nPNG (which return files that are visually very similar if the user\nhas the necessary file readers for each format) and, finally, one\nsound option for playing the MIDI sound file (using a browser\nplug-in such as QuickTime). As shown in Figure 4, while the\ndifferent file formats are clearly represented at the interface, the\ndifferent modalities of the file contents are not, even though\nthis is likely to be the most important file feature for a user.\nABC, PS and EPS cause a file to be downloaded, to be further\ninterpreted by suitable software on the user’s computer,\nwhereas the remaining formats appear without further user\nintervention in the browser window.\nFigure 5: Sound output formats in NZDL Music CollectionUsability of Musical Digital Libraries: a Multimodal Analysis\nChambers (personal communication) argues that the difference\nbetween the various graphical formats is important: that PS is\nmuch higher quality than GIF or PNG, and that in folk circles\nlower quality is generally preferred. In terms of modalities, as\nwell as the primary modality (visual–symbolic–continuous)\nthrough which the tune is represented, there is a dependent\nmodality, also visual–symbolic–continuous, that\ncommunicates the ‘folkiness’ of the paper representation.\nClearly, for some – arguably more sophisticated – users, this\nproperty of the visual representation is important, while for\nothers it is not. However, this understanding is only available\nto people who are experts in both folk music culture and\ncomputer music representational formats; for others,\nadditional explanation would be helpful, or the choice may be\nbemusing.\nWhereas in JC’s ABC Tune Finder, the largest choice of formats\nis made available for the musical score, within NZDL Music\nCollection, the greatest choice is of sound formats, as shown in\nFigure 5.\nIn practice, a different subset of these formats is likely to be\navailable on any particular computer system. To most users,\nMIDI Types 0 and 1 are indistinguishable, and most first-time\nusers of this kind of system are likely to have difficulty\nascertaining which other sound formats are actually available\nto them. If the user chooses a non-available format, the system\nresponse is unpredictable; for instance, when we selected\n‘Ulaw’ on one computer system, we waited 12 second while the\nfile downloaded, at which point a ‘broken’ QuickTime icon was\ndisplayed, with no output sound; whereas when we selected\nReal Audio, an error message was immediately displayed\nstating that the application ‘Real Player’ was not available.\nOther than the two MIDI formats, the different sound formats\nthat are available on a particular computer system typically\nhave audibly different qualities – notably of pitch and pace.\nThe user who does not have a background in audio technology\ncan only discover these differences through a process of trial\nand error. Also, ‘what I sang’ can only be played back in the\nsound format in which it was recorded, so giving the user a\nchoice is liable to lead to user errors. While this choice of\nformats may be appropriate for experts, it would ideally be\navoided for novices, by setting suitable defaults.\nWithin the NZDL Music Collection, the different types of\noutput are represented by icons next to the tune names, as\nshown in Figure 6. In this example, there are three alternatives:\nto display the musical score (the icon with the treble clef), to\nhear a MIDI sound file (the speaker icon) or to see textual\ninformation about the tune. There is a notational difference\nbetween MIDI files (which can only be played back in MIDI\nformat) and other sound files (see for example the speaker icon\nnext to ‘what you sang’), which users are expected to\nunderstand.\nFigure 6: NZDL Music Collection (MidiMini) query results\nIn the Music Videos collection, the user is presented with a\nchoice of video output format, as illustrated in Figure 7;\nsometimes there are only one or two choices, but sometimes –\nas shown here – there are three. When tested, selecting one ofthe leftmost icons resulted in the file being downloaded and\nthen played using QuickTime; selecting the centre icons\n(‘Mpeg’) resulted in the file being downloaded but then\n‘disappearing’; selecting the rightmost icon (‘Real Video’)\nresulted in an immediate error message. Since these video files\nare large (typically 4 or 5 MB), they can take several minutes to\ndownload, making the user cost of downloading high, and\nparticularly so if the resulting file cannot be played.\nFigure 7: Music Videos browsing results\nIn the Folk Music collection, the user is presented with no\nchoice of output format for the available output modalities.\nThis lack of choice leaves less scope for user error.\nIn summary, for the broad range of web users that might access\nthese systems, there has to be a clear way of presenting the\ndifferent modalities of output that are possible (e.g. sound,\nscore, text), but there should be a sensible default for file\nformats; for example, MIDI, pdf and html (respectively) would\nbe sensible defaults for web-accessible music collections.\n3.5 Interfacing with other systems\nAll the collections studied are implemented to be accessed via\na web browser, and to make use of browser plug-ins such as\nQuickTime. In an informal study with novice users, we found\nthat the boundary between the collection software and such\nplug-ins was not always obvious to them. With the exception\nof the Folk Collection, when music files are played, the user\nsees a window from which the only way to reach another page\nis via the browser’s ‘back’ button. In many cases, the moving\nposition indicator on the QuickTime audio control panel\n(Figure 8) might be considered redundant, as the user can also\nhear the music playing. However, it provides complementary\ninformation – to indicate how far through a tune we are – and\nalso might alert a user to the fact that the sound is turned down\non their computer (if they can see the indicator moving but not\nhear the melody).\nFigure 8: QuickTime audio control panel\nIf a user is trying to retrieve tunes by melody matching in the\nNZDL Music Collection, this necessitates constant switching\nbetween NZDL and sound recording software. We get a cycle of\nactivity in which the user, working with NZDL, has to exit to\nthe operating system, initialises or re-selects the sound\nrecording software, records some singing, saves that to file,\nthen returns to the web browser (i.e. NZDL Music Collection)\nand browses the disk to find the file just saved (having to\nremember the name, of course) to initiate a new search. This\nplaces a high cognitive load on the user, having to remember\nthe sequence of operations and the latest name of the file.\nRemote melody matching over the Internet places heavy\nconstraints on the developer (the melody has to be saved inUsability of Musical Digital Libraries: a Multimodal Analysis\norder to be transmitted), so this difficulty may be unavoidable,\nbut developers should be aware of the problem.\n4. CONCLUSIONS\nWe have seen that, in the web-accessible musical digital\nlibraries studied, the greatest difficulty is not the ‘medium\nmismatch problem’ [14], whereby documents and queries are\nexpressed in different media, but the data mismatch problem\nfor melody matching. This problem arises both because of the\nway that many collections have been gathered (having variable\nquality) and because of intermediate representations used in\nthe matching. Thus, there is a tension between notional ease of\nuse and usefulness and actual ease of use. While the idea of\nmelody matching is intuitive and appealing, the current state\nof the art is such that major usability difficulties still exist.\nThere have been rapid developments in web technologies. A\nrange of technical formats for each medium (text, sound,\ngraphics, video) have been developed, some of which are\nproprietary or platform-specific, others of which are now very\nwidely available. Where system developers have catered for a\nvariety of remote user systems, this can result in confusion for\nany individual user, who does not necessarily know what is\navailable on their particular system. As standards converge,\nand as it becomes increasingly possible for the web server to\nidentify features of the client (so that decisions about formats\ncan be made by systems without user intervention), it should\nbe possible to minimise or eliminate user involvement in\nformat selection.\nThere is a tension between the widespread aim of catering for\nnovice users and the need to demand a comparatively\nsophisticated understanding of the technology and concepts\nthat underpin musical digital libraries. The various libraries\nstudied here have demanded different levels of musical\nsophistication of their users: the more powerful retrieval\nmechanisms unavoidably demand a deeper understanding of\nunderlying technologies. The design challenge is to minimise\nthe understanding needed and to communicate effectively with\nusers in the users’ language.\nLooking to the future, we can identify usability requirements\nthat apply particularly to digital music libraries. One is that\nformat options should be transparent to users – or should\ndefault to common standards for novices, with choices\navailable to more sophisticated users – so that users can focus\non modality and information content options.  For example,\nnovices should be able to choose between text, score, ABC and\nsound modalities, without having to choose between (say) PS\nand PDF or AIFF and MIDI1. Here, by ‘novices’, we mean\npeople who are unfamiliar with computer music and the\nvarious computer music formats, although they may be expert\nmusicians.\nDesigners need to pay attention to the real challenge of trading\nprecision for accuracy. More seamless integration of\ntechnologies should gradually become easier to achieve. In\nthis paper, we have not considered general web [15] or library\n[6] usability issues, although universal requirements such as\nsystems being self-explanatory and giving appropriate and\ntimely feedback apply as much to music digital libraries as to\nany other.  The provision of truly usable web-accessible\nmusical digital libraries represents a huge challenge; this\nstudy has provided some pointers towards areas that need more\nattention.\n5. ACKNOWLEDGMENTS\nThis work is supported by EPSRC Grant GR/M71848. We are\ngrateful to Joanne Hyde for guidance on the application of\nEMU, to George Buchanan for technical advice, and to David\nBainbridge and John Chambers for feedback on the design ofthe NZDL Music Collection and JC’s ABC TuneFinder\n(respectively) and for constructive comments on an earlier\nversion of this paper. Figures are reproduced with permission.\n6. REFERENCES\n[1] Alty, J. L. (1991) Multi-media - what it is and how we\nexploit it. In Diaper, D. & Hammond, N. (Eds.):\nProceedings of HCI ’91. Cambridge University Press, pp.\n31-46\n[2] Bainbridge D., Nevill-Manning C., Witten I.H., Smith L.A.\nand McNab R.J. (1999) Towards a digital library of\npopular music In E.A. Fox and N. Rowe (Eds.) Proc\nFourth ACM Conference on Digital Libraries,edited, pp\n161-169. ACM.\n[3] Bernsen, N. O. (1995a) A revised generation of the\ntaxonomy of output modalities. In Bernsen, N. O.,\nJensager, F., Lu, S., Verjans, S. (eds): Information theory\nand information mapping, Amodeus project deliverable\nD15\n[4] Bernsen, N. O. (1995b) A taxonomy of input modalities.\nIn Bernsen, N. O., Jensager, F., Lu, S., Verjans, S. (eds):\nInformation theory and information mapping, Amodeus\nproject deliverable D15\n[5] Blandford, A. E. & Stelmaszewska, H. E. (2002)\nEvaluating Multimodal Usability of Musical Digital\nLibraries: a Case Study. RIDL Technical Report.\nAvailable from http://www.cs.mdx.ac.uk/ridl/\n[6] Blandford, A., Stelmaszewska, H. & Bryan-Kinns, N.\n(2001) Use of multiple digital libraries: a case study. In\nProc. JCDL 2001. 179-188. ACM Press.\n[7] Chambers, J. (personal communication) Email message\ndated 6th April 2002.\n[8] Coutaz, J., May, J., Young, R., Blandford, A., Nigay, L.,\nSalber, D. (1995) Integrating system and user modelling\nthrough abstraction: the CARE properties for reasoning\nabout multimodality. In: Nordby, K., Helmersen, P.,\nGilmore, D. J., and Arnesen, S. (eds): Human-Computer\nInteraction: Interact'95. Chapman and Hall, pp. 115-120\n[9] Cunningham, S.-J. (2002) User Studies: A  First Step in\nDesigning an MIR Testbed. In J. S. Downie (Ed.) The\nMIR/MDL Evaluation Project White Paper Collection\nEdition 1. pp. 19-21\n[10] Fuhrman, M., Gauthier, D. & Dillon, A. (2001) Usability\nTest of VARIATIONS and DML Prototype.  Available\nfrom www.dml.indiana.edu/pdf/VariationsTest.pdf\n[11] Hyde, J. K. (2002) Multi-Modal Usability Evaluation.\nPhD thesis. Middlesex University\n[12] McNab, R. J., Smith, L. A., Bainbridge, D. & Witten, I. H.\n(1997) The New Zealand Digital Library MELody inDEX,\nD-Lib Magazine, May 1997\n[13] McNab R.J., Smith L.A., Witten I.H. and Henderson C.L.\n(2000) Tune retrieval in the multimedia library\nMultimedia-Tools and Applications 10,113-132. Kluwer\nAcademic Publishers.\n[14] Meghini, C., Sebastiani, F. & Straccia, U. (2001) A model\nof Multimedia Information Retrieval. Journal of the\nACM. 48. 909-970.\n[15] Nielsen, J. (2000) Designing Web Usability: The Practice\nof Simplicity. New Riders\n[16] Parsons, D (1975) The Directory of Tunes and Musical\nThemes. Spencer Brown, Cambridge.\n[17] Purchase, H. (1999) A semiotic definition of multimedia\ncommunication. In: Semiotica, vol 123-3/4, pp 247-259"
    },
    {
        "title": "On the use of FastMap for Audio Retrieval and Browsing.",
        "author": [
            "Pedro Cano",
            "Martin Kaltenbrunner",
            "Fabien Gouyon",
            "Eloi Batlle"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415250",
        "url": "https://doi.org/10.5281/zenodo.1415250",
        "ee": "https://zenodo.org/records/1415250/files/CanoKGB02.pdf",
        "abstract": "In this article, a heuristic version of Multidimensional Scaling (MDS) named \u0002\u0004\u0003\u0006\u0005\b\u0007 \u0003 is used for audio retrieval and browsing. \u0002\u0004\u0003\u000e\u0005\u000f\u0007 \u0010\u0003\r\f , like MDS, maps objects into an Euclidean space, such that similarities are preserved. In addition of being more efficient than MDS it allows query-by-example type of query, which makes it suitable for a content-based retrieval purposes.",
        "zenodo_id": 1415250,
        "dblp_key": "conf/ismir/CanoKGB02",
        "keywords": [
            "heuristic version",
            "Multidimensional Scaling (MDS)",
            "audio retrieval",
            "Euclidean space",
            "query-by-example",
            "content-based retrieval",
            "efficient",
            "suitable",
            "preserved",
            "query"
        ],
        "content": "OntheUseofFastMapforAudioRetrievalandBrowsing\nOntheUse ofFastMap forAudio Retrie valand Browsing\nPedro Cano ,MartinKaltenbr unner ,Fabien Gouy onand Eloi Batlle\nMusic Technology Group\nUniv ersitat Pompeu Fabra\n08002, Barcelona, Spain\n3493542 2202\u0000pcano ,mkalten, fgouy on,eloi \u0001@iua.upf .es\nABSTRA CT\nInthis article, aheuristic version ofMultidimensional Scaling\n(MDS) named \u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\r\f isused foraudio retrie valandbrowsing.\u0002\u0004\u0003\u000e\u0005\u000f\u0007\n\t\u0010\u0003\r\f ,likeMDS, maps objects into anEuclidean space, such\nthatsimilarities arepreserv ed.Inaddition ofbeing more efﬁcient\nthan MDS itallowsquery-by-e xample type ofquery ,which makes\nitsuitable foracontent-based retrie valpurposes.\n1.INTR ODUCTION\nTheorigin ofthisexperiment istheresearch onasystem forcontent-\nbased audio identiﬁcation. Details onthesystem aredescribed in\n[1]. Basically thesystem decomposes songs into sequences ofan\nalphabet ofsounds, verymuch likespeech canbedecomposed into\nphonemes. Once having converted theaudio intosequences ofsym-\nbols, theidentiﬁcation problem results inﬁnding subsequences in\nasuperstring allowing errors, thatis,approximate string matching.\nIfwecompare onesequence—corresponding toanoriginal song in\nthedatabase—to thewhole database ofsequences weretrie vealist\nofsequences sorted bysimilarity tothequery .Intheconte xtof\nanidentiﬁcation system, thislistreﬂects which songs thequery—a\ndistorted version ofanoriginal recording [1]—can bemore easily\nconfused with. Ofcourse, studying thisforeach song isatedious\ntaskanditisdifﬁcult toextract information onthematching results\nforthewhole database against itself. Indeed, theresulting distances\ndisplayed inamatrix arenotveryinformati veatﬁrstsight. Onepos-\nsible waytoexplore these distances between songs bymere visual\ninspection isMultidimensional Scaling. MDS makesitpossible to\nviewadatabase ofcomple xobjects aspoints inanEuclidean space\nwhere thedistances between points correspond approximately to\nthedistances between objects. This plot helps todisco versome\nstructure inthedata inorder tostudy methods toaccelerate the\nsong matching search. Itcanalso beused asatestenvironment\ntocompare different audio parameterization aswell astheir corre-\nsponding intrinsic distances independently ofthemetrics. Finally ,\nFastMap’ sindexing capabilities also provide aninteresting toolfor\ncontent-based browsing andretrie valofsongs.\n2.RELA TED WORK\nResearch projects thatoffervisual interf aces forbrowsing arethe\u0011\u0013\u0012\u0006\u0014\u0016\u0015\u0018\u0017\u001a\u0019\u001c\u001b\u001d\u0012\u0006\u001e\u0005 \u001f\n\u001b[2]and \t\u0010\u0003\n\u001b\u0005\b!\"\u0003\u0006\u0005\u000f#\u000e$ [3].The\n\u0011\u0013\u0012\u0006\u0014\u0016\u0015\u0018\u0017\u001a\u0019\u001c\u001b\u001d\u0012\u0006\u001e\u0005 \u001f\n\u001buses\nsonic spatialization fornavigating music orsound databases. In[2]\nmelodies arerepresented asobjects inaspace. Byadding direct\nsoniﬁcation, theuser canexplore thisspace visually andaurally\nwith anewkind ofcursor function thatcreates anaura around the\ncursor .Allmelodies within theaura areplayed concurrently using\nspatialized sound. Theauthors present distances formelodic sim-\nilarity buttheyackno wledge thedifﬁculty torepresent themelodic\ndistances inanEuclidean space.\t\u000b\u0003\n\u001b\u0005\b!\"\u0003\u0006\u0005\u000f#\u000e$ isaprototype audio\nbrowser andeditor forlargeaudio collections. Itshares some con-\ncepts with the\n\u0011%\u0012\u0006\u0014&\u0015\u0018\u0017'\u0019\u001c\u001b(\u0012\u000e\u001e\u0005 \u001f\n\u001bandintegrates them inanextended\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feeprovided\nthat copies arenotmade ordistrib uted forprofitorcommercial\nadvantage andthatcopies bear thisnotice andthefullcitation on\nthefirstpage. )\n*\n2002 IRCAM -Centre Pompidouaudio editor .Tosolvetheproblem ofreducing dimensionality and\nmapping objects into2Dor3Dspaces, Principal Component Anal-\nysis (PCA) isproposed. Thedrawback ofthissolution isthatthe\nobject must beavector offeatures and, consequently ,itdoes not\nallowtheuseofe.g: theeditdistance, theinclusion ofmetadata\norother arbitrary distance metrics. Inthisarticle, theuseofMDS,\nspeciﬁcally\u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\u000f\f isproposed toaddress thisissue.\n3.MAPPING COMPLEX OBJECTS INEU-\nCLIDEAN SPACES\n3.1 Multidimensional Scaling\nMDS [5]isused todisco vertheunderlying (spatial) structure ofaset\nofdatafrom thesimilarity ,ordissimilatity ,information among them.\nIthasbeen used forsome years ine.g.social sciences, psychology ,\nmark etresearch, physics. Basically thealgorithm projects each\nobject toapoint inak-dimensional space trying tominimize the+ ,.-\"/\u000e+0+function:+ ,.-\"/\u000e+0+\u001c1\n233334\n5 687 9&:<;=\n6 9?>=\n6 9\"@(A5687 9=\nA6 9\nwhere\n=\n6 9\nisthedissimilarity measure between theoriginal ob-\njects and\n;=\n6 9\nistheEuclidean distance between theprojections.\nThe\n+<,.-\"/\u000e+0+function givestherelati veerror thatthedistances in\nk-dimensional space sufferfrom, onaverage. Thealgorithm starts\nassigning each item toapoint inthespace, randomly orusing some\nheuristics. Then, itexamines each point, computes thedistances\nfrom theother points andmovesthepoint tominimize thediscrep-\nancybetween theactual dissimilarities andtheestimated distances\nintheEuclidean space. Asdescribed in[4],theMDS suffersfrom\ntwodrawbacks:BItrequiresC\n:\u0018D\nA@\ntime, where\nDisthenumber ofitems. It\nistherefore impractical forlargedatasets.BIfused ina’query byexample’ search, each query item hasto\nbemapped toapoint inthek-dimensional space. MDS isnot\nwell-suited forthisoperation: GiventhattheMDS algorithm\nisC\n:\u0018D\nA@\n,anincremental algorithm tosearch/add anewitem\ninthedatabase would be C\n:\u0018D\n@\natbest.\n3.2 FastMap\nToovercome these drawbacks, Faloutsos andLin[4]propose an\nalternati veimplementation oftheMDS: \u0002\u0004\u0003\u000e\u0005\u000f\u0007\n\t\u0010\u0003\r\f . \u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\u000f\f con-\nsiders theobjects aspoints ofsome unkno wnk-dimensional space.\nThepoints areiterati velyprojected tothehyperplanes perpendicular\ntoanorthogonal setofk-lines passing through themost dissimilar\nobjects. Thealgorithm isfaster than MDS (being linear ,asopposed\ntoquadratic, w.r.t.thedatabase), while itadditionally allowsindex-\ning. Theypursue fastsearching inmultimedia databases: mapping\nobjects intopoints ink-dimensional spaces, theysubsequently use\nhighly ﬁne-tuned spatial access methods (SAMs) toanswer several\ntypes ofqueries, including the’Query byExample’ type. Theyaim\nattwobeneﬁts: efﬁcient retrie val,inconjunction with aSAM, as\ndiscussed above,visualization anddata-mining.OntheUseofFastMapforAudioRetrievalandBrowsing\n4.RESUL TS\nToevaluate theperformance ofboth least squaresMDSand \u0002E\u0003\u0006\u0005\b\u0007F\t\u000b\u0003\r\f ,\nweused atestbedconsisting of2data collections. One collection\nconsists in1840 commercial songs andthesecond collection in250\nisolated instrument sounds (from IRCAM’ sStudio OnLine). Sev-\neraldissimilarity matrices were calculated with different distance\nmetrics. The results ofthese experiments areshownindetail inG\u00078\u0007H\fJI KLK\n\u001eM\u001eM\u001eONP\u0015FQ\u0003\nNPQ\f0R\nN\u001f\r\u0005SK&TU\u0007WV K\n\u0011\u0013\u0012\u0006\u0014V\n\u0011&QX\u001bRY\u001f\n\u001b.InFigure 1therepre-\nsentation ofthesong collection aspoints calculated with MDS and\u0002\u0004\u0003\u000e\u0005\u000f\u0007\n\t\u0010\u0003\r\f isshown.TheMDS map takesconsiderably longer tocal-\nculate than the \u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\u000f\f ’s(894 vs18.4 seconds) although several\nruns of\u0002\u0004\u0003\u000e\u0005\u000f\u0007\n\t\u0010\u0003\r\f aresometimes needed toachie vegood visual-\nizations. Although wedidnotobjecti velyevaluate \u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\r\f and\nMDS (objecti veevaluations ofdata representation techniques are\ndiscussed in[5]), MDS maps seem ofhigher quality .Ontheother\nhand, MDS presents ahigh computational costanddoes notaccount\nfortheindexing/retrie valcapabilities ofthe\u0002\u0004\u0003\u000e\u0005\u000f\u0007\n\t\u0010\u0003\r\f approach.\n5.CONCLUSIONS\nWehavepresented theuseoftheexisting \u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\r\f method for\nimpro ving acontent-based audio identiﬁcation system. The tool\nprovestobeinteresting, notonly foraudio ﬁngerprinting research\n(visually exploring therepresentation space ofaudio data may re-\nvealthepossible weakness ofasimilarity measure), butalso asa\ncomponent ofasearch-enabled audio browser.\nWeﬁrsttested thetoolwith audio objects such asharmo nicorpercus-\nsiveisolated sounds forwhich perceptually-deri veddistances exist.\nInthiscase theresults areexcellent. Butsongs haveamore comple x\nnature, theyaccount formanyaspects ofinterest. Notonly good\nsimilarity measures arehard todesign butalso toextract automat-\nically from low-levelaudio features. Song repositories areusually\ndescribed with heterogeneous mixesofattrib utes, descriptors range\nfrom physical feature vectors (e.g. MFCCs), uptosubjecti velabels\ndeﬁned byexperts (e.g. the\"genre\").\nTheadvantage ofMDS and \u0002Z\u0003\u0006\u0005\b\u0007\n\t\u000b\u0003\r\f liesintheir generality: they\ncancombine anytype ofdata attrib utes, from low-levelattrib utes\ntometadata. Webelie vethatthisfeature isrelevantforimpro ving\nbrowsing engines.\n6.REFERENCES\n[1]P.Cano, E.Batlle, H.Mayer ,andH.Neuschmied. Robust\nSound Modeling forSong Detection inBroadcast Audio.\nIn[\n\u001b(\u0012Y\u0017\u001f\b\u001f(\\\n\u0015F\u0014V\u0006\u0005\n\u0012R]\u0007\nG\u001f_^L^0`L\u0007\nGba\nQ\\\n\u0015\u0018\u0012dce\u0014V\n\u0015F\u0014\u001f\b\u001f\n\u001b\u000f\u0015F\u0014V\n\u0011%\u0012Y\u0017 \u0015\u001f\r\u00078!fg\u0012\u0006\u0014\u0016h\u001f\n\u0014\u0007\n\u0015\u0018\u0012\u0006\u0014,Munich, 2002.\n[2]D.´O.Maidin andM.Fernstr ¨om. The Best OfTwoWorlds:\nRetrie ving andBrowsing. In [\n\u001b(\u0012Y\u0017\u001f\b\u001f\b\\\n\u0015F\u0014V\u0006\u0005\n\u0012R\u001a\u0007\nG\u001f\nfg\u0012\u0006\u0014RY\u001f\n\u001b\u001f\n\u0014\u0013\u0017\u001f\u0012\u0006\u0014$\n\u0015V\n\u0015\u0007i\u0003\"j\na\nQ\\\n\u00158\u0012UcJk\u001f\n\u0017\u00078\u0005,2000\n[3]G.Tzanetakis andP.Cook. Marsyas3D: APrototype Audio\nBrowser-Editor using aLargeScale Immersi veVisual andAu-\ndioDisplay .In[\n\u001b\u001d\u0012Y\u0017\u001f\b\u001f(\\\n\u0015F\u0014V\u0006\u0005\n\u0012Rl\u0007\nG\u001flm\n\u0014\u0007S\u001f\n\u001b\r\u0014\u0003\u0006\u0007\n\u0015\u0018\u0012\u0006\u0014\u0003\"j\nfg\u0012\u0006\u0014RY\u001f\n\u001b\rn\u001f\n\u0014%\u0017\u001f\n\u0012\u0006\u0014a\nQ\\\n\u0015\u0007\n\u0012\u0006\u001b!l$\n\u0015\u0005\u0018\f%jo\u0003\u0006! ,250-254. 2001.\n[4]C.Faloutsos andK.Lin.FastMap: AFastAlgorithm forIndex-\ning,Data-Mining andVisualization ofTraditional andMulti-\nmedia Datasets. In [\n\u001b\u001d\u0012Y\u0017\u001f\b\u001f(\\\n\u0015F\u0014V\u0006\u0005\n\u0012Rp\u0007\nG\u001fq^\u0006rLrLs\na\nf\t\n\u0011m\u000et\nn\tvuw$ ,163-174. 1995.\n[5]W.Basalaj. [\n\u001b(\u0012Yx\u0006\u0015T\n\u0015\u00078!zy\n\u0015\u0005\nQ\u0003\"j\n\u0015P{\u0003\u0006\u0007\n\u0015\u0018\u0012\u0006\u0014|\u0012R\na~}\u0005\u000f\u0007\n\u001b\u0003\n\u0017\u0007d$'\u0003\u000e\u0007i\u0003 .\nTechnical Report 509, University ofCambridge Computer\nLaboratory ,January 2001.−0.5 0 0.5 1−0.6−0.4−0.200.20.40.6MDS\n−1 −0.9 −0.8 −0.7 −0.6 −0.5−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10FASTMAP\n0.35 0.40.45 0.50.55 0.60.65 0.70.75 0.8−0.6−0.55−0.5−0.45−0.4−0.35FASTMAP (edit distance)\u001c\n\u0013\u00040\u0013\u0006% \u0006\u000f\"<\n\u0016\u0006&q\b&\u0016X&W W¡ \"¢\u001d£\u000f\u0013¤Y¦¥&§v¨¡£ª©¬«8 XJ­X\u0013®¯\u0013°Y±\u001d²W³´°\bµq«.¶_P®%®J·PX\u0013®v¥\u000f &¶_­<¸¹\n( \u0006YP\r¤0dX\u0013®¬¤\u000eW<¤\u000e·Pb¤Y&  \"\u000fJ\u0016\u0013®\r\b&\u0016&¥&§º\u000e»º¼l\n·P®\u0013X\u0013®¡½\u001a¾\u0013 \u0006d£'&X<U£¿LÀ¿q \u0006\rJ\u0006¤Y<PÁ\"·P§¸½~¾\u0016_¯\u0013°0±.²W³°(µ¦J·o ¤YX< \u0006\u000f&\u0013®\u0010\r´\bÀ\u001a®JoÂZ\"\rL¿ W¶_\n·oYo\b§¶\u000e\rÃ\u0013 \"Y¸"
    },
    {
        "title": "An Extensible Representation for Playlists.",
        "author": [
            "Amar Chaudhary"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415696",
        "url": "https://doi.org/10.5281/zenodo.1415696",
        "ee": "https://zenodo.org/records/1415696/files/Chaudhary02.pdf",
        "abstract": "The increasing availability of digital music has created a greater need for methods to organize large collections of music.  The eXtensible PlayList (XPL) representation allows users to express playlists with varying degrees of specificity.  XPL handles refer- ences to exact files or URLs as well as rules for selecting content based on metadata constraints.  XPL also allows the transitions between tracks in a playlist to be specified.  This paper describes the features of XPL, a system for rendering XPL specifications and use of an advanced XPL renderer in an existing application.",
        "zenodo_id": 1415696,
        "dblp_key": "conf/ismir/Chaudhary02",
        "keywords": [
            "digital music",
            "organize large collections",
            "methods to organize",
            "eXtensible PlayList (XPL)",
            "express playlists",
            "varying degrees of specificity",
            "references to exact files",
            "rules for selecting content",
            "metadata constraints",
            "transitions between tracks"
        ],
        "content": "An Extensible Representation for Playlists \nAn Extensible Representation for Playlists  \n Amar Chaudhary \nCreative Advanced Technology Ctr. \n1500 Green Hills Road \nScotts Valley, CA, USA \n+1-831-440-2944 \namar@atc.creative.com \nABSTRACT  \nThe increasing availability  of digital m usic has created a greater \nneed for methods to organize large collections of music.  The \neXtensible Play List (XPL) repres entation allows users to express \nplaylists with vary ing degrees of  specificity .  XPL handles refer-\nences  to exact files  or URLs  as well as  rules  for s electing content \nbased on metadata constraints.  XPL also allows the transitions \nbetween tracks  in a play list to be s pecified.  This  paper describes  \nthe features  of XP L, a s ystem for rendering XPL specifications  \nand use of an advanced XPL rende rer in an existing application.  \n1. INTRODUCTION \nThe increasing availability  of digital m usic on desktop PCs, the \nInternet and personal devices (e.g., portable MP3 devices) has \ncreated a greater need for methods to organize large collections of \nmusic.  The traditional concept of a playlist , long employ ed by  \nDJs and radio stations, has become an im portant elem ent of digital \nmusic organization and play back.  However, there are many  way s \nto describe playlists.  One could be very  specific, listing the exact \naudio files to use (e.g., Beatles_Yesterday.mp3 ), artist and \ntitle nam es (e.g., Yesterday  by the Beatles), or non-specific de-\nscriptions based on sty les, tempi,  etc (e.g., a down-tem po clas sic-\nrock ballad).  Additionally , a sophi sticated audio play er can per-\nform transitions between songs (e .g., cross fades, tempo matches, \netc.).  Thus, a com plete play list description m ay include inform a-\ntion about transitions. \nThe “eXtens ible PlayList” (XPL) repres entation allows  users to \nexpress and exchange play lists with vary ing degrees of specific-\nity. It extends the notion of a play list-description form at to include \nnot only  exact lists of audio files can be represented but also more \ngeneral rule-bas ed specifications  based on artist, genre or tempo \nconstraints  and exact trans itions  can be s pecified between succes-\nsive tracks.  \n2. The structure of XPL documents  \nThis section describes the basic st ructure of XPL documents.  It is \nnot intended as a comprehensive specification, which can be ob-\ntained els ewhere [1] . \n2.1 Playlists \nAn XPL document begins and ends with <xpl>  tags and consists \nof one or m ore play list elem ents.  A play list elem ent is enclos ed \nin <playlist>  tags, and consists of one or more track ele-\nments. \n<?xml version=\"1.0\" ?>\n<xpl version=”1.0”>\n<playlist attributes >\n  track or playlist 1   track or playlist 2 \n  … \n  track or playlist n \n</playlist>\n</xpl>\nPlaylist elem ents can also be nes ted, to form  hierarchical s truc-\ntures.  Sim ilar to HTML docum ents, which can reference external \nsub-docum ents, images, etc., play list elem ents can refer to other \nXPL files using a src attribute: \n<playlist name=”super_party_mix” >\n<playlist src=”early_evening.xpl” />\n<playlist src=”late_night.xpl” />\n</playlist>\nwhere the value of s rc can be a local file nam e or a URL.   \n2.2 Tracks \nAn XP L track elem ent des cribes  a piece of content to be played or \na rule for s electing a piece of content.  Track elem ents that specify  \na piece of content are called static tracks  and formatted as fol-\nlows: \n<track [name= name]>\n<content src= filename  />\n</track>\nwhere src is a filenam e or URL.  The file references by  src \nshould be a “playable” piece of content, such as an MP3 file (of \ncourse, the definition of “play able” will differ am ong applica-\ntions). \nTrack elem ents that s pecify  rules  are called dynamic tracks  and \ncan s pecified in different way s according to the ty pe of rule being \napplied.  The most common ty pe of rule is a query  that describes a \nset of cons traints  that an application can use to select a particular \npiece of content for the track.  A query track  is form atted as fol-\nlows: \n<track>\n<query> query specification </query>\n</track>\nThe query  specification uses an “SQL-like” language for describ-\ning cons traints , for exam ple the following track s pecification: \n<track><query>\ngenre=”Acid Jazz” ANDbpm>120\n</query></track>\nresults in the inclus ion of a piece of content attributed to the genre \n“Acid Jazz”  whose tem po is greater than 120 beats per m inute.  \nThe query  language allows arbitrarily  complex constraints to be \nspecified for satisfy ing queries in a play list.    XPL does not spec-\nify the method by  which an application satisfies the constraints in \nquery  elem ents. \nAnother clas s of rules  that can be used to describe tracks is \nweighted selection from one or more sub-play lists.  Consider the \nplaylists fast.xpl  and slow.xpl , which contain songs above \n120 bpm and below 120 bpm, respectively .  A user can create a \nnew play list that blends the two playlists by  using a “combination \ntrack” as  follows . Permission to m ake digital or  hard copies of all or  part of this \nwork for personal or  classr oom use is gr anted without fee pro-\nvided that copies are not m ade or  distr ibuted for  profit or  com -\nmercial advantage and that copies bear this notice and the full \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou <playlist name=”random_mix” loop=”1”>\n<track name=”track1”>An Extensible Representation for Playlists \n<combine>\n<playlist src=”fast.xpl” weight=”.6” />\n<playlist src=”slow.xpl” weight=”.4” />\n</combine>\n</track>\n</playlist>\nThe <combine> element specifies that the content for the track \nshould be randomly selected from the two external sub-playlists, \nwith a .6 probability that the selection will be from fast.xpl\nand a .4 probability that it will be from slow.xpl.  The loop \nattribute of the playlist is used to repeat the track and randomly \nselect content until both playlists are exhausted. Fast.xpl and \nslow.xpl can themselves be dynamic playlists represented by \nthe queries “ bpm > 120” and “ bpm < 120”, respectively.  \nThus, a new more complex rule “60% of the songs should be \ngreater than 120 bpm and 40% should be less than 120 bpm” is \ncreated.   \n2.3 Transitions \nFor some applications, it is not enough to simply declare what content to play next.  A user might also want to control the transi-\ntion between tracks, such as how to crossfade between the two \ntracks and whether or not to perfo rm beat matching. Transitions \ncan be included in a playlist by using a <transition> element \nbetween tracks: \n<track><content src=”funky.mp3” /></track>\n<transition track1Start=”3”\nfadeDurationMin=”2”\nenableBeatMatching=”1”>\n</transition>\n<track><content src=”groovy.mp3” /></track>\nIn the above example, a special transition is defined between the \ntwo tracks.  It begins 3 seconds before the end of the first track, \nwith the second track starting at this time.  The duration of the crossfade is set at 2 seconds, a nd beat-matching is enabled.  A \ncomplete list of available transiti on attributes is available in the \nXPL specification. \n3. RENDERING XPL \nAn application that renders XPL playlists must include an XML \nparser that converts the textual representation into internal repre-\nsentation.  The renderer must be able to iterate through the inter-\nnal representation and render each explicitly specified or rule-based content file in order to a suitable output device, such as a \nsoundcard, audio file or network port.  Consider the following XPL representation: \n<playlist name=”mainlist”>\n<track><content src=”first.mp3” /></track>\n<transition …></transition>\n<playlist name=”sublist” >\n<track><content src=”second.mp3” /></track>\n<track><query>\nbpm > 150 and genre=”Electronica”\n</query></track>\n</playlist>\n</playlist>\nThe file is parsed and the interpreted tracks and transitions are then sequenced in depth-first order.  A track that satisfies the constraints \nin the query specification is then located.  The resulting sequence of content references is then sent to the application’s rendering (i.e., \nplayback) engine.  If the engine supports transitions, the explicit \ntransition is used to cross-fade between first.mp3 and sec-\nond.mp3 3.1 Metadata Support \nIn order to support the dynamic queries available in XPL, the \nrenderer must have access to metadata,  or data about the musical \ncontent in a user’s library. Exampl es of metadata include textual \nelements such as track and album title, artist name, etc. as well as \naudio-derived data such as finge rprints or tempo specifications. \nSuch metadata can be made available to an XPL renderer via a metadata database, or metabase .  The renderer sends each query to \nthe metabase, which returns a list of content files that satisfy the query.  The metabase acts as a central repository for metadata \nacquired from several sources, such as ID3 tags, CDDB records \nand audio analyses of the content.  The acquisition and storage of metadata for use by XPL renderers  is an application-dependent \nprocess.  Multiple metabases are supported in XPL via the data-\nbase and language attributes of query tags.  In addition to \nsupport for multiple distributed meta bases, these attributes allow \nqueries to be defined that are not easily expressed via the default \nSQL-like language in XPL [2].  \n4. CONCLUSIONS AND FUTURE WORK \nXPL provides an expressive, extens ible and readable format for \nthe specification and exchange of playlists.  An XPL renderer has been successfully incorporated into a demonstration application \ncodenamed “VDJ” that supports load ing an rendering of static and \ndynamic XPL representations and user editing of queries and ex-\nplicit transitions.  All edit operations are live, allowing the user to dynamically shape the playlist in real time, and can also be saved \nto XPL files for later rendering and sharing with other users.   \nSuccessful rendering of dynamic XPL requires quality metadata.  \nIf tracks in a music library lack metadata or contain incorrect data (e.g., incorrect genre or tempo), rendered playlists will not match \nuser expectations.  Quality of metadata can be improved by use of \nauthoritative quality-controlled repositories and linking of meta-\ndata to audio-derived features (e.g., fingerprints) [3]. \nFuture work on XPL will include integration with content-based music-information-retrieval syst ems and distribut ed metabases. \n5. ACKNOWLEDGMENTS \nThe author would like to thank the members of the Technovation department at Creative ATC for th eir contributions and feedback \non the XPL specification and their impressive work turning the concepts of XPL and metadata-awa re rendering into a real appli-\ncation. \n6. REFERENCES \n1. Chaudhary, A., XPL Specification , . 2002, Creative Advanced \nTechnology Center: Scotts Valley, CA.  \n2. Reiss, J., J.-J. Auc outurier, and M. Sandler. Efficient multidi-\nmensional searching routines for music information retrieval . \nin International Symposium on Music Information Retrieval . \n2001. Bloomington, IN.  \n3. Allamanche, E., et al.  Content-based identification of audio \nmaterial using MPEG-7 low level description . in International \nSymposium on Music Information Retrieval . 2001. Blooming-\nton, ID. http://ismir2001.indiana. edu/pdf/allamanche.pdf ."
    },
    {
        "title": "Technology and Art - Putting Things in Context.",
        "author": [
            "Leonardo Chiariglione"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416780",
        "url": "https://doi.org/10.5281/zenodo.1416780",
        "ee": "https://zenodo.org/records/1416780/files/Chiariglione02.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416780,
        "dblp_key": "conf/ismir/Chiariglione02"
    },
    {
        "title": "An Auditory Model Based Transcriber of Singing Sequences.",
        "author": [
            "L. P. Clarisse",
            "Jean-Pierre Martens",
            "Micheline Lesaffre",
            "Bernard De Baets",
            "Hans E. De Meyer",
            "Marc Leman"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416074",
        "url": "https://doi.org/10.5281/zenodo.1416074",
        "ee": "https://zenodo.org/records/1416074/files/ClarisseMLBML02.pdf",
        "abstract": "In this paper, a new system for the automatic transcription of singing sequences into a sequence of pitch and duration pairs is presented. Although such a system may have a wider range of applications, it was mainly developed to become the acoustic module of a query- by-humming (QBH) system for retrieving pieces of music from a digitized musical library. The first part of the paper is devoted to the systematic evaluation of a variety of state-of-the art transcription systems. The main result of this evaluation is that there is clearly a need for more accurate systems. Especially the segmentation was experienced as being too error prone (\u0001 \u0001\u0002 % segmentation errors). In the second part of the paper, a new auditory model based tran- scription system is proposed and evaluated. The results of that eval- uation are very promising. Segmentation errors vary between 0 and 7 %, dependent on the amount of lyrics that is used by the singer. The paper ends with the description of an experimental study that was issued to demonstrate that the accuracy of the newly proposed transcription system is not very sensitive to the choice of the free parameters, at least as long as they remain in the vicinity of the values one could forecast on the basis of their meaning.",
        "zenodo_id": 1416074,
        "dblp_key": "conf/ismir/ClarisseMLBML02",
        "keywords": [
            "automatic transcription",
            "singing sequences",
            "pitch and duration pairs",
            "systematic evaluation",
            "state-of-the-art transcription systems",
            "auditory model based transcription",
            "experimental study",
            "accuracy of transcription system",
            "free parameters",
            "sensitivity"
        ],
        "content": "An Auditory Model Based Transcriber of Singing Sequences\nAn Auditory Model Based Transcriber of\nSinging Sequences\nL. P . Clarisse\n/BD, J. P . Martens\n/BD, M. Lesaffre\n/BE, B. De Baets\n/BF,H .D eM e y e r\n/BGand M. Leman\n/BE/BDDepartment of Electronics and Information Systems (ELIS), Ghent University;\nSint-Pietersnieuwstraat 41, 9000 Gent (Belgium). martens@elis.rug.ac.be, 0032-09-2643395./BEInstitute for Psychoacoustics and Electronic Music (IPEM), Ghent University./BFDepartment of Applied Mathematics, Biometrics and Process Control, Ghent University./BGDepartment of Applied Mathematics and Computer Science, Ghent University.\nABSTRACT\nIn this paper, a new system for the automatic transcription of singing\nsequences into a sequence of pitch and duration pairs is presented.Although such a system may have a wider range of applications, itwas mainly developed to become the acoustic module of a query-by-humming (QBH) system for retrieving pieces of music from adigitized musical library. The ﬁrst part of the paper is devoted tothe systematic evaluation of a variety of state-of-the art transcriptionsystems. The main result of this evaluation is that there is clearly aneed for more accurate systems. Especially the segmentation wasexperienced as being too error prone (/AP /BE/BC% segmentation errors).\nIn the second part of the paper, a new auditory model based tran-\nscription system is proposed and evaluated. The results of that eval-\nuation are very promising. Segmentation errors vary between 0 and7 %, dependent on the amount of lyrics that is used by the singer.The paper ends with the description of an experimental study thatwas issued to demonstrate that the accuracy of the newly proposedtranscription system is not very sensitive to the choice of the freeparameters, at least as long as they remain in the vicinity of thevalues one could forecast on the basis of their meaning.\n1. INTRODUCTION\nIt sounds appealing to have the possibility of retrieving a musicalpiece from a musical database, just by singing or humming an ex-cerpt from that piece. In general, the proposed retrieval methodol-ogy is called Query-by-Humming (QBH). Both academic interestand practical appeal have encouraged the development of QBH sys-tems over the last decade.\nIn this paper, we only consider singing sequences, be it that we\nmake a distinction between singing with lyrics (i.e. singing thewords), or singing without lyrics (i.e. singing with meaninglesssyllables like /da/, /na/, /du/, etc). Most dedicated state-of-the-artQBH systems were speciﬁcally designed for and tested on singingwithout lyrics. Some systems even put additional restrictions on thetype of syllables that can be used (mostly /da/).\nNearly all QBH system consist of two parts: (i) an acoustic module\nfor converting the acoustic input into a sequence of segments (time\nintervals) with associated discrete frequencies (notes), and (ii) a pat-tern matching module for matching this sequence to the musicaldata in a database. In case the acoustic signal is a singing sequence,notes cannot overlap in time. The result of the transcription systemshould thus be a segmentation of the signal into successive notes,optionally separated by white-spaces.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage.\nc/AD2002 IRCAM - Centre Pompidou\nMost QBH systems (see for instance [10, 15, 19, 25]) are dedicated\nsystems whose acoustic module always produces a result meetingthis constraint. However, some systems use a general purpose wav-to-midi converter instead (see for instance [8, 14]). Such a convertermay also produce overlapping notes, which may be resolved by aproper post-processing of the output before supplying it to the QBHpattern matcher.\nIn this paper we are solely dealing with the acoustic module of a\nQBH system. It is expected though, that the performance of the\nQBH system as a whole is highly dependent on the quality of the\ntranscription provided by this module. This quality can be ex-pressed in terms of the number of segmentation errors (deleted orinserted notes), substitution errors (the note was incorrect in termsof its frequency), and time alignment errors (the detected segmenthas different endpoints than the correct segment). The substitutionerrors mainly affect the transcribed melody, whereas the other errorsmainly affect the rhythm.\nSome QBH systems do not perform a segmentation (see for instance\n[5, 9, 18, 21]) and just convert the acoustic input into a pitch con-\ntour (e.g. one pitch sample per frame of 10 ms). It is our convictionhowever that similarity matching on the basis of pitch only is notpowerful enough. In fact, an obvious objection is that it relies onthe weakest point of a mediocre singer, namely the correctness ofhis pitch contour. Rhythm is also considered an important aspectof human music recognition, especially for the recognition of mu-sic with a less expressive pitch pattern, and there is no reason tobelieve that rhythm would be unimportant for an automatic QBHsystem. Therefore, all systems described in this paper intend to per-form a segmentation.\nThe structure of this paper is as follows. In section 2 we outline the\ngeneral principles underlying the acoustic modules of some state-of-the-art QBH systems. Then we describe our methodology forevaluating the transcriptions provided by such a module, and wepresent the results of an evaluation of 8 modules. Following theresults of this evaluation we have developed a more accurate tran-scription system, as described in section 3. The evaluation of thisnew system is presented and discussed in section 4. The paper endswith some conclusions.\n2. THE ACOUSTIC MODULE OF A QBH\nSYSTEM\nThe acoustic module of a QBH system always contains an acoustic\nfront-end to transform the acoustic signal into a parametric repre-sentation of the time-frequency information carried by this signal.This parametric representation is then analyzed in detail by the tran-scription module, in order to produce the requested transcription ofthe acoustic signal.An Auditory Model Based Transcriber of Singing Sequences\n2.1 The acoustic front-end\nThe acoustic front-end aims to extract features that are relevant for\nthe transcription process. The main features usually are the energy\n(or some more complex estimate of the loudness of the signal),the pitch and the degree of voicing. The features are determinedper frame of a certain length, and subsequent frames are typicallyshifted over 10 ms. If frames are chosen longer than 10 ms, subse-quent frames overlap.\nAs far as we know, only the Haus and Pollastri system has extracted\nthe degree of voicing. The extraction is based on the mean andstandard deviation of the energy and the zero-crossing rate of the\nderivative of the background noise and the signal. Using this in-\nformation, the system tries to discriminate between vowels, voicedconsonants and unvoiced sounds.\nTraditionally, pitch detection has received most attention in the\nacoustic front-end of a QBH system. By far the mostly used pitchdetermination method is the autocorrelation method (see for in-stance [5, 8, 21]). Meldex on the other hand uses the Gold-Rabineralgorithm [20].\n2.2 The transcription system\nTranscription systems consist of two parts: a segmentation part, inwhich the audio input is divided into note segments and white-spaces, and a note assignment part, in which a single note (fre-quency) is assigned to every note segment. The methods of doingsuch, vary widely from system to system. We will summarize the\nmethods adopted by two well documented systems.\nMeldex [15, 16, 17] uses a segmentation which is purely based on\nthe root mean square (RMS) - the square root of the energy - as afunction of time. A note onset is recorded when the RMS exceedssome threshold and a note offset is recorded when the RMS dropsbelow a second lower threshold. The thresholds were respectivelyset at 35% and 55% of the mean RMS over the entire signal. Anote is assigned to the segment by identifying the highest peak inthe histogram of the frame-level pitch frequencies found in the seg-\nment, and by computing the average of the pitches lying in that bin.\nThe pitch is then converted to a MIDI note using a scale which isadapting to the intonation of the user. The idea is to keep track ofthe bias in the computed frequency of the singer, and to subtract thisbefore performing the note assignment. As shown in [10] however,simply rounding the computed to the closest note frequency yieldsa better performance.\nThe system of Haus and Pollastri [10] is more elaborate. The seg-\nmentation process starts with a ﬁrst estimation of segment bound-\naries based on signal/noise discrimination, with the noise level set\nto 15% above the RMS of the ﬁrst 60 ms of the input. Next, theon/offset estimation is reﬁned by incorporating the detection ofvowels, voiced consonants and unvoiced sounds. The pitch of asegment is computed on the basis of the frames labeled as vowelin this segment. After the fundamental frequencies have been de-tected, they are median ﬁltered (mediating three subsequent frames)and checked for octave errors. Four adjacent frames with similarfundamental frequencies are grouped into a block. Legato is de-tected when subsequent blocks have pitches more than 0.8 semi-tones apart. In this case additional segment boundaries are inserted.\nJust like in the Meldex system one aims at capturing the intention\nof the singer. Conversion from frequencies to the equally temperedscale incorporates a relative scale. The relative scale is based onthe assumption that each singer has a reference tone in mind andthat the other notes are sung relative to the scale constructed on thattone. The ﬁrst thing Pollastri tries to do is to look for the reference\ntone. Global pitches of the segments are compared to an absolute\nscale and differences are represented in a histogram of overlapping\nbins of 0.2 semitones. The prominent peak is identiﬁed and an aver-age is made over this winning bin to ﬁnd the shift transforming theabsolute scale into the user scale. Shifting the absolute scale by thisamount minimizes the deviation error and thereby it is claimed thatthe user scale has been found. Further reﬁnements are made on thebasis of additional rules.\n3. EVALUATION OF TRANSCRIPTION\nSYSTEMS\nIn order to evaluate the quality of a system for the transcription of\nsinging sequences one needs (i) a representative corpus of singingsequences by naive singers, (ii) a reliable reference transcription ofthese sequences, and (iii) a good method for measuring the discrep-\nancies between the generated and the reference transcriptions in a\nquantitative manner.\n3.1 Corpus collection\nFive men and six women of different ages (between 23 and 51 yearsold) were asked to sing two excerpts from two different songs. They\nwere free to choose how they would sing: with or without lyrics.\nAll subjects were unexperienced singers. They were free to choosea melody from a list of 50 which they had in front of them. Thesubjects were invited in the room where the computer was, theywere given the list, they decided what tunes they would sing, andthey immediately started to sing. The recordings were made in anormal ofﬁce room with a home PC and a hand-held microphone(type Sony ecm ms907). The samples were recorded at a samplingfrequency of 22.05 kHz with a resolution of 16 bit, and saved as aPCM wav ﬁle.\nA typical phenomenon was that the volume (loudness) was quite\nlarge at the beginning, but much lower at the end. It also happenedfrequently during singing with lyrics that the subjects fell out ofwords and continued by singing parts without lyrics.\nIn total, 22 samples were recorded. Two recordings, one of a male\nand one of a female subject, were taken out for algorithm develop-ment and tuning, the remaining 18 samples (7 without and 11 withlyrics) were considered as an evaluation corpus. This corpus con-\nsisted of 150 seconds of acoustic signal, containing approximately\n300 notes. Obviously, this corpus is too limited to be really repre-sentative, but it was considered large enough to yield at least goodindications of expected system performances.\n3.2 Making the reference transcriptions\nIn order to get a reliable reference transcription, a musical expertwas asked to segment the speech into notes and white-spaces. Itwas found convenient to use the open source tool PRAAT [4] forthis purpose. The musical expert had to introduce time markersindicating the beginning or end of a note, and to assign a note orwhite-space label between two time markers. For doing this, theexpert had a visual image of the signal on the screen (see ﬁgure 1which shows a screen dump), and the ability to listen repeatedly toany fragment of the signal. The note labeling is found to be the mosttime consuming part of the task.\nOnce the note labeling was ready, it was saved in the TextGrid for-\nmat of Praat, and subsequently converted to a MIDI format [24], theformat that is used by most transcription systems.An Auditory Model Based Transcriber of Singing Sequences\nFigure 1: A screen dump of the image in front of the musical expert after he has introduced the note boundaries and the note labels, according\nto the annotation scheme of the Autoscore system.\n3.3 Evaluation methodology\nThe goal of the evaluation is to compare a computed transcriptionwith the reference transcription of the signal. As both can consistof a different number of segments (notes and white-spaces), a directcomparison is not straightforward. However, a simple solution isoffered by the Dynamic Time Warping algorithm (DTW) [23].\nIf the computed and reference transcriptions are characterized by/C6/CRtime markers /D8/CR/BN/CXand /C6/D6time markers /D8/D6 /BN/CYrespectively, we\nwant DTW to align each /D8/CR/BN/CXwith a /D8/D6 /BN/CYin such a way that the accu-\nmulation of local costs attached to these alignments is minimized.I.e., DTW must identify the warping path/CM /DBsatisfying/CM /DB /BP/CP /D6 /CV /D1 /CX /D2/CM /DB\n/C6/CR\n/A0 /BD/CG/CX /BP/BD\n/CR /B4 /D8/CR/BN/CX\n/BN/D8/CR/BN/CX /B7/BD\n/BN/D8/D6 /BN/DB/CX\n/BN/D8/D6 /BN/DB/CX\n/B7/BD\n/B5\nThe pairs /B4 /CX/BN /CY /BP /DB/CX\n/B5can be represented as points on a path from\n(1,1) to /B4 /C6/CR\n/BN/C6/D6\n/B5in a two-dimensional trellis (see ﬁgure 2).\nThe path consists of subsequent transitions characterized by dis-\nplacements /A1 /CX /BP /BDand /A1 /CY /BP /DB/CX\n/A0 /DB/CX /A0 /BD. In order to obtain\na sensible path, /A1 /CY /BE /B4/BC /BN /BG/B5was imposed as a constraint. Obvi-\nously, the deﬁnition of the local cost contributions will determinethe properties of the alignment one obtains for a speciﬁc transcrip-tion pair. Our ﬁrst goal was to penalize the time differences betweenthe computed and their associated reference time markers. The notefrequency discrepancies were considered as a secondary criterion.\nThis way, the alignment does not depend too much on the qual-\nity of the pitch detector. The local cost contribution of a transition/B4/A1 /CX/BN /A1 /CY /B5was therefore determined on the basis of the following\nconsiderations:/AF /A1 /CY /BP/BCmeans that two computed time markers are assigned\nto the same reference marker. This points to an inserted timemarker in the computed transcription, and is penalized withan insertion cost/CR/CX/D2/D7\n/BP/BC /BM /BL/BH./AF /A1 /CY/BQ /BCmeans that a new computed time marker is assigned\nto a new reference marker. In this case one considers twodiscrepancies: a discrepancy in the timing, and a discrep-ancy in the note frequencies of the segments starting at thesetime markers (a different note is considered as a note substi-\ntution error). The timing cost/CR/D8/CX/D1/CT is equal to the absolute\ntime difference divided by some /A1 /CC/D1/CP/DC which was set to\n0.2s. The substitution cost /CR/D7/D9/CQis equal to the minimum of\n0.5 semitones and 0.25 times the note frequency differencein semitones. The substitution cost for assigning a note to awhite-space is set to 2 semitones./AF /A1 /CY /BQ /BDmeans that some reference time marker is not as-\nsigned to any computed marker. This is penalized by an ex-\ntra deletion cost /CR/CS/CT/D0multiplied by the number of deletions\n( /A1 /CY /A0 /BD).\n1234567891 0 1 1 1 2 1 3 1 4 1 5 1 612345678910111213141516optimum path/CX\n/CY/CM /DB\nComputed markersReference markers\nFigure 2: A warping path /CM /DBrepresenting an alignment of the\nautomatic and the reference transcription of a singing sequence.\nOnce the alignment between the transcriptions is available, one can\neasily determine the number of deletions and insertions along thewarping path. For determining the number of substitutions, we dis-tinguished between exact or not, and between within a semitone ornot. In case two or more computed segments were assigned to thesame reference segment, the decision was based on a comparison ofthe frequency of the computed segment that had the largest overlapwith the reference segment.An Auditory Model Based Transcriber of Singing Sequences\n4. EVALUATION OF STATE-OF-THE-ART\nIn this section we describe an experimental evaluation of 8 differ-\nent systems which are assumed to represent the state-of-the-art in\ntranscribing singing sequences.\nBefore reviewing the systems that were tested, we recall that some\nof them allow the user to specify a lot of free parameter settings.In all cases we used the preferred settings speciﬁed in the manual.If the note range could be speciﬁed it was always set to (C2,C6) =(65 Hz,1000 Hz).\nSome programs seemed to introduce some delay. For that reason we\nallowed transcriptions to be shifted in time before supplying them tothe DTW algorithm. The results presented later always correspondto the time shift producing the lowest alignment cost.\n4.1 Evaluated transcription systems\nSome of the systems that were tested are commercial systems,which are not well documented in terms of underlying methodolo-gies. However, where references to scientiﬁc publications can bemade, they are included. Let us look at the list of the ﬁve systemsfor which detailed results are provided in table 1:\nMeldex This is maybe the most famous QBH system. For a recent\nand detailed overview, we refer to [16].\nPollastri The system of Haus and Pollastri [10] was developed in\nthe context of query by humming, with the term hummingreferring to singing without lyrics. In this case, the transcrip-tion of our material was performed by the author himself. Wegot the assurance from Pollastri that the conversion was madeunder the same conditions as speciﬁed in [10].\nAkoff Composer This is a shareware program by Andrei Kovalev\n[1] for the conversion of monophonic music waves to a MIDI\nﬁle format.\nWidi This is a polyphonic music recognition system developed by\nRussian students in physics [28]. It has a monophonic mode,\nand it is in this mode that we tested it.\nAutoscore This is another off-the-shelf monophonic music to\nMIDI converter [3]. This system has already been used forquery by humming by Naoko Kosugi [14].\nThree other systems that were tested are the commercial packages\nAudioworks [2],Digital Ear [7] and Intelliscore [12]. These sys-tems performed (according to our tests) worse than Akoff, Widi or\nAutoscore, and were therefore not included in table 1.\n4.2 Detailed evaluation results\nThe evaluation results are summarized in table 1. The results areseparated according to the singing mode: with or without lyrics.Two important conclusions can be drawn from these results:\n1. All systems make a considerable amount of deletion and in-\nsertion errors, and singing with lyrics seems to be much moredifﬁcult to segment than singing without lyrics.\n2. Although exact note recognition is low for all systems, most\nsystems provide a within 1 semitone note recognition ac-curacy of 85.00 % or more. Especially Widi seems to in-corporate an excellent pitch extractor. However, this is notnecessarily true since Widi produces many short (inserted)notes for unstable segments, and consequently there is a highchance that the longest segment in the more stable part of the\nnote has the correct pitch.\nNote that for the published systems, our evaluation results appear\nto be signiﬁcantly worse than those reported in these publications.\nOne likely explanation is that the system performances depend toomuch on the recording conditions (volume, noise, room acoustics)and the parameter settings. Another explanation may be that weused naive singers, and that for singing without lyrics, we did notforce them to use a particular syllable (e.g. the /ta/-syllable, proba-bly the most easy one to analyze).\nListening to the transcribed sequences convinced us of the absolute\nneed for more accurate segmentations. Even the best system (Pol-\nlastri) is usually unable to provide a sufﬁciently accurate segmen-\ntation of the singing with lyrics sequences. That is why we haveconceived a new transcription system that is described in the nextsection.\n5. A NEW TRANSCRIPTION SYSTEM\nThe acoustic module of our QBH system comprises an auditorymodel which is essentially the same model as that published in[26]. However, it is the ﬁrst time we have used it for the analysisof human singing. Our main motivations for preferring an auditorymodel over a more standard acoustic front-end are the following:\nTable 1: Overview of the results obtained by comparing computed and reference transcriptions using the methodology outlined in section 3.\nAkoff\n Autoscore\n Meldex\n Widi\n Pollastri\nSinging without lyrics\nnotes deleted\n 6.72 %\n 7.26 %\n 37.31 %\n 5.22 %\n 4.76 %\nnotes inserted\n 11.19 %\n 14.29 %\n 4.48 %\n 64.18 %\n 7.94 %\nnotes deleted + inserted\n 17.91 %\n 21.55 %\n 41.79 %\n 69.40 %\n 12.70 %\nexact note recognition error\n 40.71 %\n 54.26 %\n 53.73 %\n 31.15 %\n 48.31 %\nnote recognition error /BQ1 semitone\n 4.42 %\n 10.64 %\n 28.36 %\n 1.64 %\n 10.37 %\nSinging with lyrics\nnotes deleted\n 18.50 %\n 22.95 %\n 52.46 %\n 18.50 %\n 13.66 %\nnotes inserted\n 30.00 %\n 12.02 %\n 3.28 %\n 60.50 %\n 5.46 %\nnotes deleted + inserted\n 48.50 %\n 34.97 %\n 55.74 %\n 79.00 %\n 19.13 %\nexact note recognition error\n 48.34 %\n 44.27 %\n 66.23 %\n 34.72 %\n 58.39 %\nnote recognition error /BQ1 semitone\n 13.91 %\n 15.27 %\n 31.17 %\n 6.25 %\n 16.79 %\nAn Auditory Model Based Transcriber of Singing Sequences\n1. We were able to prove that the speech loudness pattern\nemerging from the model provides excellent cues for the pho-\nnetic segmentation of speech [27].\n2. The built-in pitch extractor, called AMPEX (Auditory Model\nbased Pitch Extractor) has been proven to be among the bestpitch extractors available for speech analysis [11, 22].\n3. Since the pioneering work of Davis and Mermelstein [6], the\nperceptually based MFCCs (Mel-Frequency Cepstral Coefﬁ-cients) have become the standard parametric representationfor speech recognition applications.\nIn the subsequent sections we ﬁrst describe our auditory model and\nthe improvements that were made since the original publication ofthe model. Then we introduce the segmentation and pitch assign-ment algorithms that were developed to produce the envisaged tran-scriptions.\n5.1 The auditory model\nA general outline of the auditory model is depicted in ﬁgure 3. Theacoustic signal is ﬁrst ﬁltered by a band-pass ﬁlter that models thesound transmission in the outer and middle ear. The ﬁltered signal isthen supplied to a cochlear processing block which models the con-version of the acoustic signal into neural ﬁring patterns observed ingroups of auditory nerve cells. Each group represents nerve cellsconnected to neighboring hair-cells somewhere along the BasilarMembrane (BM) in the cochlea. The number of cells in a group is\nassumed to be large enough so as to make it sensible to characterize\nthe group response by means of a time pattern representing the neu-ral ﬁring density as a function of time. Each pattern is obtained byone analysis channel consisting of a band-pass ﬁlter with a uniquetuning frequency, a non-linear hair-cell model and an envelope ex-tractor. In agreement with physiological measurements [13], theneural ﬁbers do not transmit modulation frequencies that are muchlarger than 500 Hz.\nBPF AMPEX\nCochlear\nProcessing\nFiltering/SamplingHF components\nLF componentsFrequency \nSplitter\nauditory spectrumaudio file\nneural firing patterns\n(one per channel)\n/BY/D3 V/U /CE/CT\nFigure 3: Architecture of the auditory model front-end.\nEach neural ﬁring pattern is then split into a low and a high-\nfrequency component by means of a frequency splitter with a char-acteristic frequency of 20 Hz. The low-frequency components aredecomposed of their spontaneous activity (value in the absence ofany signal), further low-pass ﬁltered and down-sampled to 100 Hzso as to form the components of an auditory spectrum. The lat-ter represents the short-term neural activity (loudness) distributionacross channels. The high-frequency components are supplied to\na pitch extraction module, called AMPEX (Auditory Model based\nPitch EXtractor). It produces one pitch per frame, and it is consists\nof three major parts:\n1. A pseudo-autocorrelation analysis of the individual high-\nfrequency components/CU/CW/D1\n/B4 /D8 /B5: /CU/CW/D1\n/B4 /D8 /B5is replaced by a se-\nquence of pulses occurring at the positions of its maxima,and a function/CA/D1\n/B4 /AS /B5very much similar to a short-time au-\ntocorrelation function is derived from this signal. The chan-nel contributions are then accumulated to a global pseudo-\nautocorrelation function/CA /B4 /AS /B5.\n2. A pitch candidate extraction algorithm that identiﬁes all rel-\nevant peaks (larger than a small threshold) in /CA /B4 /AS /B5, and thus\nproduces a set of pitch candidates /CC/CZand their corresponding\nevidences /BX/CZ\n/BP /CA /B4 /CC/CZ\n/B5for each frame.\n3. A pitch continuity analysis to retrieve the best pitch /CC/D3, its\ncorresponding voicing evidence /CE/CT, and a voiced/unvoiced\ndecision for each frame. If /CC/CY/CZ\n/BN/BX/CY/CZ( /CZ /BP /BD /BN /BM/BM/BN /C6/CY) repre-\nsent the pitch candidates and their evidences hypothesized inframe/CY, and if the frame rate is 10 ms, the voicing evidence\nof a pitch candidate /CChypothesized in frame /D2is computed\nas/CE/CT\n/B4 /CC /B5/BP\n/D2 /B7/BE/CG/CY /BP /D2 /A0 /BE\n/C6/CY/CG/CZ /BP/BD\n/BX/CY/CZ\nÆ /B4\n/CY /CC /A0 /CC/CY/CZ\n/CY\n/CC /B7 /CC/CY/CZ\n/BO/AF/CC\n/B5 (1)\nwith Æ /B4 /BM /B5being 1 if the condition is satisﬁed and 0 other-\nwise, and with /AF/CCbeing a coincidence threshold. The pitch\ncandidate with the highest /CE/CTis selected as the pitch, and\na voiced/unvoiced decision is made on the basis of this evi-dence (see [26]).\nThe auditory model is designed in such a way that it can process\na continuous audio stream. Obviously, due to the pitch continu-ity analysis, there is a delay of 20 ms between the acoustic inputand the model output. When aligning the auditory features with theacoustic signal, one has to compensate for this delay.\nSince its publication in [26], AMPEX was further improved in the\nfollowing ways:\n1. In order to make the voiced/unvoiced decision less dependent\non the signal level, the evidence assigned to a pitch candidate/CCduring the pitch candidate extraction stage is no longer not/CA /B4 /CC /B5but /CA /B4 /CC /B5 /BP /CJ /CA /B4/BC/B5 /B7 /AF/C5 /CL, with /C5being the number of\nchannels in the auditory model.\n2. In order to reduce the number of harmonic pitch errors, the\npitch evidences computed in the pitch continuity analysiswere multiplied by/BC /BM /BH/B7/BC /BM /BD /CC( /CCin ms) so as to compensate\nfor the tendency of the algorithm to produce somewhat largerevidences for smaller values of/CC.\n3. The pitch continuity analysis continues to seek for the pitch\ncandidate /CCgetting the highest evidence according to equa-\ntion (1), but it then determines the effectively generated pitch\nhypothesis as/CC/D3\n/B4 /CC /B5/BP\n/C8/D2 /B7/BE/CY /BP /D2 /A0 /BE\n/C8/C6/CY/CZ /BP/BD\n/CC/CY/CZ\n/BX/CY/CZ\nÆ /B4\n/CY /CC /A0 /CC/CY/CZ\n/CY\n/CC /B7 /CC/CY/CZ\n/BO/AF/CC\n/B5\n/C8/D2 /B7/BE/CY /BP /D2 /A0 /BE\n/C8/C6/CY/CZ /BP/BD\n/BX/CY/CZ\nÆ /B4\n/CY /CC /A0 /CC/CY/CZ\n/CY\n/CC /B7 /CC/CY/CZ\n/BO/AF/CC\n/B5(2)\nWith these improvements, the total pitch and V/U error rate for the\nspeech database used in [26] was reduced from 5.1 to 3.7 %, andthere is also a much better balance between the performance formale and female voices now.An Auditory Model Based Transcriber of Singing Sequences\nSo as to reduce the CPU time, different channels are operated at dif-\nferent sampling frequencies. The auditory model therefore contains\na decimation unit to supply down-sampled copies of the input signal\nto these channels. This unit was enhanced in two ways with respectto [26]:\n1. In order to prevent aliasing products of high-frequency tones\nto produce activity in low-frequency channels, a higher orderdecimation ﬁlter (with a high-frequency suppression of morethan 66 dB) was introduced.\n2. In order to prevent harmonics, introduced by the half- wave\nrectiﬁer in the hair-cell models, to produce low-frequencyaliasing products in the hair-cell outputs, the sampling fre-quency in a channel has to be larger than 7.2 times the centerfrequency of the channel bandpass ﬁlter (see [26]). In orderto satisfy this condition for the high frequency channels, thedecimation unit was extended to produce an up-sampled ver-sion of the input signal as well.\nIn all the experiments reported in this paper, the auditory model\ncomprises 23 channels covering the frequency range from 140 Hzto 6 kHz, and it produces one acoustic parameter vector per\n10 ms. Each vector consists of an auditory spectrum (23 values),\na voiced/unvoiced decision, a voicing evidence, a loudness valueand a pitch frequency (zero if the frame is unvoiced).\nIt is important to emphasize that all the free parameters of the au-\nditory model were optimized for normal speech processing. Theywere not changed for the analysis of the singing sequences appear-ing in the present study.\n5.2 The segmentation algorithm\nTo begin with, the auditory spectrum components of a frame are ac-cumulated across channels to produce the so called loudness of thatframe. The pitch (/BY/D3), loudness ( /C4) and voicing evidence ( /CE/CT) pat-\ntern for a two-seconds extract from a singing sequence is depictedon ﬁgure 4. These are the patterns which are further analyzed byour segmentation system.\nThe segmentation is primarily based on the loudness function,\nwhose deep minima are supposed to delimit the note segments. Inorder to obtain a robust decision, the deep minimum detection algo-rithm must be able to deal with loudness ﬂuctuations which are notreferring to note boundaries.\nWe have implemented a robust extremum detection algorithm which\nassumes that there is some silence at the beginning of the ﬁle. The\nalgorithm goes from left to right, it starts by searching for a maxi-\nmum and it proceeds according to the following principles.\n1.While searching for a maximum\nKeep track of the position and the value of the largest loud-ness (stored as the potential maximum), and consider a max-imum found at the moment the actual value is sufﬁcientlylower than the stored maximum. When a maximum is found,store the position and loudness of the actual frame as a po-tential minimum and start looking for a minimum.\n2.While searching for a minimum\nKeep track of the position and the value of the smallest loud-ness (stored as the potential minimum), and consider a min-imum found at the moment the actual value is sufﬁcientlyhigher than the stored minimum. When a minimum is found,generate a new note segment (starting at the previous mini-mum), store the position and loudness of the actual frame asa potential maximum and start looking for a maximum.To determine what sufﬁciently higher/lower is, we adopt Weber-\nFechner’s law of psycho-acoustics [29]. It states that equal\nincrements of sensation due to some energy variable are as-\nsociated with equal increments of the logarithm of that vari-able supplemented with some bias. Consequently, if/C4/CQis\nthe loudness bias, loudness /C4/BEis sufﬁciently higher/lower than/C4/BDif /CY /C4/BE\n/A0 /C4/BD\n/CY /BP /B4 /C4/BD\n/B7 /C4/CQ\n/B5exceeds some threshold /AF/C4.\n012345678910\n0 25 50 75 100 125 150 175\nt (10ms)loudness\n012345678910\n0 25 50 75 100 125 150 175\nt (10ms)voice evidence050100150200250300350400\n0 25 50 75 100 125 150 175\nt (10ms)frequency (Hz)\nFigure 4: The pitch, loudness and voicing evidence patterns\nemerging from the auditory model.\nIn order to detect white-spaces too, the extremum detection algo-\nrithm is further extended as follows. When the loudness goes undersome white-space threshold/C4/DB/D7in more than 2 successive frames,\na note segment is generated and the search for extrema is inhibiteduntil 2 successive frames with a loudness above/C4/DB/D7are encoun-\ntered. At that moment, a white-space segment is generated, and anew search for a maximum is started.\nThe white-space threshold can be made adaptive and proportional\nto the lowest loudness found over the last two seconds, but as wenormalized the energy of the singing sequences before analyzingthem, it was possible to select a ﬁxed threshold throughout the ex-periments.\n5.3 Post-processing the segments\nIt happens that low energy segments like breaths and noises appearas note segments in the computed segmentation. In a segment post-processing stage, we relabel them as white-spaces as soon as theysatisfy one of the following conditions:\n1. the maximum voicing evidence is smaller than/CE/D1/CX/D2\n2. the maximum loudness is smaller than /AB/C4/DB/D7( /AB/BQ /BD).\nThis post-processing stage completes the segmentation process.\n5.4 The pitch assignment algorithm\nIn order to determine the note label of a note segment, the pitch\ncontour is analyzed in the center part of that segment. The onsetAn Auditory Model Based Transcriber of Singing Sequences\nand offset of a note segment are excluded because pitch algorithms\nhave a tendency to make pitch errors in these areas. On the other\nhand, the more pitch values one can retain, the more accurate the\ncomputed pitch is going to be. We choose to consider the ﬁrst andlast 2 frames as the onset and offset of the note segment. The notelabel is obtained in two steps.\nStep 1: segmental pitch determination\nThe segmental pitch is computed as the average of /BY/D3over the\nframes of the segment (central part). To cope with possible oc-tave errors this average is iteratively improved by eliminating thoseframes whose pitch deviates more than a certain/A1 /BY/D3from the ac-\ntual average, and by computing a new average on the basis of theremaining frames. The process is stopped as soon as the segmentalpitch does not change anymore. Usually this happens after one ortwo iterations.\nIf one wants to maximize the note recognition within a semitone,\none intuitively feels that/A1 /BY/D3should be smaller than\n/BI\n/D4\n/BE /A0 /BD /AP/BC /BM /BD/BE. We have not tried to optimize this value, and used /A1 /BY/D3\n/BP/BC /BM /BD/BCin all our experiments.\nIn some exceptional cases, a segment may contain so many octave\nerrors that there are almost no pitch values within /A1 /BY/D3of the ﬁrst\nsegmental pitch approximation. To get the right frequency in this\ncase, an escape route is followed. It consists of constructing a his-\ntogram of the frame pitches and selecting the most likely value asthe segmental pitch.\nStep 2 : note labeling\nOnce the segmental pitch is determined, it can be converted to aMIDI note using the equally tempered frequency scale. Using theconventions that A4 corresponds to 440Hz and that MIDI note zerocorresponds to C-1, one readily ﬁnds that\nMIDI-note/B4 /BY/D3\n/B5/BP\n/D0/D3/CV /B4 /BY/D3\n/BP/BY/D6/CT /CU\n/B5\n/D0/D3/CV\n/BD/BE\n/D4\n/BE\n/BN /BY/D6/CT /CU\n/AP /BK /BM /BD/BJ/BH/BK Hz (3)\nWe always round the frequency to the nearest MIDI note. No at-\ntempt is made to adjust to the scale of the user. For the momentwe are only interested in transcribing the sequences as precisely aspossible, disregarding the intention of the singer.\n6. EXPERIMENTAL RESULTS\nOur system was evaluated in exactly the same way as the state-of-the-art systems were in section 4.\n6.1 Parameter tuning\nThe free parameters of the algorithm were optimized on the record-ings of one male and one female singer which did not contributeto the evaluation corpus (see section 3.1). In table 2 we havelisted the parameters, their meaning and their values. The param-eters are grouped according to their appearance in the segmenta-tion, the segment post-processing and the note assignment stages.\nTable 2: Internal parameters and their settings found by empirical\nevaluation.\nparameter\nmeaning\n value\n/AF/C4\n min. loudness deviation\n 35%/C4/CQ\n loudness bias\n 2.5% of maximum/C4/DB/D7\n white-space threshold\n 2.5% of maximum\n/CE/D1/CX/D2\n min. note voicing evidence\n 15% of maximum/AB\n min. note loudness vs /C4/DB/D7\n 3\n/A1 /BY/D3\n max. frequency deviation\n 10%6.2 Evaluation results\nThe results of our evaluation are labeled MAMI (after the\nname of our project: Musical Audio Mining) in table 3.\nThey are presented in opposition to the results of thebest state-of-the-art system according to our previous tests.\nTable 3: Transcription results for the proposed system (MAMI) as\ncompared to the results of the Pollastri system.\nMAMI\n Pollastri\nSinging without lyrics\nnotes deleted\n 0.00 %\n 4.76 %\nnotes inserted\n 2.24 %\n 7.94 %\nnotes deleted + inserted\n 2.24 %\n 12.70 %\nexact note recognition error\n 35.88 %\n 48.31 %\nnote recognition error /BQ1 semitone\n 1.53 %\n 10.37 %\nSinging with lyrics\nnotes deleted\n 4.92 %\n 13.66 %\nnotes inserted\n 2.19 %\n 5.46 %\nnotes deleted + inserted\n 7.10 %\n 19.13 %\nexact note recognition error\n 42.01 %\n 58.39 %\nnote recognition error /BQ1 semitone\n 6.51 %\n 16.79 %\nApparently, both types of singing sequences are much better tran-scribed by the MAMI system. The remaining 2.24% segmentationerrors in the singing without lyrics sequences all appear in one shortsequence which is sung with very unstable notes. The exact noterecognition errors are spread over the ﬁles. The note recognitionwithin a semitone is always very high (/BL/BK /BM /BH/B1on average), ensuring\nenough precision for a QBH application. Five of the seven singingwithout lyrics sequences were transcribed 100% correctly.\nSegmenting singing with lyrics has also reached an acceptable level\nnow (about 7% segmentation errors on average). The note recogni-\ntion, although not as good as for singing without lyrics, is also quitereliable (about 93.5 % on average).\nIt appears that over the whole set of 18 ﬁles no octave errors have\nbeen made. The largest note deviation is 4 semitones, and it occuredonly once.\n6.3 Sensitivity to parameter settings\nThe main parameter for controlling the segmentation algorithm is/AF/C4. It was veriﬁed experimentally that the total deletion+insertion\nerror rates are not much affected as long as /AF/C4stays in the range\nof 25% to 45%. In this range, loudness ﬂuctuations due tolegato/vibrato usually do not emerge in inserted note boundaries.\nThe only parameter that controls the pitch assignment is/A1 /BY/D3.\nChanging this parameter from 10 to 100% resulted in an increaseof the note recognition error within 1 semitone of only 2%. This isowed to the large robustness of the AMPEX pitch detector.\nOmitting the segment post-processing stage shows a 3 % increase\nof the insertion error rate. Especially in the longer sequences breathremoval seems to be necessary.\nThe bottom line is that the settings of the free parameters are not\ncritical, and the optimal settings are very much in line with whatone would expect on the basis of their meaning.\n6.4 Limitations of the present system\nAs AMPEX analyzes temporal ﬂuctuations in the envelope patterns\nof the auditory model hair-cell outputs, it cannot detect a pitch much\nlarger than 500 Hz. This means that whistling and singing with aAn Auditory Model Based Transcriber of Singing Sequences\nhigh pitch cannot be handled by the present system. In spite of this\nwe obtain good results because our corpus contains only one ﬁle\nwith some whistling in it.\nMonophonic instruments can in principle be handled adequately by\nAMPEX as long as their pitch remains below 500 Hz. However, wedid not perform any test to conﬁrm this.\nSo as to attain a higher applicability of the system, we are cur-\nrently developing a frequency-based pitch extractor to complementthe time-based AMPEX algorithm. The frequency-based extractor\nwill identify maxima in the auditory spectrum, and use them to de-\nrive a best pitch estimate and its evidence. Using this extension, itshould also become possible to handle whistling and monophonicinstruments with a high pitch.\n7. CONCLUSIONS\nWe have established that most transcription systems are incapableof accurately transcribing singing sequences of naive singers. Threeproblems were identiﬁed: (i) they offer but a poor segmentation, (ii)they can only handle singing with speciﬁed syllables (e.g. /ta/), and(iii) their performance is very sensitive to the choices of the freeparameters. Some systems even require training from the user.\nAstonished by this result, we have developed a new auditory model\nbased transcription system that seems to perform an acceptable seg-mentation and note labeling of free singing (with or without lyrics,\nand without any restrictions on the syllables used in singing with-\nout lyrics). In addition, the performance of the algorithm is not verysensitive to the settings of its free parameters.\n8. ACKNOWLEDGMENTS\nWe thank Gaetan Martens, Koen Tanghe and Dirk Van Steelant\nfor valuable discussions on the subject. We also acknowledge\nEmanuele Pollastri for testing his system on our corpus.\nThis research was supported by project ”Musical Audio Mining”\n(010035-GBOU) which is funded by the Flemish Institute for thePromotion of the Scientiﬁc and Technical Research in Industry.\n9. REFERENCES\n[1] Akoff music composer 2.0. Akoff Sound Lab.\nhttp://www.akoff.com.\n[2] Audioworks 2.15. http://www.audioworks.com.[3] Autoscore Deluxe 2.0. Wildcat Canyon Software.\nhttp://www.wildcat.com\n[4] Boersma P. and Weenink D. Praat. A system for doing pho-\nnetics by computer. Report 132, Institue of Phonetics (Ams-terdam). (http://www.praat.org)\n[5] Dannenberg R.B., Mazzoni D. (2001). Melody Matching Di-\nrectly From Audio. Procs ISMIR 2001, 17-18.\n[6] Davis S, Mermelstein P. (1980). Comparison of parametric\nrepresentations for monosyllabic word recognition in contin-uously spoken sentences. IEEE Trans. ASSP 28, 357-366.\n[7] Digitalear. Epinoisis Software. http://www.digital-ear.com[8] Francu C., Nevill-Manning C.G. (2000) Distance metrics and\nindexing strategies for a digital library of popular music.\nProc. IEEE Int. Conf. on Multimedia and Expo, 889-892.\n[9] Ghias A., Logan J., Chamerlin D., Smith B.C. (1995). Query\nBy Humming. Musical Information Retrieval in an AudioDatabase. Procs ACM Multimedia 1995. 231-236.\n[10] Haus G., Pollastri E. (2001). An Audio Front End for Query-\nby-Humming Systems. Procs ISMIR 2001, 65-72.[11] Hermes D. (1992). Pitch Aanalysis. Visual representations\nof speech analysis (edts M. Cooke, S. Beet). Wiley & Sons.\n[12] Intelliscore 4.0, Innovative Music Systems Inc.\nhttp://www.intelliscore.net\n[13] Johnson D. (1980). The relationship between spike rate and\nsynchroning in responses of auditory-nerve ﬁbers to singletones. J. Acoust. Soc. Am. 68, 1115-1122.\n[14] Kosugi N., Nishihara Y ., Sakata T., Yamamuro M. and\nKushima K. (2000). A Practical Query-By-Humming Sys-tem for a Large Music Database. Procs ACM Multimedia2000, 333-342.\n[15] McNab R.J., Smith, L.A. and Witten I.H. (1996). Signal Pro-\ncessing for Melody Transcription. Australian Computer Sci-ence Conference, 301-307.\n[16] McNab R.J., Smith L.A., Witten I.H. and Henderson C.L.\n(2000). Tune Retrieval in the Multimedia Library. Multime-dia Tools and Applications, 113-132.\n[17] Meldex is a part of the New Zealand Digital Library project.\nWebpages www.nzdl.org/musiclib and www.nzdl.org.\n[18] Nishimura T., et al.(2001). Music Signal Spotting Retrieval\nby a Humming Query Using Start Frame Feature Depen-dent Continuous Dynamic Programming. Procs ISMIR 2001,211-218.\n[19] Prechelt L., Typke R. (1998) An Interface for Melody Input.\nUnpublished (see http://wwwipd.ira.uka.de/tuneserver).\n[20] Rabiner L.R. et al. (1976). A comparative performance study\nof several pitch detection algorithms. IEEE Trans ASSP 24,399-418.\n[21] Roger Jang J.-S., Chen J.-C., Gao M.-Y . (2000). A Query-\nby-Singing System based on Dynamic Programming. Int.Workshop on Intelligent Systems Resolutions (8th BellmanContinuum), 85-89.\n[22] Rouat J., Liu Y ., Morisette D. (1997). A pitch and\nvoiced/unvoiced decision algorithm for noisy speech. SpeechCommunication 21, 191-207.\n[23] Sakoe H. and Chiba S. (1978). Dynamic programming algo-\nrithms optimization for spoken word recognition. IEEE TransASSP 26, 43-49.\n[24] Sapp C.S. Improv software MIDI-library.\nhttp://improv.sapp.org.\n[25] Sonoda T., Goto M., Muraoka Y . A WWW-based Melody\nRetrieval System (1998). Procs ICMC 1998, 349-352.\n[26] Van Immerseel L. and Martens J.P. (1992). Pitch and\nvoiced/unvoiced determination with an auditory model. J.Acoust. Soc. Am. 91, 3511-3526.\n[27] V orstermans A., Martens J.P., Van Coile B. (1996). Auto-\nmatic segmentation and labeling of multi-lingual speech data.Speech Communications 19, 271-294.\n[28] Widi music recognition system 2.7, Music Recognition\nTeam. http://www.widisoft.com.\n[29] Zwicker E. and Terhardt, E. (1974). Facts and Models in\nHearing. Springer, Berlin/Heidelberg."
    },
    {
        "title": "Automatic Music Summarization via Similarity Analysis.",
        "author": [
            "Matthew Cooper 0002",
            "Jonathan Foote"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417026",
        "url": "https://doi.org/10.5281/zenodo.1417026",
        "ee": "https://zenodo.org/records/1417026/files/CooperF02.pdf",
        "abstract": "We present methods for automatically producing summary excerpts or thumbnails of music. To find the most representative excerpt, we maximize the average segment similarity to the entire work. Af- ter window-based audio parameterization, a quantitative similarity measure is calculated between every pair of windows, and the results are embedded in a 2-D similarity matrix. Summing the similarity matrix over the support of a segment results in a measure of how similar that segment is to the whole. This measure is maximized to find the segment that best represents the entire work. We discuss variations on the method, and present experimental results for or- chestral music, popular songs, and jazz. These results demonstrate that the method finds significantly representative excerpts, using very few assumptions about the source audio.",
        "zenodo_id": 1417026,
        "dblp_key": "conf/ismir/CooperF02",
        "keywords": [
            "automatically producing summary excerpts",
            "maximizing average segment similarity",
            "quantitative similarity measure",
            "window-based audio parameterization",
            "support of a segment",
            "experimental results",
            "representative excerpts",
            "source audio",
            "jazz",
            "orchestral music"
        ],
        "content": "Automatic Music Summarization via Similarity Analysis\nAutomatic Music Summarization via Similarity Analysis\nMatthew Cooper\nFX Palo Alto Laboratory\n3400 Hillview Avenue\nBldg. 4\nPalo Alto, CA 94304 USA\ncooper@fxpal.comJonathan Foote\nFX Palo Alto Laboratory\n3400 Hillview Avenue\nBldg. 4\nPalo Alto, CA 94304 USA\nfoote@fxpal.com\nABSTRACT\nWepresentmethodsforautomaticallyproducingsummaryexcerpts\northumbnailsofmusic. Toﬁndthemostrepresentativeexcerpt,we\nmaximize the average segment similarity to the entire work. Af-\nter window-based audio parameterization, a quantitative similarity\nmeasureiscalculatedbetweeneverypairofwindows,andtheresults\nare embedded in a 2-D similarity matrix. Summing the similarity\nmatrix over the support of a segment results in a measure of how\nsimilar that segment is to the whole. This measure is maximized to\nﬁnd the segment that best represents the entire work. We discuss\nvariations on the method, and present experimental results for or-\nchestral music, popular songs, and jazz. These results demonstrate\nthat the method ﬁnds signiﬁcantly representative excerpts, using\nvery few assumptions about the source audio.\n1. INTRODUCTION\nAs digital audio collections grow in size and number, audio sum-\nmarization, or “thumbnailing” has become an increasingly active\nresearch area. Audio summaries are useful in applications such as\ne-commerceandinformationretrieval,becauseofthelargeﬁlesizes\nand high bandwidth requirements of multimedia data. Quite often\nit is not practical to audit an entire work, for example if a music\nsearch engine returns many results each lasting several minutes. A\nrepresentative excerpt that gives a good idea of the work is thus is\ndesirable. Similarly,e-commercemusicsitesoftenmakeshortsong\nsegmentsavailabletopreviewbeforepurchase. Inanaudioretrieval\nsystem, it may make sense to judge the similarity of representative\nexcerptsofaworkratherthantheworkasawhole,especiallyifthe\nanalysis is computationally expensive. There is no point analyzing\nan entire symphony if a reasonable index can be derived from a\nten-second excerpt.\nFor these applications, the segment must be a good representation\nofthelongerwork. However,existingsegmentationandexcerpting\nalgorithms do little to guarantee this. Indeed, some approaches can\nbe as crude as to present, for example, the ﬁrst thirty seconds of\nan audio track as representing the whole work. This can be highly\nunsatisfactory if, for instance, the bulk of a particular track bears\nlittle resemblance to an idiosyncratic introduction.\nWe present a method for automatically producing excerpts of lin-\near media (where “linear” implies a function of a one-dimensional\nvariable). Examples include audio and video, which are functions\nof time, and text, which is a discrete function of ﬁle position. We\nconstructsummariesusingself-similarityanalysis,whichallowsus\nto study the structure in an audio ﬁle by measuring the pairwise\nsimilarity between audio instants.\nHere we assume the optimal excerpt is most similar, in an average\nsense, to the piece as a whole. This approach does not rely on se-\nmanticcontentthatcan’tbeautomaticallyextracted,andthuscannot\nPermission to make digital or hard copies of all or part of this\nworkforpersonalorclassroomuseisgrantedwithoutfeeprovided\nthat copies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on\nthe first page. c/circlecopyrt2002 IRCAM - Centre Pompidou\ni start endj\ni\njsimilarity\nmatrixSstart\nendstream\nD(i,j)Figure 1: Embedding pairwise similarity data in a matrix.\nbeconsideredoptimalinthatsense. Forexample,asummaryofthe\nﬁrstmovementofBeethoven’sFifthSymphonywithoutthefamous\nfour-note theme would not be ideal by most standards. To rectify\nthis,theprocesscanbeweightedtoreﬂectanyavailablesemanticin-\nformation. Anotherpossibledesiderataforanaudiosummaryisthat\nit contain all representative portions. For example, a popular song\ncontainingverses,refrainsandabridgeshouldarguablybesumma-\nrizedbyanexamplecontainingportionsofallthreesegments. This\nis generally not possible with a short, contiguous excerpt. In this\npaper, our summaries will be continuous excerpts that are typically\nmuch shorter than the source audio.\n1.1 Related Work\nThere is a great deal of related work on summarization techniques\nfor text, audio, and video. Most summarization or excerpting tech-\nniques start with an analysis of the structure or semantics of the\nsource material. The work on statistical text summarization uses\nterm frequency/inverse document frequency (tf/ idf) to select para-\ngraphs[1],sentences[9],orkeyphrasesthatarebothrepresentative\nof the document and differentiate it from other documents. Au-\ndio summarization techniques typically use a segmentation phase\nfollowed by extraction of a representative excerpt from each seg-\nment. A subset of these excerpts are combined to summarize the\naudio [6]. The work on scene transition graphs is a typical ap-\nproach to abstracting video [8]. After video frames are clustered,\nthe keyframe closest to each cluster center is chosen to represent\nthat cluster. Other approaches attempt to summarize video using\nvarious heuristics, often derived from an analysis of accompanying\nclosed captions [2]. In contrast, our summarization method is not\nbased on any prior segmentation or segment clustering. The result-\ning summaries are selected to maximize quantitative measures of\nthe similarity between candidate excerpts and the source audio as a\nwhole. Summariesofanydesiredlengthcanbeextracted,tosupport\nbrowsing at varying levels of detail.Automatic Music Summarization via Similarity Analysis\n00.511.522.533.544.55\nfrequency (Hz)\ntime (seconds)01020304050607080900500100015002000250030003500400045005000\nFigure 2: Spectrogram data computed from artiﬁcial three-\ntone audio data.\n00.10.20.30.40.50.60.70.80.91\ntime (seconds)time (seconds)\n10203040506070809010010\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFigure 3: Similarity matrix for synthetic signal of Figure 2.\n2. SELF-SIMILARITY ANALYSIS\nIn our analysis, the ﬁrst step is to parameterize the audio. This is\ntypically done by windowing the audio waveform. Currently, we\nconvertthesourceaudiotoa22.05KHzmonoformatbyresampling\nordecodingacompressedformatlikeMP3(MPEGLayer2Level3).\nWethensubdividetheaudiointo2048sample(92.87ms)“frames”\nata10Hzrate. EachframeisthenwindowedwithaHammingwin-\ndow, and parameterized using Mel-Frequency Cepstral Coefﬁcient\n(MFCC) analysis (e.g. [7]). We have also successfully employed\nstandard spectrogram-based parameterizations. We achieve simi-\nlarperformancebyreducingthedimensionofuniformly-spaced64\nbin spectrograms via singular value decomposition (SVD). We are\ncurrently using MFCCs as they provide a low dimensional data-\nindependent parameterization, which may be efﬁciently calculated\nusingfreelyavailablesoftware. Manycompressiontechniquessuch\nasMP3containsimilarspectralrepresentationswhichcouldbeused\ndirectly, avoiding the expense of audio decoding and reparameter-\nizing. Regardless of the parameterization, the result is a compact\nvector of parameters for every frame. Figure 2 shows the spec-\ntrogram for a synthetic three-tone test signal. This was generated\nby concatenating 30 seconds of a 1 kHz sine wave, 40 seconds of\n500 Hz, and 30 seconds of 2 kHz to result in a one-hundred second\nsignal. Because the 500 Hz portion is the longest, the ideal sum-\nmary should consist primarily of the 500 Hz signal as opposed to\nthe shorter 1 KHz and 2 KHz segments.2.1 Distance Matrix Embedding\nOnce the signal has been parameterized, it is then embedded in\na two-dimensional representation [3, 4]. The key is a measure d\nof the (dis)similarity between pairs of parameter vectors viandvj\ncalculated from frames iandj. A simple distance measure is the\nEuclidean distance in the L-dimensional parameter space:\nde(vi, vj) =/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay\nl=1(vi(l)−vj(l))2. (1)\nAnother useful similarity measure is the scalar (dot) product of the\nvectors. Thedotproductcanbenormalizedtogivethecosineofthe\nangle between the parameter vectors:\ndc(vi, vj) =< vi, vj>\n/bardblvi/bardbl /bardblvj/bardbl. (2)\nThis is the cosine of the angle between the vectors and has the\nproperty that it yields a large similarity score even if the vectors\nare small in magnitude. For most applications, this signiﬁcantly\nimproves performance over the Euclidean distance measure by re-\nmoving dependence on signal energy.\nThe distance measure is a function of two frames, hence instants in\nthe source audio. We consider the similarity between all possible\ninstants in a signal by embedding the distance measure in a two-\ndimensional similarity matrix, as depicted in Figure 1. The matrix\nScontainsthesimilaritycomputedbetweenallframecombinations,\nsuch that the element at the ithrow and jthcolumn is\nS(i, j) =dc(vi, vj). (3)\nIn general, Swill have maximum values on the diagonal (because\nevery window will be maximally similar to itself); furthermore if d\nis symmetric then Swill be symmetric as well.\n2.2 Visualizing Similarity Matrices\nWevisualize Sbyassigningabrightnessproportionaltothesimilar-\nitymeasure dc(i, j)toeachpixel (i, j). Theresultingimagereveals\nthestructureofthesourceaudio. Regionsofhighself-similarityap-\npear as bright squares along the main diagonal. Repeated sections\nproducebrightoff-diagonalrectangles. Iftheworkhasahighdegree\nof repetition, this will be visible as diagonal stripes or rectangles,\noffsetfromthemaindiagonalbytherepetitiontime. Thesimilarity\nmatrixforthesyntheticthreetonesignalisshowninFigure3. Each\nportion of the signal is visible as self-similar white squares on the\ndiagonal. For example the 500 Hz tone extends from 30 seconds to\n70 seconds on both time axes.\n3. AUTOMATIC SUMMARIZATION\nTo ﬁnd the segment of a work that best represents the entire work,\nwewishtoﬁndthesegmentwithmaximumsimilaritytothewhole.\nInpopularmusic,whichcommonlycontainsrepeatedelementssuch\nas verses or choruses, we expect that the song’s most-repeated or\nlongest element will appear in the summary. This element is deter-\nmined from the similarity matrix.\nA simple example will illustrate the approach. Given the sequence\nABBBCC, we wish to ﬁnd the most representative subsequence of\nlength three. For simplicity, the similarity measure is chosen to\nbe one if the sequence members match and zero otherwise. We\ncan compute Susing a Hamming-like metric such that the distance\nbetweentwosequenceelementsisoneiftheelementsarethesame,\nand zero otherwise:\nS=\n1 0 0 0 0 0\n0 1 1 1 0 0\n0 1 1 1 0 0\n0 1 1 1 0 0\n0 0 0 0 1 1\n0 0 0 0 1 1\n. (4)Automatic Music Summarization via Similarity Analysis\nqr\nsimilarity\nmatrix Sstart\nrqq\nr endend start\nFigure4: Calculating ¯S(q, r)bysummingthesimilaritymatrix\nover the support of the excerpt q,· · ·, r.\n0 10 20 30 40 50 60 70 80 90 10000.20.40.60.81\nq = start time (seconds)score of excerpt starting at time q20 second excerpt\n30 second excerpt40 second excerpt *\nFigure 5: Summary scores QL(i)computed from the similarity\nmatrix of Figure 3 for L= 20,30,and40seconds.\nFor any subsequence, the average similarity between the subse-\nquence and the entire sequence can be found by summing the\ncolumns (or equivalently, rows) of Scorresponding to that sub-\nsequence. In our example, we want to ﬁnd the three-element sub-\nsequence with maximal average similarity. There are four possi-\nble subsequences: ABB,BBB,BBC, andBCCwith column sums\nseven, nine, eight, and seven, respectively. The highest scoring\nsubsequence is BBB, with a score of nine. This is the optimal\nthree-element contiguous summary of the sequence ABBBCC. Note\nthat this contains all the most frequent members ( B) of the longer\nsequence. The runner-up sequence is BBCwith a score of eight,\nwhich contains both the most frequent and second-most frequent\nmembers. The score can be normalized by the subsequence length\nso that summaries of different lengths can be compared.\nThe previous example can be generalized to arbitrary sequence\nlengths. Given a segment starting at qand ending at r, the av-\nerage similarity of the segment is calculated as the sum of the self-\nsimilarity between the segment and the entire work, normalized by\nthe segment length:\n¯S(q, r) =1\nN(r−q)r/summationdisplay\nm=qN/summationdisplay\nn=1S(m, n), (5)where Nis the length of the entire work (width and height of S).\nThis is shown schematically in Figure 4. A simple interpretation\nof¯S(q, r)is the average of similarity matrix rows over the interval\nq,· · ·, r(or equivalently, the columns). Thus intervals with large\nsimilaritytotheworkasawholewillhavealargeraverage ¯S(q, r).\nIf a weighting function wis known, it can be applied to ﬁnd a\nweighted average as:\n¯Sw(q, r) =1\nN(r−q)r/summationdisplay\nm=qN/summationdisplay\nn=1w(n)S(m, n).(6)\nThiscanbemaximizedtoﬁndtheoptimalweightedsummary. Typ-\nicalweightingfunctionsmightincludea wthatdecreaseswithtime,\nso segments at the beginning of the work are weighted more highly\nthan those at the end. Alternatively, wmight include a measure of\nloudness,favoringgenerallyloudersectionssuchas tutti(allinstru-\nmentsplaying)orchorusesratherthanverses. Anyotherinformation\nknowna priorior deduced can be incorporated into w.\nTo ﬁnd the optimal summary of length L, we ﬁnd the excerpt of\nthatlengthwiththemaximumsummaryscore(Eq. (5)). Deﬁnethe\nscore QL(i)as\nQL(i) =¯S(i, i+L) =1\nNLi+L/summationdisplay\nm=iN/summationdisplay\nn=1S(m, n)(7)\nfori= 1,· · ·, N−L. Thebeststartingpointfortheexcerptisthe\ntimeq∗\nLthat maximizes the summary score:\nq∗\nL= ArgMax\n1≤i≤N−LQL(i). (8)\nThebestsummaryisthentheexcerptoflength Lstartingat q∗\nLand\nending at time q∗\nL+L.\nFigure 5 shows values of QL(i)versus start time i, for summary\nlengths Lof 20, 30, and 40 seconds. All show a maxima or peak at\nq∗= 30,whichisthestarttimeofthe500Hztone(themostrepre-\nsentative segment of the work). The L= 20curve has a maximum\nthatextendsfrom30secondsto50seconds,becauseany20second\nexcerpt starting within that interval will consist solely of the 500\nHzrepresentativetone. Thusforthisexampleandsegmentlengths,\npicking any maximal point q∗\nLresults in an excerpted segment that\nconsists completely of the 500 Hz tone. This is the desired behav-\nior in a quasi-probabilistic sense: any inﬁnitesimally short sample\ntaken uniformly randomly from the the source signal will result in\na 500 Hz tone 40% of the time, and 1 or 2 kHz only 30%. Thus\nthe selected segment contains the excerpt most likely to be similar\nto samples from the original signal.\n4. EXPERIMENTS\n4.1 Music Visualization\nFigure 6 shows a visualization for the Spring (allegro) movement\nfrom Vivaldi’s The Four Seasons . The 22.05 KHz audio was win-\ndowedata10Hzrate. Foreachwindow,wecomputed45MFCCs.\nWe then ordered the MFCCs according to their variances across\nthe entire piece, and retained the ﬁfteen coefﬁcients with highest\nvariances. Wescaledthesecoefﬁcientstounitvariance(hencezero\nmean) across the piece, and then calculated the pairwise similarity\nusingthecosinedistanceof (2). (Wediscardedthelowvarianceco-\nefﬁcientsastheyprovidepoordiscriminationbetweenthestructural\nelements of a piece. Scaling them to unit variance will generally\namplify noise and in turn degrade the similarity analysis.)\nTheresultingsimilaritymatrixshowsthefamiliaropeningthemein\ntheﬁrstsixteenseconds. Itisrepeatedtwice,ﬁrst forte(loudly)then\na quieter repeat eight seconds later. Both repetitions look similar\nbecause of the cosine similarity measure. The theme is repeated at\nseventy-twosecondsandonehundredninetyseconds,whichcanbeAutomatic Music Summarization via Similarity Analysis\n−0.8−0.6−0.4−0.200.20.40.60.8\nTime (sec)Time (sec)\n2040608010012014016018020020\n40\n60\n80\n100\n120\n140\n160\n180\n200\nFigure 6: Similarity matrix computed for Vivaldi’s Springus-\ningMFCCfeaturesandcosinesimilaritymeasure. Theopening\ntheme is repeated at 72 and 190 seconds.\n−0.8−0.6−0.4−0.200.20.40.60.8\nTime (sec)Time (sec)\n2040608010012014016020\n40\n60\n80\n100\n120\n140\n160\nFigure 7: Similarity matrix computed for The Magical Mystery\nTourusing MFCC features and cosine similarity measure.\nseen as brighter regions along the bottom of the image. The major\nstructure of the piece is also evident in the blocks along the main\ndiagonal. For example the bright block between 30 and 70 seconds\nisasoftpassagefortwoviolins,withtherestoftheensemblequiet.\nFigure 7 shows the similarity matrix computed from The Magical\nMysteryTour byTheBeatles,usingthesameparameterizationasthe\nVivaldi. The bright white squares of high similarity show repeated\ninstances of the song’s familiar chorus (“Roll up, roll up for the\nMystery Tour”) throughout the song. The piece also features a\ndistinctivecodafrom145-167seconds,whichdifferssubstantially\nfrom the majority of the song.\n4.2 Music Summarization\nWeusethesimilaritymatrixtodeterminetheoptimalsummariesin\ntwo steps. The ﬁrst step is to evaluate the summary score QL(i)\nof (7). Next, we maximize this to ﬁnd the best start point q∗\nLof\n(8). Computing the column sums of of Sin advance can reduce\ncomputation and storage requirements. For example, in an ap-\nplication where variable length summaries of songs are provided\ndepending on available bandwidth, the column sums for each song\nFigure8: Columnsumscomputedfromthesimilaritymatrixof\nFigure 7.\ncan be pre-computed and stored, and Scan be discarded. Given a\ndesired summary length, L,QLcan be computed using solely the\ncolumn sums (the inner sum of (5)), and maximized to determine\nthe summary excerpt. The column sums computed from the matrix\nof Figure 7 appear in Figure 8.\nFigure 9 shows the summary scores Q10(i), Q20(i), and Q30(i)\ncomputed by summing the columns of the similarity matrix for for\nTheMagicalMysteryTour (Figure8). Locatingthemaximainthese\ncurves, per (8), produces the optimal summary start points shown\nin Table 1. The twenty second summary includes the ten second\nsummary and contains the familiar title refrain. The thirty second\nsummary, interestingly, is a repeat of this element of the song, but\nfrom later in the piece (after the bridge). This excerpt was selected\nbecauseitisalongerreprisalofthesametitlerefrainthanisavailable\nat the beginning of the song.\n0 20 40 60 80 100 120 140 160 180−1.5−1−0.500.51x 104\nTime (sec)Score\nQ10(i)\nQ20(i)\nQ30(i)\nFigure 9: Summary scores, QL(i), computed from the similar-\nity matrix of Figure 7 for L= 10,20,and30seconds.\nTable2showstheoptimalsummariescomputedforVivaldi’s Spring.\nAllthreesummariesincludethememorableintroductorytheme. The\nten second summary is the ﬁrst 10 seconds of the theme. The 20-\nsecond summary includes the last three repetitions. The 30-second\nsummary includes virtually the entire introduction, which exhibits\nthe highest average similarity with the overall piece.\nWealsopresentsummariesfortwoadditionalsongs. Table3shows\nthree summaries computed for Wild Honey by the band U2. All\nthree summaries include the song’s longest chorus segments. The\nchorusisabout15secondslong,sotheﬁrstsummaryonlycontainsAutomatic Music Summarization via Similarity Analysis\na portion of it, while the longer two summaries contain at least one\nof its repetitions in its entirety. Table 4 shows three summaries\ncomputedfor Takethe“A”Train performedbyDukeEllingtonand\nhis orchestra. Again, all three summaries contain the same portion\nof the piece; in this case it is a reprisal of the song’s main melody\nat the performance’s end. In each of the four cases presented, the\nresulting summaries make intuitive sense, and represent signiﬁcant\nand memorable elements of the original pieces. These results are\npromising,andmorethoroughvalidationoftheapproachisplanned.\nTheresultspresentedhereareavailableforreviewontheworldwide\nweb1.\nTable 1: Summary times for The Magical Mystery Tour .\nSegment Length Start (sec.) End (sec.)\n10 49.7 59.7\n20 44.9 64.9\n30 91.1 121.1\nTable 2: Summary times for Spring.\nSegment Length Start (sec.) End (sec.)\n10 4.3 14.3\n20 8.4 28.4\n30 2.5 32.5\nTable 3: Summary times for Wild Honey .\nSegment Length Start (sec.) End (sec.)\n10 197.1 207.1\n20 189.6 209.6\n30 181.7 211.7\n5. CONCLUSION\nWehavepresentedaquantitativeapproachtoautomaticmusicsum-\nmarizationwhichmakesminimalassumptionsregardingthesource\naudio. (Indeed, we expect this approach to work for MIDI, video,\nandothertime-dependentmediaaswell). Thepairwisesimilarityof\nthe audio feature is embedded in a similarity matrix which reveals\nthe major structure of the audio. By summing the similarity matrix\ncolumns, the most representative contiguous portions of the piece\ncanbelocatedandusedforsummariesofarbitrarylength. Wehave\npresented experimental results across a variety of genres, and in\neachcase,theresultingsummariesrepresentedsigniﬁcantelements\nof the original piece. While this approach will not always yield in-\ntuitively satisfying results, we argue that it will ﬁnd the summary\nthat is most likely to be similar to the work as a whole.\n1http://www.fxpal.com/media/musicthumbnails.htmlTable 4: Summary times for Take the “A” Train .\nSegment Length Start (sec.) End (sec.)\n10 135.2 145.2\n20 136.7 156.7\n30 135 165\nThis excerpting criteria is particularly well-suited to popular mu-\nsic which exhibits a relatively high degree of repetition of songs’\nprincipal segments.\nWe are currently integrating this summarization approach with au-\ndio segmentation [5] such that summaries will begin and end at\nmeaningful segment boundaries (such as verse/chorus transitions).\nWearealsoexaminingjointsegmentationandsummarizationtech-\nniques to develop a more complete structural characterization of\ndigital audio.\n6. REFERENCES\n[1] Abracos, J. and Lopes, G.P., Statistical methods for retriev-\ning most signiﬁcant paragraphs in newspaper articles, In\nACL/EACLWorkshoponIntelligentScalableTextSummariza-\ntion, pages 51-57, 1997.\n[2] Christel,M.,Stevens,S.,Kanade,T.,Mauldin,M.,Reddy,R.,\nand Wactlar, H. Techniques for the Creation and Exploration\nof Digital Video Libraries. In Multimedia Tools and Applica-\ntions, B. Furht, ed. Kluwer Academic Publishers, 1996.\n[3] Eckman, J.P., et al., Recurrence Plots of Dynamical Systems,\ninEurophys. Lett. 4(973) (1 November 1987).\n[4] Foote, J. Visualizing Music and Audio using Self-Similarity.\nInProc. ACM Multimedia 99 , pp. 77-80, Orlando, Florida,\nNovember 1999.\n[5] Foote,J.,AutomaticAudioSegmentationUsingAMeasureof\nAudio Novelty, Proc. ICME 2000 , vol. I, pp. 452-455, 2000.\n[6] Logan, B. and Chu, S., Music Summarization Using Key\nPhrases, in Proc. IEEE ICASSP , 2000.\n[7] Rabiner,L.andJuang,B.-H., FundamentalsofSpeechRecog-\nnition. Prentice Hall PTR. ISBN 0-13-015157-2, 1993.\n[8] Yeo, B.-L., Yeung, M.M., Classiﬁcation, Simpliﬁcation, and\nDynamic Visualization of Scene Transition Graphs for Video\nBrowsing, in Storage and Retrieval for Image and Video\nDatabases (SPIE) pp. 60-70, 1998.\n[9] Zechner, K. Fast generation of abstracts from general domain\ntext corpora by extracting relevant sentences, In Proc. of the\nInternationalConferenceonComputationalLinguistics ,1996."
    },
    {
        "title": "Pattern Discovery Techniques for Music Audio.",
        "author": [
            "Roger B. Dannenberg",
            "Ning Hu"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417177",
        "url": "https://doi.org/10.5281/zenodo.1417177",
        "ee": "https://zenodo.org/records/1417177/files/DannenbergH02.pdf",
        "abstract": "Human listeners are able to recognize structure in music through the perception of repetition and other relationships within a piece of music. This work aims to automate the task of music analysis. Music is “explained” in terms of embedded relationships, especially repetition of segments or phrases. The steps in this process are the transcription of audio into a representation with a similarity or distance metric, the search for similar segments, forming clusters of similar segments, and explaining music in terms of these clusters. Several transcription",
        "zenodo_id": 1417177,
        "dblp_key": "conf/ismir/DannenbergH02",
        "keywords": [
            "Human listeners",
            "recognize structure",
            "music through",
            "perception of repetition",
            "relationships within",
            "piece of music",
            "automate the task",
            "music analysis",
            "transcription of audio",
            "representation with similarity/distance metric"
        ],
        "content": "Pattern DiscoveryTechniques for MusicAudio\nPattern DiscoveryTechniquesfor MusicAudio\nRogerB.DannenbergandNingHu\nSchoolof ComputerScience\nCarnegieMellonUniversity\nPittsburgh,PA15213USA\n+1-412-268-3827\n{rbd,ninghu}@cs.cmu.edu\nABSTRACT\nHuman listeners are able to recognize structure in music through\nthe perception of repetition and other relationships within a pieceof music. This work aims to automate the task of music analysis.Music is “explained” in terms of embedded relationships,especially repetition of segments or phrases. The steps in thisprocess are the transcription of audio into a representation with asimilarity or distance metric, the search for similar segments,forming clusters of similar segments, and explaining music interms of these clusters. Several transcription methods areconsidered: monophonic pitch estimation, chroma (spectral)representation, and polyphonic transcription followed byharmonic analysis. Also, several algorithms that search for similarsegments are described. These techniques can be used to performan analysisofmusical structure, asillustrated byexamples.\n1. INTRODUCTION\nDigital sound recordings of music can be considered the lowestlevel of music representation. These audio representations offernothing in the way of musical or sonic structure, which isproblematic for many tasks such as music analysis, music search,and music classification. Given the current state of the art,virtually any technique that reveals structure in an audio recordingis interesting. Techniques such as beat detection, key detection,chord identification, monophonic and polyphonic transcription,melody and bass line detection, source separation, speechrecognition, and instrument identification all derive some higher-level information from music audio. There is some hope that bycontinuing to develop these techniques and combinethem, wewillbe better able to reason about, search, and classify music, startingfroman audio representation.\nIn this work, we examine ways to discover patterns in music audio\nand to translate this into a structural analysis. The main idea isquite simple: musical structure is signaled by repetition. Ofcourse, “repetition” means similarity at some level of abstractionabove that of audio samples. We must process sound to obtain ahigher-level representation before comparisons are made, andmust allow approximate matching to allow for variations inperformance, orchestration, lyrics, etc. In a number of cases, ourtechniques have been able to describe the main structure of musiccompositions.\nWe have explored several representations for comparing music.\nMonophonic transcription can be used for music where a singlevoice predominates (even in a polyphonic recording). Spectralframes can be used for more polyphonic material. We have alsoexperimented with apolyphonictranscription system.\nFor each of these representations, we have developed heuristicalgorithms to search for similar segments of music. We identify\npairs of similar segments. Then, we attempt to simplify thepotentially large set of pairs to a smaller set of clusters. Theseclusters identify “components” of the music. We can thenconstruct an explanation or analysis of the music in terms of thesecomponents. The goal is to derive structural descriptions such as“AABA.”\nWe believe that the recognition of repetition is a fundamental\nactivity of music listening. In this view, the structure created byrepetition and transformation is as essential to music as thepatterns themselves. In other words the structure AABA isimportant regardless of what A and B represent. At the risk ofoversimplification, the first two A’s establish a pattern, the Bgenerates tension and expectation, and the final A confirms theexpectation and brings resolution. Structure is clearlyimportant tomusic listening. Structure can also contribute expectations or priorprobabilities for other analysis techniques, such as transcriptionand beat detection, where knowledge of pattern and form mighthelp to improve accuracy. It follows that the analysis of structureis relevant to music classification, music retrieval, and otherautomated processingtasks.\n2. RELATEDWORK\nIt is well known that music commonly contains patterns andrepetition. Any music theory book will discuss musical form andintroduce notation, such as “AABA,” for describing musicalstructures. Many researchers in computer music have investigatedtechniques for pattern discovery and pattern search. Cope [8]searches for “signatures” that are characteristic of composers, andRolland and Ganascia describe search techniques [25]. Interactivesystems have been constructed to identify and look for patterns[29], and much of the work on melodic similarity [13] is relevantto the analysis ofmusic structure. Aucouturierand Sandlerpresentanotherapproach to findingpatternsin musicaudio. [2]\nSimon and Sumner wrote an earlypaper on music listening and its\nrelationship to pattern formation and memory [27], proposing thatwe encode melody by referencing patterns and transformations.This has some close relationships to data compression, which hasalso inspired workin musicanalysisand generation. [15] Narmourdescribes a variety of transformative processes that operate inmusicto createstructuresthat listenersperceive. [21]\nA fundamental idea in this work is to compare every point of a\nmusic recording with every other point. This naturally leads to amatrix representation in which row i, column jcorresponds to the\nsimilarity of time points iandj. A two-dimensional grid to\ncompute and display self-similarity has been used by Wakefieldand Bartsch [5] and byFooteand Cooper[12].\nMont-Reynaud and Goldstein proposed rhythmic pattern\ndiscovery as a way to improve music transcription. [20] Conklinand Anagnostopoulou examine a technique for finding significantexactly identical patterns in a body of music. [7] A differentapproach is taken by Meek and Birmingham to search forcommonlyoccurringmelodiesorothersequences. [18]\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfullcitation on thefirst page.\n©2002 IRCAM – CentrePompidouPattern DiscoveryTechniques for MusicAudio\n3. PATTERNSEARCH\nIn this section, we describe the general problem of searching for\nsimilar sections of music. We assume that music is represented asa sequence s\ni,i=0 …n–1. Asegmentof music is denoted by a\nstarting and ending point: ( i, k), 0≤i≤k<n.Similar sections\nconsists of two segments: (( i, k), (j,l)), 0≤i≤k<j≤l<n.For\nconvenience, wedo notallowoverlapped segments1, hencek<j.\nThere are O( n4) possible pairs of segments. To compute a\nsimilarity function of two segments, one would probably use adynamic programming algorithm with a cost proportional to thelengths of the two segments. This increases the cost to O( n\n6)i f\neach pair of segments is evaluated independently. However, givena pair of starting points, i,j, the dynamic programming alignment\nstep can be used to evaluate all possible pairs of segmentendpoints. There are O( n\n2) starting points and the average cost of\nthe full alignment computation is also O( n2), so the total cost is\nthen O(n4). Using frame sizes from 0.1 to 0.25 seconds and music\ndurations of several minutes, we can expect nto be in the range of\n200 to 2000. This implies that a brute-force search of the entiresegment pair space is will take hours or even days. This has led usto pursueheuristicalgorithms.\nIn our work, we assume a distance function between elements of\nthe sequence s\ni. To compute the distance between two segments,\nwe use an algorithm for sequence alignment based on dynamicprogramming. A by-product of the alignment is a sumofdistancesbetween corresponding sequence elements. This measure has theproperty that it generally increases with length, whereas longerpatterns are generally desirable. Therefore, we divide distance bylength to get an overall distancerating.\nTypically there are many overlapping candidates for similar\nsegments. Extending or shifting a matching segment by a frame ortwo will still result in a good rating. Therefore, the problem is notso much to find all pairs of similar segments but the locally “best”matches. In practice, all of our algorithms work by extendingpromising matches incrementally to find the “best” match. Thisapproach reduces the computation time considerably, butintroduces heuristics that make formal descriptions difficult.Nevertheless, we hope this introduction will help to explain thefollowingsolutions.\n4. MONOPHONICANALYSIS\nOur first approach is based on monophonic pitch estimation,which is used to transcribe music into a note-based representation.Notes are represented as a pitch (represented on a continuousrather than quantized scale), starting time, and duration (inseconds). The pitch estimation is performed using autocorrelation[24] and some heuristics for rejecting false peaks and outliers, asdescribed in an earlier paper. [9]\nWe worked with a saxophone solo, “Naima,” written and\nperformed by John Coltrane [6] with a jazz quartet (sax, piano,bass, and drums). To find matching segments in the transcription,we construct a matrix MwhereM\ni,jis the length of a segment2\nstartingat note iand matching a segment at note j.\n4.1 Algorithm1\nThe search algorithm in this case is a straightforward search of\nevery combination of i,jsuch that i<j.F o rnnotes, there are n(n-\n−1)/2 pairs. The search proceeds only if there is a close match\nbetween pitch iand pitch j. Although we could use dynamic\nprogramming for note alignment [13, 26], we elected to try asimple iterative algorithm. The algorithm repeatedly extends thecurrent pair of similar segments as long as the added notes matchin pitch and approximate duration. In addition to direct matches,the algorithm is allowed to skip one note after either segment andlook for a match, skip one short note in both segments and look\nfor a match, consolidate [19] two consecutive notes with matchingpitches to form one with a greater duration and match that to anote, or match consolidated note pairs following both segments.These rules might be extended or altered to search for rhythmicpatternsorto allowtranspositions.\nIf segment ( i,k) matches ( j,l), then in many cases, ( i+1 ,k) will\nmatch (j+1 ,l) and so on. To eliminate the redundant pairs, we\nmake a pass through the elements of M, clearing cells contained\nby longer similar segments. For example if ( i,k) matches ( j,l), we\nclear all elements of the rectangular submatrix M\ni..k,j..lexcept for\nMi,j.\nFinally, we can read off pairs of similar segments and their\ndurations by making another pass over the matrix M. Although\nthis approach works well if there is a good transcription, it is notgenerally possible to obtain a useful melodic transcription frompolyphonic audio. In the next section, we consider an alternativerepresentation.\n5. SPECTRUM-BASEDANALYSIS\nWhen transcription is not possible, a lower-level abstraction basedon the spectrum can be used. We chose to use Wakefield’schromabecause it seemed to do a good job of identifying similar\nsegments in an earlier study where the goal was to find the chorusofapop song. [4, 5]\nThe chroma is a 12-element vector where each element represents\nthe energy associated with one of the 12 pitch classes. Essentially,the spectrum “wraps around” at each octave and bins are\ncombined to form the chroma vector. Distance is then defined asEuclidean distance between vectors normalized to have a mean ofzero and a standard deviation of one. (This particular distancefunction was adopted from Bartsch and Wakefield. It seems towork at least as well as various alternatives, including simpleEuclidean distance, although there is no formal basis for thischoice.)\nThe most important feature of a chroma representation is that the\nmusic is divided into equal-duration frames rather than notes.Typically, there will be hundreds or thousands of frames asopposed to tens or hundreds of notes. Matching will tend to bemore ambiguous b ecause the data is not segmented into discrete\nnotes. Therefore, we need to use more robust (and expensive)sequence alignment techniques and therefore more cleveralgorithms.\n5.1 Brute-ForceApproach\nAt first thought, observing that dynamic programming computes aglobal solution from incremental and local properties, one mighttry to reuse local computations to form solutions to our similarsegments problem. Atypical dynamicprogrammingstep computes\nthe distance at cell i,jin terms of cells to the left ( j−1), above\n(i\n−1), and diagonal( i−1,j−1). Thevalueat i,jis:\nMi,j=di,j+m in(Mi,j−1,Mi−1,j,Mi−1,j−1)\nIn terms of edit distances, we use di,j, the distance from frame ito\nframejas either a replacement cost, insertion cost, or deletion\ncost, although many alternative cost/distance functions arepossible within the dynamic programming framework. [14]Unfortunately, even if we precompute the full matrix, it does nothelp us in computing the distance between two segments becauseof initial boundary conditions, which change for everycombination of iandj. Smith and Waterman’s algorithm [28]\ncomputes a single best common subsequence, but in our case thatwould simply be the perfect match along the diagonal. Otherrelated algorithms for biological sequence matching includeFASTA [23] and BLAST [1], but these would also report thePattern DiscoveryTechniques for MusicAudio\ndiagonal as the longest matching sequence. There are similarities\nbetween these algorithms and ours (presented below). It seemslikely that better and faster music similarity algorithms could bederived from these and other biological sequence matchingalgorithms.\nAs mentioned in the introduction, the best we can do is to\ncompute a submatrix starting at i,jfor every 0 ≤i<j<n.This\nleaves us with an O( n\n4) algorithm to compute the distance for\nevery pair (( i, k), (j, l)). To avoid very long computation times, we\ndeveloped a faster, heuristic search.\n5.2 HeuristicSearch\nWe compute the distance between two segments by finding a pathfromi,jtok,lthat minimizes the distance function. Each step of\nthe path takes one step to the right, downward, or diagonally. Inpractice, similar segments are characterized by paths that consistmainly of diagonal segments because tempo variation is typicallysmall. Thus we do not need to compute a full rectangular array tofind good alignments. Alternatively, we can compute several oreven all paths with a single pass through the matrix. This methodisdescribed here.\n5.3 Algorithm2\nThe main idea of this algorithm is to identify path beginnings andto follow paths diagonally across a matrix until the path ratingfalls below some threshold. The algorithm uses three matrices we\nwill call distance ( D), path (P), and length ( L).DandLhold real\n(floating point) values, and Pholds integers. Pis initialized to\nzero so that we can determine which cells have been computed. If\nP\ni,j= 0, we say cell i,jis uninitialized. The algorithm scans the\nmatrix along diagonals of constant i+jas shown in Figure 1,\nfilling in corresponding cells of D,P,a n dL. Acell is computed in\nterms of the cells to the left, above, and diagonal. First, computedistances and lengths as follows:\nd\nh=ifPi,j−1≠0t h e nDi,j−1+di,j,elseÿ;lh=Li,j−1+√2/2\ndv=ifPi−1,j≠0t h e nDi−1,j+di,j,elseÿ;lv=Li−1,j+√2/2\ndd=ifPi−1,j−1≠0t h e nDi−1,j−1+di,j,elseÿ;ld=Li−1,j−1+1\nThe purpose of the infinity ( ÿ) values is to disregard distances\ncomputed from uninitialized cells as indicated by P. The reader\nfamiliar with dynamic programming for string comparison mayrecognize d\nh,dv,a n dddas horizontal, vertical, and diagonal\nextensions of precomputed paths. In contrast to dynamicprogramming, we also compute path lengths l\nh,lv,a n dld.Now, let\nc=min(dh/lh,dv/lv,dd/ld). Ifcis greater than a threshold, the cell\nati,jis left uninitialized. Otherwise, we define Di,j=c,Li,j=lm,\nandPi,j=Pm, where the subscript mrepresents the cell that\nproduced the minimum value for c,either (i,j−1), (i−1,j), or\n(i−1,j−1).\nBecause of the length terms, this algorithm may not find optimal\npaths. However, we found that when we defined distance withoutthe length terms, the algorithm was difficult to tune and wouldfind eitherpathsthataretoo shortormanyspuriouspaths.\nAs described so far, this computation will propagate paths once\nthey are started, but how is a path started? When P\ni,jis left\nuninitialized by the computation described in the previousparagraph and d\ni,jis below a threshold (the same one used to cut\noff paths), Pi,jis set to a newinteger value to denote the beginning\nof a new path. We also define Di,j=di,jandLi,j=1a tt h e\nbeginningofthepath.ij\nFigure1. InAlgorithm2, thesimilarity matrix is\ncomputed along diagonals as shown.\nAfter this computation, regions of Pare partitioned according to\npath names. Every point with the same name is a candidateendpoint for the same starting point. We still need to decidewherepaths end. We can compute endpoints by reversing the sequenceofchroma frames, so that endpoints becomestartingpoints. R ecall\nthat starting points are uninitialized cells where d\ni,jis below a\nthreshold. To locate endpoints, scan the matrix in reverse fromtheoriginal order (Figure 1 shows the original order). Whenever anew path name is encountered, and the distance d\ni,jis below\nthreshold, find the starting point and output the path. An arraycankeep track of which path names have been output and where pathsbegin.\n6. POLYPHONICTRANSCRIPTION\nPolyphonic transcription offers another approach to similarity.Although automatic polyphonic transcription has rather high errorrates, it is still possible to recover a significant amount of musicalinformation. We use Marolt’s SONIC transcription program [16],which transcribes audio files to MIDI files. SONIC does notattempt to perform source separation, so the resulting MIDI datacombines all notes into a single track. Although SONIC wasintended for piano transcription, we get surprisingly good resultswith arbitrary music sources. Transcriptions inevitably havespurious notes, so we reduce the transcriptions to a chordprogression using the Harman program by Pardo [22]. Harman isable to ignore passing tones and other non-chord tones, so inprinciple, Harman can help to reduce the “noise” introduced bytranscription errors.\nAfter computing chords with Harman, we generate a sequence of\nframess\ni,0<i<n, where each frame represents an equal interval\nof time and siis a set of pitch classes corresponding to the chord\nlabel assigned byHarman.\nIn our experiments with polyphonic transcription, we developed\nyet another algorithm for searching for similar segments. Thisalgorithm is based on an adaptation of dynamic programming forcomputer accompaniment [10]. In this accompaniment algorithm,\na score is matched to a performance not by computing a full n×m\nmatrix but by computing only a diagonal band swept out by amoving window, which is adaptively centered on the “best”currentscoreposition.\n6.1 Algorithm3\nTo find similar segments, we will sweep a window diagonallyfromupper left to lower right as shown in Figure 2. When amatchis found, indicated by good match scores, the window is moved tofollow the best path. We need to decide where paths begin andend. For this purpose, we compute similarity(rather than distance)such that similarity scores increase where segments match, anddecreasewheresegmentsdo notmatch.Pattern DiscoveryTechniques for MusicAudio\nFigure2. InAlgorithm3, thesimilarity matrix iscomputed\nin diagonal bands swept out along the path shown. The\nshaded area shows a partially completed computation.\nAn example function for similarity of chords is to count the\nnumber of notes in common minus the number of notes not incommon. For chords A and B (sets of pitch classes), the similarityis:\nσ(A, B) =|A ∩B|−|A∪B−A∩B|,\nwhere |X|isthenumber ofelementsin (cardinalityof) set X. Other\nfunctions were tried, including a count of the number of commonpitches, but this has the problem that a dense chord will matchalmost anything. (A similarity function based on probabilitiesmight work better than our ad hocapproach. This is left for future\nwork.) We will write σ\ni,jto denote σ(si,sj), the similarity between\nchords at frames iandj.\nWhen we compute the matrix, we initialize cells to zero and store\nonly positive values. A path begins when a window elementbecomes positive and ends when the window becomes zero again.Thecomputation foramatrixcellis:\nM\ni,j=max(Mi,j−1−p,Mi−1,j−p,Mi−1,j−1)+σi,j−c\nwherepis a penalty for insertions and deletions, and cis a bias\nconstant, chosen so that matching segments generate increasingvalues along the alignment path, and non-matching segmentsquicklydecrease to zero.\nThe computation of Mproceeds as shown by the shaded area in\nFigure 2. This evaluation order is intended to find locally similarsegments and follow their alignment path. The reason for\ncomputing in narrow diagonal bands is that if Mwere computed\nentire row by entire row, all paths would converge to the maindiagonal where all frames match perfectly. At each iteration, cellsare computed along one row to the left and right of the currentpath, spanning data that represents a couple of seconds of time.Because of the limited width of the path, references will be made\nto uninitialized cells in M. These cells and their values are ignored\nin themaximumvaluecomputation.\nThis algorithm can be further refined. The score along an\nalignment path will be high at the end of the similar segments,after which the score will decrease to zero. Thus, the algorithmwill tend to compute alignment paths that are too long. We canimprove on the results by interactively trimming a frame fromeither end of the path as long as the similarity/length quotientincreases. This does not always work well because of localmaxima. Another heuristic we use is to trimthe final part ofapathwhere the slope is substantially off-diagonal, as shown in Figure3.\nBecause the wi ndow has a constant size, this algorithm runs in\nO(n\n2) time, and by storing only the portion of the matrix swept by\nthe window, O( n) space. The algorithm is quite efficient in\npractice.Figure 3. The encircled portion of the alignment path is\ntrimmedbecause it represents anextreme difference intempo.\nThe remainder determines a pair of similar segments.\n7. CLUSTERING\nAfter computing pairs of similar segments with any of the threepreviously described algorithms, we need to form clusters toidentify where segments occur more than twice in the music. Forexample, if segment A is similar to segment B (as determined byalgorithm 1, 2, or 3), and B is similar to C, we expect A to besimilar to C, forming the cluster {A, B, C}. Essentially, we arecomputing the transitive closure of a “similarity” relation overthese segments, where “similarity” means either the segments areidentified as similar by Algorithm 1, 2, or 3, or the segmentsoverlap significantly (typically, the segment starting and endingpoints match within 10 percent of the segment durations). Thetransitive closure generates an equivalence relation, which in turnimplies a partition over the set of segments. This partition is theclusteringwe are after.\nThe algorithm is simple: Start with a set of similar pairs, as\ncomputed by Algorithms 1, 2, or 3. Remove any pair from the setto form the first cluster. Then search the set for pairs ( a,b)s u c h\nthat either aorb(or both) is an approximate match to a segment\nin the cluster. If a(orb) is not already in the cluster, add it to the\ncluster. Continue extending the cluster in this way until there areno more similar segments in the set of pairs. Now, repeat thisprocess to form the next cluster, etc., until the set of pairs isempty.\nSometimes, asegment in aclusterwill correspond to asubsegment\nof a pair, e.g. (10, 20) overlaps halfofthe first segment ofthe pair((10, 30), (50, 70)). We do not want to add (10, 30) or (50, 70) tothe cluster because these have length 20, whereas the clusterelement (10, 20) only has length 10. However, it seems clear thatthere is a segment similar to (10, 20) starting at 50. In thissituation, we split the pair proportionally to synthesize a matchingpair. In this case, we would create the pair ((10, 20), (50, 60)) andadd (50, 60)to thecluster.\n8. ANALYSIS AS EXPLANATION\nThe final step is to produce an analysis of the musical structureimplied by the clusters. We like to view this as an “explanation”process. For each section of music, we “explain” the music interms of its relationship to other sections. If we could determinerelationships of transposition, augmentation, and other forms ofvariation, these relationships would be part of the explanation.With only similarity, the explanation amounts to labeling musicwith clusters.\nTo build an explanation, recall that music is represented by a\nsequence s\ni,0≤i<n. Our goal is to fill in an array Ei,0≤i<n,\ninitiallynil, with cluster names, indicating which cluster (if any)\ncontains a note or frame of music. The explanation Eserves to\ndescribe the music as a structure based on the repetition andorganization ofpatterns.Pattern DiscoveryTechniques for MusicAudio\nRecall that a cluster is a set of intervals. For each iin some\nmember ofthe cluster, we set Eito the name ofthe cluster. (Names\nare arbitrary, e.g. “A”, “B”, “C”, etc.) We then continue searchingfor the next isuch that E\ni=nilandiis in some new cluster. We\nthen label additional points in Eiwith this new cluster. However,\nonce a label is set, we do not replace it. This gives priority tomusical material that is introduced the earliest, which seems to beareasonableheuristicto resolveconflictswhen clustersoverlap.\n9. EXAMPLES\nWe present results from monophonic pitch estimation andchroma-based analyses, and we describe some preliminary resultsusingpolyphonictranscription.\n9.1 Transcription and Algorithm1\nFigure 4 illustrates an analysis of “Naima” using monophonictranscription and Algorithm 1 to find similar segments. Audio isshown at the top to emphasize the input/output relationships forthe casual reader. (The authors realize that very little additionalinformation is revealed by these low-resolution waveforms.)Clusters are shown as heavy lines, which show the location ofsegments, connected by thin lines. The analysis is shown at thebottom of the figure. The simple “textbook” analysis of this piece\nwould be a presentation of the theme with structure AABA,followed by a piano solo. The saxophone returns to play BAfollowed by a short coda. In the computer analysis, furtherstructure is discovered within the B part (the bridge), so thecomputer analysis might be written as AABBCA, where BBCformsthebridge.\nThe transcription failed to detect more than a few notes of the\npiano solo. There are a few spurious matching segments here.After thesolo, theanalysisshowsarepetition ofthebridgeand theA part: BBCA. This is followed by the coda in which there issome repetition. Aside from the solo section, the computeranalysis corresponds quite closely to the “textbook” analysis. Itcan be seen that the A section is half the duration of the B part,which is atypical for an AABA song form. If the program hadadditional knowledge of standard forms, it might easily guess thatthis is a slow ballad and uncover additional structure such as thetempo, number of measures, etc. Note, for example, that once thepiece is subdivided into segments, further subdivisions areapparent in the RMS amplitude of the audio signal, indicating aduple meter. Additional examples of monophonic analysis arepresented in anotherpaper. [11]\n0 20 40 60 80 100 120 140\nTime(s)\nFigure 5. Analysis of Beethoven’s Minuet in G performed on piano. The structure, shown at the bottom, is clearly\nAABBCCDDAB.\n0 50 100 150 200 250\nTime(s)\nFigure 4. Analysis of Naima. Audio is shown at top. Below that is a transcription shown in piano roll notation.\nNext isa diagramof clusters. At bottomis the analysis; similar segments are shadedinthe same pattern.\nThe formis AABA,where the B part has additional structure that appears as two solid black rectangles\nandonefilledwitha“///” pattern. The middlesectionisapianosolo. Thesaxophonereenters at the\nB section, repeats the A part, andends witha coda consisting of another repetition.Pattern DiscoveryTechniques for MusicAudio\n9.2 Chroma and Algorithm2\nFigure 5 illustrates an analysis of Beethoven’s “Minuet in G”\n(performed on piano) using the chroma representation andAlgorithm 2 for finding similar segments. Because the repetitionsare literal and the composition does not involve improvisation, theanalysis is definitive, revealing that the structure is:AABBCCDDAB.\nFigure 6 applies the same techniques to a pop song [3] with\nconsiderable repetition. Not all of the song structure wasrecovered because the repetitions are only approximate; however,the analysis shows a structure that is clearly different from theearlier pieces byColtrane and Beethoven.\n9.3 PolyphonicTranscription &Algorithm3\nSo far, polyphonic transcription has not yielded good results asanticipated. Recall that we first transcribe a piece of music andthen construct a harmonic analysis, so the final representation is asequence of frames, where each frame is a chord. When we listento the transcriptions, we can hear the original notes and harmonyclearly even though many errors are apparent. Similarly, theharmonic analysis of the transcription seems to retain theharmonic structure of the original music. However, the resultingrepresentation does not seem to have clear patterns that aredetectable using Algorithm 3. On the other hand, using syntheticdata, Algorithm3 successfullyfindsmatchingsegments.\nThe observed problems are probably due to many factors. The\nanalysis often reports different chords when the music is similar;for example, an A minor chord in one segment and C major in theother. Since these chords have 2 pitch classes in common and 2\nthat are different, σ(Amin, Cmaj) = 0, whereas σ(Cmaj, Cmaj) =3.\nPerhaps there is a better similarity function that gives less penaltyfor plausible chord substitutions. In addition, chord progressionsin tonal music tend to use common tones and are based on the 7-note diatonic scale. This tends to make any two chords chosen atrandom from a given piece of music more similar, leading to falsepositives. Sometimes Algorithm 3 identifies two segments thathave the same single chord, even though the segments are nototherwise similar. A better similarity metric that requires morecontext might help here. Also, there is a fine line between similarand dissimilar segments, so finding a good value for the biasconstant cis difficult. Finally, the harmonic analysis may be\nremovinguseful information alongwith the“noise.”\nTo get a better idea of the information content of this\nrepresentation, Figure 7 is based on an analysis of “Let it Be”performed by the Beatles [17], using polyphonic analysis andchord labeling. After a piano introduction, the vocal melody startsat about 13.5s and finishes the first 4 measures at about 27s. Thisphrase is repeated throughout the song, so it is interesting tomatch this known segment against the entire song. Starting atevery possible offset, we can search for the best alignment withthe score and plot the distance (negative similarity). The distance\nis zero at 13.5s because the segment matches itself perfectly. Thesegment repeats almost exactly at about 27s, which appears as adownward spike at 27s. From the graph, it is apparent that thesegment also appears with the repetition several other times, asindicated bypairsofdownward spikesin thefigure.\nCorrelation With First 4 Bars\n020406080100120\n0 50 100 150 200 250Time Offset (s)Distance\nFigure7. Thesegmentfrom13.5sto27sisalignedatevery\npointing the score and the distance is plotted. Downward\nspikes indicate a similar segments, of whichthere are several.\nFigure 7 gives a clear indication that the representation contains\ninformation and in fact is finding structure within the music;otherwise, the figure would appear random. In this case, we aregiven the similar segment and only ask “where else does thisoccur?” Further workisrequired to usethisinformation to reliablydetect similar segments, where the segments are not given a\npriori.\n10. SUMMARYANDCONCLUSIONS\nMusic audio presents very difficult problems for music analysisand processing because it contains virtually no structure that isimmediately accessible to computers. Unless we solve thecomplete problem of auditory perception and human intelligence,we must consider more focused efforts to derive structure fromaudio. In this work, we constructed programs that “listen” tomusic, recognize repeated patterns, and explain the music in termsofthesepatterns.\nSeveral techniques can be used to derive a music representation\nthat allows similarity comparison. Monophonic transcriptionworks well if the music consists primarily of one monophonicinstrument. Chroma is a simplification ofthe spectrumand appliesto polyphonic material. Polyphonic transcription simplified byharmonic analysis offers another, higher-level representation.Three algorithms for efficientlysearching for similar patterns werepresented. One of these works with note-level representationsfrom monophonic transcriptions and two work with frame-basedrepresentations. We demonstrate through examples that the\n0 20 40 60 80 100 120 140 160 180\nTime(s)\nFigure6. Analysisofapopsong (SamanthaMumba, “Baby Come OnOver”), showing many repetitions ofasingle\nsegment. Similar segments existaround20s and60s, but this similarity was not detected.Pattern DiscoveryTechniques for MusicAudio\nmonophonic and chroma analysis techniques recover a significant,\nand in some cases, essentially complete top-level structure fromaudio input.\nWe find it encouraging that these techniques apply to a range of\nmusic, including jazz, classical, and popular recordings. Of\ncourse, not all music will work as well as our examples. Inparticular, through-composed music that develops and transformsmusical material rather than simply repeating it cannot beanalyzed with our systems. This includes improvised jazz and rocksoloing, many vocal styles, and most art music. In spite of thesedifficulties, we believe the premise that listening is based onrecognition of repetition and transformation is still valid. Thechallenge is to recognize repetition and transformation even whenit isnot so obvious.\nSeveral areas remain for future work. We are working to better\nunderstand the polyphonic transcription data and harmonicanalysis, which offer great promise for finding similarity in theface ofmusicalvariations. Itwould beniceto haveaformalmodelthat could help to resolve structural ambiguities. For example, amodel could enable us to search for patterns and clusters that givethe “best” global explanation for observed similarities. Thedistance metrics used for finding similar segmentscould also useamore formal approach. Distance metrics should reflect theprobability that two segments are not similar. Anotherenhancement to our work would be the use of hierarchy inexplanations. This would, for example, support a two-levelexplanation of the bridge in “Naima.” It would be interesting tocombine data from beat tracking, key analysis, and othertechniques to obtain a more accurate view of music structure.Finally, it would be interesting to find relationships other thanrepetition. Transposition of small phrases is a commonrelationship within melodies, but we do not presently detectanything other than repetition. Transposition often occurs in veryshort sequences, so a good model ofmusicalsequencecomparisonthat incorporates rhythm, harmony, and pitch seems to benecessaryto separaterandommatchesfromintentional ones.\nIn conclusion, we offerasetofnewtechniquesand ourexperience\nusing them to analyze music audio, obtaining structuraldescriptions. These descriptions are based entirely on the musicand its internal structure of similar patterns. Our results suggestthis approach is promising for a variety of music processing tasks,including music search, where programs must derive high-levelstructures and features directlyfromaudio representations.\n11. ACKNOWLEDGMENTS\nThis work was supported by the National Science Foundationunder award number 0085945. Ann Lewis assisted in thepreparation and processing of data. Matija Marolt offered the useof his SONIC transcription software, which enabled us to explorethe use of polyphonic transcription for music analysis. MarkBartsch and Greg Wakefield provided chroma analysis software.We would also like to thank Bryan Pardo for his Harman programand assistance using it. Finally, we thank our other colleagues atthe University of Michigan for their collaboration and manystimulatingconversations.\n12. REFERENCES\n[1] Altschul, S.F., Gish, W., Miller, W., Myers, E.W. and\nLipman, D.J. A Basic Local Alignment Search Tool. Journal\nof MolecularBiology ,215. 403-410.\n[2] Aucouturier, J.-J. and Sandler, M., Finding Repeating\nPatterns in Acoustic Musical Signals: Applications for AudioThumbnailing. in AES22 International Conference onVirtual, Synthetic and Entertainment Audio , (Espoo, Finland,\n2002), Audio EngineeringSociety, to appear.\n[3] Bagge, A., Birgisson, A. and Mumba, S. Baby Come On\nOverBabyComeOn Over(CDSingle) , Polydor, 2001.\n[4] Bartsch, M. and Wakefield, G.H., To Catch a Chorus: Using\nChroma-Based Representations For Audio Thumbnailing. inProceedings of the Workshop on Applications of Si gnal\nProcessing to Audio and Acoustics , (2001), IEEE.\n[5] Birmingham, W.P., Dannenberg, R.B., Wakefield, G.H.,\nBartsch, M., Bykowski, D., Mazzoni, D., Meek, C., Mellody,M. and Rand, W., MUSART: Music Retrieval Via AuralQueries. in International Symposium on Music Information\nRetrieval, (Bloomington, Indiana, 2001), 73-81.\n[6] Coltrane, J. Naima GiantSteps , AtlanticRecords, 1960.\n[7] Conklin, D. and Anagnostopoulou, C., Representation and\nDiscovery of Multiple Viewpoint Patterns. in Proceedings of\nthe 2001 International Computer Music Conference , (2001),\nInternationalComputerMusicAssociation, 479-485.\n[8] Cope, D. Experiments in Musical Intelligence . A-R Editions,\nInc., Madison, Wisconsin, 1996.\n[9] Dannenberg, R.B. Listening to \"Naima\": An Automated\nStructural AnalysisfromRecorded Audio, 2002, (in review).\n[10] Dannenberg, R.B., An On-Line Algorithm for Real-Time\nAccompaniment. in Proceedings of the 1984 International\nComputer Music Conference , (Paris, 1984), International\nComputer Music Association, 193-198.http://www.cs.cmu.edu/~rbd/bib- accomp.html#icmc84.\n[11] Dannenberg, R.B. and Hu, N., Discovering MusicalStructure\nin Audio Recordings. in International Conference on Music\nand Artificial Intelligence , (2002), Springer, (to appear).\n[12] Foote, J. and Cooper, M., Visualizing Musical Structure and\nRhythm via Self-Similarity. in Proceedings of the 2001\nInternational Computer Music Conference , (Havana, Cuba,\n2001), InternationalComputerMusicAssociation, 419-422.\n[13] Hewlett, W. and Selfridge-Field, E. (eds.). Melodic\nSimilarity: Concepts, Procedures, and Applications .M I T\nPress, Cambridge, 1998.\n[14] Hu, N. and Dannenberg, R.B., A Comparison of Melodic\nDatabase Retrieval Techniques Using Sung Queries. in Joint\nConference on Digital Libraries , (2002), Association for\nComputingMachinery.\n[15] Lartillot, O., Dubnov, S., Assayag, G. and Bejerano, G.,\nAutomatic Modeling of Musical Style. in Proceedings of the\n2001 International Computer Music Conference , (2001),\nInternationalComputerMusicAssociation, 447-454.\n[16] Marolt, M., SONIC: Transcription of Polyphonic Piano\nMusic With Neural Networks. in Workshop on Current\nResearch Directions in Computer Music , (Barcelona, 2001),\nAudiovisual Institute, Pompeu FabraUniversity, 217-224.\n[17] McCartney, P. LetItBe L e tI tB e, AppleRecords, 1970.\n[18] Meek, C. and Birmingham, W.P., Thematic Extractor. in 2nd\nAnnual International Symposium on Music InformationRetrieval, (Bloomington, Indiana, 2001), Indiana University,\n119-128.\n[19] Mongeau, M. and Sankoff, D. Comparison of Musical\nSequences. in Hewlett, W. and Selfridge-Field, E. eds.Melodic Similarity Concepts, Procedures, and Applications ,\nMITPress, Cambridge, 1990.Pattern DiscoveryTechniques for MusicAudio\n[20] Mont-Reynaud, B. and Goldstein, M., On Finding Rhythmic\nPatterns in Musical Lines. in Proceedings of the\nInternational Computer Music Conference 1985 ,\n(Vancouver, 1985), International Computer MusicAssociation, 391-397.\n[21] Narmour, E. Music Expectation by Cognitive Rule-Mapping.\nMusic Perception ,17(3). 329-398.\n[22] Pardo, B. Algorithms for Chordal Analysis. Computer Music\nJournal,26(2). (in press).\n[23] Pearson, W.R. Rapid and Sensitive Sequence Comparison\nwith FASTP and FASTA. Methods in Enzymology ,183. 63-\n98.\n[24] Roads, C. Autocorrelation Pitch Detection. in The Computer\nMusicTutorial , MITPress, 1996, 509-511.\n[25]Rolland, P.-Y. and Ganascia, J.-G. Musical pattern extraction\nand similarity assessment. in Miranda, E. ed. Readings in\nMusic and Artificial Intelligence , Harwood Academic\nPublishers, 2000, 115-144.\n[26] Sankoff, D. and Kruskal, J.B. Time Warps, String Edits, and\nMacromolecules: The Theory and Practice of SequenceComparison . Addison-Wesley, Reading, MA, 1983.\n[27] Simon, H.A. and Sumner, R.K. Pattern in Music. in\nKleinmuntz, B. ed. Formal Representation of Human\nJudgment , Wiley, NewYork, 1968.[28]Smith, T.F. and Waterman, M.S. Identification of Common\nMolecular Subsequences. Journal of Molecular Biology ,147\n(1). 195-197.\n[29] Stammen, D. and Pennycook, B., Real-Time Recognition of\nMelodic FragmentsUsingtheDynamicTimewarp Algorithm.inProceedings of the 1993 International Computer Music\nConference , (Tokyo, 1993), International Computer Music\nAssociation, 232-235.\n1To understand why, assume there are similar segments, (( i,\nk), (j,l)), that overlap, i.e. 0 ≤i<j ≤k<l<n. Then, there\nis some subsegment of ( i, k) we will call ( i, m),m<k,\ncorresponding to the overlapping region ( j, k)a n ds o m e\nsubsegment of ( k, l) we will call ( p, l),p>j, corresponding\nto (j, k). Thus, there are three similar segments ( i, m), (j, k),\nand (p, l) that provide an alternate structure to the original\noverlapping pair. In general, a shorter, more frequentpattern is preferable, so we do not search for overlappingpatterns.\n2An implementation note: for eachpair ofsimilar segments,\nthe starting points are implied bythe coordinates i, j,but we\nneed to store durations. Since we only search half of thematrix due to symmetry, we store one duration at location i,\njand the other at j, i."
    },
    {
        "title": "Managing Metadata.",
        "author": [
            "Dave Datta"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415230",
        "url": "https://doi.org/10.5281/zenodo.1415230",
        "ee": "https://zenodo.org/records/1415230/files/Datta02.pdf",
        "abstract": "The All Media Guide (AMG) is a technology company that maintains the world’s largest database of metadata relating to the entertainment industries. This document describes some of the goals of AMG, the issues uncovered during the evolution of our databases, and discusses some of the implementations we have chosen.",
        "zenodo_id": 1415230,
        "dblp_key": "conf/ismir/Datta02",
        "keywords": [
            "All Media Guide",
            "technology company",
            "world’s largest database",
            "metadata",
            "entertainment industries",
            "goals",
            "issues",
            "evolution",
            "databases",
            "implementations"
        ],
        "content": "Managing Metadata \nManaging Metadata \n David Datta \nAll Media Guide \n301 E. Liberty \nAnn Arbor, MI 48108 \ndavdat@allmusic.com  \nABSTRACT  \nThe All Media Guide (AMG) is a technology company that \nmaintains the world’s largest da tabase of metadata relating to  the \nentertainment industries. This document describes some of the \ngoals of AMG, the issues uncove red during the evolution of our \ndatabases, and discusses some of  the implementations we have \nchosen. \n1. INTRODUCTION \nThe All Media Guide began in 1991 as a hobbyist project to produce the book “The All Music Guide.” The book became the \nseed for a database that has now grown into the world’s largest \npublished collection of informa tion about music, movies, and \ngames.  \nAMG’s mission has e xpanded over the years. As a book, the \nmission was to “provide a consumer resource to the best \nrecordings.” As a database, the goal was to “provide complete e-\ncommerce enabling entertainment databases.” Now, as a technology company, AMG’s mission is to “Provide the industry \nstandard for entertainment c ontent management through the \ndevelopment of product inform ation databases, e-commerce \nenabling tools and proprietary cont ent technologies.” As the goals \nof the project have evolved, the da ta structures, data elements, and \nprocessing methods needed to suppor t them have changed to meet \nnew demands.  \nAMG's products are licensed by business customers who utilize the databases in a wide variety of  applications that were never \nenvisioned when the first All Music Guide book was created. E-commerce and content websites, encoding companies, consumer \nelectronics devices, media technol ogy companies, and in-store \nkiosks all make use of AMG metadata.  \nIn addition to licensing database content, AMG hosts a network of \npopular websites which serve as a showcase for AMG data and \ntechnology. The All Music Guide website ( www.allmusic.com ) \nthe All Movie Guide (www.allmovie.com), and All Game Guide (\nwww.allgame.com ) are invaluable resources for industry \nprofessionals. The All Music Guide by itself receives over 2 million hits a day. \n2. DEFINING THE PURPOSE OF YOUR \nDATA \nThe first step in the creation of a database is to define how the \ndata will be used. While there is a natural tendency to rush in and \nbegin collecting album titles, performer credits, and track titles, the time and effort spent planning prior to this step will greatly \nimpact on the sustainability and effect success of the project. \nInitially, and for that matter at every step of the way, these two statements will be true: (1) You will be able to think of more \ntypes of information to collect than you have the resources to \nsupport, and (2) No matter how long your “wish list,” you can’t \npossibly anticipate everything that will come up as the database \nmatures. \nThe end purpose of the database defines which elements are collected. Distributors and sales organizations typically keep Artist Name, Album Title, Record Label/Catalogue number, and UPC codes. File encoding and song identification companies \ntypically collect artist names, album titles and track titles. For \nthese companies, the metadata they collect is adequate for their \npurposes. At AMG we handle the product once and we enter \nevery objective element we can find. This includes extended \nmetadata elements such as track level copyrights, composers, and performers. \nScope of coverage is an importa nt design factor. Are out-of-print \ntitles needed? Is coverage of domestic catalogue enough? Do you \nhave international users? Consider  the question of in-print versus \nout-of-print. The current in-print  U.S. music catalogue is about \n185,000 albums; the out-of-print universe is somewhere between \n700,000 and 900,000 albums. \nThe increasing reality of modern technology is that there is no such thing as music which is out-of-print. It is expected that all \nrecorded music will become available in digital form. Early \ncoverage of this additional materi al provides a head start as music \nis re-issued. Presentation of info rmation about out-of-print music \nis useful in almost all applicati ons. Limiting searches to in-print \nalbum titles means many searches lead to no useful results. Users \ncan quickly reach a dead end; the inclusion of out-of-print titles \nprovides launching points which c onnect users to other albums \nand artists available to be purchased.  \n3. DATA ORGANIZATION \nOnce data elements and scope have been defined, data \norganization can begin. Choosing a good structure for musical \ninformation requires extensive knowledge of music, the product \nyou have available to collect data from, and the planned uses. \nMaking a wrong choice during the da tabase design stage impacts \nall aspects of the process; from data entry to publication. One \nmust consider not only database th eory and data structure size, but \nalso ease of maintenance, and ease of publication. There are as many approaches to the storage of metadata information about \nmusic as there are applications for the data itself. Structures that \nare suitable for importing and exporting between disparate systems (XML) are usually inefficient when applied to actual data \nmaintenance. AMG evolved its data  structures over many years to \nobtain good balances between data maintenance and data publication. \nConsider the following information about the pop and classical \nmusical catalogues.  \n Pop Classical \nAlbums 544,000 84,000 \nTracks/Performances 4,000,000 740,000 \nSongs/Works 150,000 200,000 \nComposers 133,000 18,000 \nPerformers 450,000 54,000 \n Managing Metadata \nPeopleProduction/\nPerformance\nCredits\nCompositions Performance s\nAlbums\n \nFigu re 1: Bas ic structural overview  of mu sic metad ata \n \nTypical for a pop album: \n1. Album  titles are almost alway s present. W hen not \npresent, conventions are known. (For example Peter \nGabriel’s albums are recognized as “Peter Gabriel 1,” \n“Peter Gabriel 2,” and “ Peter Gabriel 3.”) \n2. Composers often perform on the recording. \n3. Tracks, both CD and LP, contain a single song. \n4. Work/Song titles are individually  distinctive and \nfrequently  unique. \n5. Works/Songs are performed and released in their \nentirety . \nTypical for a clas sical album : \n1. Album  titles are frequently  not present. Of the 84,000 \nalbum s, less than 60,000 have actual titles and are not \nsimply a list of works on the album . \n2. Composers seldom perform on the recording. \n3. CD tracks  occur whenever th ere is a logical break in a \nwork being performed. Tracks on LPs are often absent \nwith the exception being side changes. \n4. Work/Song titles are descriptively  repetitive -- for \nexample: sonata, fugue, prelude, etc.. \n5. Works/Songs are seldom pe rformed in their entirety . \nWhile both pop and classical shar e the same basic blocks of \ninformation, designing a unified stru cture is not a trivial task and \nmay not be appropriate approach. In a clas sical databas e, most \ncompositions are well documented;  the structure to support the \namount of information available for classical works would be \nempty  for most pop songs. Similarly , an efficient structure for pop \ntracks would be extrem ely repetitive when used on a classical \nalbum  where a single work m ay be split over ten tracks with no \ninformation given as to how the work is divided. In pop, the main \nentry  points to the collection are via albums and names. It is \nusually  the case that a person w ill search for an album  title, a \nperform ing artist, or song title. In classical, a user will search for a \ncomposer, a com position title or a perform er. Searching for an \nalbum  title is frequently  a futile process. \nTo accom modate the different needs  of the us ers of thes e \ndatabases, AMG has im plemented separate structures and product \nhandling systems for pop and classical. Many  artists are logically  \nclassified as both pop and classical. Because of this AMG \nprocesses crossover product twice, once for each data set.  \nA single album may  have two re views , one with a classical \norientation, and one with a pop or ientation. Similarly , a person can have two biographies. Paul McCartney  is a minor footnote \nwithin the classical world. Luci ano Pavarotti’s videos are seldom  \nshown on MTV. \n4. INFORMATION AVAILABILITY \nThe lifecy cle of a music product begins months in advance of \nrelease. Beginning with the initial announcem ent and continuing \nlater as the product nears its releas e date, artists, record labels and \nprom otional agencies  make inform ation available. At this  stage \ncover im ages, track lis tings , and s ome perform er credits  may be \nfound at business-to-business web sites. In some cases, \npromotional copies of the album are sent out for review. \nEventually , the product is releas ed. AMG considers the release \nversion of the product to be the definitive source of metadata \ninform ation. The database will only be as good as the sources \nfrom which the information is gathered. You must know how to \nefficiently  replace and add data as the lifecy cle of an announced \nproduct evolves. \n5. LINKING AND INTEGRATION \nAfter identify ing data elem ents and sources, there is still the \nlogistical process of getting info rmation into the database and \nkeeping errors out. W hile it is a trivial task to collect metadata \nfrom products, data inconsistencie s are unavoidable.  Normalizing \nand linking data is the most complicated part of the creation \nprocess. \nThe s ame data can com e from  a variety  of sources ; each of these \nsources will have their own idio syncrasies. For exam ple, which \nEury thmics song title is correct?  “The King and Queen of \nAmerica” or “ The King & Queen of America” Depending on the \nsource, either form  of this title m ay appear. While one could argue \nthe product is the final arbiter, different releases of the same \nproduct may contradict each other. In som e cases, the product \nitself may  display  both. Names ha ve the additional problem of \nidentification. Which “John Smith” are y ou looking for?  AMG \nuses program matic tools to genera te context-sensitive suggestions \nthat expert editors  then confirm , revis e, or reject. \nEven the best integration and linking sy stem s will have cases that \nslip through. Automated and manua l feedback analy sis processes \nare needed to help detect data problem s.  \n6. ADDING CREATIVE CONTENT TO \nMETADATA \nOne of the m ost interesting facts we have discovered is that only  \n4% of the AM G webs ite hits  are the res ults of s earches . The \nwebs ites are an exploratory  tool; us ers becom e immersed in the \ninformation and follow links from person to person to album and \nbeyond. This type of browsing experience is only  possible \nbecaus e of the creation of relational content.  \nOur editorial s taff and freelancers , often in cons ultation with \nindustry  professionals, assign descri ptive content such as genres, \nstyles, key words, and moods. Analy sis of this descriptive data, \ncombined with editorially  created connections , recom mendations , \nand reviews provides AMG with the backbone to fulfill its goal of \nenabling people in their entertainm ent choices .  \n7. MAKING IT WORK; BREAKING DOWN \nTHE SKILL SETS \nAMG has five functional ty pes of st aff: Strategic, Editorial, Data \nEntry , Data Integration, and Support. \nThe Strategic group defines the purpose of the database. This \ngroup decides what data AMG will keep, how it will be used, and \ncoordinates efforts between the groups to ensure proper \nimplem entation. Managing Metadata \nThe job of the Editorial group, the experts, is to organize and \ncategorize the data. They  define the genres  and s tyles, create \nstandards for writing, and determin e how to apply  rating sy stems. \nThey  also resolve the inevitable am biguities that arise when \nworking with large amounts of data.  \nThe Data Entry  staff transcribes inform ation directly  from  the \nproduct into a format usable by the integration team.  \nThe Integration and Data P rocessing team  integrates  the entered \ndata into the m ain databases. They  are the experts in \nmanipulating, linking, and normaliz ing data. This team  is also \nresponsible for maintaining database  integrity  and data cleaning.  \nThe Support staff consists of ev eryone else. This group includes \nprogrammers, customer support, and all other non-data related \nfunctions. \n \nData Entry\nIntegration\nmaintenanc e\nDatabaseEditorial\nPublication\n(Databases, Books,\nWebsites)Data Integrity\nChecksCont ent Crea tion\n(Writing, Rating,\nCategori zation)\n \nFigure 2: Generaliz ed data flow  diagram \n8. LEGAL ISSUES \nThe notes  below are areas  to cons ider when us ing elem ents that \nare ty pically  used along with me tadata. Rights management is a \ncomplicated topic that requires  expertis e, therefore it is  important \nthat you consult with an intellectual properties lawy er to verify  \nthe legality  of your publications.  Factual m etadata data \nObjective facts are not covered under copy right. Pink Floyd is the \ncredited perform er on the album  titled “ The Dark Side Of The \nMoon” is an objective fact. It is generally  accepted that objective \nfactual inform ation can be published at will. \nAlbum cover artwork \nRights for cover images usually  are held by  the record label, the \nartist, the creator of the im age, or  subsequent rights holder. At this \ntime in the United S tates, it is  generally  accepted that album  \nartwork may  be used in conjuncti on with the sale of the album . \nSound Samples \nRoyalties m ust be paid each tim e a sam ple is played. The rights \nholder of the performance of the song owns sound samples. \nRecord labels , the m ost com mon owners , are extrem ely protective \nand have not hesitated to sue for infringement. \nLyrics \nThe Harry  Fox Agency  is one of the m any com panies  that acts  as \na clearing house for the publication of music in the United States. \nThey  are very protective of the rights of their m embers and will \nnot hesitate to sue for infringement. In the mid 1990’s they \nthreatened to sue all of the free acces s on-line databas es of ly rics \nand the sites were shut down. \nReviews and Biographies \nThe copy right laws dealing with text depend on the contractual \nagreements between th e author, the publisher, and the country  in \nwhich the text was  created. Re-publication alway s requires  \npermission and one must arrange proper clearances. \n9. TAKE THESE CONCEPTS HOME WITH \nYOU \nThis document has only  introduced a few of the more important \nissues y ou will encounter when developing y our m etadata \ndatabase. Each of the processes described here c ould be expanded \ninto its own paper. While y our ope rations may not need to be as \nlarge or com plex as  thos e of All Media Guide, to be successful, \nyou will need to im plem ent m any of these sy stem s. W hat I really  \nwant to em phasize is  whatever choices y ou make in database \ndesign, the most important choices will be those y ou m ake in \ndefining purpose and planning flexible way s to achieve it.   \n10. ACKNOWLEDGMENTS \nThanks to Richard Gilliam  for his assistance in the preparation of \nthis paper."
    },
    {
        "title": "A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music.",
        "author": [
            "Shyamala Doraisamy",
            "Stefan M. Rüger"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416022",
        "url": "https://doi.org/10.5281/zenodo.1416022",
        "ee": "https://zenodo.org/records/1416022/files/DoraisamyR02.pdf",
        "abstract": "In this paper we investigate the retrieval performance of monophonic queries made on a polyphonic music database using the n-gram approach for full-music indexing.  The pitch and rhythm dimensions of music are used, and the musical words (a term coined by Downie [2]) generated enable text retrieval",
        "zenodo_id": 1416022,
        "dblp_key": "conf/ismir/DoraisamyR02",
        "keywords": [
            "N-grams",
            "Polyphonic music",
            "Query-by-humming (QBH)",
            "Fault-tolerance",
            "Music information retrieval",
            "Monophonic queries",
            "Interval encoding",
            "Rhythmic ratios",
            "Full-music indexing",
            "MIDI format"
        ],
        "content": "A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \nA Comparative and Fault-tolerance Study of the Use of  \nN-grams with Polyphonic Music \nShyamala Doraisamy \nDept. of Computing \nImperial College \nLondon SW7 2BZ \n+44-(0)20-75948180 \nsd3@doc.ic.ac.uk Stefan Rüger \nDept. of Computing \nImperial College \nLondon SW7 2BZ \n+44-(0)20-75948355 \nsrueger@doc.ic.ac.uk\nABSTRACT \nIn this paper we investigate the retrieval performance of \nmonophonic queries made on a polyphonic music database using \nthe n-gram approach for full-music indexing.  The pitch and \nrhythm dimensions of music are used, and the musical words (a \nterm coined by Downie [2]) generated enable text retrieval \nmethods to be used with music retrieval.  We outline an \nexperimental framework for a comparative and fault-tolerance \nstudy of various n-gramming strategies and encoding precision \nusing six experimental databases.  For monophonic queries we \nfocus in particular on query-by-humming (QBH) systems.   Error \nmodels addressed in several QBH studies are surveyed for the \nfault-tolerance study.  Our experiments show that different n-\ngramming strategies and encoding precision differ widely in their \neffectiveness.  We present the results of our comparative and fault-\ntolerance study on a collection of 5380 polyphonic music pieces \nencoded in the MIDI format.   \n1. INTRODUCTION \nWith the advances in computer and network technologies, large \ncollections of digital music documents are being created and \nstored.  Managing these large collections requires effective \ncomputer-based music information retrieval (Music IR) systems \nwhere documents relevant to a user query can be retrieved \nquickly.   \nMusic documents are stored digitally in many formats and \ntherefore Music IR system designs frequently need to include \nsophisticated document and query pre-processing modules.  These \nformats have been generally categorized into (i) highly structured \nformats where every piece of musical information on a piece of \nmusical score is encoded, (ii) semi-structured formats in which \nsound event information is encoded and (iii) highly unstructured \nraw audio that encodes only the sound energy level over time.  \nMost current Music IR systems adopt a particular format and \ntherefore queries and indexing techniques are based upon the \ndimensions of music information that can be extracted or inferred \nfrom that particular encoding method [11]. An example would be \nto input queries by humming in the audio format to a database \nindexed on a collection of themes encoded using the Parson’s \ncode (a simple encoding that reflects only the directions of \nmelodies; each pair of consecutive notes is coded as “U” for up, \n“D” for down and “R” for repeat, i.e. when pitches are equal) [8].  \nWith query-by-example (QBE), one can input a recording where \nenergy profiles of the query and of the collection of audio \nrecordings are compared.  These are, however, still in early \nresearch stages [13].  There are several systems that use text-based input modes, of which many are designed for those musically \nliterate, where inputs are music notation keyed into text-boxes \n[14] or played on a graphically visualised keyboard.  These \nsystems usually do include options for those who are not \nmusically literate such as melodic contour inputs similar to the \nParson’s code.  The user would have the additional task of \nworking out the contour using this input mode. \nOf the various technologies of query processing and interface, one \nthat has been gaining popularity is query-by-humming (QBH).  \nThis can be said to be appealing to a large number of users, \nwhether musically literate or not.  A number of studies on QBH \nhave been performed and systems developed in recent years [4-9].  \nFor many who are shy singing in front of others [8], QBH may \nstill be an appealing choice with options such as private listening \nbooths available in music stores that can simply be extended as \nprivate QBH booths as well.  \nA known problem with IR systems in general is query precision, \nwhere documents are not retrieved with rank number one due to \nqueries not being specified precisely or just simply erroneous.  \nWith Music IR, the user-friendly, or rather music-friendly, queries \nthat are hummed are highly likely to be incorrect. What is required \nis that Music IR systems (in this case QBH systems) should work \nfor everybody – perfect singers or not [8].  Fault-tolerant or error-\ntolerant QBH systems are necessary to provide for the large \nnumber of unprofessional singers who wish to use the system.  \nMost of the QBH studies have been based on monophonic music.  \nThe query in this context, which is essentially monophonic (unless \na choir squeezes onto a microphone!), is made against a collection \nof monophonic pieces.  The vast majority of data collections \navailable today are in the polyphonic form.  With automated \nmelody and theme extraction systems still in research stages [3, \n15,16], extracting and indexing themes for the development of \nmonophonic databases that can be queried by humming is an \nonerous task in itself.    \nWe aim to study the querying of a database of polyphonic pieces \nwith a monophonic music sequence using the n-gram approach to \nfull-music indexing. An experimental design is outlined for a \ncomparative study and fault-tolerance study investigating various \nn-gramming strategies and encoding precision values of the \nmusical n-grams.  A survey of error models from several QBH \nstudies is done for the fault-tolerance study.    \nThe paper is structured as follows:  Section 2 presents a number of \nQBH studies and the error models that have been addressed.  \nSection 3 describes the n-gram approach to polyphonic music \nretrieval and the fault-tolerance of this approach.  Section 4 \noutlines the experimental setup using 5380 polyphonic music \npieces and discusses the error model adopted.  The retrieval \nperformance evaluation using the precision-at-15 measure is \npresented in Section 5. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM – Centre Pompidou A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \n2. QBH AND ERROR MODELS \nWork on Music IR systems has, in general, focused more on \ndevelopment rather than evaluation [2].  However, music IR \nevaluation test-beds and performance measures similar to the \nCranfield model in text retrieval are in their early development \nstages [17].  With a high probability of query imprecision or \ninaccuracies with QBH, predefined queries, responses and metrics \nfor evaluation would need to be based on QBH error models - \nhow well a system performs under such erroneous inputs.   A \nnumber of QBH studies have addressed error models [5-7].  \nIn an experimental study by McNab et al on the development of a \nQBH system [5], ten songs and ten singers were used to get an \nidea of the kind of input they could expect in their music retrieval \nsystem - to find out how people sang well known tunes.  The \nfindings had also been used by Downie [2] for the performance \nevaluation of the n-gram approach towards monophonic music \nretrieval.  The types of errors had been grouped into 4 classes: \n1. Expansion – a tendency to expand smaller intervals that \nfell within 1 and 4 semitones. \n2. Compression – a tendency to compress larger intervals \nthat were larger than 5 semitones. \n3. Repetition – a tendency to incorrectly repeat notes. \n4. Omission – a tendency to simply omit an interval. \nThe audio query transcription algorithm developed by Haus and \nPollastri [6] was based on an error model that is contrary to the \nidea from the study by McNab et al that singers tend to compress \nwide leaps and expand sequences of smaller intervals.  They \nassumed constant sized errors based on the idea that every singer \nhas his/her own reference tone in mind and dealt with obtaining \nthis reference tone in their study.  Singers would simply sing each \nnote relative to the scale constructed on their own reference tone, \napart from some small increases with the size of the interval. \nA tempo analysis was done in the QBH study by Kosugi et al [7].  \nIt was observed that singers decided on what tempo to maintain, \nwhich was not necessarily the same as that of the original song.  \nThe assumption adopted on tempo in their study was that for faster \nsongs there was a tendency for users to choose a tempo that was \nhalf the correct one. Fault-tolerance was addressed in the database \nby making two copies of songs of fast tempos, one at the original \ntempo and the other at half the tempo. \nIn these studies, the error models were based on a definition of \nhumming as singing with the syllable ta, da or la, and not \nwhistling or singing with syllables derived from lyrics.  Removing \nlyrics was suggested to remove another possible source of errors \nthat is difficult to quantify [6].  There is also the consideration \nthat, with singing based on lyrics, one could possibly remember \nthe tune better when it is associated with words. \n3. N-GRAMS AND FAULT TOLERANCE \nN-grams have been widely used in text retrieval, where query \nterms are decomposed into their constituent n-grams.  For \nexample, the word ‘music’ comprises the bi-grams mu,us,si and ic.  \nA character string formed from n adjacent characters within text is \ncalled an n-gram [1].  In music retrieval, an in-depth study of \nmelodic n-grams has been performed by Downie using \nmonophonic music data and interval-only representation [2].  A \ndatabase of folksongs was converted to an interval-only \nrepresentation of monophonic melodic strings.  Using a gliding \nwindow, these strings were fragmented into length-n subsections, \nor windows, called melodic n-grams. \nA study on the use of n-grams with polyphonic music retrieval, \nwhere the interval-only representation had been extended to include rhythmic information, was done in a previous paper [11].  \nThe strategy is to use all combinations of monophonic musical \nsequences from polyphonic music data: The gliding window \napproach is used to divide a music piece into overlapping \nwindows of n different adjacent onset times.  All possible \ncombinations of melodic strings from each window form musical \nn-grams.  To incorporate interval and rhythm information, for a \nsequence of n onset times, n-grams are constructed in the pattern \nform of: \n[ Interval1 Ratio1.. Intervaln-2 Ration-2Intervaln-1 ] \nFor a sequence of n pitches, an interval sequence is derived with \nn-1 intervals by: \ni i i Pitch Pitch Interval − = +1   (1)   \nFor a sequence of n onset times, a rhythmic ratio sequence is \nderived with n-2 ratios obtained by: \u0000\u0001\u0002\u0003\u0004\u0005−−=\n++ +\ni ii i\niOnset OnsetOnset OnsetRatio\n11 2 (2)   \nMusical words [2] obtained from encoding the n-grams with text \nletters are used in the indexing and database construction.  Queries \nare processed similarly, which means that the queries might be \npolyphonic – although we use monophonic queries in this study. \nWith queries generated from erroneous inputs, such as in QBH, \ncorrect retrieval is possible using the n-gram approach to music \nretrieval.  An erroneous query string would generate a number of \nn-grams that are incorrect out of the total number of n-grams \nconstructed.  The probability of retrieving a query correctly would \ndepend on the number of n-grams that are incorrect. Using \nMcNab’s error model described in Section 2 and the value of n=4, \nwe illustrate the fault-tolerance of the n-gram approach using the \ntheme from Mozart’s Variations in C, K265, “Ah! Vous dirai-je, \nMaman” (Twinkle Twinkle Little Star), adapted from Barlow and \nMorgenstern [12] as shown in Figure 1. \n \n \n \n \n \n \n            Figure 1. Theme from “Ah! Vous dirai-je, Maman” \nIn using interval-only representation, n-grams are constructed \nbased on the interval distance (in semitones) and direction using \nEquation 1. Therefore, the n-gram constructed using the melodic \nsequence from the first window would be represented as [0 +7 0].  \nWith the gliding window approach, n-grams would be repeatedly \ngenerated in this pattern to the end of the excerpt.  To obtain \n‘musical words’, the n-grams are encoded using text letters, see \nfor details below. \nThe set of n-grams generated from Figure 1 are: {[0 +7 0], [+7 0 \n+2], [0 +2 -2], [+2 0 -2], [0 -2 -2], [-2 0 -2], [0 -2 0], [-2 0 -1], [0 \n-1 0], [-1 0 -2], [0 -2 0], [-2 0 +2], [0 +2 -4]}. \nTo illustrate the fault-tolerance, two examples of possible errors, \ncompression and omission, are incorporated into the query string \nas shown in Figure 2.  \n \n[0  +7  0] \n[+7  0  +2] [-2  0  +2] \n[0  +2  -4] A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \n \n \n \n \n \n \n \n \n \n \nFigure 2. Compression and omission errors \n \nThe set of n-grams generated from Figure 2 are: {[0 +6 0], [+6 0 \n+3], [0 +3 0], [+3 0 -2], [0 -2 0], [-2 0 -2], [0 -2 0], [-2 0 -1], [0 -1 \n0], [-1 0 -2], 0 -2 0], [-2 0 -2]}. \nIn this example, around 60% of the n-grams generated from the \nerroneous input of Figure 2 are similar to those from the excerpt in \nFigure 1.  If this number of n-grams still sufficiently represents the \nindexed relevant document unambiguously, perfect retrieval is \nhighly feasible.  \nIn our previous study, we had investigated the fault-tolerance of \nthe encoding precision of the n-grams. With large numbers of \npossible interval values and ratios to be encoded, and a limited \nnumber of text representations, classes that clearly represented a \nparticular range of intervals and ratios without ambiguity were \ninvestigated.  Based on an analysis of the interval distribution, the \ninterval encoding precision was varied based on a mapping \nfunction given by :   \u0000\u0000\u0001\u0002\u0003\u0003\u0004\u0005\u0000\u0001\u0002\u0003\u0004\u0005=−\nYIntervaX Code1nltanh int   (3) \nIn Equation (3), X is a constant set to 27 in this study to limit the \ncode ranges to 26 text letters.  With Y = 24, a 1-1 mapping of \nsemitone differences in the range [-13, 13] is obtained.  Less \nfrequent semitone differences (which are bigger in size) are \nsquashed and have to share codes.  Y determines the rate at which \nclass sizes increase as interval sizes increase.  This is a trade-off \nbetween classes of small (and frequent) versus large (and rare) \nintervals.  The codes obtained were then mapped to the ASCII \ncharacter values for letters.  To encode the interval direction, \npositive intervals were encoded as uppercase letters A-Z and \nnegative intervals were encoded with lower case letters a-z and, in \nthe centre, code 0 represented with the numeric character 0. \nIn this study we continue to adopt the mapping function for the \ninterval encoding in studying fault-tolerance. With the possible \nerrors from the example above, in using a 2-1 mapping, the \nintervals +6 and +7 would have been encoded with the same text \nletter.  This would have been more fault-tolerant towards the \ncompression error in Figure 2. More detailed interval \nclassification schemes were investigated by Downie [2] and it was \nconcluded in his study that the expected fault tolerance through \nthe application of the classification scheme was not evident. The \nParson’s code used in the QBH study performed by Prechelt and \nTypke [8] was said to be highly fault-tolerant on a database of \nmonophonic themes.  However, we performed a preliminary test, \nretrieving from a database of polyphonic pieces using the n-grams \napproach with Parson’s code where hardly any queries were \nretrieved, hence this approach was not adopted either.   In order to represent the rhythm dimension, we base our ratio \nencoding on the frequency distribution of ratio values of the data \ncollection.  A graph of frequency versus the logarithm of the ratios \n(onset times were obtained in units of milliseconds) was used to \nfind peaks that clearly identified significant ratios. Ratio bins were \nconstructed based on the midpoints between the identified ratio \npeaks.  These bins provided appropriate quantisation ranges for \ntiming deviations with performance data.  Ratio 1 had the highest \npeak and other peaks occurred in a symmetrical fashion where for \nevery peak ratio, there was a symmetrical peak value of 1/peak \nratio.  The peaks identified as ratios greater than 1 were 6/5, 5/4, \n4/3, 3/2, 5/3, 2, 5/2, 3, 4 and 5.  Ratio 1 was encoded as Z.  The \nbins constructed based on peak ratios greater than 1 were encoded \nwith uppercase letters A-I.  Any ratio above 4.5 was encoded as Y.  \nThe bins for ratios smaller than 1 based on the peaks 5/6, 4/5, 3/4, \n3/2, 3/5, 1/2, 2/5, 1/3, 1/4 and 1/5 were encoded with lowercase \nletters a-i and y respectively.  Wider ratio bins (merging A and B, \nC and D, etc) were investigated for fault-tolerance with rhythmic \ndeviations.  \n4. EXPERIMENTAL SETUP \nThe aim of this study is to test the retrieval performance of \nquerying a polyphonically encoded database with monophonic \nqueries using the n-gram approach to full-music indexing, similar \nto full-text indexing in text IR. Various n-gramming strategies and \nencoding precision of the musical n-grams are investigated.  A \nfault-tolerance study based on the error model used in the QBH \nstudy of McNab et al [5] is adopted.  \nA collection of 5380 polyphonic music pieces in the MIDI format \nwas used for the experimental databases.  10 monophonic excerpts \nextracted from this collection were used as experimental queries.  \nThese were popular tunes of various genres, where the main theme \nof a particular piece was extracted.  For the classical pieces, \nthemes were adapted from the Dictionary of Musical Themes [12].  \nFor each query, there were several performances that were \nconsidered as documents relevant to the retrieval process in this \ncontext.  The list of songs and the number of associated relevant \ndocuments in the collection are listed in Table 1.   \nTable 1. Song list \nSong ID Song Title No. relevant \n1 Alla Turka (Mozart) 5 \n2 Happy Birthday 4 \n3 Chariots of Fire  3 \n4 Etude No. 3 (Chopin) 1 \n5 Eine kleine Nachtmusik \n(Mozart) 5 \n6 Symphony No. 5 in C \nMinor, (Beethoven) 8 \n7 The WTC, Fugue 1, Bk \n1 (Bach) 2 \n8 Für Elise (Beethoven) 3 \n9 Country Gardens 2 \n10 Hallelujah (Händel) 7 \n \nWe adopted a similar assumption for relevance as Uitdenbogerd \nand Zobel [3] where an arrangement was considered to be distinct \nfrom other arrangements if one of the following conditions held: \n(i) it was in a different key, (ii) there was a different number of compression omission \n[0 +6 0] \n[+6 0 +3] [-2 0 -2] [0 –2 0] A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \nparts in the arrangement or (iii) there were differences in rhythm, \ndynamics or structure.  Query lengths varied between 15-25 notes \nfor eight of the songs.  Song 6 had just 8 notes and song 10 was \nwith 285 notes. \n4.1 Database development \nSix experimental databases were developed using the value of n=4 \nwith various n-gramming strategies and encoding precision values. \nThe Lemur Toolkit was used in the database development; it is a \nresearch-based toolkit that supports the construction of basic text \nretrieval systems based on language models [10].  Other retrieval \nmodels are also supported and these include the vector space \nmodel and the probabilistic model.  These models use the tf-idf \n[20] and the Okapi BM25 retrieval function for weighting [19], \nrespectively.  Based on several initial tests on the tool using \nknown-item searches, the probabilistic model with the Okapi \nBM25 function for weighting performed best with musical n-\ngrams and hence was adopted for this study.  A description of the \ndatabases developed follows:   \nPR4: The pitch and rhythm dimensions are used for the n-gram \nconstruction as described in Section 3.  For interval encoding, the \nvalue of Y in Equation 3 is set to 24.  All bin ranges that had been \nidentified as significant, as listed in Section 3, were used for the \nratio encoding. \nPR4CA: The pitch and rhythm dimensions are used for the n-\ngram construction, as described in Section 3.  For interval \nencoding precision that is coarser, the value of Y in Equation 3 is \nset to 48 for a 2-1 mapping of most intervals smaller than 20 \nsemitones (1 character now covers at least 2 semitones).  For \ncoarser ratio encodings, wider ratio bin ranges are used.  Half of \nthe text alphabets used with PR4 were used, where one character \ncovers two ratio bins.  \nAL1: In querying a polyphonic database using a monophonic \nmelodic sequence, n-grams generated from polyphonic documents \nthat are likely to include accompaniment would be matched \nagainst n-grams generated from a simple melody.  A possible \nstrategy to overcome the problem of n-gramming based on these \nintercepting accompaniment onsets is to reduce n-grams that are \ngenerated from only accompaniment onsets.  This database was \nindexed using n-grams generated from alternate onsets and not \nevery adjacent onset as in the two previous databases.  Interval \nand ratio encoding is similar to PR4. \nAL2:  Similar to AL1 but n-grams were generated not from every \nother onset of the gliding window approach but by skipping two \nonsets.   \nAM: An approach to automate the development of databases of \nmonophonic melodies from a polyphonic music collection by \napplying melodic extraction algorithms was proposed by \nUitdenbogerd and Zobel [3].  In the study, several melodic \nextraction algorithms were investigated and the approach in which \nall top notes were extracted for a given polyphonic music piece \nperformed best.  All MIDI channels were combined and the \nhighest note from each simultaneous note event was kept.  The \nmethod had been referred to as all-mono.  We developed this \ndatabase by n-gramming the highest note from all simultaneous \nnote events and not all possible patterns within a gliding window \nas described in Section 3.  Interval and ratio encoding is similar to \nPR4.  \nP4:  Only the pitch dimension was used for the n-gram \nconstruction.  With QBH, there is a high probability of rhythm not \nbeing adhered to at all.  This database was developed to study the \nusefulness of retrieving with pitch only.  The encoding of pitch is \nagain similar to PR4.   4.2 Error simulation \nFor the error simulation in our fault-tolerance study, we adopted \nthe error model based on the study by McNab et al [5] as \ndiscussed in Section 2.  The possible errors on scale differences \nassumed in the study by Haus and Pollastri [6] and tempo \ndifferences by Kosugi et al [7] were not adopted for this study. \nThe use of intervals and rhythmic ratios in the n-gram \nconstruction is fault-tolerant to such errors where intervals are \ninvariant to transpositions and scale differences and rhythmic \nratios are invariant to augmentation and diminution of tempo.  \nIn the study by Downie [2] that adopted the similar error model \nbased on the study by McNab et al [5], the query lengths and \nnumber of notes that were simulated with errors were constant.  \nHowever, with real queries possibly varying in length, we do not \nfix the query length. For this initial study using varying query \nlengths, error probability at 10% and 20% for each query note was \ninvestigated.    \n5. RESULTS \nFor the performance evaluation, we used the precision-at-15 \nmeasure, in which the performance of a system is measured by the \nnumber of relevant melodies amongst the first k retrieved, with \nk=15 in our case.  The results are shown as the percentage \nretrieved from the number of relevant documents for each song \nquery in Table 2.  The retrieval performances of the 10 queries are \naveraged, weighted by the relevant documents for each of the \ndatabase indexing method in the last column (W.A.).   The values \nhave been rounded to the nearest integer. \nTable 2. Percentage retrieved with perfect queries \nSong \nID PR4 PR4CA AL1 AL2 AM P4 \n1 100 0 0 0 100 0 \n2 50 25 50 25 50 25 \n3 0 0 0 0 0 0 \n4 0 0 0 0 100 0 \n5 100 40 100 100 100 0 \n6 13 0 0 0 100 0 \n7 100 50 100 50 100 0 \n8 33 33 33 0 67 67 \n9 50 50 50 50 50 0 \n10 86 14 71 43 86 0 \nW.A. 58 18 40 34 80 8 \n \nIn investigating methods of querying a polyphonic database with a \nmonophonic query, it is clear from the results that preprocessing a \npolyphonic database for indexing with a melody extraction \nalgorithm is a feasible approach as had been studied previously \n[3].  On average, 80% of the relevant documents were retrieved \nwithin rank 15.   \nHowever, looking at full-music indexing of polyphonic music, \nPR4 performed well despite the large number of index terms \ngenerated from n-gramming all possible patterns of polyphonic \nmusic data.  58% of the relevant documents were retrieved on \naverage within rank 15.  The performance of PR4 clearly indicates A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \nthat using n-grams in querying a polyphonically encoded database \nwith monophonic query is a promising approach.   \nIn comparing the retrieval performance of the various songs \nbetween AM and PR4, two of the largest retrieval measure \ndifferences were of Songs 4 and 6.  Song 4 had a large number of \naccompaniment notes interleaved between the melody lines in \ncomparison to the other songs retrieved perfectly by AM, namely \nSongs 1, 5 and 7. This is one possible reason for the poor retrieval \nof this song that requires further investigation.  The query length \nof Song 6 of just 8 notes was just not sufficient for retrieval of this \nlarge movement of a symphony.   \nThe problem of intercepting accompaniment onsets was not \novercome by alternating n-grams with AL1 and AL2.  Skipping \nmore onsets would have to be investigated.  Other songs not \nretrieved based on this problem as well were Song 3 and versions \nof songs 2, 8, 9 and 10.  With AL2, short query length as with \nSong 6 posed a problem where no query document could possibly \nbe generated.  The addition of the rhythm dimension clearly \nimproves retrieval, as can be seen from the weak retrieval \nperformance of P4 in comparison to PR4.   \nFault-tolerance investigation was performed by simulating errors \nin the queries with the probability of error levels at 10% and 20% \nfor each of the query notes. The erroneous notes were simulated \nwith errors based on the study by McNab et al [5] using a \nprobability of 0.4 for Repetition, 0.4 for Modification (Expansion \nor Compression) and 0.2 for Omission.  The retrieval performance \nmeasures were obtained by averaging the performance results of \nten retrieval runs for each song.  The results are shown in Table 3 \nand Table 4. \nThe retrieval performance for all databases deteriorated under \nerror condition as expected with the increase of erroneous notes.  \nIt is also clear from the results that retrieval was not completely \nlost due to erroneous query notes with the n-gram approach as \ndiscussed in Section 3.  The performance of AL1 remained almost \nsimilar at 40% and 39% under perfect and error conditions (of \nerror probability of 10%) respectively.  This fault-tolerance would \nbe further investigated. \n  \nTable 3. Percentage retrieved with error probability of 10% \nSong \nID PR4 PR4CA AL1 AL2 AM P4 \n1 10 0 0 4 60 0 \n2 40 5 43 16 50 10 \n3 0 0 0 0 0 0 \n4 0 0 0 20 100 0 \n5 92 28 90 86 92 0 \n6 10 0 10 0 89 0 \n7 85 45 85 45 90 0 \n8 30 30 26 0 47 27 \n9 50 40 50 40 50 0 \n10 86 19 71 40 86 0 \nW.A. 43 14 39 25 70 3 \n \n Table 4. Percentage retrieved with error probability of 20% \nSong \nID PR4 PR4CA AL1 AL2 AM P4 \n1 0 0 0 0 2 0 \n2 28 3 23 5 50 0 \n3 0 0 0 0 0 0 \n4 0 0 0 0 90 0 \n5 82 24 84 9 90 0 \n6 9 0 0 0 78 0 \n7 70 15 75 35 70 0 \n8 30 23 7 0 40 0 \n9 50 40 50 50 50 0 \n10 86 9 71 26 82 0 \nW.A. 38 9 32 10 58 0 \n \n6. FUTURE WORK \nOur experiments suggest following future directions: The problem \nof poor retrieval for documents with large number of intercepting \naccompaniment onsets compared to the melody line has to be \ninvestigated alongside optimal query lengths.  The indexing and \nquery processing strategies would be investigated more \nexhaustively, such as querying a database indexed with the PR4 \nmethod with queries processed with the AL1 and AL2 method.  \nThe possibility of results list fusion where each of the independent \nretrieval strategies is merged in some way that would improve \nretrieval effectiveness [18] would be investigated.   \nFurther study would be required for rhythmic error models.  Error \nmodels not just on studies done on QBH but aural and perception \nstudies would be incorporated.  Coarser encodings for rhythm and \ninterval information would be further investigated individually for \nfault-tolerance.   \nA small query sample has been used for this initial investigation.  \nLarger samples from various genres would have to be investigated \nindependently. \n7. CONCLUSIONS \nThis study shows that full-music indexing of a polyphonic music \ncollection using the n-gram approach is promising.  The strategy \nof using all combinations of monophonic musical sequences from \npolyphonic music data does not altogether overwhelm the \nindexing and retrieval of polyphonic music.  With large \ncollections of polyphonic music available and the complexity of \nautomating melody extraction for the development of monophonic \ndatabases, we have shown a method of retrieving from a \npolyphonically encoded database without the need to preprocess \nthe database with melody extraction algorithms. \nWith the high probability of queries generated from erroneous \ninputs with QBH, a framework for a fault-tolerance study based \non QBH error models has been outlined.  Results show that a \nfusion of retrieval strategies may be needed in the development of \na more fault-tolerant system.  \nThe use of n-grams with full-music indexing in polyphonic music \nretrieval is a promising approach for both queries in the A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music \npolyphonic form as shown in our previous study [11] and with \nmonophonic queries in this study. \n8. ACKNOWLEDGEMENTS \nThis work is partially supported by the EPSRC, UK. \n9. REFERENCES \n[1] H.S. Heaps, Information Retrieval: Computational and \nTheoretical Aspects, Academic Press, 1978. \n[2] J. Stephen Downie, Evaluating A Simple Approach to Music \nInformation Retrieval: Conceiving Melodic N-Grams As \nText, PhD Thesis, University of Western Ontario,1999.  \n[3] Alexandra Uitdenbogerd and Justin Zobel, Melodic Matching \nTechniques for Large Databases, ACM Multimedia ’99, \nOrlando, FL, USA. \n[4] Asif Ghias, Jonathan Logan, David Chamberlin and Brian C. \nSmith, Query By Humming – Musical Information Retrieval \nin an Audio Database, ACM Multimedia ’95 – Electronic \nProceedings, San Francisco, CA, Nov. 1995. \n[5] Rodger J. McNab, Lloyd A. Smith, Ian H. Witten, Clare L. \nHenderson and Sally Jo Cunningham, Towards the Digital \nMusic Library: Tune Retrieval from Acoustic Input, DL ’96, \nBethesda MD USA. \n[6] Goffredo Haus and Emanuele Pollastri, An Audio Front End \nfor Query-by-Humming Systems, 2nd International \nSymposium on Music Information Retrieval, ISMIR2001, \nIndiana, USA, Oct 2001, pp 65-72. \n[7] Naoko Kosugi, Yuichi Nishihara, Tetsuo Sakata, Masahi \nYamamuro and Kazuhiko Kushima, A Practical Query-By-\nHumming System for a Large Music Database, ACM \nMultimedia 2000, Los Angeles, CA., Nov. 2000. \n[8] Lutz Prechelt and Rainer Typke, An Interface for Melody \nInput, ACM Transactions on Computer-Human Interaction, \nVol. 8, No.2, June 2001, pp 133-149. \n[9] Pierre-Yves Rolland, Gailius Raskinis and Jean-Gabriel \nGanascia, Musical Content-Based Retrieval: An Overview of \nthe Melodiscov Approach and System, ACM Multimedia ’99, \nOrlando, FL, USA. \n[10] Lemur toolkit, http://www-2.cs.cmu.edu/~lemur [11] Shyamala Doraisamy and Stefan Rüger, An Approach \nTowards A Polyphonic Music Retrieval System, 2nd \nInternational Symposium on Music Information Retrieval, \nISMIR2001, Indiana, USA, Oct 2001, pp 187-193 \n[12] Harold Barlow and Sam Morgenstern, A Dictionary of \nMusical themes, London: Ernest Benn, 1949.  \n[13] Jonathan Foote, ARTHUR: Retrieving Orchestral Music by \nLong-Term Structure, 1st International Symposium on Music \nInformation Retrieval, ISMIR2000, Massachusetts, USA, Oct \n2000. \n[14] Andreas Kornstadt, Themefinder: A Web-Based Melodic \nSearch Tool, Computing in Musicology 11, 1998, MIT Press. \n[15] Lloyd Smith and Richard Medina, Discovering Themes by \nExact Pattern Matching, 2nd International Symposium on \nMusic Information Retrieval, ISMIR2001, Indiana, USA, Oct \n2001, pp 31-32. \n[16] Colin Meek and William P. Birmingham, Thematic \nExtractor, 2nd International Symposium on Music \nInformation Retrieval, ISMIR2001, Indiana, USA, Oct 2001, \npp 119-128. \n[17] J. Stephen Downie, Wither Music Information Retrieval: Ten \nSuggestions to Strengthen the MIR Research Community, \n2nd International Symposium on Music Information \nRetrieval, ISMIR2001, Indiana, USA, Oct 2001, pp 219-222. \n[18] Alan F. Smeaton and Francis Crimmins, Using a Data \nFusion Agent for Searching the WWW, WWW6 Conference, \nStanford,USA,1997. \n[19] S. Walker, S.E. Robertson, M. Boughanem, G.J.F. Jones, K. \nSpärck Jones, Okapi at TREC-6: Automatic ad hoc, VLC, \nrouting, filtering and QSDR, NIST Special Publication 500-\n240, The Sixth Text Retrieval Conference (TREC-6), 1998. \n[20] Gerard Salton, Automatic Text Processing: The \nTransformation, Analysis, and Retrieval of Information by \nComputer, Addison-Wesley, 1989."
    },
    {
        "title": "Toward a Theory of Music Information Retrieval Queries: System Design Implications.",
        "author": [
            "J. Stephen Downie",
            "Sally Jo Cunningham"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417565",
        "url": "https://doi.org/10.5281/zenodo.1417565",
        "ee": "https://zenodo.org/records/1417565/files/DownieC02.pdf",
        "abstract": "This paper analyzes a set of 161 music-related information requests posted to the rec.music.country.old-time newsgroup. These postings are categorized by the types of detail used to characterize the poster's information need, the type of music information requested, the intended use for the information, and additional social and contextual elements present in the postings. The results of this analysis suggest that similar studies of 'native' music information requests can be used to inform the design of effective, usable music information retrieval interfaces.",
        "zenodo_id": 1417565,
        "dblp_key": "conf/ismir/DownieC02",
        "keywords": [
            "Music Information Retrieval",
            "Queries",
            "System design",
            "User needs",
            "Bibliographic information",
            "Lyrics",
            "Genre",
            "Social context",
            "Newsgroups",
            "Data analysis"
        ],
        "content": "Toward a Theory of Music Information Retrieval Queries: System Design Implications\nToward a Theory of Music Information Retrieval Queries:\nSystem Design Implications\nJ. Stephen Downie\nGraduate School of Library and Information Science\nUniversity of Illinois at Urbana-Champaign\nRoom 103 LIS Building\nChampaign, Illinois, USA\nPhone: +1(217) 351-5037\njdownie@uiuc.eduSally Jo Cunningham\nDepartment of Computer Science\nUniversity of Waikato\nPrivate Bag 3105\nHamilton, New Zealand\nPhone: 64-7-838-4402\nsallyjo@cs.waikato.ac.nz\nABSTRACT\nThis paper analyzes a set of 161 music-related information\nrequests posted to the rec.music.country.old-time newsgroup.\nThese postings are categorized by the types of detail used to\ncharacterize the poster's information need, the type of music\ninformation requested, the intended use for the information, and\nadditional social and contextual elements present in the\npostings. The results of this analysis suggest that similar studies\nof 'native' music information requests can be used to inform the\ndesign of effective, usable music information retrieval\ninterfaces.\n1. INTRODUCTION\nInterest in the development of content-based music information\nretrieval (MIR) systems is growing rapidly. The MIR research\ncommunity consists of a multidisciplinary amalgam of\nlibrarians, digital librarians, information scientists, computer\nscientists, musicologists, audio engineers, lawyers and business\npersons. This multidisciplinary approach has given rise to\nsignificant technological advancements in retrieval algorithms,\naudio interfaces and data representation schemes.\nNotwithstanding these technological advancements, MIR\nresearch is currently a systems-centered research domain. For a\nvariety of reasons—including intellectual property law, limited\naccess to substantial, multi-genre, multi-format collections and a\nlack of a historical user-base—MIR research has hitherto been\nunable to develop and exploit data concerning the nature of real-\nworld user needs and use of music information.  Although [2]\nand [1] have both commented on the paucity of research\ninvolving real users of MIR systems, there have been limited\nattempts to garner information about and from users. The two\nmost notable examinations of MIR system usage in the research\nliterature are a partial analysis of the OPAC transaction logs\nfrom a music library [4] and a log analysis of text and audio\nqueries to a testbed collection of bibliographic and MIDI files\n[6]. The insights gained in these studies are necessarily limited\nto the systems that generated the usage data—that is, the studies\nprovided valuable hints as to the usability of features\nimplemented in the two MIR systems, but the data could not\nprovide insight into what additional search facilities or output\ntypes that users desire. Moreover, the testbed collection whose\nusage was studied in [6] is an artificial collection created to test\nthe facilities of the MELDEX MIR software [5]—and so it is notclear that this log analysis describes ‘real’ information needs.\nMIR research is being hindered by its lack of user-centered\ntheoretic or empirical groundings for its systems design\ndecisions. It is the purpose of this paper to address this obvious\nneed by laying out an empirically derived theoretic framework\nfor the categorization of MIR queries through the context of the\nneeds and uses made manifest in a collection of unsolicited and\nspontaneous real-world music queries. We will also demonstrate\nthe possible design implications that emerge from our query\nanalyses and classifications.\n2. DATA GATHERING\nThis study analyzed 161 music queries posted to the\nrec.music.country.old-time  Usenet newsgroup [3] during the\nperiod July 2001 – Jan 2002. During this period, approximately\n760 ‘threads’ are represented. Each thread was manually\nexamined, and only the 161 postings which began a thread and\nwhich contained a music-related request were retained. This\nnewsgroup was selected as a source of queries for several\nreasons: it is an active newsgroup (by February 2002, the\ngroup’s archive contained over 32,800 discussion ‘threads’); the\ngroup focuses tightly on discussions of music and music events;\nand the discussions are relatively ‘serious’—that is, the\nparticipants focus on the genre, rather than on personalities or\ngossip. Satisfying an information need related to old-time music\nis not a simple matter of browsing in a CD shop or running a\nquick search on a peer-to-peer file-sharing service\nWhy analyze postings to a newsgroup? Given the dearth of query\ndata available for analysis from formal MIR systems, we sought\na source of authentic music information requests. This\nnewsgroup provides such a source of queries, and in practice\nfunctions as a human-based MIR ‘system’: it is effectively an\nold-time music reference desk, run by volunteers keen to share\ntheir expertise in and passion for old-time music. The ‘interface’\nto this people-powered system is flexible and usable: the users\ncan post natural language requests, expressing their information\nneeds in their own words and unrestricted by the artificial\nconstraints of a search syntax. These requests contain rich\ncontextual details and background information, permitting the\nresearcher a greater insight into search motivations than is\ngenerally available in, for example, a transaction log from a\ncomputer-based IR system.  Further, information requests to a\nformal IR system are often constrained by the user’s pre-\nconception of what types of information or document formats are\navailable—the user tailors requests (consciously or\nunconsciously) to what s/he thinks could be retrieved from that\nsystem. In contrast, newsgroup readers recognize that posted\nrequests to a newsgroup could literally retrieve anything—a\ndesired fact, an opportunity to purchase a much-desired album, aPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on the\nfirst page.\n© 2002 IRCAM – Centre PompidouToward a Theory of Music Information Retrieval Queries: System Design Implications\npointer to an MP3 file, and so forth. The text-based format of\nnewsgroup postings limits this study to primarily text-based\nqueries, and so we cannot examine in any depth the audio or\nsymbolic techniques that people may wish to use when\ndescribing a music information need. Even in the spare, ASCII\nnewsgroup interface—newsgroup posters include transcriptions\nand links to audio files in their queries.\n3. ANALYTIC CATEGORIES AND DATA\nFour principal analytic categories emerged from our reading of\nthe queries:\n• Information need  (see Table 1)\n• Desired outcomes (see Table 2)\n• Intended uses of information (see Table 3)\n• Social and contextual elements\nThe data from the first three analytic categories are easily\nquantified and are presented in relatively self-evident summary\nform below (unfortunately, space precludes more detailed\nexplication of the categories). The fourth category, however,\ndefies prima facie  quantitative summarization as it involves the\nqualitative description of extra-musical information that users\npresented to better contextualize their queries. For example,  an\nassociative or environmental context of the desired music-\nrelated information was mentioned in 18% (30) of the postings\nanalyzed . By this, we mean that the posting mentioned a social\nconnection that brought the music to the requestor’s attention, or\nan emotional/social memory associated with the music, or that\nthe music is associated with an event other than simply listening\nto a CD or LP (“I've heard it at a couple of jam sessions”, “ex-\nfriend of mine … used to sing a song when bowling”, “a\nmandolin class”).\nIn some cases, this associative context may be helpful in\nsatisfying the information need; for example, the detail that a\nsong was heard by the requestor “Last week at the Fraley\nfestival” is potentially useful to responders, as the responders\nmay have also attended the festival, may have access to the\nfestival programme, or may use background knowledge about\nthe type of music included in that festival to identify the song in\nquestion.\nTable 1. Features used to describe the information need.\n Information need description  Percentage  Occurrences\n BIBLIOGRAPHIC  75.2%  121\n LYRICS  14.3%  23\n GENRE  9.9%  16\n SIMILAR WORKS  9.9%  16\n AFFECT  7.5%  12\n LYRIC STORY  6.8%  11\n TEMPO  2.5%  4\n EXAMPLE  1.8%  3\nTable 2. Characterizations of desired information\n Outcome type  Percentage  Occurrences\n BIBLIOGRAPHIC  35.4%  57\n LYRICS  29.8%  48\n RECORDING  16.8%  27\n NOTATION  13.0%  21\n BACKGROUND  13.0%  21\n RESOURCE  5.0%  8\n OTHER  2.5%  4Table 3 . Intended uses for requested music information.\nCategory Percentage Occurrences\nLOCATE 49.7% 80\nRESEARCH 19.3% 31\nPERFORM 18.6% 30\nCOLLECTION BUILDING 18.0% 29\nLISTEN 6.8% 11\n4. SUMMARY AND FUTURE RESEARCH\nOur study analyzed a set of music information queries across\nseveral facets: we examined the types of details presented to\ndescribe the information need, the types of information that are\ndesired to satisfy the information need, the explicitly described\nintended uses for the information, and the\nsocial/environmental/associative contexts mentioned in the\nrequest.  This study suggests that findings from studies of music\ninformation requests can be used to inform the development of\neffective, usable MIR system interfaces, as well as to indicate\nthe types of documents or document representations required to\nsupport specific information needs.\nHowever, these queries are drawn from a single source and\ndescribe music information needs based around a single genre.\nFurther studies from a variety of sources of music information\nrequests, across a variety of musical genres drawn from diverse\ncultures, are required to validate the categories presented in\nmusic groups are a rich source of music query data, as are the\nmultitudinous email discussion lists and website dedicated to\nparticular types of music or groups of music appreciators.  It is\nalso important to move beyond the limitations of text-based\nquery description, to capture the multi-modal richness of real-\nworld, spontaneous “music” inputs—for example, to record and\nanalyze queries including hummed and sung snippets of songs.\nThese latter types of queries are more difficult to obtain; we\nplan, for example, to negotiate with producers of a popular radio\nprogramme to gain access to the musical queries phoned in to\ntheir on-air experts.\n5. REFERENCES\n[1] Byrd, D., and Crawford, T. Problems of music information\nretrieval in the real world. Information Processing &\nManagement  38(2), 249-272.\n[2] Downie, J.S. Music information retrieval. To appear in\nAnnual Review of Information Science and Technology 37 .\nLearned Information, Medford NJ, (in press).\n[3] FAQ of rec.music.country.old-time. Available at\n<URL:http://home.earthlink.net/~stevesag/otfaq.html> ,\nviewed Feb. 21 2002.\n[4] Itoh, M. Subject Search for Music: Quantitative Analysis of\nAccess Point Selection. In Proceedings of the 1st Int.\nSymposium on Music Information Retrieval . Center for\nIntelligent Information Retrieval, Amherst MA, 2000.\nAvailable at <URL :http://ciir.cs.umass.edu/music2000/>.\n[5] McNab R.J., Smith L.A., Witten I. H. and Henderson C. L.\nTune retrieval in the multimedia library. In Multimedia-\nTools and Applications 10 , 113-132. Kluwer Academic\nPublishers, 2000.\n[6] McPherson, J.R., and Bainbridge, D. Usage of the\nMELDEX digital music library. Proceedings of the 2nd Int.\nSymposium on Music Information Retrieval  (Bloomington\nIN, USA ; October 15-17, 2001), 19-20. 2001."
    },
    {
        "title": "The Quest for Ground Truth in Musical Artist Similarity.",
        "author": [
            "Daniel P. W. Ellis",
            "Brian Whitman",
            "Adam Berenzweig",
            "Steve Lawrence"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415602",
        "url": "https://doi.org/10.5281/zenodo.1415602",
        "ee": "https://zenodo.org/records/1415602/files/EllisWBL02.pdf",
        "abstract": "It would be interesting and valuable to devise an automatic measure of the similarity between two musicians based only on an analysis of their recordings. To develop such a measure, however, presupposes some ‘ground truth’ training data describing the actual similarity between certain pairs of artists that constitute the desired output of the measure. Since artist similarity is wholly subjective, such data is not easily obtained. In this paper, we describe several attempts to construct a full matrix of similarity measures between a set of some 400 popular artists by regularizing limited subjective judgment data. Wealsodetail ourattemptstoevaluatethesemeasures by comparison with direct subjective similarity judgments collected via a web- based survey in April 2002. Overall, we find that subjective artist similarities are quite variable between users—casting doubt on the concept of a single ‘ground truth’. Our best measure, however, gives reasonable agreement with the subjective data, and forms a useable stand-in. In addition, our evaluation methodology may be useful for comparing other measures of artist similarity.",
        "zenodo_id": 1415602,
        "dblp_key": "conf/ismir/EllisWBL02",
        "keywords": [
            "automatic measure",
            "similarity between musicians",
            "limited subjective judgment data",
            "full matrix of similarity measures",
            "web-based survey",
            "user variability",
            "ground truth",
            "direct subjective similarity judgments",
            "reasonable agreement",
            "evaluation methodology"
        ],
        "content": "TheQuest for Ground Truth in Musical Artist Similarity\nThe Quest for Ground Truth in Musical Artist Similarity\nDaniel P .W. Ellis\nColumbia University\nNew York NY U.S.A.\ndpwe@ee.columbia.eduBrian Whitman\nMIT Media Lab\nCambridge MA U.S.A.\nbwhitman@media.mit.eduAdam Berenzweig\nColumbia University\nNew York NY U.S.A.\nalb63@columbia.eduSteve Lawrence\nNEC Research Institute\nPrinceton NJ U.S.A.\nlawrence@necmail.com\nABSTRACT\nIt would be interesting and valuable to devise an automatic measureof the s imilarity between two musicians based only on an analysis of\ntheir recordings. To develop such a measure, however, presupposes\nsome ‘ground truth’ training data describing the actual similarity\nbetween certain pairs of artists that constitute the desired output of\nthe measure. Since artist similarity is wholly subjective, such datais not easily obtained. In this paper, we describe several attempts to\nconstruct a full matrix of similarity measures between a set of some\n400 popular artists by regularizing limited subjective judgment data.We also deta il our attempts to evaluate these measures by comparison\nwithdirect subjective similarity judgments collected via a web-\nbased survey in April 2002. Overall, we ﬁnd that subjective artistsimilarities are quite variable between users—casting doubt on the\nconcept of a single ‘ground truth’. Our best measure, however, givesreasonable agreement with the subjective data, and forms a useablestand-in. In addition, our evaluation methodology may be useful for\ncomparing other measur es of artist s imilarity.\n1. INTRODUCTION\nThere is a strong appeal to the notion that the similarity between\ntwo artists can be somehow measur ed. It seems particularly obvi-\nous that the similarity between certain pairs of artists can be judgedas greater than between other pairs. Even though the concept ofas ingle numerical similarity score between every pair of a set of\nartists raises serious epistemological problems, being able to gen-\nerate such a score would be very useful in music recommendation\nand organization applications, and several researchers have pursuedvariati ons of this idea. A typical goal would be an automatic system\nthat uses examples of the music of two artists to generate a ratingof their similarity. This raises the problem of assessing the quality\nof the automatic ratings, and/or choosing the ideal outcomes with\nwhich to train such a system.\nThe current paper seeks to address this last problem: can we come\nupwith a quantitative set of similarity scores, for a limited range of\nartists, which are as close as possible to the ‘ground truth’ that wewould wish for as the output of signal analysis based methods? We\nwant the ground truth values to capture the subjective impressions\nof the average user, giving a continuously-valued similarity score\nfor a large number of artist pairs, including, crucially, both similar\nand dissimilar pairs. Assuming th is data existed, it could be used\nto train automatic algorithms by providing a set of ‘target’ ratingswith which to set system parameters, and to assess the accuracy ofthe automatic systems by measurin gh o ww e l l their s cores matched\nthe id eal.\nBeforeconsidering how such a set of ground truth values might be\nestimated, we need to examine some of the problems that beset fromthis idea:\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use i sgranted without fee provided\nthat copies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on\nthe first page. c/circlecopyrt2002 IRCAM - Centre Pompidou\n•Individual variation: That pe ople have individual tastes and\npreferences is central to the very idea of music and humanity.Bythesame token, subjective judgments of the similarity\nbetween speciﬁc pair so fa r tists are not consistent between\nlisteners and may vary with an individual’s mood or evolve\nover time. In pa rticular, music that holds no interest for a\ngiven subject very frequently ‘all sounds the same’.\n•Multiple dimensions: The question of the similarity between\ntwo artists can be answered from multiple perspectives: Music\nmay be similar or distinct in terms of genre, geographical ori-\ngin, instrumentation, lyric content, historical timeframe, etc.While these dimensions are not independent, it is clear that\ndifferent emphases w ill result in different artists. That both\nPaul Anka and Alanis Morissette are from Canada might beof paramount signiﬁcance to a Canadian cultural nationalist,\nalthough another person might not ﬁnd them at all similar.\n•Asymmetry: Deﬁning a single similarity value for a pair\nof artists suggests that their similarity is symmetric, but, asdiscussed in [8] and elsewhere ,subjective s imilarity is often\nasymmetric. We might say that the 90s LA pop musician\nJason Falkner is similar to the Beatles, but we would be lesslikely to say that the Beatles are similar to Jason Falkner,not only because the Beatles recorded most of their musicbefore Falkner was born, but also because the much betterknown Beatles serve as a prototype ,i nc ontrast to the spe-\nciﬁc instance of Falkner. Asymmetry is one of the issues\nthat undermines a geometric (Euclidean) model of similarity,\nwhichisnonetheless a widely used assumption in similarity\nmeasures.\n•Variability and span: Few artists are truly a single ‘point’ in\nany imaginable stylistic space, but undergo changes throughtheir careers, and may consciously span multiple styles withinas ingle album. Trying to deﬁne a single distance between any\nartist and widely-ranging long-lived musicians such as DavidBowie or Prince seems unlikely to yield satisfactory results.\nDespite these problems, we believe that there is utility to the idea\nthat an ‘average’ set of similarity judgments, that would mostly\nagree with most people, could be constructed. In the remainder ofthe pap er, we pursue this idea. Section 2 brieﬂy reviews related\nprior work in music sim ilarity. In section 3, we describe our general\napproach, and deﬁne the several different data sources and metrics\nwe have developed for this task. Section 4 explains our evaluation\nprocedure, in which an independent dataset was collected speciﬁ-cally to compare the success of each metric. Finally, in section 5,wediscuss the results of our evaluation, and draw conclusions about\nthe best practice for re searchers interested in artist similarity.\n2. PRIOR WORK\nComputationally, musical simila rity has been studied from the score\nlevel, the audio level, and the cu ltural level .Each type of study in-\nformsthe next in hypothesis (that music can be modeled statistically\nand measured against other pieces) but not approach (where modelsTheQuest for Ground Truth in Musical Artist Similarity\nwidely differ.) However, all hav et h es a m ec a veat: that different\nideas of computationally- derived similarity cannot be compared to\none another, because the methods are as of now lacking a groundtruth.\nAtthescore level (MIDI ﬁles, transcribed music or CSound scores)\nsystems can extract style and similarity using the performance char-acteristics of the piece along with the key and frequently used pro-gressions, where such feature extraction is discretized and deﬁnite.\nAny system trained to do genre or style detection can infer up a\nleveltoperform similarity computations by studying the posterior\nprobabilities. In [4], various machine learning classiﬁers are trained\non performance characteristics of the score, and in [2] three types offolk music were separated using a Hidden Markov Model. Recent\nworkin[6]studies the cognitive background of melodic similarity\nfrom score data.\nWhen considering the audio domain, spectral information has proven\nto be instructive but not the only feature necessary to infer acoustic\nsimilarity. A system trained on a song identiﬁcation task (for copy-right protection or query-by-example), such as [5], would need only\nthespectral information, but systems that need to understand what\nconstitutes a similar piece of audio usually need help from higher-\nlevel extr acted features. In [12], attempts are made to abstract the\ncontent from the style of the audio in a manner that could recognize“cover versions” of songs already in a database. Genre identiﬁ-cation work undertaken in [9] aims to understand acoustic contentenough to classify into a small set of related clusters. The idea of\nparsing audio w ith the intent of creating an “eigen-artist” trained to\nclassify future work by the same artist (a speciﬁc form of similarity)wasﬁrstundertaken in [10] and then improved on in [1] with more\nmusical knowledge. Both genre and artist identiﬁers can claim tocompute musical similarity, but both have the inherent advantage ofawell-deﬁned ground truth (in genre’s case, the record industry’s\nmarketing-led genres, and in artists’ case, the actual artist.)\nCultural similarity (in which the listener or collection of listeners\ndeﬁne the similarity) can beneﬁt from attempting to express innatenon-acoustic and non-musical features about a speciﬁc piece of mu-sic. [11] deﬁnes community metadata concerning music as a feature\nvector that changes over time, reﬂecting the public’s perception\nof an artis t. (Their “Klepmit” and “OpenNap” datasets are used\nas sim ilarity in this article.) Related work in [3] computes music\nrecommendations based on similar artists found together in users’\n“favorite ar tist lists.”\n3. APPROACH\nThebasis for a ‘ground truth’ artist similarity measure must be the\nsubjective judgments of music listeners, but problems arise whenconverting subjective opinions into quantitative values, and when\nextending sparse coverage to give similarity judgments between any\npair from a large list of artists. In particular, while we can easily\nagree that the Backstreet Boys are very similar to N’Sync, judgments\nabout dissimilar artists are less common and more difﬁcult to quan-\ntify: how much are Backstreet Boys unlike Velvet Underground?\nHow does that compare to their dissimilarity to Sade?\nWe have in vestigated several different basic sources for our subjec-\ntive information, and s everal different mechanisms for ‘regularizing’\nthat information into a relatively comprehensive matrix of judgments\nbetween a large number of artists. Each measure is described in more\ndetail below.\n3.1 Measures\n3.1.1 Artist Selection\nWechose 412 artists to be included in our evaluation space. The\nartists were chosen automatically as the most popular artists on apopular peer-to-peer network as of August, 2001 (see below for amore detailed description of the peer-to-peer data collection com-\nponent.) Because of their selection criteria, the genre of the artists\ndoes not stray far from pop or rock, but has the advantage of beingrecognizable by almost any arbiter of current culture.\nEach similarity meas ure described deﬁnes its output as a similarity\nmatrix on the 412x412 artist space, where S(a,b)isacontinuous\nreal-valued function describing the relation of artist atob.S o m e\nmeasures give distances rather than similarity; this distinction is\nunimportant for simple rankings (providing the correct sense is ap-\nplied).\n3.1.2 Erd ¨os\nOne promisi ng data source is a published music guide, in which\nprofessional editors write brief descriptions for a large number ofpopular musical artists, often including a list of similar artists.\nWe extr acted the sim ilar artist lists from the All Music Guide\n(www.amg.com), giving for each member of our 412 artist list anaverage of 5.4 sim ilar artists also within the list (31 of the artists\nhad no neighbors in the set, and were effectively excluded from thismeasure).\nToconvert these descriptions of the immediate neighborhood of each\nartist into a more extensive measure, we adopted the technique used\namong mathematicians to gauge their relationship to Paul Erd ¨os\n:those who have co-authored papers with the proliﬁc Hungarian\nmathematician have an Erd ¨os number of 1; co-authoring with one\nof those authors will earn you an Erd ¨os number of 2, and so on.\n(This principle is applied to mov ieactors in the game known as “Six\ndegrees of Kevin Bacon”).\nThe largest distance in our Erd ¨os matrix is 13, corresponding to the\nmaximally dissimilar pair “Miles Davis” and “Wade Hayes”. Our\nconstruction of the Erd ¨os measure is symmetric, since links between\nartists were treated as nondirectional. Erd ¨os measures in trinsically\nobeythetriangle inequality, since the distance between any two\npoints cannot exceed the sum of the distances to a third point - since\nthis sum describes a valid Erd ¨os path.\nErd¨os distances are of course always integers, meaning that the\ndistance measures are highly quan tized. For any given source-target\npairing of artists, there will likely be a number of other artists at\nexactly the same ‘distan ce’ from the source. This is clearly an\nartifact and can be a nuisance, for example when trying to constructasingle, canonical ordered list.\n3.1.3 Res istive Erd ¨os\nAnobjection to the technique above might be that it is subject to\nthe whims of the human experts who created the original lists of\n“sim ilar artists”. The criteria used to create the lists are not well-\ndeﬁned, and it is likely that no two experts would create the samelists.Furthermore, the expert’s decisions about who to include or\nomit from each list becomes set in stone, because, for example, only\nthe artistsBincluded in artistA’slist, or vice-vers a, can have Erd ¨os\ndistanced(A,B)=1 .B u tw h a ti f there is another artist, C,t h a ti s\nvery mu chlike artistA,b u ti tw a s overlooked by the expert? Assume\nfurther that t he expert did note that both artists A and C are similar to\nseveral others (D,E,F )?I n s o m e cases it might seem reasonable\nthatd(A,C)should be even less than d(A,B),becauseAandC\nshare so many mutual intermediaries and thus must resemble one\nanother.\nThis intuition is captured by the Resistive-Erd ¨os measure. The\ndesired property, namely that nodes connected by many alternativepaths of lengthlare more similar than nodes connected by only\nas ingle path of length l,can be modeled by electrical resistance\nin a network. Resisto rs connected in parallel add as reciprocals\n(R\neq=1\n1/R1+1/R2), so the equivalent resistance between two\nnodes connected by multiple paths is l ess than the resistance of anyTheQuest for Ground Truth in Musical Artist Similarity\nsingle path.\nThe Resistive-Erd ¨os sim ilarity measure between two artists is de-\nﬁned as the equivalent resistance between the nodes in the Erd ¨os\ngraph if each edge is a resistor of 1 ohm. An all-pairs version of theSPS (Series-Parallel-Star) tree algorithm [7] was used to computethe resistances, written recursively to avoid recomputing intermedi-ate steps when computing resistances between all pairs of nodes inan e t w o r k .\nOneproblem with the measure is that it is biased towards popular\nartists (nodes with high degree in the Erd ¨os graph) because the many\nalternate paths lower t he to tal resistance. Attempts to compensate\nby using heavier resistance on edges incident to popular artists werenot successful, but perhaps improvements can be made in the future.\n3.1.4 OpenNap Peer-To-Peer Cultural Similarity\nSimilarity can be inferred from observation: clusters of music gener-ated from lis tening patterns are a direct measure of cultural similarity\nand can show relations between artists that could never come out\nof an edited list or the musical content. We used user preference\ndata (userihas ar tistxintheir collection) to generate a continuous\nmatrix of similarity.\nWeretrieveduser collection data from OpenNap, a popular music\nsharing service (we did not download any audio ﬁles). About 1.6million user-to-song relations were retrieved, indicating that a userhas a particular song in their collection. After processing the data\nfor typos and misspellings, and re moving unknown artists, we were\nleftwithabout 400,000 user-to-song relations covering about 3,000\nunique artists.\nWedeﬁne a collection as the set of artists a user had songs by on\ntheir shared folder. If two artists frequently occur together in usercollections, we consid er them sim ilar via this measure of commu-\nnity metadata, since even if users are striving for variety in their\ncollections, it is signiﬁcant if they ﬁnd variety in the same artists.\nWealso deﬁne a collection count C(artist)which equals the num-\nber o fu s e r st h a th a v eartist in their set.C(a,b),likewise, is the\nnumber of users that have both artists aandbin their set.\nOneproblem of this method is that extremely popular artists (such\nas Madonna) occur in a large percentage of users’ collections,which down-weights similarity between lesser-known artists. We\ndeveloped a scoring metric that attempts to alleviate this problem.\nGiven two artistsaandb,w h e r eaismore popular than b(i.e.,\nC(a)>C(b)), and a third artist cwhoisthe most popular artist in\nthe set;aandbare considered similar w ith normalized weight:\nS(a,b)=C(a,b)\nC(b)(1−|C(a)−C(b)|\nC(c))( 1 )\nThesecond term is a “popularity cost” which down-weights rela-\ntionships of artists in which one is very popular and the other is very\nrare.\n3.1.5 Community Metadata-derived Similarity\nAnother more formal model of cultural similarity is provided by\nthe “Klepmit” system, described in detail in [11]. “Klepmit” pro-vides a continuous measure of cultural similarity by analyzing thecommunity me tadata associ ated with a particular artist (e.g., the text\ncontent of web pages returned by a s earch on the artist’s name). This\nmetadata is deﬁned as a feature vector of textual terms (adjectives,unigrams, bigrams, and noun phrases) and similarity is computedby determining a weighted overlap via a Gaussian window over thetf·idf values.\nf\nte−(log(fd)−µ)2\n2σ2(2)\nHere,fdis the doc ument frequency of a term, ftthe term freque ncy\nof a term, andµandσare parameters indicating the mean and0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.80.20.40.60.811.21.41.61.822.2\nbilly joelbonnie tyler\nbryan adams\ncrowded housedire straits\nelton johnelvis costello\neric clapton\nfleetwood macgary wright\njohn lennonkenny logginslove\nmark \u001f\nknopfler\nmelissa etheridge\nmen at workpat benatar\npeter gabrielphil collinspink floyd\nrichard marxrod stewartsantana\nstevie nicks\nstingthe beatlestom petty\nFigure 1: Artists embe dded in a 2-D space. This is a small\nportion of the full space derived from the Erd ¨os measure.\ndeviation of the Gaussian window. See Table 1 for example returned\nvectors.\nThismodel attempts to measure the popular opinion regarding an\nartist, and has the valuable property of being time-aware: community-\nmetadata crawled only weeks apart can return widely varying results\nfor a single artist. This data, arranged as a trajectory along time, canuniquely identify similarities of artists at any point in their career,\nas opposed to other models of similarity that treat artists as staticindices in thei rd a tabase.\nFor the purposes of this experiment, we generated a matrix of simi-\nlarities comparing each artist in our set with each other, along each\nof the different term types com puted in the community metadata\nfeature space.\n3.2 Geometr ic Embe dding\nIn addition to extending the covera ge of a metric beyond directly-\nspeciﬁed subjective comparisons, regularization may be required to\ngive a particular me tric properties such as symmetry and transitivity\n(i.e. the triangle inequality); one extreme way to ensure these prop-erties is to convert a set of distance judgments into a set of points in aEuclid ean space such that the Euclidean distances between the points\ndo the best job of approximating the original distances. These points\nmay be found via a straightforward gradient descent in a procedureoften known as Multidimensional Scaling (MDS). A typical choice\nfor the global error to be minimized is the root-mean-square (RMS)\n‘stress’ along all links, i.e. the proportional difference between ideal\nand actual lengths. The ﬁnal stress is also a measure of how success-\nful MDS was in ﬁtting the original distance measures. Points can be\nembeddedinaspace of arbitrary dimensionality; more dimensions\nafford more degrees of freedom and hence a lower stress. 2 and 3dimensional embeddings have the attraction of permitting visualiza-\ntion of the dataset’s geometric conﬁguration; a small portion of a2-D embedding of the Erd ¨os distance is shown in Figure 1. For our\nartist similarity data, a 3D space provides for reasonably low-stress\nembedding, and we saw a plateau in RMS stress at 4 dimensions;using higher order spaces gave negligible improvements in ﬁt.\nEmbe dding can be applied to any of the measures. Where a sim-\nilarity between 0 and 1 is provided (as with the OpenNap andTheQuest for Ground Truth in Musical Artist Similarity\nTable 1: Top 10 terms for various community metadata vectors of the gr oup Portishead. He re, the noun phrase and adjective terms\nseem to give the best descriptions and are imperative identiﬁers for uncovering cultural similarities.\nn1 T erm\n Score\ngibbons\n 0.0774\ndummy\n 0.0576\ndispleasure\n 0.0498\nnader\n 0.0490\ntablets\n 0.0479\ngodrich\n 0.0479\nirks\n 0.0467\ncorvair\n 0.0465\ndurban\n 0.0461\nfarﬁsa\n 0.0459\nn2 Term\n Score\nbeth gi bbons\n 0.1310\nsour times\n 0.0954\nblue lines\n 0.0718\n17 feb\n 0.0675\nlumped into\n 0.0665\nwhich come\n 0.0635\nmellow sound\n 0.0573\nin together\n 0.0519\nmusicians will\n 0.0494\nenough like\n 0.0494\nnp T erm\n Score\nbeth gi bbons\n 0.1648\ntrip hop\n 0.1581\ndummy\n 0.1153\ngoosebumps\n 0.0756\nsoulful melodies\n 0.0608\nrounder records\n 0.0499\ndante\n 0.0499\nmay1997\n 0.0499\nsbk\n 0.0499\ngrace\n 0.0499\nadj Term\n Score\ncynical\n 0.2997\nproduced\n 0.1143\nsmooth\n 0.0792\ndark\n 0.0583\nparticular\n 0.0571\nloud\n 0.0558\namazing\n 0.0457\nvocal\n 0.0391\nunique\n 0.0362\nsimple\n 0.0354\nKlepmit measures), it can be converted to a distance via dist=\n(−log(sim))k.H e r e,kimplements an arbitrary power-law mono-\ntonic transformation of every dist ance; in all cases, we searched over\nar ange of such transformations ( kbetween 0.1 and 3.0) to ﬁnd the\none giving the lowest stress solution, since the relations of the mea-\nsures to Euclidean distances are only speciﬁed up to a monotonic\ntransfor mation.\n4. EV AL UATION\nHaving produced various alternative candidate ground-truth mea-\nsures, we are faced with the problem of trying to compare theirquality. Again, this needs to be relat ed to true subjective judgments,\nbut to use the sam ei n f orma tion as was the basis for one or more\nof the measures would be circular and misleading. Therefore, wecollected a completely separate set of judgments for the speciﬁcpurpose of evaluating our measures. First, we will describe the datacollection, then how we used it to evaluate the measures.\n4.1 Evaluation Collection Web Site\nFor the purposes of collecting large-scale evaluation data, we devel-oped a web-based game and survey termed ‘MusicSeer’ (which iscurrently av ailable at http://musicseer.com/ ). Usin gt h e 412\nartists in our set, MusicSeer collects subjective human responsesabout artist to artist relationships .The system has two modes (freely\nselectable by the informant), both with their own speciﬁc purpose.\n4.1.1 Artist Survey\nIn the more direct route, we can as ki n f ormants “given an artist x,\nwho is the most similar?” This is t he approach of the artist survey\nmode, but with a few twists to make the data more valuable.\n•Pre-sel ected Choices: The survey a utomatica lly selects a\nsource artist and 10 target artists from the list of 412 artists.Thesource artist is selected from amongst popular artists, or\nartists that the user is familiar with (see below), while the target\nartists are randomly selected from the top 10 most similar\nartists according to the following three similarity metrics:OpenNap, Klepmit noun phrases, and Erd ¨os.\n•Triplet Encoding: Along with the pair of source artist, target\nartistthat each judgment contains, we also store the remaining\nartists that were notselected. This allows us to understand\nacertain hierarchical ordering (over many judgments) from\naparticular source artist. For each selected artist, then, we\nactually stor en i n e‘ triplets’ source artist, target artist (is more\nsimilar to source than...), unselected artist .\n•Bad Judgment Detection: Peppered throughout the survey\nare a small amount of randomly generated ‘fake band names.’We devel oped a set of sta tistically average artist name gram-\nmars and ran the terms used in current band names throughthem. Inf ormants that select such red herrings as “Sleep-\nlessness Explosive” or “Blonde and Bipolar” are treated with\nsuspicion in later processing.\n•Unknown Artists: Thesurvey has an option to skip respond-\ning if the user is unfamiliar with the source artist, or with mostof the targe ta rtists.\n•Adaptive Artist Selection: The survey k eeps track of artists\nthat the user knows (the source or the selected target from\nprior responses) and does not know (the survey assumes the\nsource artist is unknown when the “unknown” option is se-\nlected). Source artists are initially chosen from the mostpopular artists. After 5 responses, source artists are chosenfromamongst the known artists 80% of the time, and from\npopular artists 20% of the time. Artists that we know the useris unfamiliar with are never chosen as source artists.\nAtthetimeofwriting, the survey has generated over 6,200 responses\n(roughly 56,000 triplets.)\n4.1.2 Erd ¨os Game\nThe Erd ¨os Game (also known as ‘poperdos’ or ‘the rabbit game’)\ncame about from the uniqueness of the Erd ¨os distance measure ex-\ntracted from the All Music similarities. Links between relativelydistant artists were exciting to study (how could you get from Mar-\nilynManson to ABBA?) and we felt that a game founded on this\ndata could attract a ttention to our data collection effort.\nIn the game, the informant is asked to select a target artist to go\nwitharandomly chosen source artist, and is immediately presented\nwith the pre-computed Erd ¨os distance. The informant is then asked\nto match or beat that di stance by moving along a chain of similar\nartists. Some pressure is added by the compelling back story of a\nlost rabbit trying to ﬂee the clutches of an evil record store owner,\nwho is curi ously bent on denying the rabbit his favorite carrots and\nraisins.\nAt each ‘hop,’ the informant is presented with a list of immediate\nneighbors, from whom the artist most similar to the desired ultimatedestination should be chosen. For example, at each hop in a MarilynManson to ABBA game, the user must select the closest artist to\nABBA among the present similarity list. The list of possible artists\nis based on our existing metric set, slightly augmented from thebasic All Music data, so that it is sometimes possible to beat the\nErd¨os distance.TheQuest for Ground Truth in Musical Artist Similarity\n-2 -1.5 -1 -0.5 00\n0.50.5\n11\n1.5 2010002000300040005000\nNormalized differenceFrequency countOpenNap\nWeighting\nErdos-3D''\nFigure 2: Histograms of the scores when the evaluation triplets\nare converted into the difference in distance between selected\nand unselected targets, and normalized by the magnitudes of\neach distance according to the internal noise model. Note theslight bias visible in the distribution of the Erd ¨os -3D data to-\nwards the positive (agreement) side. Superimposed is the erf\nsigmoid weighting used to weight these histograms before inte-grating to give the overall weighted agreement.\nFrom our own experience, we realized that informants’ judgments\nvary in nature and quality depending on the stage in the game. In\nearlier steps, judgments are for artists who may be very dissimilar,andwhile this is unique and valuable data, we also record the position\nwithinthegame in our database in case we should wish to ﬁlter on\nthis attribute at a later stage.\nThe Erd ¨os Game has currently attracted 7,400 selections (over\n82,000 triplets). Figure 3 shows sample screenshots of the webinterface to both the su rvey and game modes.\n4.2 Evaluation Measures\nThe web site has collect ed over 13,000 total selections, giving some\n138,000 (source, target, unselected) relative similarity triplets withwhich to test our metrics. We use this data in two ways:\n•Aver age ranki ng:For each selection, we use the metric under\ntest to sort the list, then record the ranking of the actual item\nselected by the informant. Each ranking is normalized to a\nscale of 1 to 10 (for lists that contain greater or fewer thantenitems), then averaged across all the judgments. A metric\nthat perfectly predicted informant responses would give an\naverage ranking of 1; random orderings should give a ranking\naround 5.5.\n•Aver age unweighted/weighted agreement: As i m p l ew a yt o\nuse the data triplets is to count the cases in which the inferredsubjective judgment (that the source is more similar to the tar-get than to the unselected alternative) agree with the distancesgiven by the metric. This measure, the average unweightedagreement, has the disadvanta ge that it makes no distinction\nbetween a disagreement over ar tists of a pproximately equal\nsimilarity to the source (which is not serious), and the moresigniﬁcant situation in which an informant chooses a targetthat the metri cr a ted as vastly inferior.\nThis leads to the we ighted agreement measure: We can model\ntheinformant’s judgment as the comparison of ‘true’ similar-\nity measur es that have been corrupted by an internal noise\nsource. If we assume the noise has a standard deviation inproportion to the magnitude of th esimilarities, th en the sig-\nniﬁcance of each triple becomes a function of the differencebetween their metric distances divided by the expected error\nmargin i.e. (d(S,T)−d(S,U))//radicalbig\n(d(S,T)2+d(S,U)2).\nWhendis a distance, values less than zero indicate agreement\nbetween informant a nd metr ic. Positive or negative values\nclose to zero are rela tively insigniﬁcant, since the internal\nnoise could easily cause an error in this range. A histogram of\nthis normalized difference over the entire evaluation set gives\naquick summary of the metric’s performance, showing the\nextent to which it is biased to the ‘agreement’ side. Figure 2\nshows examples for the OpenNap measure and the distancesmeasured from the embedding of the Erd ¨os measure in a 3-D\nspace.\nTo conv ert the histogram to a single score, we can sum the\nhistogram bins, individually weighted to indicate their cor-rectness and signiﬁcance. The sigmoid function shown over-laidon the histogram provides such a weighting; judgments\nclearly reversed from the metric’s predictions score 0, highlyconsistent judgments score 1, and ambiguous judgments land\nup in the middle of the histogram and have a weight of around\n0.5. The width of the sigmoid transition corresponds to an\nassumption of the magnitude of the internal noise, i.e. overwhat range the choice between similar distances should be dis-counted. Arbitrarily, we used the large value illustrated in theﬁgure, where the unweighted agreement would correspond to\nazerotransition widht.\nAveraging the weighted or unweighted counts over all the\nknown-artist eva luation triplets giv es an indication of how\nstrongly the metric agreed (or disagreed, for a score below50%) with the subjective data.\nOne issue that arose in using the evaluation website was that in\nmany cases some of the artists on a list may be unknown to theinformant. In this case, the selection cannot be accurately interpreted\nas meaning that the informant judged the selected target as more\nsimilar to the source than the unknown, unselected alternative. We\ndevised a conservative procedure for ensuring that our data excludedsuch invalid triplets: Over the entire history of selections made by aparticular informant (tracked via an anonymous web cookie), a listof ‘known’ artists is constructed as all the artists ever selected, on\nthe assumption that informants would never select artists with whom\nthey were not familiar. Then the triplets are ﬁltered to retain only\nthose in which both target and unselected alternate are afﬁrmatively\nknown by the informant. This removes about two thirds of the datatriplets.\n4.3 Results\nTable 2 lists the results of our evalua tion sch emes. Average rankings\nare reported for each measure ove rfour subsets of the evaluation\ndata, broken down into the two modes (survey and game) and into\nall results, or known artists only. Restricting the ranking to the\nsmaller set of artists known to each informant greatly reduces theeffective list length and tends to increase average rankings. This maybe because the unknown artists are more likely to be dissimilar tothe known source artist, and hence we are removing items primarily\nfrom the bottom of the list before renormalizing to the 1-10 scale.\nThe ranking numbers are unfamiliar and we have been unable to\ncalculate an ap r i o r i signiﬁcance bound. However, some feeling for\nthe stability of this data can be gain ed by looking at the variation in\ntheranking score of the random measure across the different subsets\nof the evaluation data. We expect the average score to be 5.5 (theaverage of values uniformly distributed in the range 1-10); there\nappears to be a slight negative bias, but the ranking values appear to\nbe reliable at least to the ﬁrst decimal place. We have adopted anaverage ranking difference of 0.1 as our signiﬁcance threshold for\nthis data.TheQuest for Ground Truth in Musical Artist Similarity\nFigure 3: Screenshots of the web interfaces used to collect the evaluation data. Top pane: Survey mode. Bottom pane: The Erd ¨os\ngame.TheQuest for Ground Truth in Musical Artist Similarity\nTable 2: Evaluat ion results. Each column describes a different metric, being: opt - the ‘optimized’ measure derived from the survey\ndata; cmb - similarities from Erd ¨os and OpenNap measures combined by simple averaging; erd - the plain Erd ¨os distance; e2d -\nErd¨os distance embedded in 2-dimensional space, then converted back into a similarity matrix based on the actual Euclidean distances;\ne3d - the same for a 3D space; e4d - the same for a 4D space; Rer - the “resistive” Erd ¨os extensio n; onp - the OpenNap measure;\nkn1 - unigram features from the Klepmit data; kn2 - Klepmit bigram features; knp - Klepmit noun-phrase features; kaj - Klepmitadjectives; rnd - a random similarity matrix included for compar ison. (Since rankings are normalized to fall between 1 and 10, we\nexpect random choices to average out to around 5.5, as observed). Each row presents a different quality index for the metrics; the\nﬁrst four rows present average rankings of the user selection under each metric, broken up according to the collection mode (survey\nor game), and both with (all) and w ithout (known) ratings involving artists that the informant may not know. 3D embedding stress\nis the ﬁnal stress when the metric is embedded in a 3D space, and i so fc o u r s e zero for the metrics derived from Euclidean spaces of\nthat size or smaller (e2d and e3d); the low embedding stress of the ‘opt’ measure arises because it deﬁnes only a small proportion of allthe possible distances. Average unweighted agreement gives the proportion of collected judgment triplets that agree with the metric;average weighted agreement weights this value to discount errors where the artists in question are almost equivalent, as described in\nthe text. In both cases, random agreement should score 50%.\nMode\n opt\ncmb\nerd\ne2d\ne3d\ne4d\nRer\nonp\nkn1\nkn2\nknp\nkaj\nrnd\nSurvey,all (6177 resp, 8.97 av.choices)\n 1.98\n3.52\n3.83\n4.26\n4.08\n4.05\n4.14\n4.06\n4.53\n4.55\n5.20\n4.72\n5.42\nSurvey,known(4802 resp, 3.59 av.choices)\n 2.24\n4.26\n4.07\n4.50\n4.26\n4.22\n4.92\n5.14\n4.62\n4.46\n4.66\n4.96\n5.44\nGame, all (7421 resp, 11.10 av.choices)\n 1.91\n4.41\n4.50\n4.64\n4.56\n4.54\n4.77\n4.65\n5.44\n5.37\n5.39\n5.57\n5.49\nGame, known (6515 resp, 4.72 av.choices)\n 2.68\n5.02\n4.87\n4.94\n4.88\n4.90\n5.31\n5.35\n5.42\n5.36\n5.35\n5.57\n5.45\n3D embe dding stress (%)\n 15.3\n23.7\n20.8\n0.0\n0.0\n13.5\n20.3\n27.9\n34.1\n34.5\n34.7\n36.2\n35.8\nAverage unweighted agreement (%)\n 85.0\n56.5\n58.5\n57.4\n58.9\n59.1\n52.3\n50.6\n56.6\n57.4\n56.0\n54.2\n50.6\nAverage weighted agreement (%)\n 71.6\n52.6\n56.6\n55.9\n56.7\n56.6\n51.4\n49.8\n51.3\n51.6\n51.4\n51.1\n50.2\nThere is a question over the internal consistency of the survey data:\ninview of the introductory discussion, is it even possible for a single\nsimilarity measure to have good agreement with the judgments frommore than 1,100 informants logged by the site? To answer this, we\ndeveloped an optimal ‘cheating’ metric, constructed to have the best\npossible agreement with the survey data. For each source artist, wesearched for an optimal ordering of the remaining artists by testingeach referenced target artist at every point in the list and calculatingthe resulting agreement with all the judgments related to that source.\nThis gave the “optimal” metric shown in the tables, which agrees\nwith88.2% of the collected judgments; we conclude that there is a\ngood degree of consistency within the ratings. Note, however, thatthischeating metric fares poorly by our original standards - it has\nno trans itivity or symmetry (there is no effort to relate d(A,B)to\nd(B,A)), and it speciﬁes relations for each source artist only for the\nother artis ts with comparisons in the evaluation data - an average of\n83.4 artists each, or about 20% of the total similarity matrix.\n5. DISCUSSION AND CONCLUSIONS\nThe results show that on both the average ranking and the weighted\nagreement measures, the plain Erd ¨os score performs the best among\nthevarious base measures we have proposed. Geometric embed-\ndings of Erd ¨os become increasingly similar to the plain measure as\nthe dimensionality increases to 4 (and have the advantage of beingtrue metrics, reﬂected in their low 3D embedding stresses). Resis-tive Erd ¨os appears inferior to plain Erd ¨os , although as discussed\nabove there may be other forms of this measure that will performbetter.\nThe Ope nNap measure performs quite well on the rankings but not\non the weighted agreement; as seen in Figure 2, this reﬂects the\ntight bunching of the length differences around zero for this mea-\nsure. (The poor correlation between the weighted agreement and theaverage rankings in this case seems to imply that more sophisticated\nnormalization is required within the weighted agreement calcula-tion.) The various Klepmit similarities seem less promising than\nOpenNap. Notice that t he embedding stress of these metrics is\nsimilar to the value for the random similarity matrix, implying thatgeometric embedding is not at all appropriate for this data, at leastas we have impl emente dit.Apart from the ‘optimal’ measure (which cannot be fairly compared,\nsince it uses prior knowledge of the evaluation data to optimize itsscore), the best rankings are obtained by the combined measure thataveragessimilarities from the Erd ¨os and OpenNap sets. It seems log-\nical that a combination should be able to outperform either measure\nalone, since the combined measure draws on the pooled knowledgerepresented by the subjective judgments underlying each measure.\nOur combination scheme, however, is very simple. It seems likelythat a more sophisticated and better-performing combination mea-\nsure could be found.\nDifferences between the survey and the game in the absolute values\nof the average ranking scores are to be expected because the cohorts\nfrom which user choices are made are very different: Game choicesare made among a set of similar artists (the neighbors of the current‘position’), whereas survey sets come from a broader range. Thus,\nweexpect non-cheating measures to do worse on the more closely-\nbunched game choices.\nReturning to our original goal of constructing a full matrix of sim-\nilarities among a given set of artists that could be used to train an\nautoma tic measure of artist similarity, the combined measure is at\nleast a usable starting point. It may be, however, that the evaluation\nmethodology and the judgments collected though the web site areequally useful; in our own current work developing signal-basedmusic similarity measures, this evaluation procedure has turned out\ntobe very valuable as a way to judge progress and reﬁne our algo-\nrithms.\n5.1 Summary and Conclusions\nWehave investigated the feasibility of deriving the ‘ground truth’\nthat underlies subjective assessments of artist similarities. This task\nisdaunting, not only because such values defy direct measurement,\nbutalsobecause several considerations imply that a single metric\ncannot exist.\nNevertheless, we were able to coerce relatively modest amounts\nof subjective rating data from var ious sources into full similarity\nmatrices with varying properties. In order to evaluate the differentmetrics, we collected a new dataset consisting of direct judgmentsof artist similarity. Under the various indices we devised to rateTheQuest for Ground Truth in Musical Artist Similarity\nour metrics against this evaluation data we found that several met-\nrics performed quite well, and a simple combination of the metrics\nperformed still better.\nThe motivation o ft h i sw ork was to deﬁne consistent measures over a\nlarge set of artists to be used as training data for automatic similaritymeasures based on audio data. We feel that the results of our best-performing combined metric is suitable for this task, although theevaluation methodolgy and data may turn out to be the more useful\ncontribution. We plan to make the data from this metric, as well as\nthe raw data used inour evaluation, freely available as a resource\nfor the research community.\n6. ACKNOW LEDGMENTS\nThisworkwasextensively supported in part by the NEC Research\nInstitute, whose contribution is gratefully acknowledged.\n7. REFERENCES\n[1] A. Berenzweig, D. Ellis, and S. Lawrence. Using voice seg-\nments to improve artist classiﬁcation of music. In AES 22nd\nInternational Conference ,Espoo, Finland, June 15–17 2002.\n[2] W. Chai a nd B. Vercoe. Folk music classiﬁcation using hidden\nMarkov models. In Proceedings of International Conference\nonArtiﬁcial Intelligence ,2001.\n[3] W. W .Cohen and W. Fan. Web-collaborative ﬁltering: rec-\nommending music by crawling the web. WWW9 / Computer\nNetworks ,33(1-6):685–698, 2000.\n[4] R. B. Da nnenberg, B. Thom, and D. Watson. A machine learn-\ning a pproach to musical style recognition. In In Proceedings\nof the 1997 International Computer Music Conference ,p a g es\n344–347. International Computer Music Association., 1997.[5] J. Herre, E. Allamance, and O .H ellmut h. Robust matching of\naudio signals using spectral ﬂatness features. In Proceedings of\nthe2001 IEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics ,pages 127–130, Mohonk, New York,\n2001.\n[6] L. Hofma nn-Engl. Towards a cognitive model of melodic sim-\nilarity. In Proceedings of the 2nd Annual International Sympo-\nsium on Music Information Retrieval ,pages 143–151, Bloom-\nington, Indiana, 2001.\n[7] J. Mauss and B. Neumann. Qua litative reasoning about electri-\ncal circuits using series- parallel-star trees. In 1st International\nWorkshop on Model-based Systems and Qualitative Reason-\ning, ECAI’96 W orkshop W23 ,Budapest, 1996.\n[8] A. Tversky. Features of similarity. Psychological Review ,\n84(4):327–352, July 1977.\n[9]G.Tzanetakis, G. Essl, and P. Cook. Automatic musical genre\nclassiﬁcation of audio signals. In Proc. I nt. Symposium on\nMusic Inform. Retriev. (ISMIR) ,pages 205–210, October 2001.\n[10] B. Whitman, G. Flake, and S. Lawrence. Artist detection in\nmusic with Minnowmatch. In Proceedings of the 2001 IEEE\nWorkshop on Neural Networks for Signal Processing ,p a g es\n559–568, Falmouth, Massachusetts, September 10–12 2001.\n[11] B. Whitman and S. Lawrence. Inferring descriptions and sim-\nilarityformusic from community metadata. 2002. in prepara-\ntion.\n[12] C. Yang. Music database retrieval based on spectral similarity.\nInProceedings of the 2nd Annual International Symposium\non Music Information Retrieval ,pages 37–38, Bloomington,\nIndiana, 2001."
    },
    {
        "title": "Popular Music Retrieval by Independent Component Analysis.",
        "author": [
            "Yazhong Feng",
            "Yueting Zhuang",
            "Yunhe Pan"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416098",
        "url": "https://doi.org/10.5281/zenodo.1416098",
        "ee": "https://zenodo.org/records/1416098/files/FengZP02.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416098,
        "dblp_key": "conf/ismir/FengZP02",
        "content": "Popular Music Retrieval by Independent Component Analysis \nPopular Music Retrieval by Independent Component \nAnalysis \nYazhong Feng Yueting Zhuang Yunhe Pan \nDepartment of Computer Science, Zhejiang University \nHangzhou 310027, China \n86-571-87951853 \nfengyz_zju@263.net  yzhuang@cs.zju.edu.cn panyh@sun.zju.edu.cn \n \n1.  INTRODUCTION \nDigital music download activity is becoming the dominant traffic \nstream on Internet; research on content-based music retrieval tool is increasing. A rich range of researchers are contributing to content-based music retrieval systems, most of the systems deal with MIDI music and use melody contour to represent music and string matching strategies to retrieval music. \n2. OUR APPROACH \nIn published literature, usually, acoustic input singing or humming is pitch-tracked and segmented into notes or converted into three or five level melody contour, melody contour is also extracted from music in database, our approach (Figure 1) does not try to segment notes at all, we employ statistic model to extract singing from popular music, user’s input singing and extracted singing are converted to self-similarity sequence, which is a curve in 2D space, music retrieval is equivalent to comparison of these curves. Indices on music database are the weights of recurrent neural network; similarity of query key with music in database is represented by their correlation degree. \n \nFigure 1. The diagram of our approach. \n3. EXTRACTING SINGING FROM RAW \nAUDIO MUSIC \nIndependent Component Analysis (ICA) is a statistical and \ncomputational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals. The application of ICA to singing extraction is straightforward and the result is so good that it is worthy of doing more research on this direction. \n3.1 Independent Component Analysis \nAssume that N linear mixtures \n[]T\nNx xx ,,,2 1!=x ofM independent components []T\nMs ss ,,,2 1!=s are observed \nAsx= ,     E q .  1  \nWhere A is a full rank MN× scalar matrix. In the ICA model, \nassume that each mixturejxas well as each independent \ncomponentksis a random variable instead of a time signal. \nWithout loss of generality, assume that both the mixture variables \nand the independent components have zero mean. If the multivariate probability density function (pdf) of scan be written \nas the product of the marginal independent distributions, \n∏\n==M\nii isp p\n1)( )(s                                                 Eq. 2 \nThe components of sare such that at most one source is normally \ndistributed, and then it is possible to extract the sources from the \nmixtures. This statistical model is called independent component analysis (ICA).  \n3.2 Singing Extraction from stereo Popular \nSongs \nThe number of independent components is equal to that of \nobserved variables in classic ICA, that is N M= , FastICA [2] \ncan be used to perform independent component analysis.  \nApplication of ICA to singing extraction from stereo popular music is straightforward; just regard two channels of signal as observed values, singing and accompaniment as two resources. \nExperiments show that FastICA performs very well in extracting \nsinging from popular music except that drum blurs the singing in some cases.  \n3.3 Singing Extraction from Single Channel \nPopular Song \nTo extract two sources, singing and accompaniment from single \nchannel recording  Y, we assume that  \n2 1Y YY+= ,     Eq. 3 \nii i x Yλ= ,     Eq. 4 \nwhere { }],1[)( T ttyi i ∈=Y . It is forced that \n12 1=+λλ ,     E q .  5  \nwe use an exponential power density for resource s, which is \nzero mean, e.g. \n(ps −∝exp() s)q\n,    Eq. 6 Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM – Centre Pompidou  Popular Music Retrieval by Independent Component Analysis \nat every time point ]1 ,1[+−∈ NT t  a segment )(1ty of \ncontiguous Nsamples is extracted from iY. Then independent \nsource can be inferred as\n11\n1)(λ=ts W1 )(1ty . Figure 2 shows \na segment of single channel music and its components, the singing \nand accompaniment. \n \nFigure 2. Single channel original music is separated to \naccompaniment and singing. \n4. SELF-SIMILARITY SEQUENCE \nMusic is self-similar, lay people tend to singing with their own style, they introduce some errors such as, tempo variation, insertion or deletion of notes, but they also tend to keep the same error. These evidences support our using of self-similar sequence to represent input singing and entities in music database in our music retrieval system. \n4.1 MFCCs as Features \nMFCCs has been used to model music or audio, some audio retrieval system based on a cepstral representation of sounds, because that the use of Mel scale for modeling music is at least not harmful in speech/music discrimination, we also use MFCCs as the feature to form the self-similarity sequence.  \n4.2 Self-similarity of Audio \n[1] represents acoustic similarity between any tow instants of an audio recording in a 2D representation, similarity matrix, we borrow this idea to form the self-similarity sequence, define \n),( ),(j ivv diffjis=     E q .  7  \nas the element of similarity matrix Sof an audio segment, where \nivis the feature vector of thiframe, ],1[, N ji∈ , there \nareNframes in this segment, define the self-similarity sequence \nof the same segment of audio as  \n],1[,1)1,(\n)( N ii NjS diag\nissN\nij∈−+−\n=∑\n=.  Eq. 8 \nSelf-similarity sequence is a fault-tolerant representation of music \nand input singing b ecause of its not using the exact acoustic i nput \nor music information but retain their latent structures. In our system, Self-similarity sequence is used as the index of music database. \n \nFigure 3.  Self-similarity sequences: red solid curve is the \nextracted singing of 15 seconds segment in song “Happy birth day to you.”, blue dash curve is input singing.  \n5. INDEXING ON MUSIC DATABASE \nAfter singing is extracted from music in database and being converted into self-similarity sequence, recurrent neural network (RNN) is employed to remember this sequence, for each piece of music, we train a corresponding RNN. It is obvious that index size is linear to the size of music database. We do not know in previous which part of a piece of music users will singing, so the system must be robust enough for users to singing any part of the song. Recurrent neural network is of strong ability in time series prediction, the node size of input layer, output layer, hidden layer and context layer is 1, 1, 10, 10, respectively in our system, the weights between different layers store what information the network remembers.  \nWhen feeding self-similarity sequence of input singing to RNNs, \nwe obtain a corresponding sequence from output layer, calculate the correlation degree [3] of the input and output, the bigger the correlation degree is, the more similar the input is with the music represented by this RNN. \n6. EXPERIMENT RESULT \nOur test database is composed of 120 pi eces of raw audio popular \nmusic, our approach achieves the successful rate of top 1 and top3 is 79% and 86%, respectively. The database is rather small, but is enough to test our idea. The in accurate retrieval results may result \nfrom bad singing extraction because of too many drum s ound and \nimproperly selected features used to calculate self-similarity sequence. Further research should be done on more accurate extraction algorithms; performance evaluation of ICA should be paid attention. Self-similarity is an interesting character of music, but which feature is more appropriate for its calculation is open for discussion.  \n7. \nREFERENCES  \n[1] Foote, J. Visualizing Music and Audio using Self-Similarity. \nIn Proceedings of ACM on Multimedia, 1999. \n[2] Hyvärinen, A. and Oja, E. Independent Component Analysis: \nAlgorithms and applications. Neural Networks, 13(4-5): 411-430, 2000. \n[3] Feng, Y.Z., Zhuang, Y.T. and Pan, Y.H. Query Similar Music by Correlation Degree. In Proceedings of IEEE PCM, 2001, pp.885-890."
    },
    {
        "title": "Audio Retrieval by Rhythmic Similarity.",
        "author": [
            "Jonathan Foote",
            "Matthew Cooper 0002",
            "Unjung Nam"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417603",
        "url": "https://doi.org/10.5281/zenodo.1417603",
        "ee": "https://zenodo.org/records/1417603/files/FooteCN02.pdf",
        "abstract": "We present a method for characterizing both the rhythm and tempo of music. We also present ways to quantitatively measure the rhythmic similarity between two or more works of music. This allows rhythmically similar works to be retrieved from a large col- lection. A related application is to sequence music by rhythmic similarity, thus providing an automatic “disc jockey” function for musical libraries. Besides specific analysis and retrieval methods, we present small-scale experiments that demonstrate ranking and retrieving musical audio by rhythmic similarity.",
        "zenodo_id": 1417603,
        "dblp_key": "conf/ismir/FooteCN02",
        "keywords": [
            "characterizing",
            "rhythm",
            "tempo",
            "quantitatively",
            "measure",
            "rhythmic",
            "similarity",
            "works",
            "collection",
            "automatic"
        ],
        "content": "Audio Retrieval by Rhythmic Similarity\nABSTRACT\nWe present a method for characterizing both the rhythm and tempo\nof music. We also present ways to quantitatively measure the\nrhythmic similarity between two or more works of music. This\nallows rhythmically similar works to be retrieved from a large col-\nlection. A related application is to sequence music by rhythmicsimilarity, thus providing an automatic “disc jockey” function for\nmusical libraries. Besides specific analysis and retrieval methods,\nwe present small-scale experiments that demonstrate ranking andretrieving musical audio by rhythmic similarity.\n1. INTRODUCTION\nWe present audio analysis algorithms that can automatically rank\nmusic by rhythmic and tempo similarity. Music in a user’s collec-\ntion is analyzed using the “beat spectrum,” a novel method of auto-\nmatically characterizing the rhythm and tempo of musical\nrecordings [1]. The beat spectrum is a measure of acoustic self-similarity as a function of time lag. Highly repetitive music will\nhave strong beat spectrum peaks at the repetition times. This\nreveals both tempo and the relative strength of particular beats, and\ntherefore can distinguish between different kinds of rhythms at the\nsame tempo. Unlike previous approaches to tempo analysis, thebeat spectrum does not depend on particular attributes such as\nenergy, pitch, or spectral features, and thus will work for any music\nor audio in any genre. The beat spectrum is calculated for everymusic file in the user’s collection. The result is a collection of\n“rhythmic signatures” for each file. We present methods of measur-\ning the similarity between beat spectra, and thus between the origi-\nnal audio. Given a similarity measure, files can be ranked by\nsimilarity to one or more selected query files, or by similarity withany other musical source from which a beat spectrum can be mea-\nsured. This allows users to search their music collections by rhyth-\nmic similarity, as well an enable novel applications. For example,\ngiven a collection of files, an application could sequence them by\nrhythmic similarity, thus functioning as an “automatic DJ.” \n2. BEAT SPECTRAL SIMILARITY\nDetails of the beat-spectral analysis are presented in [1]; weinclude a short version here for completeness. The beat spectrum is\ncalculated from the audio using three principal steps. First, the\naudio is parametrized into a spectral or other representation. Thisresults in a sequence of feature vectors. Second, a distance measure\nis used to find the similarity between all pairwise combinations of\nfeature vectors, hence times in the audio. This is embedded into a\ntwo-dimensional representation called a similarity matrix. The beat\nspectrum results from finding periodicities in the similarity matrix,using diagonal sums or autocorrelation.Given two works, we can\ncompute two beat spectra B\n1(l) and B2(l); both are 1-dimensionalfunctions of lag time l. In practice, l is truncated to some number L\nof discrete values. This yields L-dimensional vectors, from which\nthe Euclidean or other distance functions can be computed.\n2.1 Experiment 1\nIn this experiment, we determine how well Euclidean distancebetween beat spectra measures tempo difference. We generated dif-\nferent-tempo versions of the identical musical excerpt using com-mercially available music editing software, which can change the\nduration of a musical waveform without altering the pitch. This\nmusical excerpt consists of 16 4/4 bars of live vocals and instru-mentals over a rhythmic beat. The original excerpt was played at\n120 beats per minute (bpm; also denoted MM). Ten tempo varia-\ntions were generated at 2 bpm intervals from 110 to 130 bpm. \n1\nA first test of measuring beat spectral difference is a simple Euclid-ean distance between beat spectra. To this end, beat spectra were\ncomputed for each excerpt, and the squared Euclidean distance\ncomputed for all pairwise combinations. Figure 1 shows the result.Each line shows the Euclidean distance between one source\nexcerpt and all other files. This graphically demonstrates that the\nEuclidean distance increases relatively monotonically for increas-ing tempo differences. Of course, this is a highly artificial case in\nthat examples of the same music at different tempos are relatively\nrare. Still, it serves as a “sanity check” that the beat spectrum doesin fact capture useful rhythmic information. \n2.2 Experiment 2\nThe corpus for this experiment are 10-second excerpts taken fromthe audio track of “Musica Si,” available as item V22 of theMPEG-7 Content Set [2]. There were four songs that were long\nenough to extract multiple ten-second samples. Each song is repre-Audio Retrieval by Rhythmic Similarity\nJonathan Foote Matthew Cooper Unjung Nam\nFX Palo Alto Laboratory, Inc. FX Palo Alto Laboratory, Inc. CCRMA\n3400 Hillview Ave. 3400 Hillview Ave. Department of Music\n Building 4  Building 4 Stanford University\nPalo Alto, CA 94304 USA Palo Alto, CA 94304 USA Stanford, CA 94305 USA\nfoote@fxpal.com cooper@fxpal.com unjung@stanford.edu\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-vided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the fullcitation on the first page. \n© 2002 IRCAM – Centre Pompidou\n1This audio data can be found at http://www.fxpal.com/people/\nfoote/musicr/tempor.html00.511.5\nTempo (bpm)squared Euclidean distance\n110 130 120 122 124 126 128 118 116 114 112 \nFigure 1. Euclidean Distance vs. Tempo110\n112\n114\n116120\n122\n124\n126\n128\n130Audio Retrieval by Rhythmic Similarity\nsented by three ten-second excerpts, save for a pop/rock song\nwhose chorus and verse are each represented by three excerptsrespectively. Each excerpt was assumed to be relevant to other\nexcerpts from the same tune, and not relevant to all other excerpts.\nThe one exception is that the verse and chorus of the pop/rock songwere markedly different in rhythm and so are assumed to not berelevant to each other. Thus we have three ten-second excerptsfrom each of five relevance classes (three songs plus two song sec-tions), for a total of 15 excerpts as shown in Table 1. \nEach beat spectrum was normalized by scaling so the peak magni-\ntude (at zero lag) was unity. Next, the mean was subtracted fromeach vector. Finally the beat spectra were truncated in time.Because the short-lag spectra is similar across all files and thus notinformative, the first 116 ms was truncated, as well as lags longerthan 4.75 s.t The result was a zero-mean vector having a length of200 values, representing lags from 116 ms to 4.75 s for each musi-cal excerpt. (The effect of varying the truncation regions was notexamined, and it is unlikely that the ones chosen are optimal.)\nSeveral distance measures were investigated. First, each of the 15\ncorpus documents was then ranked by similarity to each queryusing Euclidean distance. Each query had 2 relevant documents inthe corpus, so this was the cutoff point used to measure retrievalprecision. After ranking by increasing Euclidean distance, 24 of the30 possible documents were relevant, giving a retrieval precisionof 80%. A second measure used is a cosine metric, or the inner\nproduct of the vectors normalized by the product of their magni-\ntudes. This measure performed significantly better than the Euclid-ean distance: 29 of the 30 documents retrieved were relevant,giving a retrieval precision of 96.7% at this cutoff.\nThe final distance measure is based on the Fourier coefficients of\nthe beat spectrum. The fast Fourier transform was computed foreach beat spectral vector. The log of the magnitude was then deter-mined, and the mean subtracted from each coefficient. Becausehigh “frequencies” in the beat spectra are not rhythmically signifi-cant, the transform results were truncated to the 25 lowest coeffi-cients. The cosine distance metric was computed for the 24 zero-mean Fourier coefficients, which served as the final distance met-ric. Experimentally, this measure performed identically to thecosine metric using an order of magnitude fewer parameters.\n3. CONCLUSION\nWe have presented an approach to finding rhythmically similarmusic and audio. Though the experiments here are on an admit-tedly very small corpus, there is no reason that these methods couldnot be scaled to thousands or even millions of works.In contrast to other approaches, the beat spectrum does not depend\non assumptions such as silence, periodic peaks, or particular time\nsignatures in the source audio. Practical applications include an“automatic DJ” for personal music collections, and we are cur-\nrently prototyping such a system. We hypothesize that a satisfying\nsequence of arbitrary music can be achieved by minimizing thebeat-spectral difference between successive songs. This ensures\nthat song transitions are not jarring, for example following a partic-\nularly slow or melancholic song with a rapid or energetic one.Additionally, rhythmic retrieval could usefully be with other sys-\ntems that retrieve music by pitch or timbral similarity [7]. Such a\nhybrid retrieval engine might allow users to trade off spectral andrhythmic similarity to suit their particular information needs.\n4. REFERENCES\n[1] Foote, J. and Uchihashi, S. “The Beat Spectrum: A New\nApproach to Rhythm Analysis,” in Proc. International Con-\nference on Multimedia and Expo  2001. http://\nwww.fxpal.com/people/foote/papers/ICME2001.htm\n[2] MPEG Requirements Group. “Description of MPEG-7\nContent Set,” http://www.darmstadt.gmd.de/mobile/MPEG7/Documents/N2467.html\n[3] Scheirer, E., “Tempo and Beat Analysis of Acoustic Musi-\ncal Signals,” in J. Acoust. Soc. Am.  103(1), Jan. 1998, pp\n588-601.\n[4] G. Tzanetakis, P. Cook, “Automatic Musical Genre Classifi-\ncation of Audio Signals,” in Proc. ISMIR 2001\n[5] Wold, E., Blum, T., Keislar, D., and Wheaton, J., “Classifi-\ncation, Search and Retrieval of Audio,” in Handbook of\nMultimedia Computing , ed. B. Furht, pp. 207-225, CRC\nPress, 1999. \n[6] Cliff, David, “Hang the DJ: Automatic Sequencing and\nSeamless Mixing of Dance Music Tracks,” HP Technical\nReport HPL-2000-104 , Hewlett-Packard Labs, Bristol UK\n[7] Pye, D., “Content-based Methods for the Management of\nDigital Music,” in Proc. ICASSP 2000,  vol. IV pp 2437.\n−0.200.20.40.60.81\ntime (s)0 1 2 3 42\n4\n6\n8\n10\n12\n14\nFigure 2. Beat spectra of retrieval data set.Table 1. Retrieval set: 10-second excerpts from “Musica Si” [2 ]\nTime (mm:ss) Song Title (approximate) Description  Set\n09:12 “Toto Para Me” acoustic guitar + vocals A09:02 “Toto Para Me” acoustic guitar + vocals A\n08:52 “Toto Para Me” acoustic guitar + vocals A\n07:26 “Never Loved You Anyway” pop/rock chorus B06:33 “Never Loved You Anyway” pop/rock chorus B\n06:02 “Never Loved You Anyway” pop/rock verse C\n05:52 “Never Loved You Anyway” pop/rock verse C05:30 “Never Loved You Anyway” pop/rock chorus B\n04:53 “Never Loved You Anyway” pop/rock verse C\n01:39 “Everybody Dance Now” dance + rap vocals D01:29 “Everybody Dance Now” dance + rap vocals D\n01:19 “Everybody Dance Now” dance + vocals D\n00:25 “Musica Si Theme” theme + vocals E00:15 “Musica Si Theme” theme + vocals E\n00:05 “Musica Si Theme” theme intro E"
    },
    {
        "title": "Digital Image Capture of Musical Scores.",
        "author": [
            "Ichiro Fujinaga",
            "Jenn Riley"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416554",
        "url": "https://doi.org/10.5281/zenodo.1416554",
        "ee": "https://zenodo.org/records/1416554/files/FujinagaR02.pdf",
        "abstract": "Musical scores have small details and complex markings, and are difficult to digitally capture and deliver well. All capture decisions should be made with a clear idea of the purpose of the resulting digital images, but master images must be flexible enough to fulfill unanticipated future uses. In order to provide a framework for decision-making in musical score digitization projects, best practices for detail and color capture are presented. Recommendations for file formats for archival storage, web delivery and printing of musical materials are presented.",
        "zenodo_id": 1416554,
        "dblp_key": "conf/ismir/FujinagaR02",
        "keywords": [
            "Musical scores",
            "small details",
            "complex markings",
            "digital capture",
            "flexible enough",
            "future uses",
            "best practices",
            "file formats",
            "archival storage",
            "web delivery"
        ],
        "content": "Digital Image Capture of Musical Scores \nDigital Image Capture of Musical Scores \nIchiro Fujinaga \nPeabody Conservatory of Music \nJohns Hopkins University \n1 E. Mt. Vernon Pl. \nBaltimore, MD 21202 \n+1 (410) 659-4100 \nich@jhu.edu Jenn Riley \nDigital Library Program \nIndiana University \n1320 E. 10th St., E170 \nBloomington, IN 47405 \n+1 (812) 856-5759 \njenlrile@indiana.edu\nABSTRACT  \nMusic al scores have sma ll details and complex markings, and are \ndifficult to digitally  capture and deliver well. All capture \ndecisions should be made with a clear idea of the purpose of the \nresulting digital im ages, but m aster images must be flexible \nenough to fulfill unanticipated future uses. In order to provide a \nframework for decision-making in  musical score digitization \nprojects , best practices  for detail and color capture are presented. \nRecom mendations  for file form ats for archival storage, web \ndelivery  and printing of musical materials are presented. \n1. INTRODUCTION \nMusic inform ation retrieval res earch deals  with resource \ndiscovery . However, the quality  of the data itself is a critical part \nof the retrieval sy stem, as cont ent-based retrieval cannot work on \ninferior content. Musical scores are difficult to properly  digitally  \ncapture and deliver for several r easons. They  contain small details \nsuch as  staff lines , dots , and bars  that are es sential to the meaning \nof the notation. They  come in a wide variety  of formats (printed \nand manuscript) and sizes. This paper is intended to provide an \noverview of the decisions that mu st be made when undertaking a \nmusical score digital imaging pr oject and provide some best \npractices  for m aking thes e decis ions. \n2. THE PURPOSE OF SCANNING \nThe firs t step in planning any  imaging project is to determ ine the \npurpos e of im age capture. S ome form ats, such as  original \nmanuscripts, rare im prints, a nd pages with important pencil \nmarkings or annotations, should be captured with the intent of \nreproducing the current appearance of the page as is. For many  \nother formats, the current conditi on of the pages is not important, \nbut instead the intent is to capture only  the musical content \ncontained within. The guidelines below offer advice on two types \nof capture: artifactual and conten t-based. Some imaging projects \nfocus on capturing waterm arks on the paper, but this will not be \ncovered here [a] [1][2][3]. \nFor rare and fragile materials, analog capture on film  may be a \nbetter option than digital im aging,  as digital cam eras do not y et \nperform as well as flatbed and film scanners. Generally  4”x5” \ncolor film  is m ost suitable for this purpose [b] . \n3. MASTER FILE SPECIFICATIONS \n3.1 Resolution \nMusical notation must be scanne d at a res olution s ufficient to \ncapture all significant  detail. For a collection with little variation \nin print size, it may be appropriate to apply  a form ula to determ ine the m inimum capture resolution based on the stroke width of the \nsmallest detail [4] . We have found this smallest detail is generally  \nthe white s pace between beam s. 3 pixels  per detail, rather than 2 \npixels, is required for successful Optical Musical Recognition \n(OMR) with the forthcoming Gamera software. [5]  \nHowever, this approach presen ts som e problem s. Details in \nmusical notation are consistently  smaller than 1mm, requiring \nspecialized measuring equipm ent. Als o, few collections  are \nhomogeneous, which would require  resolution calculation for \neach item , adding a great deal of tim e to the capture proces s.  \nFor most projects it m ay be m ore practical to sim ply capture all \nimages at the same resolution. Our tests have found that 600dpi is \nsufficient to capture all significant detail for m ost printed m usical \nnotation, as seen in Figures 1 and 2, where the 600dpi scan more \nadequately  renders the ledger line and the sharp sign.  600dpi will \ncapture detail as small as .005 in (.027mm) with 3 pixels. Our \npreliminary  studies show that  going above 600dpi does not offer \nmuch advantage for the purpose of viewing, printing, or OMR, \neven for extrem ely small notation. However, tes ting with specific \nhardware and materials is require d to develop a final resolution \nspecification for any  given project. \n \n  \nFigure 1. 300dpi small detail. Figure 2. 600dpi small detail. \n3.2 Color Reproduction and Bit Depth \nMost printed musical scores do not use color. 8-bit or higher \ngrayscale capture of these is most appropriate. S heet m usic \ncovers, original manuscripts, a nd other color pages should be \ncaptured in 24-bit RGB. If the purpose of capture is both to \nrepres ent the page as  an artifact and to gain a file appropriate for \nOMR, 12-, 14-, or 16-bit per channel capture may  be needed. \nIt is essential that all devices in the im aging sy stem , including \nscanner, monitor and printer (if printed output is desired), be \ncharacterized and managed via International Color Consortium \n(ICC) profiles  [c]. Locally -created ICC profiles  for each device \nare preferable to generic profiles [d] . Color management with ICC \nprofiles, while not perfect, is the best current solution to the \nproblem  of accurate tonal capture. S canning for artifactual \npurposes is then sim ply capturi ng the item  in a fully  calibrated \nsystem with no manual adjustments.  Use of ICC profiles is also \npreferred for content-bas ed capture to ens ure that the visual image \nthe operator is  viewing when m aking adjus tments is accurate.  Permission to m ake digital or  hard copies of all or  part of this \nwork for per sonal or  classr oom use is gr anted without fee \nprovided that copies ar e not made or distr ibuted for  profit or  \ncommercial advantage and that copies bear this notice and the full \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou Digital Image Capture of Musical Scores \nFor content-only capture, automatic or manual tonal adjustments \nmay be desirable in order to ma ximize the contrast between the \nmusical notation and background of  the page. Pages should have \ncompletely filled-in note heads, solid staff lines, and clean white \nspace between lines when viewed at 100% magnification. \nDeskewing and sharpening are also options. \n3.3 Master File Formats \nImaging best practices to da te have almost unanimously \nadvocated uncompressed TIFF for st orage of master files [e]. \nHowever, the TIFF specification is a de facto rather than a true standard. The PNG format may be an emerging replacement for \nTIFF for this purpose [f]. PNG has the capability to store all \nrelevant information captured according to these guidelines, and as such would be technically suitable for a master file format for \nmusical score images. PNG uses lossless compression, and \ncompressed PNG files are significantly smaller than \nuncompressed TIFF files. It may be  some time before it is clear \nwhether the digital library community as a whole accepts PNG for \nthis purpose. \n4. WEB DELIVERY  \nFor web delivery, only JPEG, GIF, PNG, and PDF are supported by majority of web browsers with common plug-ins. Thus formats \nsuch as TIFF and DjVu [6][g] are not currently appropriate for \nthis purpose. Delivery by single pa ge file formats such as JPEG, \nGIF, and PNG requires a page-tur ning mechanism for navigation. \nWhile the PDF format includes this functionality, it was not designed primarily for navigating raster image files, so the file \nsizes of full musical scores in PD F format are prohibitively large.  \nDigital imaging projects generally  create images for web viewing \nbased on a fixed pixel size. Howeve r, for musical notation, this \napproach must be abandoned in favor of one that ensures all \nsignificant detail is visible. Fo r this reason, use of thumbnail \nimages may not be necessary. Gr ayscale or color images at 100-\n200dpi (based on original page size)  generally display all relevant \ndetail on the screen, as seen at <http://www.dlib.indiana.edu/ \n~jenlrile/ismir2002/webdelivery/>. In  grayscale, JPEG images at \nmedium-high to high quality levels are smaller than GIF files, and do not show obvious compression artif acts. File sizes are similar \nfor JPEG and PNG, but PNG files decompress faster [h]. \n5. PRINTING \nScore pages intended for use in practice or performance must be \nprintable. Ideally, a multi-page file format would be used, but \nPDF, DjVu, and multi-page TIFF all present size or accessibility \nproblems. For laser printing, bitonal TIFF, GIF, and PNG all \nprovide similar results at 250-400dpi [i]. For lower resolutions in \nthis range, bitonal PNG files are smaller, while at higher \nresolutions, CCITT Group 4 compressed TIFF files are smaller, as \nshown in Table 2. \nTable 2. Average file size for 8.5”x11” bitonal images. \n PNG  TIFF (Group 4)  \n800dpi 329 KB 192 KB \n400dpi 183 KB 146 KB \n250dpi 90 KB 96 KB \n200dpi 64 KB 71 KB \n100dpi 25 KB 38 KB \n6. CONCLUSION \nConsistent and useful informati on retrieval depends heavily on the \npresence of adequate data. The overarching philosophy of capturing and presenting to the user all relevant data in the source \nmaterial should guide digitizati on decisions. These guidelines are based on current best practices in archival digital imaging, \nadapted to suit the unique capture and delivery requirements of \nmusical notation. \n7. ACKNOWLEDGMENTS \nIchiro Fujinaga would like to thank Michael Droettboom, Karl \nMacMillan, and Asha Srinivasan fo r their help in the preparation \nof this paper. This research is funded in part by the NSF's DLI-2 initiative (#981743), IMLS National Leadership Grant, and \nsupport from the Levy Family. \n8. NOTES \n[a] Watermarks may be obtained by tracings, transmitted light \nphotography, photosensitive paper technique (Dylux); beta \nradiography, or microfocus radiography. Image processing \ncombining reflective and transmissive scans may offer \ninexpensive and relatively accurate results. \n[b] Ilford's Ilfochrome Micrographic film has an estimated life \nexpectancy of over 300 years. \n<http://www.microcolour.com/mci03.htm> \n[c] The ICC’s home page is at <http://www.color.org/>. \n[d] Software packages to perform profiling on your own devices \nare commercially available, such as those from Monaco \nSystems <http://www.monacosys.com/index.html>. \n[e] NARA <http://www.nara.gov/nara /vision/eap/digguide.pdf> \nand LC <http://memory.loc.gov/ammem/formats.html> have taken the lead in documenting best practices for digital \nimaging. \n[f] PNG (ISO/IEC 15948) is currently under publication.  \n[g] DjVu holds promise in this area, as it was designed for raster image compression. Files are extremely small, and as the \nformat becomes more pervasive DjVu may become a \npractical option. \n[h] Browsers not supporting PNG are now extremely rare:  <http://www.thecounter.com/sta ts/2002/April/browser.php>.\n \n[i] See <http://www.dlib.indiana.edu/~jenlrile/ismir2002/print/> for sample images for printing.\n \n9. REFERENCES \n[1] Edge, D. 2001. The digital imaging of watermarks. Computing in Musicology 12: 261-74. \n[2] Stewart, D., R. A. Scharf, a nd J. S. Arney. 1995. Techniques \nfor digital image capture of watermarks. Journal of Imaging \nScience and Technology 39(3): 261--7. \n[3] Wenger, E. et al. 1995. Image analysis for dating of old manuscripts. Lecture Notes in Computer Science  1024:522-3. \n[4] Kenney, A., and O. Rieger. 2000. Moving Theory into \nPractice.  Mountain View, California: Research Libraries \nGroup. 46-47. <http://www.library .cornell.edu/preservation/ \ntutorial/conversion/c onversion-05.html>. \n[5] MacMillan, K., M. Droettboom and I. Fujinaga. 2001. Gamera: A structured document recognition application \ndevelopment environment. Proceedings of the International \nSymposium on Music Information Retrieval. 15-6. \n[6] Bottou, L., P. Haffner, P. G. Howard, P. Simard, Y. Bengio, \nand Y. Le Cun. 1988. High quality document image \ncompression with DjVu. Journal of Electronic Imaging  7(3): \n410-25."
    },
    {
        "title": "Interdisciplinary Communities and Research Issues in Music Information Retrieval.",
        "author": [
            "Joe Futrelle",
            "J. Stephen Downie"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416406",
        "url": "https://doi.org/10.5281/zenodo.1416406",
        "ee": "https://zenodo.org/records/1416406/files/FutrelleD02.pdf",
        "abstract": "Music Information Retrieval (MIR) is an interdisciplinary research area that has grown out of the need to manage burgeoning collections of music in digital form. Its diverse disciplinary communities have yet to articulate a common research agenda or agree on methodological principles and metrics of success. In order for MIR to succeed, researchers need to work with real user communities and develop research resources such as reference music collections, so that the wide variety of techniques being developed in MIR can be meaningfully compared with one another. Out of these efforts, a common MIR practice can emerge.",
        "zenodo_id": 1416406,
        "dblp_key": "conf/ismir/FutrelleD02",
        "keywords": [
            "Music Information Retrieval",
            "Interdisciplinary research area",
            "Managing music collections",
            "Need for real user communities",
            "Methodological principles",
            "Metrics of success",
            "Common research agenda",
            "Developing research resources",
            "Reference music collections",
            "Meaningful comparison"
        ],
        "content": "Interdisciplinary Communities and Research Issues in Music Information Retrieval \n Interdisciplinary Communities and Research Issues in \nMusic Information Retrieval \nJoe Futrelle \nGraduate School of Library and Information Science \nUniversity of Illinois \nChampaign, IL 61820  \n+1 217 265 0296 \nfutrelle@uiuc.edu J. Stephen Downie \nGraduate School of Library and Information Science \nUniversity of Illinois \nChampaign, IL 61820 \n+1 217 265 5018 \njdownie@uiuc.edu \nABSTRACT \nMusic Information Retrieval (MIR) is an interdisciplinary research \narea that has grown out of the need to manage burgeoning \ncollections of music in digital form. Its diverse disciplinary \ncommunities have yet to articulate a common research agenda or \nagree on methodological principles and metrics of success. In \norder for MIR to succeed, researchers need to work with real user \ncommunities and develop research resources such as reference \nmusic collections, so that the wide variety of techniques being \ndeveloped in MIR can be meaningfully compared with one \nanother. Out of these efforts, a common MIR practice can emerge. \n1. INTRODUCTION \nMusic Information Retrieval (MIR) is a rapidly growing \ninterdisciplinary research area encompassing computer science \nand information retrieval, musicology and music theory, audio \nengineering and digital signal processing, cognitive science, \nlibrary science, publishing, and law. Its agenda, roughly, is to \ndevelop ways of managing collections of musical material for \npreservation, access, research, and other uses. In this way it \nresembles traditional library science, and indeed, libraries have \nhistorically led the development of music collections. The idea of \napplying automatic information retrieval (IR) techniques to music \ndates back to the 1960’s (Kassler 1966). But in particular, MIR \nhas grown recently out of an explosion of interest in networked \ncollections of musical material in digital form, precipitated by the \ndevelopment of compression technologies such as mp3, online \nservices such as Napster, advances in optical musical recognition \n(OMR), and the ever-plummeting costs of digital storage and \nbandwidth. In this sense MIR is closely related to Digital \nLibraries. \nAs in other interdisciplinary fields, discourse in MIR is impeded \nat disciplinary boundaries by unfamiliar jargon, differing \nmethodology, and even philosophical and ethical differences. To \nunderstand the field, it is currently necessary to acquire at least a \ncursory understanding of each of the disciplines, and MIR \nresearchers are undertaking this as they begin to develop a \ncommon practice (Downie 2001). \nThis paper investigates MIR’s interdisciplinary communities and \nresearch issues by surveying the proceedings of the International \nSymposia on Music Information Retrieval (ISMIR 2000 and \n2001). The ISMIR series of meetings is an explicit attempt to \ngather together all of the disciplines and research areas pertinent \nto MIR. ISMIR can also claim to be the only conference series \nexclusively devoted to the advancement of MIR research. For these reasons, we believe the proceedings of ISMIR provide a \nrepresentative “snapshot” of the major issues comprising MIR \nresearch and development. Based on these proceedings and with \nreference to components of their supporting literatures, this paper \nwill characterize the field, outline the major research communities \ninvolved in the field, assess the state of the art in each community, \nidentify coverage gaps, and propose a research agenda aimed at \naddressing those gaps. \n2. WHAT IS MIR RESEARCH? \nWhat are MIR researchers trying to build and what problems are \nthey trying to solve?  MIR researchers often characterize their \nmotivations by pointing out that the increasing volume of digital \nmusic available necessitates new retrieval techniques (Durey et al. \n2001; Hoos et al. 2001; Kornstädt 2001; Yang 2001). However, \nthe lack of effort to assess this volume, its rate of growth, and/or \ncompare its rate of growth against the cost of bandwidth, storage, \nand processing power, and the relative scarcity of research such as \nJang et al. (2001) focusing on scaling existing techniques, indicate \nthat MIR researchers are primarily concerned with larger, more \nfundamental problems. \nMIR researchers understand that the increasing availability of \ndigital music is merely an aggravating factor of a more significant \nissue: few effective retrieval techniques exist for digital music \ncollections. The problem has existed since music was first \nencoded digitally, but has become pressing only recently as the \ncost of storing large digital music collections has dropped to \nalmost nothing and the number of such collections has \nconsequently exploded. For MIR, developing effective retrieval \ntechniques is basic research, which continues to advance on a \nnumber of interrelated fronts. \nDeveloping IR techniques for music is challenging because of the \nwide variety of ways music is produced, represented, and used \n(Smiraglia 2001). Basic research in MIR can be categorized \nroughly by the kind of music representation employed.  \nTable 1 shows some representations and the kinds of MIR \nresearch being applied to them. \nIn addition to the variety of music representations, their \ncomplexity presents a problem as well. Like language, music in \nvirtually all of its representations contains difficult-to-extract \nlayers of significance, such as harmony, polyphony, and timbre. \nEven the most robust representations still require sophisticated \nprocessing techniques to extract some of these features, and \ndeveloping these techniques is an active area of MIR research. \nThis area is often called “content-based” MIR, to distinguish it \nfrom more traditional digital and pre-digital approaches based on \nmanually-produced metadata of bibliographic and related \nvarieties. \nAs basic MIR research begins to produce results, it raises the \nquestions of what kind of MIR systems can be built, what their Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM – Centre Pompidou Interdisciplinary Communities and Research Issues in Music Information Retrieval \nuser interfaces would be, and what their institutional or economic \ncontexts would be. These questions occupy a broad swath of MIR \nresearch, ranging from system architecture and testbeds (such as \nVARIATIONS (Dunn et al. 1999; Dunn 2000; Dunn et al. 2001), \nMELDEX (Bainbridge et al. 1999; Bainbridge 2000), and the \nLevy Sheet Music Collection (Choudhury et al. 2000)) to \nIntellectual Property rights (Levering 2000). \nTable 1: Music Representations in MIR \nRepresentation Description Research \nSymbolic Notation (scores, \ncharts), Event-based \nrecordings (MIDI), \nHybrid representations Matching, \nTheme/Melody \nExtraction, Voice \nSeparation, Musical \nAnalysis \nAudio Recordings, Streaming \nAudio, Instrument \nLibraries Sound/Song \nSpotting, \nTranscription, \nTimbre \nClassification, \nMusical Analysis \nVisual Scores Score Reading \n(“Optical Music \nRecognition”) \nMetadata Cataloging, \nBibliography, \nDescriptions Library Testbeds, \nTraditional IR, \nInteroperability \n3. MIR COMMUNITIES \nUnderstanding a multi-disciplinary field requires understanding \nthe disciplines involved and the variety of research interests they \nrepresent. Table 2 summarizes the research communities involved \nin MIR along with their typical home institutional settings and \ntypical areas of research. \nTable 2: MIR Communities \n \nTable 3 describes major research areas in MIR, along with basic \nresearch questions and exemplar papers in those areas. Table 3: MIR Research Areas \nResearch Area Description \nRepresentation How should musical material be \nrepresented in digital form?  What aspects \nof music are critical to represent for the \npurpose of building music collections?  At \nwhat level of granularity can we represent \nmusic?  What kinds of representation are \nthe most efficient? How can markup \nlanguages be applied to music?  (Good \n2000; Hoos et al. 2001; Lindsay et al. \n2001; Maidín et al. 2001) \nIndexing How can database indexing techniques be \napplied to musical material so it can be \nretrieved effectively and efficiently? \n(Downie 1999; Chen 2000; Pickens 2000) \nRetrieval What kinds of queries can we perform on \nindexed collections of musical material?  \nHow can the performance of these queries \nbe evaluated and improved? (Hsu et al. \n2001; Lemström et al. 2001) \nUser Interface Design How can user interfaces be built which \nenable users to effectively find and use \ndigital musical material from a collection? \n(Fernström et al. 2001; Kornstädt 2001; \nMacMillan et al. 2001) \nCompression How can audio be encoded more \nefficiently?  What are the implications for \nMIR of various emerging compression \ntechnologies? (Lindsay et al. 2001) \nFeature Detection How can distinguishing features of music \nbe detected from audio signals?  How can \nthese techniques be applied to MIR \nsystems?  (Foote 1999; Ismirli 2000; \nLogan 2000; Nam et al. 2001; Tzanetakis \net al. 2001)   \nMachine Learning How can we deduce or induce aggregate \nmusical features of collections, so that they \ncan be organized for retrieval?  What are \nthe most efficient and effective ways of \nrepresenting these aggregate features? (cf. \nClassification) \nClassification What kinds of classification techniques \nand schemes can/should be applied to \ndigital music collections? (cf. Machine \nLearning) (Herrera et al. 2000; Reiss et al. \n2001) \nMusical Analysis How is a musical composition organized?  \nHow is it similar to, or different from, \nother pieces of music?  How can MIR \nsystems meet the needs of musicologists?  \n(Bonardi 2000; Larson 2000; Cope 2001; \nKornstädt 2001) \nMetadata What kinds of descriptive or contextual \ninformation about musical material can \nand/or should be managed by an MIR \nsystem, and how should such metadata be \nrepresented? (cf. Representation) \n(Choudhury et al. 2000; Smiraglia 2001) \nUser Studies What kinds of MIR capabilities do users \nneed?  How do users search for musical \nmaterial, and why?  What would be the \nideal MIR system for a given user \ncommunity? (Itoh 2000; Selfridge-Field \n2000; McPherson et al. 2001) Community Type of \nInstitution(s) Research Areas \nComputer Science, \nInformation \nRetrieval Academic, \nCommercial Representation, \nIndexing, Retrieval,  \nMachine Learning, \nUser Interface Design \nAudio Engineering, \nDigital Signal \nProcessing Academic, \nCommercial Compression, Feature \nDetection, Pitch \nTracking, Machine \nLearning, \nClassification, Musical \nAnalysis \nMusicology, Music \nTheory Academic Representation, \nMusical Analysis \nLibrary Science Libraries, Academic Representation, \nMetadata, User \nStudies, Classification, \nIntellectual Property \nRights, User Interface \nDesign \nCognitive Science, \nPsychology, \nPhilosophy Academic Representation, \nPerception, User \nStudies, Ontology \nLaw Government, Legal \nProfession, \nAcademic Intellectual Property \nRights Interdisciplinary Communities and Research Issues in Music Information Retrieval \nIntellectual Property \nRights Who owns musical material?  Under what \nconditions?  Under what arrangement can \ndigital libraries of musical material and \nowners of IP rights for musical material \npeacefully coexist?  (Levering 2000) \nPerception How to people perceive music?  How can \nmusic perception inform the design of MIR \nsystems?  What is music?  How is musical \nsimilarity perceived? (cf. Ontology) (Perrot \net al. 1999; Huron 2000; Hofmann-Engl \n2001) \nEpistemology/Ontology What is music?  What is a musical \ncomposition?  What is the relationship \nbetween different representations of ªthe \nsameº piece?  How do improvised aspects \nof music relate to composed aspects with \nrespect to collections of musical material? \n(Smiraglia 2001) \n \n3.1 Computer Science, Information Retrieval \nOf course, virtually all MIR research employs techniques from \ncomputer science. But there is an important subset of MIR \nresearch whose origins can be traced back to the Information \nRetrieval research on bibliographic text retrieval systems in the \nearly 1960’s. This ongoing research emphasizes techniques for \nlocating items in a collection or index which match a query, rather \nthan techniques for analyzing aggregate properties of collections \nof items (e.g. data mining).  For an overview of traditional IR see \nBaeza-Yates et al. (1999). MIR research based on traditional IR is \ntypically aimed at supporting a scenario in which users know \ncharacteristics of the music they desire, and use an MIR system to \nlocate musical material that most closely matches those \ncharacteristics. Downie (1999) calls this a “locating” MIR system. \nIn traditional IR, a query on a collection can be thought of as a \nfragment or reduced form of the desired item from the collection. \nFor text collections, the query is often a word that occurs in the \ndesired document. MIR researchers have taken some pains to \ndevise MIR systems which fit this model, most notably Downie \n(1999) who reduces the music in his collection to n-grammed \nsequences of intervals, which can then be indexed using inverted \nfiles. Other research on locating MIR systems uses other \ntraditional IR strategies, such as probabilistic modeling (Pickens \n2000) and approximate string matching (Lemström et al. 2001). \nSome work also addresses IR issues such as relevance and ranking \n(Uitdenbogerd 2000). Most of this research is based on symbolic \nmusic representations, but it has also been applied to audio which \nis pre-processed and converted to symbolic sequences of audio \nfeature classes as in Aucouturier (2001) and Batlle et al. (2000). \nA great deal of attention has been paid to so-called “Query by \nHumming” (QBH) systems, which retrieve pieces based on \nmelodic fragments sung by the user. QBH systems typically \ncombine melody extraction from an audio query with a locating \nMIR system to match the melody against a target database. In the \nISMIR 2001 proceedings, 10 out of 43 papers, posters, and talks \n(23%) concerned QBH systems. \n3.2 Audio Engineering, Digital Signal \nProcessing \nA major category of MIR research concerns audio representations \nof music (i.e. recordings, audio streams, or live performance). \nTechniques used in this area grow out of decades of work in \nDigital Signal Processing (DSP) and speech recognition. A good \noverview of these techniques is found in Foote (1999). The \ntechniques are applied to an interrelated set of problems: · What are the most important features of audio \nrepresentations of music for MIR, and how can they be \nextracted from audio?  (e.g. melodies, harmonies, \ninstrument timbres, etc.) \n· Given a set of features extracted from audio, what \ntechniques can be used to understand the relationships \nbetween those features in an audio collection? \n· How can we use audio to perform structural analyses of \nmusic, and how can these be used to improve MIR \nsystems? \nA number of audio features have been used in MIR research. \nVirtually every audio MIR system uses some kind of frequency-\ndomain transformation of the signal, such as the Fast Fourier \nTransform (FFT) (Brigham 1988) or its more musically-relevant \nderivative, mel-frequency cepstral coefficients (MFCC) (Logan \n2000). A number of other features are used, including time-\ndomain autocorrelation (mostly used for pitch tracking) and \nwavelet transforms (Tzanetakis et al. 2001), but most of the \nfeatures employed are statistics computed from the FFT. The \nreason frequency-domain transformations are so prevalent is the \nprimacy of periodicity in the perception of musical aspects such as \npitch, timbre, and rhythm. \nOnce a set of features is selected and can be reliably extracted \nfrom audio, the problem is essentially one of multivariate analysis, \nin which each piece of music in a collection can be conceptualized \nas a vector in n-dimensional feature space. Traditional \nmultivariate techniques as well as probabilistic machine-learning \ntechniques such as Hidden Markov Models (HMM) and neural \nnetworks can be applied to identify salient features and perform \ndata reduction, often through classification. A good overview of \nthese and other classification techniques for audio can be found in \nHerrera et al. (2000). \n3.3 Musicology, Music Theory \nThe study of music is an important application area for MIR, and \nthus drives much of MIR research.  Musicology is an ancient and \ninterdisciplinary field which has been transformed by \ncomputational techniques (Bel et al. 1993) and promises to be \nfurther transformed by ready access to large digital music \ncollections.  Musicology-related MIR research ranges from \ncomputational music analysis, as in Cope (2001), to MIR systems \nspecialized for musicologists, as in Bonardi (2000) and Kornstädt \n(2001).  An interesting tension exists in MIR between musical \nanalysis which concerns the nature of music per se and is often \nqualitative as in Larson (2000), and approaches that attempt to \nempirically demonstrate improved retrieval performance and thus \nrely on quantitative techniques such as statistical analysis and \nmachine learning. Cope’s work with computational musical \nanalysis (Cope 2001) and algorithmic composition (Cope 1992) \ninterestingly bridges these two very different perspectives by using \ncomputational techniques to divine aspects of music, such as style, \nwhich have traditionally been investigated qualitatively or with \nexhaustive manual effort as in Van der Merwe (1989).  There is \nclearly some middle ground that remains unexplored, since \naspects of music that have currently only been characterized by \nmusicologists may yet prove useful in the design of MIR systems. \n3.4 Library and Information Science \nLibraries and library scientists are involved in MIR as part of their \nongoing effort to cope with ballooning multimedia collections. \nLibraries face all of the issues raised by MIR, from basic research \nquestions such as how to represent and index musical material, to \napplied information technology issues such as integrating \ntraditional bibliographic systems with advanced MIR tools, to Interdisciplinary Communities and Research Issues in Music Information Retrieval \npolicy issues such as how to manage intellectual property rights \nfor the producers and users of music collections. Of particular \nimportance to MIR are a number of testbed projects being \nundertaken at academic libraries and digital library research \nfacilities, including Indiana’s Digital Music Library projects \n(Dunn et al. 1999; Dunn 2000; Dunn et al. 2001), the University \nof Waikato’s MELDEX digital library (Bainbridge et al. 1999; \nBainbridge 2000; McPherson et al. 2001), and the Levy sheet \nmusic collection at Johns Hopkins (Choudhury et al. 2000). These \ntestbeds integrate a variety of MIR tools with significant music \ncollections in order to explicitly address issues such as usability, \nscale, and multi-modal access to musical works (e.g. linking \nscores with recordings). They also support the application of MIR \ntools and collections to specialized use cases such as music theory \neducation. \nTestbeds are a good way to begin to evaluate who the potential \nusers of MIR systems are and what features they are most \ninterested in. Preliminary user studies such as McPherson et al. \n(2001) indicate a trend away from speculative user requirements \nanalysis such as that found in, for example, Bonardi (2000) \ntowards empirically grounded approaches and techniques. The \nuser modeling methods put forward by Rolland (2001)  suggest \nthat future MIR systems can be tailored to meet the needs of a \nvariety of user communities. \n3.5 Cognitive Science, Psychology, Philosophy \nA small subset of MIR research concerns the implications of \nmusic perception on the design of MIR systems. Research efforts \nrange from models of music perception such as Dannenberg \n(2001) and Hofmann-Engl (2001) to epistemological analysis of \nmusic information such as Smiraglia (2001). Much research has \nbeen done on music perception in psychology, music psychology \nand cognitive science (Deliège et al. 1997; Cook 1999). There \nalso have been notable efforts in both music philosophy (Adorno \n1973) and cultural studies (Attali 1985; McClary 1991) to \ncharacterize how music is understood as a social and cultural \nphenomenon. Significantly, however, MIR researchers have so far \nrarely adopted work in these areas as a basis for MIR studies. \n3.6 Law \nHigh-profile cases such as Napster demonstrate that MIR systems \nare being developed in an uncertain regulatory environment, and \nlegal issues will continue to be important to MIR researchers until \nthis situation changes. Issues such as the Digital Millennium \nCopyright Act (Levering 2000), intellectual property rights \nmanagement, and researcher access to music databases (Byrd \n2001; Downie 2001) will be important to the field indefinitely. To \na large extent, copyright law is a policy issue rather than a \ntechnical issue, but legal issues dramatically affect the priorities of \ncommercial and non-commercial agencies funding MIR research, \nand thus are of critical importance. \n4. CRITICAL ANALYSIS OF COVERAGE \nGAPS IN MIR RESEARCH \nMIR’s newness and multi-disciplinary constituency make the field \nstrong on innovation and basic research, but weak on evaluation \nand application to real user communities. The problem is twofold: \n1. There are no commonly accepted means of comparing \nthe efficacy of retrieval techniques; and, \n2. There have been few if any attempts to study potential \nusers of MIR systems to find out what they need.  \nThe two problem areas are interrelated in that meaningful \nevaluation of retrieval techniques must be grounded in a \nsignificant understanding of user requirements. In addition to these evaluation-related gaps, there are also areas of \nbasic research that are receiving more and less attention than they \nshould. In particular, the amount of emphasis on QBH systems \nappears to be unsupportable given doubts about their usefulness \n(McPherson et al. 2001) and scalability (Sorsa et al. 2001). \nResearch on recommendation systems, common in the DL and \ncommercial communities, is inexplicably rare in the MIR \ncommunity. User interface research, now undertaken most often as \nan afterthought to research into retrieval techniques, is clearly \nunder-emphasized, especially since retrieval interfaces may have \nto incorporate complex audio strategies such as those explored by \nFernström et al.  (2001).  And finally, MIR research as a whole \nhas failed to significantly address music outside of the common-\npractice Western music canon. \n4.1 Difficult To Compare Techniques \nResearch into MIR techniques rarely presents results that can be \ncompared with other research. Some studies such as Spevak et al. \n(2001) and Rolland (2001) do not present evaluation results at all. \nOthers present results based on very small sample sets. For \nexample, the particular technique of Mazzoni et al. (2001) is \nevaluated on a database with less than 20 pieces of music. Other \npapers report overall results without reference to any common \nmeasure of significance: for example Yang (2001) reports “90% \nretrieval accuracy” without explaining what constitutes “accurate” \nretrieval, Nishimura et al. (2001) report a “search rate” computed \nby averaging precision and recall together, etc.  \nWe believe this inconsistency largely arises from MIR’s \ninterdisciplinary nature. Evaluation metrics that are well \nunderstood in one field, such as precision and recall in traditional \nIR, are new and unfamiliar to other fields such as audio \nengineering. In addition, there are no community-wide music \ncollections against which researchers can cross-evaluate a wide \nvariety of different techniques, a problem which the community is \neager to address (Byrd 2001; Downie 2001). So far, the most \nsophisticated attempts to rigorously compare a variety of  MIR \ntechniques are being done in limited domains as in Hsu et al. \n(2001) and Uitdenbogerd (2000). \n4.2 Few Attempts to Assess User \nRequirements \nJef Raskin’s talk at ISMIR 2001 about how to make computer \nsystems more usable was notable in that it suggested using \ntheoretical models of users to guide user interface design choices \nrather than involving users in the design process (Raskin 2001). \nThis emphasis on basic research over application to, and \ninvolvement with, users is common in MIR research, and may \nresult from the influence of the computer science and audio \nengineering communities. \nAlready, MIR is beginning to emphasize certain areas of research \nwithout having identified user communities and evaluated whether \nthe techniques developed will meet the needs of those \ncommunities. As mentioned before, QBH systems are being \nintensively developed, but there is virtually no evidence cited that \nusers prefer these systems, and even some that suggests that they \ndo not (McPherson et al. 2001). QBH papers typically begin with \nspeculations that such a system would be useful, such as “singing \nis naturally used as input” (Haus et al. 2001), or anecdotal \nevidence such as \nThe potential utility of such systems is attested to by \nmusic librarians, who report that library patrons often \nhum or whistle a phrase of music and ask them to \nidentify the corresponding musical work. (Smith et al. \n2001). Interdisciplinary Communities and Research Issues in Music Information Retrieval \nEven MIR research focusing on usability rarely involves user \nstudies. For instance, Kornstädt (2001) presents a graphical user \ninterface apparently tailored for the needs of musicologists, but \ncites neither research into what kinds of tools musicologists need \nnor any evaluation of the system by musicologists. Research into \ngenre classification from audio (e.g., Tzanetakis et al. (2001)) \nrepeatedly cites Perrot et al. (1999) as a basis for its assumption \nthat users can make effective genre judgments based on short \nmusical examples, but does not evaluate the performance of genre \nclassification techniques against real users’ genre judgments. One \nexception to the dearth of user studies is Fernström et al. (2001) \nwhich at least attempts a preliminary user evaluation of a new user \ninterface design based on a perceptual phenomenon known as the \n“cocktail party effect”. \nTo some extent, this is a chicken-and-egg problem; MIR \nresearchers cannot evaluate techniques that have not yet been \ndeveloped. Even worse, it is difficult to meaningfully study user \nbehavior without users having access to large, relatively \ncomprehensive collections of music with which they can \nspontaneously interact. As testbed projects continue to develop, \nthey will be in the best position to analyze their own users (e.g.,  \nMcPherson et al. (2001) and Dunn (2000)), which should provide \nvaluable guidance to basic researchers. However, it still remains to \nbe seen whether or not real user needs match the interests or \ntechnological capabilities of the many disciplines currently \ninvolved in MIR research. \n4.3 Undue Emphasis on Western Music \nAnother significant coverage gap in MIR concerns non-Western \nmusic. The music used in MIR studies is predominantly common-\npractice Western music. This is primarily a problem with symbolic \nMIR systems, which tend to use representations derived from \ncommon-practice Western music notation. Notable exceptions \ninclude Linardis et al. (2001) who describe a retrieval system \nbased on Byzantine neumatic notation. Audio MIR systems are \npresumably more flexible than symbolic MIR systems because the \naudio representations and features they employ are presumably \nmore culturally neutral, but no audio MIR research has \nspecifically investigated this hypothesis. \nAddressing the undue emphasis on common-practice Western \nmusic in MIR research requires, finally, a radical rethinking of \nMIR research practice.  Assumptions commonly made by MIR \nresearchers about music – that it has melodies, that its rhythm is \nmetrical, and that it can treated as re-contextualizable information \nobjects – must be replaced by provisional sets of assumptions \nresonant with the cultural milieus of real user communities. \n5. FIRST PRINCIPLES AND A MIR \nRESEARCH AGENDA \nTo best overcome the gaps in current MIR research, we believe \nthat MIR research must embrace as sine qua non the following \nthree principles: \n1. MIR systems are developed to serve the needs of \nparticular user communities. \n2. MIR techniques are evaluated according to how well \nthey meet the needs of user communities. \n3. MIR techniques are evaluated according to agreed-upon \nmeasures against agreed-upon collections of data, so \nthat meaningful comparisons can be made between \ndifferent research efforts. \nTo realize these first principles, we believe the following steps \nmust be taken: First, MIR research should begin by assessing existing MIR \nsystems (broadly defined to include both digital and traditional \nformats), including libraries, music retailers, on-line media \nmerchants, and individual collections. Existing practice should be \nevaluated to establish baselines of usability upon which new MIR \nsystems must improve. These evaluations should be systematic \nand empirical and involve the participation of both users and \nmaintainers of existing systems, rather than being based on the \nopinions and speculations of MIR researchers. These same \ntechniques can then be used to evaluate new MIR systems, and the \nresults can be compared. It is imperative that studies of existing \nmusic systems include non-Western resources and their use in \nnon-Western contexts.  \nSecond, the investigation of existing MIR practice should identify \ndistinct user communities and investigate what they need from \nMIR systems. As MIR researchers have already pointed out, the \nmusicological community has, for example, quite different \nrequirements for MIR systems than other communities. Future \nMIR research should explicitly identify which community’s needs \nit is attempting to address. \nThird, MIR research programs should also agree upon evaluation \nmeasures. Retrieval accuracy and system effectiveness should be \nmeasured using clearly delineated, agreed-upon methodologies \nand reported consistently across studies. To this end, MIR \nresearchers should share music collections, so that a variety of \ntechniques can be applied to the same collection and results \nreplicated or refuted by independent research teams. The \ndevelopment of a set of “universal” test collections as called for \nby the ISMIR 2001 “resolution” (see http://music-\nir.org/mirbib2/resolution) is an important step in this direction. \nFourth, and finally, MIR researchers should develop under-\nrepresented research areas such as recommendation, browsing, \nand user interface design. Advances into these areas should be \ngrounded in existing knowledge of user requirements and music \nperception. \nIf the MIR research community embraces these principles and this \nresearch agenda, we believe that future MIR systems will better \nprovide real users with the tools, features, and ease-of-use they \nneed to get the most out of rich and comprehensive collections of \nmusic in digital form.  \n6. REFERENCES \nAdorno, T. W. (1973). Philosophy of Modern Music. New York, \nSeabury Press. \nAttali, J. (1985). Noise: the political economy of music. \nMinneapolis, University of Minnesota Press. \nAucouturier, J.-J. and M. Sandler (2001). Using Long-Term \nStructure to Retrieve Music: Representation and \nMatching. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 1-2. \nBaeza-Yates, R. and B. d. A. N. Ribeiro (1999). Modern \ninformation retrieval. New York, ACM Press. \nBainbridge, D. (2000). The Role of Music IR in the New Zealand \nDigital Library Project. International Symposium on \nMusic Information Retrieval. \nBainbridge, D., C. G. Nevill-Manning, et al. (1999). Towards a \nDigital Library of Popular Music. Fourth ACM \nConference on Digital Libraries, Berkeley, California. \nBatlle, E. and P. Cano (2000). Automatic Segmentation for Music \nClassification using Competitive Hidden Markov \nModels. International Symposium on Music Information \nRetrieval. Interdisciplinary Communities and Research Issues in Music Information Retrieval \nBel, B. and B. Vecchione (1993). \"Introduction.\" Computers and \nthe Humanities 27(1 -- special issue on Computational \nMusicology). \nBonardi, A. (2000). IR for Contemporary Music: What the \nMusicologist Needs. International Symposium on Music \nInformation Retrieval. \nBrigham, E. O. (1988). The Fast Fourier Transform and Its \nApplications. Englewood Cliffs, New Jersey, Prentice-\nHall. \nByrd, D. (2001). Music-Notation Searching and Digital Libraries. \nACM/IEEE Joint Conference on Digital Libraries, \nRoanoke, VA: 239-246. \nChen, A. L. P. (2000). Music Representation, Indexing and \nRetrieval at NTHU. International Symposium on Music \nInformation Retrieval. \nChoudhury, G. S., T. DiLauro, et al. (2000). Optical Music \nRecognition System within a Large-Scale Digitization \nProject. International Symposium on Music Information \nRetrieval. \nCook, P. R. (1999). Music, cognition, and computerized sound : \nan introduction to psychoacoustics. Cambridge, Mass., \nMIT Press. \nCope, D. (1992). \"Computer modeling of musical intelligence in \nEMI.\" Computer Music Journal 16(2): 69-83. \nCope, D. (2001). Computer Analysis of Musical Allusions. \nInternational Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 83-84. \nDannenberg, R. B. (2001). Music Information Retrieval as Music \nUnderstanding. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 139-142. \nDeliège, I. and J. A. Sloboda (1997). Perception and cognition of \nmusic. Hove, East Sussex, Psychology Press. \nDownie, J. S. (1999). Evaluating a Simple Approach to Music \nInformation Retrieval: Conceiving Melodic N-grams as \nText. Graduate Program in Library and Information \nScience. London, Ontario, University of Western \nOntario: 179. \nDownie, J. S. (2001). Music Information Retrieval Annotated \nBibliography Website Project, Phase I. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 5-7. \nDownie, J. S. (2001). Whither Music Information Retrieval: Ten \nSuggestions to Strengthen the MIR Research \nCommunity. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 219-222. \nDunn, J. W. (2000). Beyond VARIATIONS: Creating a Digital \nMusic Library. International Symposium on Music \nInformation Retrieval. \nDunn, J. W., M. W. Davidson, et al. (2001). Indiana University \nDigital Music Library Project: An Update. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 137-138. \nDunn, J. W. and C. A. Mayer (1999). VARIATIONS: A Digital \nMusic Library System at Indiana University. Fourth \nACM Conference on Digital Libraries, Berkeley, \nCalifornia. \nDurey, A. S. and M. A. Clements (2001). Melody Spotting Using \nHidden Markov Models. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n109-117. \nFernström, M. and D. Ó. Maidín (2001). Computer-supported \nBrowsing for MIR. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 9-10. \nFoote, J. (1999). \"An Overview of Audio Information Retrieval.\" \nACM Multimedia Systems 7(1): 2-11. Good, M. (2000). Representing Music Using XML. International \nSymposium on Music Information Retrieval. \nHaus, G. and E. Pollastri (2001). An Audio Front End for Query-\nby-Humming Systems. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n65-72. \nHerrera, P., X. Amatriain, et al. (2000). Towards instrument \nsegmentation for music content description: a critical \nreview of instrument classification techniques. \nInternational Symposium on Music Information \nRetrieval. \nHofmann-Engl, L. (2001). Towards a Cognitive Model of Melodic \nSimilarity. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 143-151. \nHoos, H. H., K. Renz, et al. (2001). GUIDO/MIR -- an \nExperimental Musical Information Retrieval System \nbased on GUIDO Music Notation. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 41-50. \nHsu, J.-L. and A. L. P. Chen (2001). Building a Platform for \nPerformance Study of Various Music Information \nRetrieval Approaches. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n153-162. \nHuron, D. (2000). Perceptual and Cognitive Applications in \nMusic Information Retrieval. International Symposium \non Music Information Retrieval. \nIsmirli, O. (2000). Using a Spectral Flatness Based Feature for \nAudio Segmentation and Retrieval. International \nSymposium on Music Information Retrieval. \nItoh, M. (2000). Subject search for music: Quantitative analysis of \naccess point selection. International Symposium on \nMusic Information Retrieval. \nJang, J.-S. R., J.-C. Chen, et al. (2001). MIRACLE: A Music \nInformation Retrieval System with Clustered Computing \nEngine. International Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 11-12. \nKassler, M. (1966). \"Toward Musical Information Retrieval.\" \nPerspectives of New Music 4(2): 59-67. \nKornstädt, A. (2001). The JRing System for Computer-Assisted \nMusicological Analysis. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n93-98. \nLarson, S. (2000). Searching for Meaning: Melodic Patterns, \nCombinations, and Embellishments. International \nSymposium on Music Information Retrieval. \nLemström, K., G. A. Wiggins, et al. (2001). A Three-Layer \nApproach for Music Retrieval in Large Databases. \nInternational Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 13-14. \nLevering, M. (2000). Intellectual Property Rights in Musical \nWorks. International Symposium on Music Information \nRetrieval. \nLinardis, P., D. Politis, et al. (2001). Musical Information \nRetrieval for Delta and Neumatic Systems. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 23-24. \nLindsay, A. and Y. Kim (2001). Adventures in Standardization, \nor, how we learned to stop worrying and love MPEG-7. \nInternational Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 195-196. \nLogan, B. (2000). Mel Frequency Cepstral Coefficients for Music \nModelling. International Symposium on Music \nInformation Retrieval. \nMacMillan, K., M. Droettboom, et al. (2001). Gamera: a \nStructured Document Recognition Application Interdisciplinary Communities and Research Issues in Music Information Retrieval \nDevelopment Environment. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n15-16. \nMaidín, D. Ó. and M. Cahill (2001). Score Processing for MIR. \nInternational Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 59-64. \nMazzoni, D. and R. B. Dannenberg (2001). Melody Matching \nDirectly from Audio. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n17-18. \nMcClary, S. (1991). Feminine endings: music, gender, and \nsexuality. Minneapolis, University of Minnesota Press. \nMcPherson, J. R. and D. Bainbridge (2001). Usage of the \nMELDEX Digital Music Library. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 19-20. \nNam, U. and J. Berger (2001). Addressing the \"Same but different \n- different but similar\" Problem in Automatic Music \nClassification. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 21-22. \nNishimura, T., H. Hashiguchi, et al. (2001). Music Signal Spotting \nRetrieval by a Humming Query Using Start Frame \nFeature Dependent Continuous Dynamic Programming. \nInternational Symposium on Music Information \nRetrieval, Bloomington, IN, USA: 211-218. \nPerrot, D. and R. O. Gjerdigen (1999). Scanning the Dial: an \nExploration of the Factors in the Identification of \nMusical Style. Society for Music Perception and \nCognition, Evanston, IL: 88. \nPickens, J. (2000). A Comparison of Language Modeling and \nProbabilistic Text Information Retrieval Approaches to \nMonophonic Music Retrieval. International Symposium \non Music Information Retrieval. \nRaskin, J. (2001). Making Machines Palatable. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 39-40. \nReiss, J., J.-J. Aucouturier, et al. (2001). Efficient \nMultidimensional Searching Routines for Music Information Retrieval. International Symposium on \nMusic Information Retrieval, Bloomington, IN, USA: \n163-171. \nRolland, P.-Y. (2001). Adaptive User Modeling in a Content-\nBased Music Retrieval System. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 27-30. \nSelfridge-Field, E. (2000). What Motivates a Musical Query? \nInternational Symposium on Music Information \nRetrieval. \nSmiraglia, R. P. (2001). Musical Works as Information Retrieval \nEntities: Epistemological Perspectives. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 85-91. \nSmith, L. and R. Medina (2001). Discovering Themes by Exact \nPattern Matching. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 31-32. \nSorsa, T. and J. Huopaniemi (2001). Melodic Resolution in Music \nRetrieval. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 33-34. \nSpevak, C. and R. Polfreman (2001). Sound Spotting -- A Frame-\nBased Approach. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 35-36. \nTzanetakis, G., G. Essl, et al. (2001). Automatic Musical Genre \nClassification of Audio Signals. International \nSymposium on Music Information Retrieval, \nBloomington, IN, USA: 205-210. \nUitdenbogerd, A. L. (2000). Music IR: Past, present, and future. \nInternational Symposium on Music Information \nRetrieval. \nVan der Merwe, P. (1989). Origins of the popular style : the \nantecedents of twentieth-century popular music. Oxford \nOxfordshire, Clarendon Press. \nYang, C. (2001). Music Database Retrieval Based on Spectral \nSimilarity. International Symposium on Music \nInformation Retrieval, Bloomington, IN, USA: 37-38."
    },
    {
        "title": "Carnatic Ragas as Music Information Retrieval Entities.",
        "author": [
            "Gordon Geekie"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415994",
        "url": "https://doi.org/10.5281/zenodo.1415994",
        "ee": "https://zenodo.org/records/1415994/files/Geekie02.pdf",
        "abstract": "Carnatic music is the ‘art’ music of the four southern States of India (Andhra Pradesh, Karnataka, Kerala and Tamilnadu).  One difference between Carnatic music and the better-known Hindusthani music of North India is its embeddedness in a religious-philosophical context.  This context crucially determines the objects of knowledge organization and the indigenous theory of musical affect.  The author presents the view that a digital library of Carnatic music should contain the objects of knowledge organization and their interrelationships as conceived by indigenous practitioners and audiences, rather than by Western specialists or North Indian practitioners.  The author demonstrates how three features of Carnatic music (viz. aural transmission, improvisation and cultural context) have particular implications for the development of a digital library.  Aural transmission results in musical documents being less important sources of information than recordings.  Improvisation results in a highly transformational and often ambiguous relationship between (intra)musical signifiers and signified, causing problems of classification and machine recognition. The cultural context favours the prioritisation of emotional affect over introductory ease of listening and even technical recording quality in the selection of the recordings to be included in a digital library of Carnatic music.",
        "zenodo_id": 1415994,
        "dblp_key": "conf/ismir/Geekie02",
        "keywords": [
            "Carnatic music",
            "art music",
            "four southern States",
            "religious-philosophical context",
            "objects of knowledge organization",
            "indigenous theory of musical affect",
            "digital library",
            "aural transmission",
            "improvisation",
            "cultural context"
        ],
        "content": "Carnatic Ragas as Information Retrieval Entities \nCarnatic Raga s as Music Information Retrieval Entities \nGordon Geekie  \nDepartment of Information and Communication \nManchester Metropolitan University \nRosamund Street West, \nManchester M15 6LL \n+44-0161-247-3039 \ng.geekie@mmu.ac.uk\nABSTRACT \nCarnatic music is the ‘art’ music of the four s outhern S tates of \nIndia (Andhra Pradesh, Karnataka, Kerala and Tamilnadu).  One \ndifference between Carnatic music and the better-known \nHindusthani music of North India is its embeddedness in a \nreligious-philosophical context.  Th is context crucially  determines \nthe objects of knowledge organization and the indigenous theory  \nof m usical affect.  The author pres ents the view that a digital \nlibrary  of Carnatic music shoul d contain the objects of knowledge \norganization and their interrela tionships as conceived by  \nindigenous practitioners and audiences, rather than by Western \nspecialists or North Indian practiti oners.  The author dem onstrates \nhow three features  of Carnatic music (viz. aural transm ission, \nimprovisation and cultural context) have particular implications \nfor the developm ent of a digital library .  Aural transm ission \nresults in m usical docum ents be ing less important sources of \ninformation than recordings.  Improvisation results in a highly  \ntransformational and often am biguous relationship between \n(intra)musical signifiers and signified, causing problems of \nclassification and m achine recognition. The cultural context \nfavours the prioritisation of em otional affect over introductory  \nease of listening and even technical recording quality  in the \nselection of the recordings to be included in a digital library  of \nCarnatic m usic. \n1. CARNATIC MUSIC \nCarnatic music is the ‘art’ music of the four s outhern S tates of \nIndia (Andhra Pradesh, Karnatak a, Kerala and Tamilnadu).  \nTraditionally  this m usic was perform ed by  members of the \nBrahmin and Barber castes, in Hindu temples and the courts of \nHindu rulers.  Its exponents (called vidwan s: literally  ‘people with \nknowledge’) regard it as the indigenous Indian m usic tradition \nwhich, during the period of the Muslim invasions of the sub-\ncontinent, becam e merged with ‘P ersian’ m usic (their term ) in the \ncourts of Muslim rulers to b ecome ‘Hindusthani’ music (their \nterm for North Indian art m usic, with which W estern audiences  \nare m uch m ore fam iliar).   \nIn the 20th Century , Carnatic m usic did not adapt as  well as  the \nmore secularised Hindusthani music to concert and recording \ncontexts.  Carnatic music theory  is based upon the religious-\nphilosophical tenets of the Brahmin caste in southern India. These \nderive from Vedanta Hinduism and Samkhy a phy sics and define \nthe three marga  (‘paths towards self-development’) of bhakti  \n(devotion towards the Supreme God), jnana  (meditational pursuit \nof the experiential Unity  that is ‘bey ond the dualities of thought’) \nand laya (phy sical y ogas based upon homologies between the human body  and the cosmos, which posit nada  (vibration) as the \norigin of matter and consciousness).  The practice of Carnatic \nmusic is itself conceived as  a marga , called sangita  (‘music’) \nmarga . \nIn Carnatic music terminology , laya literally  means ‘rhy thm’. \nHowever,  vidwans  and audiences articulate homologies between \nCarnatic music and Samkhy a phy sics, upon which the laya marga  \nis based.  Most evidently , the sound of nada  is said to be ‘like the \nbuzzing of a bee’.  This is direc tly related to the tim bre of the \ntambura , which sounds the drone-tonic throughout the \nperform ance, and the vina (lute).  The seven notes of the musical \nscale are related to the s even chakras  of the body /cosmos.  Ascent \nand descent of nada  between the chakras  is related to musical \nascent and des cent.   \nCarnatic vidwan s conceptualis e musical affect in term s of bhava  \n(‘emotional’ affect) and ganam  (‘intellectual’ affect).  Bhava  is \nconceived as being achieved through words (sahitya ) and raga. \nGanam is conceived as being achieved through improvisation \n(manudharma : literally  ‘duty  of the m ind’):  the listener’s \nexperiences of climax and resolution are conceived as being \nachieved through the performer’s ex temporisation of patterns of \nmusical repetition and variation.   \nBhava  is c lassifie d in terms of seven rasas (literally , ‘tastes’).  \nMost (and all the ‘serious’) Carnatic raga s express bhakti-karuna  \n(literally  ‘devotion-pathos) rasa, which is described as the \ndevotee’s expression of love for the Supreme God and the misery  \n(duhkha ) endured in His absence, having the intention of \npersuading Him to appear before the devotee and thereby  grant \nmukti  (the guarantee of releas e from  the cy cle of rebirths ).  The \nexpression sukha-duhkha  (‘sweet misery ’) is also used to describe \nthis rasa. Vocalists employ  what, in Western or Hindusthani \nterms, would be des cribed as  elem ents of speech intonation, for \nexam ple the ‘s ob’ to expres s sorrow.  Instrumentalis ts \nintentionally  imitate these. Such  intonational features  are an \naspect of raga .   \nCarnatic perform ance item s, which m ay last from  5 minutes to \none hour, are ‘in’ a single raga .  They  typically  start with alapana  \n(improvisation within the raga ), followed by  a kriti (composition: \nliterally  ‘work’).  Kriti s are als o ‘in’ a s ingle raga .  Thousands of \nCarnatic raga s exist.  W ithin one of 72 scale ty pes ( melakarta s), a \nraga  is distinguished from all other raga s by  the sequence of \nswara s (notes ) specified in as cent from  tonic to supertonic \n(arohana : ascent pattern) com bined with the sequence of swara s \nspecified in des cent from  supertonic to tonic ( avarohana : descent \npattern).  The ascent and des cent patterns  can be s calar or vakra \n(i.e. containing subsidiary  ascents  and des cents ).   \nIn addition to an as cent-des cent pattern, a raga  is conceived as \nhaving a set of (approximately  20 to 30) sanchara prayogas  \n(‘characteris tic melodic phras es’).  Thes e prayoga s are conceived \nas deriving from the set of kritis in the raga .  Many  sanchara \nprayogas  adhere to the ascent-descent pattern of the raga  but are \ndistinguished by  gamaka  (glissando and microt onal ornamentation Permission to m ake digital or  hard copies of all or  part of this \nwork for per sonal or  classr oom use is gr anted without fee \nprovided that copies ar e not made or distr ibuted for  profit or  \ncommercial advantage and that copies bear this notice and the full \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou Carnatic Ragas as Information Retrieval Entities \nof some notes).  Other sanchara prayogas  depart  from the ascent-\ndescent pattern of the raga by omitting a note in ascent but \nincluding it in descent (called langhana swara ) or by omitting \ntonic, supertonic or perfect fi fth in both directions (called varjya \nswara ).  Very few of the possible note  combinations thus \ngenerated are sanchara prayogas of a raga. \nWhen improvising within a raga, Carnatic vidwan s distinguish \nbetween three types of prayoga (melodic phrase): either they \n1. adhere to the ascent-descent pattern of the raga (referred to by \nvidwans  as ‘playing in scales’); \n2. are sanchara prayogas  of the raga;  ( Vidwans say that this \nachieves the strongest expression of raga bhava.) or \n3. are prayogas which may conform with langhana or varjya \nswara  but which are not sanchara prayogas  of the raga.  Vidwans \nsay that such prayogas must be used only with extreme care, \notherwise raga bhava is destroyed.  They also define their \nobjective in raga improvisation as seeking to play all possible \ncombinations of notes, thereby risking destroying raga bhava. \n Carnatic music is aurally transmitted.  Kriti s are attributed to \nindividual composers but are not ‘published’ at their time of \ncomposition. They are aurally transmitted in vidwan-disciple \n‘lineages’.  The vidwan  dictates a simplified, ‘student’ version of \nthe kriti.  Once learned, this is significantly elaborated by purely \naural transmission.  The com positions which are published in \nSouth India, usually in commemorative compilations made after \nthe vidwan ’s death, comprise these ‘student’ versions.  In \nperformance, variations are extemporised (an aspect of ganam ).  \nSignificant variations between  versions of the same kriti recorded \nby performers from different regions and vidwan-disciple \n‘lineages’ are evident: for example, from listening to All-India Radio. \n2. IMPLICATIONS FOR MIR \nThe “objects of knowledge organization” [Smiraglia 2001 ., P. 88] \nand their interrelationships which are represented within a digital \nlibrary of Carnatic music should id eally be those employed by its \npractitioners (and audiences). The academic study of Carnatic \nragas has been claimed by Indic Musicology, which regards them \nas transformations from Hindusth ani equivalents and insists upon \nthe employment of North Indian objects of knowledge \norganization in their analysis [e.g. Powers, 1971] which have little \nrelation to the Carnatic conception detailed in Section 1.  It is \ntherefore necessary to look outside area studies for academic debate on the development of a digital library of Carnatic music. \nIn MIR, whilst Smiraglia’s [ ibid.] conception of the musical \n‘work’ and its ‘instantiations’ is adequate for the representation of \nkriti, raga constitutes a separate object of knowledge organization \nhaving a specific relationship to kriti. This relationship ideally \nneeds represented in order to expos e to the library user important \n(intra)musical meanings.   \nThree features of Carnatic music pose particular problems for the development of a digital library: viz. aural transmission, \nimprovisation and cultural context.  An important consequence of \naural transmission is the unreliability of documents.  Many \npublished versions of kritis and the sanchara prayogas of ragas \nare available (and in English) but vidwan s warn students against \ntheir use because they are out of date (by more than 100 years) and incomplete.  The music not ation used requires detailed knowledge of the raga in order to replicate a notated kriti in sound \nor to utilise it as as input for m eaningful musical analysis.  In \nconsequence, the most valuable entities in a digital library of \nCarnatic music are recordings. \nImprovisation in Carnatic music essentially results from a set of interactions: between musicians and between musicians and \naudience.  The goal of these inter actions is to achieve experiential \noutcomes in the listener.  The communication medium for the interactions consists in sets of expectations in the listener \nconcerning what will be played next.  These expectations are \ngenerated from past experience as well as experience of the current performance (on many levels  of time-frame).  In such an \nimprovisational tradition, the role of compositions is that of a ‘package of expectations’ to be worked with in performance: they \nsupply pre-existing vehicles  for communication between \nperformer and listener.  In the traditional performance context, the \nperformer interacts with accompanists and the vidwan  listeners \nlocated in front of him whilst the rest of the audience observe these interactions.  This set of interactions is difficult to achieve in \nthe Westernised concert or recording studio context. \nRecent work on the development of a digital library of Carnatic music reveals a highly transfor mational relationship between \nsanchara proyogas of ragas as performed in their ‘original’ kritis \nand as performed in alapana .  In alapana, the performer changes \nthe timing, repeats parts, transfers between sanchara prayogas in \nmid-phrase and runs them into each other.  The manipulation of the listener’s expectations in alapana evidently results in subtle \nand often indeterminate (intra)m usical references which are \nproblematic both to classify and to automate the identification of.   \nCarnatic music is deeply embedded in a particular (religious-philosophical) cultural context.  Explanation of this context for \nusers of a digital library of Carn atic music is necessary but, more \nfundamentally, the recordings th emselves must reinforce this \ncontext.  For example, I am currently mastering a series of \nrecordings of the Mysore vina made in 1974 - 6.  These were \nrecorded mainly in a traditional music school by a single \nperformer (i.e. with no tambura or percussion accompaniment).  \nRoom echo and the periodic ringing of bells convey the social context of a Hindu temple and the solo performance conveys the \ncultural context of the individual devotee in the act of bhakti .  In \nmy editing, I have emphasised rather than sought to minimise \nthese features because the previously available set of (studio) \nrecordings of the Mysore vina [EMI, 1971], whilst suffering none \nof these technical deficiencies, nonetheless, in seeking popular appeal through short alapana and a high proportion of dance \ncompositions, arguably fails ade quately to convey the cultural \ncontext of bhakti .   \nREFERENCES \nEMI, 1971 - Doreswamy Iyenga r, ‘Mysore Veena’,  The \nGramophone Company of India Ltd, ECSD 2491, 1971. \nPowers, 1971 - Harold Powers, ‘An historical and comparative approach to the classification of ragas’, ‘Selected Reports’, \nInstitute of Ethnomusicology, UCLA, 1971. \nSmiraglia, 2001 - Richard P. Sm iraglia, ‘Musical Works as \nInformation Retrieval Entities: episte mological perspectives’, in J. \nStephen Downie and David Ba inbridge (eds.), ISMIR 2001: \nProceedings of 2nd Annual International Symposium on Music \nInformation Retrieval, Bloomi ngton, Indiana, 2001, Pp. 85 - 91."
    },
    {
        "title": "RWC Music Database: Popular, Classical and Jazz Music Databases.",
        "author": [
            "Masataka Goto",
            "Hiroki Hashiguchi",
            "Takuichi Nishimura",
            "Ryuichi Oka"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416474",
        "url": "https://doi.org/10.5281/zenodo.1416474",
        "ee": "https://zenodo.org/records/1416474/files/GotoHNO02.pdf",
        "abstract": "This paper describes the design policy and specifications of the RWC Music Database, a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",
        "zenodo_id": 1416474,
        "dblp_key": "conf/ismir/GotoHNO02",
        "keywords": [
            "RWC Music Database",
            "commonly available music database",
            "research purposes",
            "Royalty-Free Music Database",
            "Classical Music Database",
            "Jazz Music Database",
            "standard MIDI files",
            "text files of lyrics",
            "available at cost",
            "encouraging further advances"
        ],
        "content": "RWCMusic Database: Popular,Classical, and Jazz MusicDatabases\nRWCMusic Database:\nPopular,Classical, and JazzMusic Databases\nMasatakaGoto\nNationalInstitute of Advanced IndustrialScience and Technology(AIST)\n& “Information and HumanActivity”, PRESTO,JST\nIT,AIST,1-1-1 Umezono,Tsukuba,Ibaraki 305-8568,JapanHiroki Hashiguchi\nMejiro University\n4-31-1 Naka-Ochiai, Shinjuku-ku,\nTokyo161-8539, Japan\nTakuichiNishimura\nNationalInstitute of Advanced IndustrialScience and Technology(AIST)\nCARC, AIST,2-41-6 Aomi, Koto-ku, Tokyo135-0064, JapanRyuichi Oka\nUniversity of Aizu\nAizu-Wakamatsu,Fukushima965-8580, Japan\nABSTRACT\nThis paper describes the design policy and speciﬁcations of the\nRWC Music Database , a music database (DB) that is available to\nresearchers for common use and research purposes. Various com-\nmonly available DBs have been built in other research ﬁelds andhave made a signiﬁcant contribution to the research in those ﬁelds.The ﬁeld of musical information processing, however, has lacked\na commonly available music DB. We therefore built the RWC Mu-\nsicDatabase which contains four original DBs: the Popular Music\nDatabase (100 pieces), Royalty-Free Music Database (15 pieces),\nClassical Music Database (50 pieces), and Jazz Music Database\n(50 pieces). Each consists of originally-recorded music compact\ndiscs, standard MIDI ﬁles, and text ﬁles of lyrics. These DBs are\nnowavailableinJapanatacostequaltoonlyduplication,shipping,andhandlingcharges(virtuallyforfree),andweplantomakethemavailableoutsideJapan. WehopethatourDBwillencouragefurther\nadvancesin musical information processingresearch.\n1. INTRODUCTION\nWe believe that research into musical information processing willbeadvancedifmusicdatabases(DBs)becomeavailablethatcanbe\nused by various researchers. The main purposes and advantages ofsuchcommonly available DBs canbe summarized asfollows:\n•Researchers will be able to use the DBs as a common bench-\nmark for comparing and evaluating various methods related to\nmusicalinformationprocessing. ThelackofcommonmusicDBsavailable worldwide for research purposes at almost no cost hasmadeitdifﬁculttoestablishbenchmarks(evaluationframeworks)\nfor much of the research done regarding musical information\nprocessing.\n•TheDBswillacceleratetheprogressofvariousformsofresearch\nusingstatisticalmethods. Recentprogressintheuseofstatisticalmethods in other research ﬁelds such as speech recognition has\nbeenlargelydueto the availability oflargeDBs.\n•Researchers will be able to use the DBs for research publica-\ntionandpresentationwithoutconventionalcopyrightrestrictions.It has been difﬁcult to demonstrate research using copyrighted\nmusical pieces that will be included in, for example, conference\nvideosand CD-ROMs.\nAlthough there is an enormous amount of music available on com-\nmercially distributed compact discs, it is difﬁcult to use this mu-\nsicfor the above purposes because of copyright issues. Commonly\navailable DBs with copyright-cleared pieces are therefore essentialtoencourage the healthy developmentof this ﬁeldof research.\nVarious commonly available DBs have been built in other re-\nsearchﬁeldssincetheimportanceandsigniﬁcanceofsuchDBshave\nbeenwidelyrecognized. Intheresearchﬁeldofspeechinformation\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for proﬁt or\ncommercial advantage and that copies bear this notice and thefullcitation on theﬁrst page.\nc/circlecopyrt2002 IRCAM– Centre Pompidouprocessing,forexample,manyDBs(corpora)havebeenbuilt. Fur-\nthermore, recognition of the need for commonly available DBs hasled to the creation of the Linguistic Data Consortium (LDC) in theUSA and the European Language Resources Association (ELRA)\ntosupportthedevelopmentandsharingofresources. Therearealso\nseveral DBs in the ﬁeld of image processing. On the other hand, inthe ﬁeld of musical information processing, there is no commonlyavailable musicDB containing hundredsof musical pieces.\nWe therefore have built a music DB — the RWC (Real World\nComputing) Music Database — that gives researchers freedom of\ncommonuseandresearchuse. (TheDBisintendedtobeusedonlyfor research purposes.) The RWC Music Database is composed of\nfour original DBs: the Popular MusicDatabase ,Royalty-FreeMu-\nsicDatabase ,ClassicalMusicDatabase ,andJazzMusicDatabase .\nInthefollowingsections,wedescribethedesignpolicyandprovidean overviewof each ofthe four DBs.\n2. OVERVIEWOF THE RWCMUSIC\nDATABASE\nWehadtoaddressvariousdesign,trade-off,andcopyrightissuesin\nbuildingthe RWCMusicDatabase . Inthissection,wediscusssome\nof themost important issues.\n•Contents of thedatabase (DB)\nAn ideal music DB would contain many richly varied musical\npieces, in various genres, of the highest quality possible. For\npracticalpurposes,though,wehadtobuildourDBunderproduc-tion resourceconstraints such asour budget andavailable time.Wetherefore took up three major music genres — popular, clas-\nsical, and jazz — and tried to include as many realistic pieces as\npossible in a way that reﬂected the complexity and diversity ofreal-world music. For example, as well as ensuring that variousstyles, moods, tempi, and lengths were represented, we also in-\ncluded as many professional composers, lyric writers, arrangers,\nsingers, andplayers as ourresources allowed.To achieve sound quality as high as that of commercially dis-tributed compact discs, we used professional digital equipment\nfor allrecording, mixdown, andmastering processes.\n•Copyrights of musicalpieces\nTo make our DB available to researchers around the world, wehad to obtain all necessary copyrights and neighboring rights\nfor research purposes.\n1We therefore included 215 pieces (for\nthe four DBs) that were all originally performed and recordedfor the purpose of inclusion in the DB. For the Popular Music\nDatabase ,weincluded100piecesthatwereoriginallycomposed\nand arranged in modern popular music styles (the lyrics were\nalso originally written). For the Royalty-Free Music Database ,\nwe included 15 public-domain traditional popular-music piecesthat were originally arranged for this DB. For the Classical Mu-\nsic Database , we selected 50 well-known public-domain pieces.\n1Note that our DB is not copyright-free even if it is available for free:\nwe reserveall necessary copyrights and neighboring rights. All users of theDBmustsubmittheuseragreementformtothegeneralmanagerofthe RWC\nMusicDatabase (Masataka Goto,contact: m.goto@aist.go.jp).RWCMusic Database: Popular,Classical, and Jazz MusicDatabases\nTable1. List of music compactdiscs for distribution(Popular,Royalty-Free,Classical, and JazzMusic Databases).\nContents (Version) # of discs Catalog number # of pieces Piecenumber\nPopularMusicDatabase (OriginalVersion: M ixed) 7RWC-MDB-P-2001-M01 −M07 100No. 1−100\nRoyalty-FreeMusic Database (Original Version: M ixed) 1RWC-MDB-R-2001-M01 15No. 1−15\nClassicalMusic Database (Original Version: M ixed) 6RWC-MDB-C-2001-M01 −M06 50No. 1−50\nJazzMusicDatabase (OriginalVersion: M ixed) 4RWC-MDB-J-2001-M01 −M04 50No. 1−50\nCatalognumber: RWC-MDB-[Contents]-[Year]-[Version][VolumeNo.], Contents: the ﬁrstletter , Year:Made in2001, Version: M ixed\nFor theJazz Music Database , we included 50 pieces where four\nwell-known public-domain pieces were originally arranged andtheother 46 pieces wereoriginally composed andarranged.\n•Standard MIDI ﬁles(SMFs)\nWe prepared transcribed SMFs for all 215 pieces. These were\nstoredin SMF format 1 (multiple tracks) and conform to the GSformat. Given audio signals, most of them were transcribed byear. For music genres such as popular and jazz where there are\ntypicallynodetailedscores,theseSMFscan beusedaseffective\nsubstitutesforscores. Evenforclassicalmusicwithscores,SMFsthat can be freely used for research purposes are valuable. Thelyricsof songs are providedas text ﬁles.\nWeusedmusiccompactdiscs(CD-DA:CompactDisc-Digital\nAudio) as the medium for distributing the audio signals of the DBpieces to researchers. The list of the compact discs and their cata-\nlog numbers are shown in Table1. Each piece has a unique “piece\nnumber” numbered in consecutive order from 1 within each DB.The volume number (the last two digits of the catalog number) isused only for putting pieces onto the compact discs and should not\nbe used for reference: a piece should be referred to by the piece\nnumberfor research use (e.g.,\nRWC-MDB-P-2001 No. 28).\n2.1 PopularMusic Database\nThePopular Music Database consists of 100 songs — 20 songs\nwith English lyrics performed in the style of popular music typicalofsongsontheAmericanhitchartsinthe1980s,and80songswithJapanese lyrics performed in the style of modern Japanese popular\nmusic typical of songs on the Japanese hit charts in the 1990s. All\n100songs with vocals were originally produced in as rich a varietyas our resources allowed. The songs were recorded by 148 peo-ple including 25 composers, 30 lyric writers, 23 arrangers, and 34\nsingers. As a result of our attempts to achieve a good male-female\nbalanceinthe100songsandtoincludesongsbyvocalgroups,thereare50songsby15malesingers,44songsby13femalesingers,and6songs by 6 vocalgroups.\n2.2 Royalty-FreeMusic Database\nTheRoyalty-Free Music Database consists of 15 songs, 10 well-\nknownstandardpopularsongswithEnglishlyricsand5well-known\nchildren’s songs with Japanese lyrics. All 15 public-domain songs\nwere originally arranged and recorded. This DB was built to con-tainwell-knownpopular songs, while the PopularMusic Database\ncontains only original popular songs. The songs were recorded by\n16people, including two arrangersand three singers.\n2.3 ClassicalMusic Database\nTheClassical Music Database consistsof 50 pieces:\n•Symphony: 4\n•Chamber: 10•Concerto: 2\n•Solo: 24•Orchestral: 4\n•Vocal: 6\nAll 50 public-domain pieces were originally recorded for our DB\n(not all movements were recorded: a certain movement was se-lected and recorded for several categories such as symphony and\nconcerto). These pieces were selected to represent a rich variety\nof instrumentation, style, period, composer, and mood. We did notintend to produce a mere anthology of well-known musical pieces:wetriedtoincludepiecesthathavebeenpreviouslyusedinresearch\nor have interesting aspects from a research viewpoint. The pieces\nwererecordedby115peopleincludingaphilharmonicorchestra(72playerswith 1 conductor), 16pianists, and 4violinists.2.4 Jazz MusicDatabase\nTheJazzMusic Database consistsof 50 pieces:\n•Instrumentationvariations: 35(5pieces ×7instrumentations)\n•Stylevariations: 9 •Fusion (crossover): 6\nAll 50 pieces were originally produced for our DB, except for the\ncomposition and lyric writing of four style-variation pieces. First,\ntheinstrumentation-variation pieceswere recordedto obtaindiffer-\nent arrangements of the same piece: ﬁve standard-style jazz pieceswereoriginally composed andthen performedin modern-jazzstyleusing seven instrumentations: 1) piano solo, 2) guitar solo, 3) duo\n(vibraphone+piano,ﬂute+piano,orpiano+bass),4)pianotrio,5)\npianotrio+trumpetortenorsaxophone,6)octet(pianotrio+guitar+ alto saxophone + baritone saxophone + two tenor saxophones),and 7) piano trio + vibraphone or ﬂute. Second, the style-variation\npieces were recorded to represent various styles of jazz. The nine\npieces, which include four well-known public-domain pieces, con-sistofvocaljazz(two),bigbandjazz(two),modaljazz(two),funkyjazz(two),andfreejazz(one)pieces. Finally,thefusionpieceswere\nrecorded to obtain music that combines elements of jazz with other\nstyles such as popular, rock, or latin. All the pieces were recordedby 53people, including fourcomposers and onelyric writer.\n3. CONCLUSION\nThe building and sharing of commonly available databases (DBs)willclearlymakeanimportantcontributiontotheresearchintomu-sical information processing. With the four DBs that compose theRWC Music Database , researchers can now use copyright-cleared\nmusicalpiecesforeachstageofproblemﬁnding,problemsolution,\nimplementation, evaluation,and presentation.\nTheRWC Music Database was built in ﬁscal 2000 and 2001\nbytheRWCMusicDatabaseSub-WorkingGroup(chair: Masataka\nGoto)intheRealWorldComputingPartnership(RWCP)fundedby\ntheMinistryofEconomy,TradeandIndustryofJapan[1,2]. Whileour DB was built for general purposes related to musical informa-tionprocessingandwasdesignedindependentlyoftheISMIR2001\nresolution(ontheneedtocreatestandardizedMIRtestcollections),\nit is consistent with the resolution and can provide useful test setsfor variousforms of music-relatedresearch.\nWe plan to make our DB available for researchers around the\nworld. In the future, it will be necessary to add various annotations\nto the DB pieces in cooperation with other researchers. We hopethat our DB will be widely used worldwide, and that various otherDBs willfollow,thus expeditingprogress in thisﬁeld of research.\n4. ACKNOWLEDGMENTS\nWethankeveryonewhohasmadethisDBprojectpossible. Yuzuru\nHiragaandKeijiHiratadevotedlyassistedusindesigningtheClas-sical and Jazz Music Databases, respectively. Satoru Hayamizu,Hideki Asoh, and Katunobu Itou advised us concerning the devel-opment and distribution of the DB. This project has also been sup-portedbymanypartiesinvolvedintheRWCproject. TheCMUSICCorporation carriedout the productionof all thepieces.\n5. REFERENCES\n[1] M. Goto et al., “RWC Music Database: Popular music database and\nroyalty-freemusicdatabase( inJapanese ),”IPSJSIGNotes2001-MUS-\n42-6, vol. 2001,no. 103,pp. 35–42, 2001.\n[2] M. Goto et al., “RWC Music Database: Classical music database and\njazz music database ( in Japanese ),”IPSJ SIG Notes 2002-MUS-44-5 ,\nvol. 2002, no.14, pp.25–32, 2002."
    },
    {
        "title": "A Highly Robust Audio Fingerprinting System.",
        "author": [
            "Jaap Haitsma",
            "Ton Kalker"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417973",
        "url": "https://doi.org/10.5281/zenodo.1417973",
        "ee": "https://zenodo.org/records/1417973/files/HaitsmaK02.pdf",
        "abstract": "Imagine the following situation. You’re in your car, listening to the radio and suddenly you hear a song that catches your attention. It’s the best new song you have heard for a long time, but you missed the announcement and don’t recognize the artist. Still, you would like to know more about this music. What should you do? You could call the radio station, but that’s too cumbersome. Wouldn’t it be nice if you could push a few buttons on your mobile phone and a few seconds later the phone would respond with the name of the artist and the title of the music you’re listening to? Perhaps even sending an email to your default email address with some supplemental information. In this paper we present an audio fingerprinting system, which makes the above scenario possible. By using the fingerprint of an unknown audio clip as a query on a fingerprint database, which contains the fingerprints of a large library of songs, the audio clip can be identified. At the core of the presented system are a highly robust fingerprint extraction method and a very efficient fingerprint search strategy, which enables searching a large fingerprint database with only limited computing resources.",
        "zenodo_id": 1417973,
        "dblp_key": "conf/ismir/HaitsmaK02",
        "keywords": [
            "audio fingerprinting system",
            "query on fingerprint database",
            "identify unknown audio clip",
            "large library of songs",
            "robust fingerprint extraction method",
            "efficient fingerprint search strategy",
            "limited computing resources",
            "email with supplemental information",
            "mobile phone",
            "supplemental information"
        ],
        "content": "A Highly Robust Aud io Fingerprinting System  \nA Highly Robust Audio Fingerprinting System  \nJaap Haitsma  \nPhilips Research  \nProf. Holstlaan 4, 5616 BA,  \nEindhoven, The Netherlands  \n++31 -40-2742648  \nJaap.Haitsma@philips.com  Ton Kalker  \nPhilips Researc h \nProf. Holstlaan 4, 5616 BA,  \nEindhoven, The Netherlands  \n++31 -40-2743839  \nTon.Kalker@ieee.org  \nABSTRACT  \nImagine the following situation. You’re in your car, listening to \nthe radio and suddenly you hear a song that catches your attention. \nIt’s the best ne w song you have heard for a long time , but you \nmissed the announcement and don’t recognize the artist. Still, you \nwould like to know more about this music. What should you  do? \nYou could call the radio station, but that’s too cumbersome. \nWouldn’t it be nice if you could push a few buttons on your \nmobile phone  and a few seconds later the phone would respond \nwith the name of the artist and the title of the music you’re \nlistening to ? Perhaps e ven send ing an email t o your default email \naddress with some supplemental information . In this paper we \npresent an audio fingerprinting system, which makes the above \nscenario possible. By using the fingerprint of an unknown audio \nclip as a query on a fingerprint database, whic h contai ns the \nfingerprints of a large library of songs, the audio clip can be \nidentified. At the core of the presented system are a highly robust \nfingerprint extraction method and a very efficient fingerprint \nsearch strategy, which enables searching a  large fingerprint \ndatabase with only limited computing resources.  \n1. INTRODUCTION  \nFingerprint systems are over one hundred years old. In 1893 Sir \nFrancis Galton was the first to “prove” that no two fingerprints of \nhuman beings were alike. Approximately 10 ye ars later Scotland \nYard accepted a system designed by Sir Edward Henry for \nidentifying fingerprints of people. This system relies on the pattern \nof dermal ridges on the fingertips and still forms the basis of all \n“human” fingerprinting techniques of today.  This type of forensic \n“human” fingerprinting system has however existed for longer  \nthan a century , as 2000 years ago Chinese emperors were already \nusing thumbprints to sign important documents. The implication is \nthat already those  emperors (or at least their administrative \nservants) realized that every fingerprint was unique. Conceptually \na fingerprint can be seen as a “human” summary or signature  that \nis uniqu e for every human being. It is important to note that a \nhuman fing erprint differs from a textual summary in that it does \nnot allow the reconstruction of other aspects of the original. For \nexample, a human fingerprint does not convey any information \nabout the color of the person’s hair or eyes.  \nRecent years have seen a gr owing scientific and industrial interest \nin computing fingerprints of multimedia objects [1][2][3][4] \n[5][6]. The growing industrial interest is shown among others by a \nlarge number of (startup) companies [7][8][9][10][11][12][13] \nand the recent request for information on audio fingerprinting \ntechnologies by the International Federation of the Phonographic \nIndustry (IFPI) and the Reco rding Industry Association of \nAmerica (RIAA) [14].  The prime objective of multimedia fingerprinting is an efficient \nmechanism to establish the perceptual equality of two multimedia \nobjects: not by comparing the (typically large)  objects themselves , \nbut by comparing the associated fingerprints (small by design). In \nmost systems using fingerprinting technology, the fingerprints of a \nlarge number of multimedia objects , along with their associated \nmeta -data (e.g. name of a rtist, title and album) are stored in a \ndatabase. The fingerprints serve as an index to the meta -data. The \nmeta -data of unidentified multimedia content are then retrieved by \ncomputing a fingerprint and using this as a query in the \nfinger print/ meta -data database.  The advantage of using \nfingerprints instead of the multimedia content itself is three -fold:  \n1. Reduced memory/storage requirements as fingerprints \nare relatively small;  \n2. Efficient comparison as perceptual irrelevancies have \nalready been removed from fingerprints;  \n3. Efficient searching as the dataset to be searched is \nsmaller.  \nAs can be concluded from above, a fingerprint system generally \nconsists of two components: a method to extract fingerprints and a \nmethod to efficiently sear ch for matching fingerprints in a \nfingerprint database.  \nThis paper describes an audio fingerprinting system that is suitable \nfor a large number of applications. After defining the concept of \nan audio fingerprint in Section 2 and  elaborating on possible \napplications in Section 3, we focus on the technical aspects of the \nproposed audio fingerprinting system. Fingerprint extraction is \ndescribed in Section 4 and fingerprint search ing in Section 5.  \n2. AUDIO FINGERPRINTING CONCEPTS  \n2.1 Audio Fingerprint Definition  \nRecall that an audio fingerprint can be seen as a short summary of \nan audio object. Therefore a fingerprint function F should map an \naudio object X, consisting of a large number of bits , to a \nfingerprint of only a limited number of bits.  \nHere we can draw an analogy with so -called hash functions1, \nwhich are well known in cryptography. A cryptographic hash \nfunction H maps a n (usually large) object X to a (usually small) \nhash value (a.k.a. message digest). A cryptographic hash function \nallows compari son of  two large objects  X and Y, by just \ncomparing their respective hash values H(X) and H(Y). Strict \nmathematical equality of the latter pair implies e quality of the \nformer, with only a very low probability of error. For a properly \ndesigned cryptographic hash function this probability is 2-n, where \nn equals the number of bits of the hash value. Using cryptographic \nhash functions, an efficient me thod exists to check whether or not \na particular data item X is contained in a given and large data set \nY={Yi}. Instead of storing and comparing with all of the data in Y, \n                                                                 \n1 In the literature fingerprinting is somet imes also referred to as \nrobust or perceptual hashing [5]. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull cit ation on the first page.  \n© 2002 IRCAM – Centre Pompidou  A Highly Robust Aud io Fingerprinting System  \nit is sufficient to store the set of hash values { hi = H (Yi)}, and to \ncompare H(X) with this set of hash values.  \nAt first  one might think that cryptographic hash functions are a \ngood candidate for fingerprint functions. However recall from the \nintroduction that, instead of strict mathematical equality, we are \ninterested in percep tual similarity. For example, an original CD \nquality version of ‘Rolling Stones – Angie’ and an MP3 version at \n128Kb/s sound the same to the human auditory system, but their \nwaveforms can be quite different.  Although the two versions are \nperceptual ly similar  they are mathematically quite different. \nTherefore cryptographic hash functions cannot decide upon \nperceptual equality of these two versions. Even worse, \ncryptographic hash functions are typically  bit-sensitive: a singl e \nbit of difference in the original object results in a completely \ndifferent hash value.  \nAnother valid question the  reader might ask is: “Is it not possible \nto design a fingerprint function that produces mathematically \nequal fingerprints for per ceptually similar objects?” The question \nis valid, but the answer is that such a modeling of perceptual \nsimilarity is fundamentally not possible. To be more precise: it is a \nknown fact that perceptual similarity is not transitive. Perceptual \nsimilarity of a pair of objects X and Y and of another pair of \nobjects Y and Z does not necessarily imply the perceptual \nsimilarity of objects X and Z. However modeling perceptual \nsimilarity by mathematical equality of fingerprints would lead to \nsuch a relationship.  \nGiven the above arguments , we propose to construct a fingerprint \nfunction in such a way that perceptual similar audio objects result \nin similar fingerprints. Furthermore , in order to be able \ndiscriminate between different audio objects, there must be a very \nhigh probability that dissimilar audio object s result  in dissimilar \nfingerprints. More mathematically, for a properly designed \nfingerprint function F, there should be a threshold T such that \nwith very high probability ||F(X)-F(Y)||≤T  if objects X and Y are \nsimilar and ||F(X)-F(Y)||>T when they are dissimilar.  \n2.2 Audio Fingerprint System Parameters  \nHaving a proper definition of an audio fingerprint we now focus \non the different par ameters of an audio fingerprint system. The \nmain parameters are:  \n• Robustness : can an audio clip still be identified after \nsevere signal degradation? In order to achieve high \nrobustness the fingerprint should be based on perceptual \nfeatures  that  are i nvariant (at least to a certain degree) \nwith respect to signal degradations. Preferably , severely \ndegraded audio still leads to very similar fingerprints. \nThe false negative rate is generally used to express the \nrobustness. A false negative occurs when the  \nfingerprints of perceptually similar audio clips are too \ndifferent to lead to a positive match . \n• Reliability : how often is a song incorrectly identified ? \nE.g. “Rolling Stones – Angie” being identified as \n“Beatles – Yesterday”. The rat e at which this occurs is \nusually referred to as the false positive rate.  \n• Fingerprint size : how much storage is needed for a \nfingerprint? To enable fast searching, fingerprints are \nusually stored in RAM memory. Therefore the \nfingerprint size, usuall y expressed in bits per second or \nbits per song, determines to a large degree the memory \nresources that are needed for a fingerprint database \nserver.  \n• Granularity : how many seconds of audio is needed to \nidentify an audio clip? Granularity is a parameter tha t can depend on the application. In some applications the \nwhole song can be used for identification, in others one \nprefers to identify a song with only a short excerpt of \naudio.  \n• Search speed and scalability: how long does it take to \nfind a fingerprint in a  fingerprint database? What if the \ndatabase contains thousands and thousands of songs? \nFor the commercial deployment of audio fingerprint \nsystems, search speed and scalability are a key \nparameter. Search speed should be in the order  of \nmilliseco nds for a database containing over 100,000 \nsongs using  only limited computing resources (e.g. a \nfew high -end PC’s).  \nThese five basic parameters have a large impact on each other. For \ninstance, if one wants a lower granularity, one needs to extract a \nlarger fingerprint to obtain the same reliability. This is due to the \nfact that the false positive rate is inversely related to the \nfingerprint size. Another example: search speed generally \nincrease s when one designs a more robust fingerprint. This is d ue \nto the fact that a fingerprint search is a proximity search . I.e. a \nsimilar (or the most similar) fingerprint has to be found. If the \nfeatures are more robust the proximity is smaller. Therefore the \nsearch speed can increase . \n3. APPLICATIONS  \nIn this sectio n we elaborate on a number of applications for audio \nfingerprinting.  \n3.1 Broadcast Monitoring  \nBroadcast monitoring is probably the most well known application \nfor audio fingerprinting [2][3][4][5][12][13]. It refers to the \nautomatic playlist generation of radio, television or web \nbroadcasts for, amon g others, purposes of royalty collection, \nprogram verification, ad vertisement  verification and people \nmetering. Currently  broadcast monitoring is still  a manual process: \ni.e. organizations interested in playlists, such as performance \nrights  organizations, currently have “real” people listening to \nbroadcasts and filling out scorecards.  \nA large -scale broadcast monitoring system based on fingerprinting \nconsists of several monitoring sites and a central site where the \nfingerprint server is loca ted. At the monitoring sites  fingerprints \nare extracted from all the (local) broadcast channels.  The central \nsite collects the fingerprints  from the monitoring sites . \nSubsequently, the fingerprint server, containin g a huge fingerprint \ndatabase, produces the playlists of all the broadcast channels.  \n3.2 Connected Audio  \nConnected audio is a general term for consumer applications \nwhere music is somehow connected to additional and supporting \ninformation. The example given in  the abstract, using a mobile \nphone to identify a song is one of these examples. This business is \nactually pursued by a number of companies [10][13]. The audio \nsignal in t his application is severely degraded due to processing \napplied by radio stations, FM/AM transmission, the acoustical \npath between the loudspeaker and the microphone of the mobile \nphone, speech coding and finally the transmission over the mobile \nnetwork. Th erefore, from a technical point of view, this is a very \nchallenging application.  \nOther examples of connected audio are (car) radios with an \nidentification button or fingerprint applications “listening” to the \naudio streams leaving or entering a soundcard o n a PC. By \npushing an “info” button in the fingerprint application, the user \ncould be directed to a page on the Internet containing information \nabout the artist. Or by pushing a “buy” button the user would be A Highly Robust Aud io Fingerprinting System  \nable to buy the album on the Internet. In other  words, audio \nfingerprinting can provide a universal linking system for audio \ncontent.  \n3.3 Filtering Technology for File Sharing  \nFiltering refers to active intervention in content distribution. The \nprime example for filtering technology for file sharing was \nNapster [15]. Starting in June 1999, users who downloaded the \nNapster client could share and download a large collection of \nmusic for free. Later, due to a court case by the music industry,  \nNapster users were forbidden to dow nload copyrighted songs. \nTherefore in March 2001 Napster installed an audio filter based \non file names, to block downloads of copyrighted songs. The filter \nwas not very effective, because users started to intentionally \nmisspell filenames. In May 2001 Napst er introduced an audio \nfingerprinting system by Relatable [8], which aimed at filtering \nout copyrighted material even if it was misspelled. Owing to  \nNapster ’s clos ure only two months later, the effectiveness of that  \nspecific fingerprint system is, to the best of the author’s \nknowledge, not publicly known.  \nIn a legal file sharing service one could apply  a more refined \nscheme than just filtering out copyrighted material. One could \nthink of a scheme with free music , different kinds of premium \nmusic (accessible to those with a proper subscription) and \nforbidden music.  \nAlthough from a consumer standpoint, audio filtering could be \nviewed as a negative technology, there are also a number of \npotential benefits to th e consumer. Firstly it can organize music \nsong titles in search results in a consistent way by using the \nreliable meta -data of the fingerprint database. Secondly, \nfingerprinting can guarantee that what is downloaded is actually \nwhat it says it is.  \n3.4 Automatic Music Library Organization  \nNowadays many PC users have a music library containing several \nhundred , sometimes  even thousands, of songs. The music is \ngenerally stored in compressed format (usually MP3) on their \nhard-drives. When these songs are obtained from different \nsources, such as ripping from a CD or downloading from file \nsharing networks, these libraries are often not well organized. \nMeta -data is often inconsistent, incomplete and sometimes even \nincorrect. Assuming that the fingerpr int database contains correct \nmeta -data, audio fingerprinting can make the meta -data of the \nsongs in the library consistent, allowing easy organization based \non, for example , album or artist. For example, ID3Man [16], a \ntool powered by Auditude [7] fingerprinting technology is already \navailable for tagging unlabeled or mislabeled MP3 files. A similar \ntool from Moodlogic [11] is ava ilable as a Winamp plug -in [17]. \n4. AUDIO FINGERPRINT EXTRACTION  \n4.1 Guiding Principles  \nAudio fingerprints intend to capture the relevant perceptual \nfeatures of audio. At the same time extracting and searching \nfingerprints should be  fast and easy, preferably with a small \ngranularity to allow usage in highly demanding applications (e.g. \nmobile phone recognition). A few fundamental questions have to \nbe addressed before starting the design and implementation of \nsuch an audio fingerprint ing scheme. The most prominent \nquestion to be addressed is: what kind of features are the most \nsuitable. A scan of the existing literature show s that the set of \nrelevant features can be broadly divided into two classes : the class \nof semantic fea tures and the class of non -semantic features. \nTypical elements in the former class are genre, beats -per-minute, \nand mood . These types of features usually have a direct interpretation, and are actually used to classify music, generate \nplay-lists and more. T he latter class consists of features that have a \nmore mathematical nature and are difficult for humans to ‘read’ \ndirectly from music. A typical element in this class is \nAudioFlatness  that is proposed in MPEG -7 as an audio descriptor \ntool [2]. For the work described in this paper we have for a \nnumber of reasons explicitly chosen to work with non -semantic \nfeatures:  \n1. Semantic features don’t always have a clear and \nunambiguous meaning. I.e. personal opinions differ over \nsuch cl assifications. Moreover, semantics may actually \nchange over time. For example, music that was \nclassified as hard rock  25 years ago may be viewed as \nsoft listening  today. This makes mathematical analysis \ndifficult.  \n2. Semantic features are in general more d ifficult to \ncompute than non -semantic features.  \n3. Semantic features are not universally applicable. For \nexample, beats -per-minute  does not typically apply to \nclassical music.  \nA second question to be addressed is the representation of \nfingerprints. One obviou s candidate is the representation as a \nvector of real numbers, where each component express es the \nweight of a certain basic perceptual feature. A second option is to  \nstay closer in spirit to cryptographic hash functions and represent \ndigital fingerprin ts as bit -strings. For reasons of reduced search \ncomplexity we have decided in this work for the latter option. The \nfirst option would imply a similarity measure involving real \nadditions/subtractions and depending on the similarity measure \nmaybe even real multiplications.  Fingerprints that are based on bit \nrepresentations can be compared by simply counting bits. Given \nthe expected application scenarios , we do not expect a high \nrobustness for each and every bit in such a binary fingerprint. \nTherefore, in contrast to cryptographic hashes that typically have a \nfew hundred bits at the most, we will allow fingerprints that have \na few thousand  bits. Finge rprints containing a large number bits \nallow reliable identification even if the percentage of non-\nmatching bits is relatively high.   \nA final question involves the granularity of fingerprints. In the \napplications that we envis age there is no guarantee that the audio \nfiles that need to be identified are complete. For example, in \nbroadcast monitoring, any interval of 5 seconds is a unit of music \nthat has commercial value, and therefore may need to be identified \nand recognized. Also, in security applications such a s file filtering \non a peer -to-peer network, one would not  wish  that deleti on of the \nfirst few seconds of an audio file would prevent  identification. In \nthis work we therefore adopt the policy of  fingerprints streams  by \nassigning sub-fingerprints  to sufficiently small atomic intervals \n(referred to as  frames ). These sub -fingerprints might not be large \nenough to identify the frames themselves, but a longer interval, \ncontaining sufficiently many frames, will allow robust and reliable \nidentification.  \n4.2 Extraction Algorithm  \nMost fingerprint extraction algori thms are based on the following  \napproach. First the audio  signal is segmented into frames. For \nevery frame a set of features is computed. Preferably the features \nare chosen such that they are invariant (at least to a certain degree) \nto signal degradations. Features that have been proposed are well \nknown audio features such as Fourier coefficients [4], Mel \nFrequency Cepstral Coefficients (MFFC) [18], spectral flatness \n[2], sharpness [2], Linear Predictive Coding (LPC) coefficients \n[2] and others. Also derived quantities  such as derivatives, means \nand variances of audio features are used. Generall y the extracted A Highly Robust Aud io Fingerprinting System  \nfeatures are mapped into a more compact representation by using \nclassification algorithms, such as Hidden Markov Models [3], or \nquantization [5]. The compact representation of a single f rame \nwill be referred to as a sub-fingerprint . The global fingerprint \nprocedure converts a stream of audio into a stream of sub -\nfingerprints. One sub -fingerprint usually does not contain \nsufficient data to identify an audio clip. The basic unit that \ncontai ns sufficient data to identify an audio clip (and therefore \ndetermining the granularity) will be referred to as a fingerprint -\nblock.  \nThe proposed fingerprint extraction scheme is based on this \ngeneral streaming approach. It extracts 32 -bit sub -fingerpri nts for \nevery interval of 11.6 milliseconds. A fingerprint block consists of \n256 subsequent sub -fingerprints, corresponding to a granularity of \nonly 3 seconds. An overview of the scheme is shown in Figure 1. \nThe audio signal is first segmented into overla pping frames. The \noverlapping frames have a length of 0.37 seconds and are \nweighted by a Hanning window with an overlap factor of 31/32. \nThis strategy results in the extraction of one sub -fingerprint for \nevery 11.6 milliseconds. In the worst -case scenario the frame \nboundaries used during identification are 5.8 milliseconds off with \nrespect to the boundaries used in the database of pre -computed \nfingerprints. The large overlap assures that even in this worst -case \nscenario the sub -fingerprints of the audio cli p to be identified are \nstill very similar to the sub -fingerprints of the same clip in the \ndatabase.  Due to the large overlap subsequent sub -fingerprints \nhave a large similarity and are slowly varying in time. Figure 2a \nshows an example of an extracted fin gerprint block and the slowly \nvarying character along the time axis.  \nThe m ost important perceptual audio features live in the frequency \ndomain. Therefore a spectral representation is computed by \nperforming a Fourier transform on every frame. Due to the \nsensitivity of the phase of the Fourier transform to different frame \nboundaries and the fact that the Human Auditory System (HAS) is \nrelatively insensitive to phase, only the absolute value of the \nspectrum, i.e. the power spectral density, is retained.  \nIn or der to extract a 32 -bit sub -fingerprint value for every frame, \n33 non -overlapping frequency bands are selected. These bands lie \nin the range from 300Hz to 2000Hz (the most relevant spectral \nrange for the HAS) and have a logarithmic spacing. The \nlogarithmic  spacing is chosen, because it is known that the HAS \noperates on approximately logarithmic bands (the so -called Bark  \nscale). Experimentally it was verified that the sign of energy \ndifferences (simultaneously along the time and frequency axes) is \na prope rty that is very robust to many kinds of processing. If we \ndenote the energy of band m of frame n by E(n,m)  and the m-th bit \nof the sub -fingerprint  of frame n by F(n,m) , the bits of the sub -\nfingerprint are formally defined as (see also the gray block in  Figure 1, where T is a delay element):  \n \nFigure 2 shows an example of 256 subsequent 32 -bit sub -\nfingerprints (i.e. a fingerprint block), extracted with the above \nscheme from a short excerpt of  “O Fortuna”  by Carl Orff.  A ‘1’ \nbit corresponds to a white p ixel and a ‘0’ bit to a black pixel. \nFigure 2a and Figure 2b show a fingerprint block from an original \nCD and the MP3 compressed (32Kbps) version of the same \nexcerpt, respectively. Ideally these two figures should be identical, \nbut due to the compression s ome of the bits are retrieved \nincorrectly. These bit errors, which are used as the similarity \nmeasure  for our fingerprint scheme, are shown in black in Figure \n2c.  \nThe computing resources needed for the proposed algorithm are \nlimited. Since the algorithm o nly takes into account frequencies \nbelow 2kHz the received audio is first down sampled to a mono \naudio stream with a sampling rate of 5kHz. The sub -fingerprints \nare designed such that they are robust against signal degradations. \nTherefore very simple down  sample filters can be used without \nintroducing any performance degradation. Currently 16 tap FIR \nfilters are used . The most computationally  demanding operation is \nthe Fourier transform of every aud io frame. In the down sampled \naudio signal a frame has a length of 2048 sample s. If the Fourier \ntransform is implemented as a fixed point real -valued FFT the \nfingerprinting algorithm has been shown to run efficiently on \nportable devices such as a PDA or a  mobile phone.  \n4.3 False Positive Analysis  \nTwo 3 -second audio signals are declared similar if the Hamming \ndistance (i.e. the number of bit errors) between the two derived \nfingerprint blocks is below a certain threshold T. This threshold \nvalue T directly determ ines the false positive rate Pf, i.e. the rate at \nwhich audio signals are incorrectly declared equal: the smaller T, \nthe smaller the probability Pf will be. On the other hand, a small \nvalue of T will negatively effect the false negative probability Pn, Figure 2. (a) Fingerprint block of original music clip, \n(b) finge rprint block of a compressed version, (c) the \ndifference between a and b showing the bit errors in \nblack (BER=0.078).  \n Original\nTime (Frames)B31..………….……………….B0 B31..………….……………….B0 B31..………….……………….B0MP3@128Kbps Bit Errors\n(a) (b) (c)\n0\n256Original\nTime (Frames)B31..………….……………….B0 B31..………….……………….B0 B31..………….……………….B0MP3@128Kbps Bit Errors\n(a) (b) (c)\n0\n256Figure 1. Overview fingerprint extraction scheme.  F ABS+\n∑x2\n∑x2-T-\n+>0 F(n,0)\n+\n-T-\n+>0 F(n,1)\n+\n∑x2\n∑x2-T-\n+>0 F(n,31)Bit DerivationEnergy\nComputationBand\nDivision\nFourier\nTransform Framing\nF ABS+\n∑x2\n∑x2-TT-\n+>0 F(n,0)\n+\n-TT-\n+>0 F(n,1)\n+\n∑x2\n∑x2-TT-\n+>0 F(n,31)Bit DerivationEnergy\nComputationBand\nDivision\nFourier\nTransform Framing\n (1) ( )\n( ) \n≤+−−−−+ −>+−−−−+ −=0)1,1(),1( )1,(),(  if  00)1,1(),1( )1,(),(  if   1),(m nE m nE mnE mnEm nE m nE mnE mnEmnFA Highly Robust Aud io Fingerprinting System  \ni.e. the probability that two signals are ‘equal’, but not identified \nas such.  \nIn order to analyze the choice of this threshold T, we assume that \nthe fingerprint extraction process yields random i.i.d. \n(independent and identically distributed) bits. The numb er of bit \nerrors will then have a binomial distribution (n,p) , where n equals \nthe number of bits extracted and p (= 0.5) is the probability that a \n‘0’ or ‘1’ bit is extracted. Since n (= 8192  = 32 × 256) is large in \nour application, the binomial distributi on can be approximated by \na normal distribution with a mean m = np and standard deviation \ns =Ö(np(1 -p)). Given a fingerprint block F1, the probability that a \nrandomly selected fingerprint block F2 has less than T = a n errors \nwith respect to F1 is given by:  \n()()\n\n−= =∫∞\n−−n dx e Pnx\nf\n2)21(erfc21\n21\n2122 a\npaa   \nwhere a denotes the Bit Error R ate (BER).  \nHowever, in practice the sub -fingerprints have high correlation \nalong the time axis. This correlation is due  not only  to the inherent \ntime correlation in audio, but also by the large overlap of the \nframes used in fingerprint extraction. Higher correlation implies a \nlarger standard deviation, as shown by the following argument.  \nAssume a symmetric binary source with alphabet { -1,1} such that \nthe probability that symbol xi and symbol xi+1 are the same is \nequals to q. Then one may easily show  that \n , ] E[||k\nkiia xx =+ (3) \nwhere a = 2·q-1. If the source Z is the exclusive -or of two such \nsequences X and Y, then Z is symmetric and  \n . ] E[||2k\nkiia zz =+ (4) \nFor N large, the standard deviation of the average NZ over N \nconsecutive samples of Z can be approximately described by a \nnormal distribution with mean 0 and standard deviation equal to  \n .\n) 1(1\n22\na Na\n−+ (5) Translating the above back to the case of fingerprints bits, a \ncorrelation factor a betwee n subsequent fingerprint bits implies an \nincrease in standard deviation for the BER by a factor  \n .11\n22\naa\n−+ (6) \nTo determine the distribution of the BER with real fingerprint \nblocks a fingerprint database of 10,000 songs was generated. \nTherea fter the BER of 100,000 randomly selected pairs of \nfingerprint blocks were determined. The standard deviation of the \nresulting BER distribution was measured to be 0.0148, \napproximately 3 times higher than the 0.0055 one would expect \nfrom random i.i.d. bits . \nFigure 3 shows t he log Probability Density Function  (PDF)  of the \nmeasured BER distribution and a normal distribution with mean \nof 0.5 and a standard deviation of 0.0148. The PDF of the BER is \na close approximation to the normal distribution . For BERs below \n0.45 we observe some outliers , due to insufficient statistics . To \nincorporate the larger standard deviation of the BER distribution \nFormula (2) is modified by inclusion of a factor 3.  \n() \n\n−= n Pf\n23)21(erfc21 aa \nThe threshold for the BER used during experiments was α = 0.35. \nThis means that out of 8192 bits there must be less than 2867 bits \nin error in order to decide that the fingerprint blocks originate \nfrom the same song. Using formula ( 7) we  arrive at a very low \nfalse positive rate of erfc(6.4)/2= 3.6 ·10-20.  \n4.4 Experimental Robustness Results  \nIn this subsection we show the experimental robustness of the \nproposed audio fingerprinting scheme. That is, we try to answer \nthe question of whether o r not the BER between the fingerprint \nblock of an original and a degraded version of an audio clip \nremains under the threshold a (=0.35).  \nWe selected four short audio excerpts (Stereo, 44.1kHz, 16bps) \nfrom songs that belong to different musical genres: “O Fortuna”  \nby Carl Orff, “Success has made a failure of our home”  by Sinead \no’Connor, “Say what you want”  by Texas and “A whole l ot of \nRosie”  by AC /DC.  All of the excerpts were subjected to the \nfollowing signal degradations:  \n• MP3 Encoding/Decoding at 128 Kbps and 32 Kbps.  \n• Real Media Encoding/Decoding at 20 Kbps.  \n• GSM Encoding at Full Rate with an error -free channel and \na channel with  a carrier to interference (C/I) ratio of 4dB \n(comparable to GSM reception in a tunnel).  \n• All-pass Filtering using the system function: H(z)=(0.81z2-\n1.64z+1)/ (z2-1.64z+0.81) . \n• Amplitude Compression  with the following compression \nratios: 8.94:1 for |A| ≥ -28.6 dB; 1.73:1 for -46.4 dB < |A| < -\n28.6 dB; 1:1.61 for |A| ≤  -46.4 dB.  \n• Equalization  A typical10 -band equalizer with the following \nsettings:  \nFreq.(Hz)  31 62 125 250 500 1k 2k 4k 8k 16k \nGain(dB)  -3 +3 -3 +3 -3 +3 -3 +3 -3 +3 \n• Band -pass Filtering  using a s econd order Butterworth filter \nwith cut -off frequencies of 100Hz and 6000Hz.  \n• Time Scale Modification  of +4% and -4% . Only the tempo \nchanges, the pitch remains unaffected.  (2) (7) 0.44 0.46 0.48 0.5 0.52 0.54 0.56-3-2-1012\nBit Error Ratelog(pdf)\n0.44 0.46 0.48 0.5 0.52 0.54 0.56-3-2-1012\nBit Error Ratelog(pdf)\nFigure 3. Comparison  of the probability density function of \nthe BER plotted as ‘+’ and the normal distribution . A Highly Robust Aud io Fingerprinting System  \n• Linear Speed Change  of +1%, -1%, +4% and -4%. Both \npitch and tempo change.  \n• Noise Add ition with uniform white noise with a maximum \nmagnitude of 512 quantization steps.  \n• Resampling consisting of subsequent down and up sampling \nto 22.05 kHz and 44.10 kHz, respectively.  \n• D/A A/D Conversion  using a commercial analog tape \nrecorder.  \nThereafter the  BERs between the fingerprint blocks of the original \nversion and of all the degraded versions were determined for each \naudio clip. The resulting BERs are shown in Table 1. Almost all \nthe resulting bit error rates are well below the threshold of 0.35, \neven for GSM encoding2. The only degradations that lead to a \nBER above threshold are large linear speed changes. Linear speed \nchanges larger then +2.5% or –2.5% percent generally result in bit \nerror rates higher than 0.35. This is due to misalignment of the \nframing (temporal misalignment) and spectral scaling (frequency \nmisalignment). Appropriate pre -scaling (for example by \nexhaustive search) can solve this issue.  \n \n5. DATABASE SEARCH  \n5.1 Search Algorithm  \nFinding extracted fingerprints in a fingerprint database is a non-\ntrivial task. Instead of searching for a bit exact fingerprint (easy!), \nthe most similar fingerprint needs to be found. We will illustrate \nthis with some numbers based on the proposed fingerprint scheme. \nConsider a moderate size fingerprint database co ntaining 10,000 \nsongs with an average length of 5 minutes. This corresponds to \napproximately 250 million sub -fingerprints. To identify a \nfingerprint block originating from an unknown audio clip we have \nto find the most similar fingerprint block in the data base. In other \nwords, we have to find the position in the 250 million sub -\nfingerprints where the bit error rate is minimal. This is of course \npossible by brute force searching. However this takes 250 million \nfingerprint block comparisons.  Using a modern P C, a rate of \n                                                                 \n2 Recall that a GSM codec is optimized for speech, not for general \naudio.  approximately of 200,000 fingerprint block comparisons per \nsecond can be achieved. Therefore the total search time for our \nexample will be in the order of 20 minutes! This shows that brute \nforce searching is not a viable solution for practical  applications.  \nWe propose to use a more efficient search algorithm. Instead of \ncalculating the BER for every possible position in the database, \nsuch as in the brute -force search method, it is calculated for a few \ncandidate positions only. These candidates contain with very high \nprobability the best matching position in the database.  \nIn the simple version of the improved search algorithm, candidate \npositions are generated based on the assumption that it is very \nlikely that at least one sub -fingerprint has an exact match at the \noptimal position in the database [3][5]. If this assumption is valid, \nthe only positions that need to be checked are the ones where one \nof the 256 sub -fingerprints of t he fingerprint block query matches \nperfectly. To verify the validity of the assumption, the plot in \nFigure 4 shows the number of bit errors per sub -fingerprint for the \nfingerprints depicted in Figure 2. It shows that there is indeed a \nsub-fingerprint that does not contain any errors. Actually 17 out of \nthe 256 sub -fingerprints are error -free. If we assume that the \n“original” fingerprint of Figure 2a is indeed loaded in the Table 1. BER for different kinds of signal degradations.  \nProcessing  Orff  Sinead  Texa s AC/DC \nMP3@128Kbps  0.078  0.085  0.081  0.084  \nMP3@32Kbps  0.174  0.106  0.096  0.133  \nReal@20Kbps  0.161  0.138  0.159  0.210  \nGSM  0.160  0.144  0.168  0.181  \nGSM C/I = 4dB  0.286  0.247  0.316  0.324  \nAll-pass filtering  0.019  0.015  0.018  0.027  \nAmp. Compr.  0.052  0.070  0.113  0.073  \nEqualization  0.048  0.045  0.066  0.062  \nEcho Addition  0.157  0.148  0.139  0.145  \nBand Pass Filter  0.028  0.025  0.024  0.038  \nTime Scale +4%  0.202  0.183  0.200  0.206  \nTime Scale –4% 0.207  0.174  0.190  0.203  \nLinear Speed +1%  0.172  0.102  0.132  0.238  \nLinear Speed -1% 0.243  0.142  0.260  0.196  \nLinear Speed +4%  0.438  0.467  0.355  0.472  \nLinear Speed -4% 0.464  0.438  0.470  0.431  \nNoise Addition  0.009  0.011  0.011  0.036  \nResampling  0.000  0.000  0.000  0.000  \nD/A A/D  0.088  0.061  0.111  0.076  \n \n50 100 150 200 250051015202530\nFrame numberBit errors per sub -fingerprint (        )\n051015202530\nMost reliable erroneous bit (        )\n50 100 150 200 250051015202530\nFrame numberBit errors per sub -fingerprint (        )\n051015202530\nMost reliable erroneous bit (        )\nFigure 5. Bit errors per sub -fingerprint (gray line) and the \nreliability of the most reliable erroneous bit (black line) for \nthe “MP3@32Kbps version” of ‘O Fortuna’ by Carl Orff.  50 100 150 200 250012345678910\nFrame numberBit errors per sub -fingerprint (        )\n50 100 150 200 250012345678910\nFrame numberBit errors per sub -fingerprint (        )\nFigure 4. Bit errors per sub -fingerprint for the “MP3@ \n128Kbps version” of excerpt of  ‘O Fortuna’ by Carl Orff.  A Highly Robust Aud io Fingerprinting System  \ndatabase, its position will be among the selected candidate \npositions for the “MP3@12 8Kbps fingerprint” of Figure 2b.  \nThe positions in the database where a specific 32 -bit sub-\nfingerprint is located are retrieved using the database architecture \nof Figure 6. The fingerprint database contains a lookup table \n(LUT) with all possible 32 bit su b-fingerprints as an entry. Every \nentry points to a list with pointers to the positions in the real \nfingerprint lists where the respective 32 -bit sub -fingerprint are \nlocated.  In practical systems with limited memory3 a lookup table \ncontaining 232 entries is often not feasible, or not practical , or \nboth. Furthermore the lookup table will be sparsely filled, because \nonly a limited number of songs can reside in the memory. \nTherefore , in practice, a hash table [19] is used  instead of a lookup \ntable.  \nLet us again do the calculation  of the average number of \nfingerprint block comparisons per identification for a 10,000 -song \ndatabase. Since the database contai ns approximately 250 million \nsub-fingerprints , the average number of positions in a list will be \n0.058(=250·106 / 232). If we assume that all possible sub -\nfingerprints are equally likely , the average number of fingerprint \ncomparisons p er identification is only 15 (=0.058 × 256). \nHowever we observe in practice that, due to the non -uniform \ndistribution of sub -fingerprints, the number of fingerprint \ncomparisons increases roughly by a factor of 20. On average 300 \ncomparisons are needed, yie lding an average search time of 1.5 \nmilliseconds on a modern PC. The lookup -table can be \nimplemented in such a way that it has no impact on the search \ntime. At the cost of a lookup -table , the proposed search algorithm \nis approximately a factor 800,000 time s faster than the brute force \napproach.  \nThe observing reader might ask: “But, what if your assumption \nthat one of the sub -fingerprints is error -free does not hold?” The \nanswer is that the assumption almost always holds for audio \n                                                                 \n3 For example a PC with a 32 -bit Intel processor has a memory \nlimit of 4 GB.  signals with “mild” audio signal degradations (See also Section \n5.2). However, for heavily degraded signals the assumption is \nindeed not always valid. An example of a plot of the bit errors per \nsub-fingerprint for a fingerprint block  that does not contain any \nerror -free sub -fingerprints, is shown in Figure 5. There are \nhowever sub-fingerprints that contain only one error . So instead of \nonly checking positions in the database where one of the 256 sub -\nfingerprints occurs, we can also check all the positions where sub -\nfingerprints occur which have a Hamming distance of one (i.e. one \ntoggled bit) with respect to all the 256 sub -fingerprints. This will \nresult in 33 times more fingerprint comparisons, which is still \nacceptable. However , if we want to cope with situations t hat for \nexample the minimum number of bit errors per sub -fingerprint is \nthree (this can occur   in the mobile phone application), the number \nof fingerprint comparisons will increase with a factor of  5489 , \nwhich leads to unacceptable sear ch times. Note that the observed \nnon-uniformity factor of 20 is decreasing with increasing number \nof bits being toggled. If for instance all 32 bits of the sub -\nfingerprints are used for toggling, we end up with the brute force \napproach again, yielding a mu ltiplication factor of 1.  \nSince randomly toggling bits to generate more candidate positions \nresults very quickly in unacceptable search times, we propose to \nuse a different approach that uses soft decoding information. That \nis, we propose to estimate and u se the probability that a fingerprint \nbit is received correctly.  \nThe sub -fingerprints are obtained by comparing and thresholding \nenergy differences (see bit derivation block in Figure 1). If the \nenergy difference is very close to the threshold, it is reaso nably  \nlikely that the bit was received incorrectly (an unreliable bit). On \nthe other hand, if the energy difference is much larger than the \nthreshold the probability of an incorrect bit is low (a reliable bit). \nBy deriving reliability in formation for every bit of a sub -\nfingerprint, it is possible to expand a given fingerprint into a list of \nprobable sub -fingerprints. By assuming that one of the most \nprobable sub -fingerprints has an exact match at the optimal \nposition in the database, the fingerprint block can be identified as \nbefore. The bits are assigned a reliability ranking from 1 to 32, \nwhere a 1 denotes the least reliable and a 32 the most reliable bit. 0xAB569834\n0x75CFD5640x014AB461\n0x90FB12AC0x512BAD78\n0x81235AB6Song 1 Song 2 Song N\n0x00000000\n0xFFFFFFFF±25000LUT\n0x00000001\n2320x000000000x00000000\n0x000000010xFFFFFFFF0xFFFFFFFF0x000000010x7390832A\n256Fingerprint\nBlock\nTimeTime0x000000000xAB569834\n0x75CFD5640x014AB461\n0x90FB12AC0x512BAD78\n0x81235AB6Song 1 Song 2 Song N\n0x00000000\n0xFFFFFFFF±25000LUT\n0x00000001\n2320x000000000x00000000\n0x000000010xFFFFFFFF0xFFFFFFFF0x000000010x7390832A\n256Fingerprint\nBlock\nTimeTime0x00000000\nFigure 6. Fingerprint database layout . A Highly Robust Aud io Fingerprinting System  \nThis results in  a simple way to generat e a list of most probable \nsub-fingerprints by toggling only the most unreliable bits. More \nprecisely, the list consists of all the sub -fingerprints that have the \nN most reliable bits fixed and all the others variable. If the \nreliability information is perfect, one expects that in the case \nwhere a sub -fingerprint has 3 bit errors , the bits with reliability 1, \n2 and 3 are erroneous. If this is the case, fingerprint blocks  where \nthe minimum number of bit errors per sub -fingerprint is 3, can be \nidentified by generating candidate  positions with only 8 (=23) sub -\nfingerprints per sub -fingerprint. Compared to the factor 5489 \nobtained when  using all sub -fingerprints with a Hamming distance \nof 3 to generate candidate positions , this is an improvement with a \nfactor of approximately 6 86. \nIn practice the reliability information is not perfect (e.g. it happens \nthat a bit with a low reliability is received correctly and vice -\nversa), and therefore the improvements are less spectacular, but \nstill significant. This can for example be seen fr om Figure 5. The \nminimum number of bit -errors per sub -fingerprint is one. As \nalready mentioned before, the fingerprint block can then be \nidentified by generating 33 times more candidate positions. Figure \n5 also contains a plot of the reliability for the m ost reliable bit that \nis retrieved erroneously. The reliabilities are derived from the  \nMP3@32Kbps version using the proposed method. We see that \nthe first sub -fingerprint contains 8 errors. These 8 bits are not the \n8 weakest bits because one of the erron eous bits has an assigned \nreliability of 27. Thus, the reliability  information is not always \nreliable . However if we consider sub -fingerprint 130, which has \nonly a single bit -error, we see that the assigned reliability of the \nerroneous bit is 3. Therefore  this fingerprint block would have \npointed to a correct location in the fingerprint database when \ntoggling only the 3 weakest bits. Hence the song would be \nidentified correctly.  \nWe will finish this sub -section by again referring  to Figure 6 and \ngiving an example of how the proposed search algorithm works. \nThe last extracted sub -fingerprint of the fingerprint block in \nFigure 6 is 0x00000001. First the fingerprint block is compared to \nthe positions in the database where sub -fingerprint 0x00000001 is located. The LUT is pointing to only one position for sub -\nfingerprint 0x00000001, a certain position p in song 1. We now \ncalculate the BER between the 256 extracted sub -fingerprints (the \nfingerprint block) and the sub -fingerprint values of song 1 from \npositi on p-255 up to position p. If the BER is below the threshold \nof 0.35, the probability is high that the extracted fingerprint block \noriginates from song 1. However if this is not the case, either the \nsong is not in the database or the sub -fingerprint cont ains an error. \nLet us assume the latter  and that  bit 0 is the least reliable bit . The \nnext most probable candidate is then sub-fingerprint 0x00000000. \nStill referring to Figure 6, sub -fingerprint 0x00000000 has two \npossible candida te positions: one in song 1 and one in song 2. If \nthe fingerprint block has a BER below the threshold with respect \nto the associated fingerprint block in song 1 or 2, then a match \nwill be declared for song 1 or 2 , respectiv ely. If n either  of the two \ncandidate positions give a below threshold  BER , either other \nprobable sub -fingerprints are used to generate more candidate \npositions, or there is a switch to one of the 254 remaining sub -\nfingerprints where the proc ess repeats itself. If all 256 sub -\nfingerprints and their most probable sub -fingerprints have been \nused to generate candidate positions and no match below the \nthreshold has been found the algorithm decides that it cannot \nidentify the song.  \n5.2 Experimental Results  \nTable 2 shows how many of the generated candidate positions are \npointing to the matching fingerprint block in the database for the \nsame set of signal degradations used in the robustness \nexperiments. We will refer to this number as the number of hits in \nthe database. The number of hits has to be one or more to identify \nthe fingerprint block and can be maximally 256 in the case that all \nsub-fingerprints generate a valid ca ndidate position .  \nThe first number in every cell is the number of hits in case only \nthe sub -fingerprints themselves are used to generate candidate \npositions  (i.e. no soft decoding information is use d). We observe \nthat the majority of the fingerprint blocks can be identified, \nbecause one or more hits occur. However a few fingerprint blocks , \nmainly originating from more severely degraded audio, such as at \nGSM with C/I of 4dB, do not generate any hits. This setting of the \nsearch algorithm can be used in applications, such as broadcast \nmonitoring and automated labeling of MP3’s, where only minor \ndegradations of the audio are expected.  \nThe second number in every cell corresponds to the number of \nhits with a setting that is used to identify heavily distorted audio \nas, for example , in the mobile phone application. Compared  to the \nprevious setting the 1024 most probable sub -fingerprints of every \nsub-fingerprint are a dditionally  used to generate candidates. In \nother words, the 10 least reliable bits of every sub -fingerprint are \ntoggled to generate more candidate positions.  The resulting \nnumber of hits are higher , and even the “GSM C/I = 4dB \nfingerprint blocks” can be identified. Most of the finge rprint \nblocks with linear speed changes still do not have any hits. The \nBER of these blocks is already higher than the threshold and for \nthat reason they cannot be identified even if hits occur. \nFurthermore one has to keep in mind that with approp riate pre -\nscaling , as proposed in Section 4.4, the fingerprint blocks with \nlarge linear speed changes can be identified rather easily.  \n6. CONCLUSIONS  \nIn this paper we presented a new approach to audio fingerprinting. \nThe fingerpr int extraction is based on extracting a 32 bit sub -\nfingerprint every 11.8 milliseconds. The sub -fingerprints are \ngenerated by looking at energy differences along the frequency \nand the time ax es. A fingerprint block, co mprising  256 subsequent  \nsub-fingerprints, is the basic unit  that  is used to identify a song. Table 2. Hits in the database for different kinds of signal \ndegradations. First number indicates the hits for using only \nthe 256 sub -fingerprints to generate candidate positions. \nSecond number indicates hits when 1024 most probable \ncandidates for every sub -fingerprint are also used.  \nProcessing  Orff  Sinead  Texas  ACDC  \nMP3@128Kbps  17, 170  20, 196  23, 182  19, 144  \nMP3@32Kbps  0, 34  10, 153  13, 148  5, 61  \nReal@20Kbps  2, 7 7, 110  2, 67  1, 41  \nGSM  1, 57  2, 95  1, 60  0, 31  \nGSM C/I = 4dB  0, 3 0, 12  0, 1 0, 3 \nAll-pass filtering  157, 240  158, 256  146, 256  106, 219  \nAmp. Compr.  55, 191  59, 183  16, 73  44, 146  \nEqualization  55, 203  71, 227  34, 172  42, 148  \nEcho Addition  2, 36  12, 69  15, 69  4, 52  \nBand Pass Filter  123, 225  118, 253  117, 255  80, 214  \nTime Scale +4%  6, 55  7, 68 16, 70  6, 36  \nTime Scale –4% 17, 60  22, 77  23, 62  16, 44  \nLinear Speed +1%  3, 29  18, 170  3, 82  1, 16  \nLinear Speed -1% 0, 7 5, 88  0, 7 0, 8 \nLinear Speed +4%  0, 0 0, 0 0, 0 0, 1 \nLinear Speed -4% 0, 0 0, 0 0, 0 0, 0 \nNoise Addition  190, 256  178, 255  179, 256  114, 225  \nResampling  255, 256  255, 256  254, 256  254, 256  \nD/A A/D  15,149  38, 229  13, 114  31,145  \n A Highly Robust Aud io Fingerprinting System  \nThe fingerprint database contains a two -phase search algorithm \nthat is based on only performing full fingerprint comparisons at \ncandidate positions  pre-selected by a sub -fingerprint search. With \nreference to the parameters that were introduced in Section 2.2, \nthe proposed system can be summarized as follows:  \n• Robustness:  the fingerprints extracted are very robust. \nThey can even be used to identify  music recorded and \ntransmitted by a mobile telephone.  \n• Reliability : in Section 4.3 we derived a model for the \nfalse positive rate, which was confirmed by experiments.  \nBy setting the threshold to 0.35 a false positive rate of \n3.6·10-20 is achieved.   \n• Fingerprint size : a 32 bit fingerprint is extracted every \n11.8 ms, yielding a fingerprint size of 2 .6kbit/s  \n• Granularity: a fingerprint block consisting of 256 sub -\nfingerprints and corresponding to 3 seconds of  audio  is \nused as th e basic unit for identification.  \n• Search speed and scalability:  by using a two -phase \nfingerprint search algorithm a fingerprint database \ncontaining 20,000 songs and handling dozens of \nrequests per second can run on a modern PC.  \nFuture research will focus on  other feature extraction techniques \nand optimization of the search algorithm.  \n7. REFERENCES  \n[1] Cheng Y., “Music Database Retrieval Based on Spectral \nSimilarity”, International Symposium on Music Information \nRetrieval (ISMIR) 2001, Bloomington, USA, October 200 1.  \n[2] Allamanche E., Herre J., Hellmuth O., Bernhard Fröbach B. \nand Cremer M., “AudioID: Towards Content -Based \nIdentification of Audio Material”, 100th AES Convention, \nAmsterdam, The Netherlands, May 2001.  \n[3] Neuschmied H., Mayer H. and Battle E., “Identificati on of \nAudio Titles on the Internet”, Proceedings of International \nConference on Web Delivering of Music 2001, Florence, \nItaly, November 2001.  \n[4] Fragoulis D., Rousopoulos G., Panagopoulos T., Alexiou C. \nand Papaodysseus C., “On the Automated Recognition of Seriously Distorted Musical Recordings”, IEEE Transactions \non Signal Processing, vol.49, no.4, p.898 -908, April 2001.  \n[5] Haitsma J., Kalker T. and Oostveen J., “Robust Audio \nHashing for Content Identification, Content Based \nMultimedia Indexing 2001, Brescia, It aly, September 2001.  \n[6] Oostveen J., Kalker T. and Haitsma J., “Feature Extraction \nand a Database Strategy for Video Fingerprinting”, 5th \nInternational Conference on Visual Information Systems, \nTaipei, Taiwan, March 2002. Published in: \"Recent advances \nin Vis ual Information Systems\", LNCS 2314, Springer, \nBerlin. pp. 117 -128.  \n[7] Auditude website < http://www.auditude.com > \n[8] Relatable website < http://www.relatable.com > \n[9] Audible  Magic website < http://www.audiblemagic.com > \n[10] Shazam website < http://www.shazamentertainment.com > \n[11] Moodlogic website < http://www.moodlogic.com > \n[12] Yacast website < http://www.yacast.com > \n[13] Philips (audio fingerprinting) website  \n<http://www.research.philips.com/InformationCenter/Global/\nFArticleSummary.asp?lNodeId=927&channel=927&channelI\nd=N927A2568 > \n[14] RIAA -IFPI. Request for information on audio fingerprinting \ntechnologies, 2001.  \n<http://www.ifpi.org/site -content/press/20010615.html >  \n[15]  Napster website < http://www.napster.com > \n[16] ID3Man website  < http://www.id3man.com > \n[17] Winamp website < http://www.winamp.com > \n[18] Logan B., “Mel Frequency Cepstral Coefficients for Music \nModeling”, Proceeding of the Int ernational Symposium on \nMusic Information Retrieval (ISMIR) 2000, Plymouth, USA, \nOctober 2000.  \n[19] Cormen T.H., Leiserson C.H., Rivest R.L., “Introduction To \nAlgorithms”, MIT Press, ISBN 0 -262-53091 -0, 1998"
    },
    {
        "title": "Locating Segments with Drums in Music Signals.",
        "author": [
            "Toni Heittola",
            "Anssi Klapuri"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418137",
        "url": "https://doi.org/10.5281/zenodo.1418137",
        "ee": "https://zenodo.org/records/1418137/files/HeittolaK02.pdf",
        "abstract": "A system is described which segments musical signals according to the presence or absence of drum instruments. Two different yet approximately equally accurate approaches were taken to solve the problem. The first is based on periodicity detection in the amplitude envelopes of the signal at subbands. The band-wise periodicity estimates are aggregated into a summary autocorrelation function, the characteristics of which reveal the drums. The other mechanism applies straightforward acoustic pattern recognition with mel-frequency cepstrum coefficients as features and a Gaussian mixture model classifier. The integrated system achieves 88 % correct segmentation over a database of 28 hours of music from different musical genres. For the both",
        "zenodo_id": 1418137,
        "dblp_key": "conf/ismir/HeittolaK02",
        "keywords": [
            "system",
            "musical signals",
            "drum instruments",
            "periodicity detection",
            "amplitude envelopes",
            "subbands",
            "summary autocorrelation function",
            "acoustic pattern recognition",
            "mel-frequency cepstrum coefficients",
            "Gaussian mixture model classifier"
        ],
        "content": "Locating Segments with Drums in Music Signals \nLocating Segments with Drums in Music Signals\nToni Heittola \nTampere University of Technology \nP.O.Box 553 \nFIN-33101 Tampere, Finland \n+358 3 3115 3788 \ntoni.heittola@tut.fi Anssi Klapuri \nTampere University of Technology \nP.O.Box 553 \nFIN-33101 Tampere, Finland \n+358 3 3115 2124 \nklap@cs.tut.fi\n  \nABSTRACT \nA system is described which segments musical signals according \nto the presence or absence of drum instruments. Two different yet \napproximately equally accurate approaches were taken to solve \nthe problem. The first is based on periodicity detection in the \namplitude envelopes of the signal at subbands. The band-wise \nperiodicity estimates are aggregated into a summary \nautocorrelation function, the characteristics of which reveal the \ndrums. The other mechanism applies straightforward acoustic \npattern recognition with mel-frequency cepstrum coefficients as \nfeatures and a Gaussian mixture model classifier. The integrated \nsystem achieves 88 % correct segmentation over a database of 28 \nhours of music from different musical genres. For the both \nmethods, errors occur for borderline cases with soft percussive-\nlike drum accompaniment, or transient-like instrumentation \nwithout drums. \n1. INTRODUCTION \nThe presence/absence of drum instruments is an important high-\nlevel descriptor for music classification and retrieval. In many \ncases, exactly expressible descriptors are more efficient for \ninformation retrieval than more ambiguous concepts such as \nmusical genre. Information about the drums can also be used in \naudio editing, or in further analysis, e.g. in music transcription, \nmetrical analysis, or rhythm recognition. \nThe aim of this paper is to present a drum detection system, \nwhich would be as generic as possible. The problem of drum \ndetection in music is more difficult than what it seems at a first \nglance. For a major part of techno or rock/pop music, for \nexample, detection is more or less trivial. However, a detection \nsystem designed for these musical genres does not generalize to \nthe others. Music contains a lot of cases that are much more \nambiguous. Drums go easily undetected in jazz/big band music, \nwhere only hihat or cymbals are softly played at the background. \nOn the other hand, erroneous detections may pop up for pieces \nwith acoustic steel-stringed guitar, pizzicato strings, cembalo, or \nstaccato piano accompaniment, to mention some examples. \n2. METHODS \nDrum instruments in Western music typically have a clear \nstochastic noise component and they can be recognized based on \nthat stochastic component [1]. A sinusoids+noise spectrum model \nwas used to extract the stochastic parts of acoustic musical \nsignals, because residual signal has significantly better  “drums-\nvs-other” ratio than the input signal [2].  2.1 Periodicity Detection Approach \nPeriodicity is characteristic for musical rhythms. Drum events \ntypically form a pattern which is repeated and varied over time. \nAs a consequence, the time-varying power spectrum of the signal \nshows clear correlation with a time shift equal to the pattern \nlength in the drum track. We propose that the presence of drums \ncan be detected by measuring this correlation in musical signals. \nThis evaluates an underlying hypothesis that periodicity of \nstochastic signal components is a universally characteristic of \nmusical signals with drums. In order to alleviate the interference \nof other musical instruments, periodicity measurement is \nperformed in the residual signal after preprocessing with a \nsinusoidal model. \nBand energy ratio (BER) feature was used to model signals rough \nspectral energy distribution. BER is defined as the ratio of the \nenergy at a certain frequency band to the total energy [3]. Since \nhuman auditory perception does not operate on a linear frequency \nscale, we apply a filter bank consisting of triangular filters spaced \nuniformly on the mel-scale. At each frequency band, an \nautocorrelation function (ACF) is calculated over the BER values \nwithin a three-second long sliding analysis window, intended to \ncapture a few patterns of even the slowest rhythms. Despite the \npreprocessing, also other instruments cause peaks to the \nbandwise autocorrelation functions. Its effects are minimized \nwith weighting bands differently before forming the summary \nautocorrelation function (SACF). The SACF will be finally mean-\nnormalized to get real peaks step out better from the SACF [4]. \nOverview of whole system is shown in Figure 1..                . \nSummary\nautocorrelationFeature V ectors\nFeature V ectors\nFeature V ectors\nFeature V ectors\nFeature V ectors\nACF\nACF\nACF\nACF\n\nNormalization\nand\nDetectionWeighting\n \nFigure 1. System overview. \nAs can be seen in Figure 2, periodic drum events produce also a \nperiodic SACF. In order to robustly detect this, time-scaled (by \nfactors 2 and 3) versions of SACF are added to the original SACF \nto yield enhanced SACF (ESACF). Thus peaks at integer \nmultiples of a fundamental tempo are used to enhance the peaks \nof a slower tempo. This technique has been adopted from [5].  \nThe region of interest in the ESACF is determined by reasonable \ntempo limits (35 beats per minute and 120 beats per minute). It \nshould be noted that due to the above describe enhancement \nprocedure, these limits actually corresponds to 35 and 360 in Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the \nfirst page.  \n© 2002 IRCAM – Centre Pompidou Locating Segments with Drums in Music Signals \nSACF. This wide tempo range is essential because the rate of \nplaying certain drum instruments is typically an integer multiple \nof tempo, and causes a clear peak in the SACF. Final detection is \ncarried out by measuring the absolute maximum value of ESACF \nwithin the given tempo limits. \nSegment without drumsSwanlake: S panish dance [Classical]\nSegment with drums\n00.511.52\nSegment without drumsEagles - Hotel California [Rock / Pop]\nSegment with drums00.511.52\n02468\n01234lag lag\nlag lag \nFigure 2. Representative SACFs (gray line) and ESACFs \n(black line) (Tempo limits marked in the plots). \n \nDrums are absent\nDrums are present\n1.6 2.7 4.5 7.4 12.2 20.1 33.1 54.6 valuedensity\n \nFigure 3. Unit area normalized feature value distributions.  \n2.2 Acoustic Pattern Recognition Approach \nMotivated by characteristic spectral energy distributions of drum \nsounds, we studied the ability of Mel-frequency cepstral \ncoefficients (MFCC) to indicate the presence of the drums in \nmusical signals. We used 16 coefficients (both static and delta), \ncalculated in 20ms frames with ¼ overlap, as features for a \nclassifier. Two different classifiers were used, one based on \nGaussian Mixture Models (GMM) and k-nearest neighbour \nclassifier (k-NN) with Mahalanobis distance measure.  \n3. SIMULATIONS AND RESULTS \nA database of 397 entire musical pieces from different genres \nwas used to evaluate the two drum detection schemes. “Presence \nof drums” was defined to include the requirement that the drum \nis played in a rhythmic role. An individual piece was randomly \nselected either to the training set or the test set. All detection \nresults are shown in Table 1. \nFor periodicity detection approach, training set was used to \nestimate the distribution of maximum values in ESACF within \nthe given tempo limits (see Figure 3), and to determine a \nthreshold value used in detection. The reason why the \ndistributions of the two classes overlap rather much is that the \nstochastic residual contains harmonic components and beginning \ntransients from other instruments, too, and in some cases these \nshow very much drum-like periodicity. Thus the starting \nhypothesis that periodic stochastic components reveal drum \nevents was still mainly right. More attention should be paid for \nthe preprocessing system in order to make concluding remarks.  \nIn order to perform classification with GMM training sets were \nused to estimate model parameters for the two classes, one model \nfor music with drums and another for music without drums. Performance was slightly better than with the system periodicity \ndetection approach, but performance was not evenly distributed \nwithin different musical styles. Although a high performance is \nobtained for one class (e.g. drums present), the other fails within \nthe individual musical style. In other words, the system starts to \nrecognize the musical style rather than the drums.  \nTable 1. Detection results. Table notation: <detection rate> \n(<rate for segments with drums> / <without drums>) \nGenre¹ Periodicity  GMM ² k-NN ³ \nClassical (27%) 83% (84/ 78) 90% (97/ 39) 83% (88/44) \nElectronic/Dance (7%) 91% (61/ 96) 89% (49/96) 86% (25/96) \nHip Hop/Rap (3%) 87% (70/ 88) 94% (26/ 98) 95% (11/99) \nJazz/Blues (16%) 75% (38/ 79) 74% (58/ 76) 77% (47/80) \nRock/Pop (29%) 83% (82/ 83) 92% (68/ 95) 89% (46/93) \nSoul/RnB/Funk (11%) 78% (80/ 78) 91% (77/ 93) 89% (46/93) \nWorld/Folk (7%) 69% (52/ 92) 68% (48/ 95) 60% (32/95) \nTotal 81% (77/83) 87% (84/88) 83% (71/89) \n¹ Portion of evaluation database is put in brackets. \n² GMM classification with two models. (MFCC + ΔMFCC,  \n  model order 24, three-second test excerpts) \n³ k-NN classification with k=5 and three-second test excerpts. \nSince two drum detection systems are based on different \ninformation, one would thus guess that the combination of the \ntwo systems would perform more reliably than either of them \nalone. But only minor improvement (1-2%) was achieved with \nintegration (Periodicity + GMM). This is due to the fact that both \nof the systems typically misclassify within the same intervals. For \nexample, jazz pieces where drums are played quite softly with \nbrush, or ride cymbal is continually tapped are likely to be \nmisclassified with both systems. In some cases, the \nmisclassification might be acceptable, since the drums are \ndifficult to detect even for a human listener.  \n4. SUMMARY AND CONCLUSIONS \nTwo different drum detection schemes were described and \nevaluated. The obtained results are rather close to each other and, \nsomewhat surprisingly, the combination performs only slightly \nbetter. Achieved segmentation accuracy of the integrated system \nwas 88 % over a database of varying musical genres. In order to \nconstruct a substantially more accurate system, it seems that \nmore complicated sound separation and recognition mechanism \nwould be required. In non-causal applications, longer analysis \nexcerpts and the global context can be used to improve the \nperformance. \n5. REFERENCES \n[1] Fletcher, N. H. and Rossing, T. D., The Physics of Musical \nInstruments. Springer-Verlag, New York, 1991. \n[2] Virtanen, T., Audio signal modeling with sinusoids plus \nnoise. Master’s thesis, Department of Information \nTechnology, Tampere University Of Technology, 2000 \n[3] Peltonen,V., Computational Auditory Scene Recognition. In \nProc. ICASSP, Orlando, Florida, May 2002. \n[4] de Cheveigné, A. and Kawahara, H., YIN a fundamental \nestimator for speech and music. JASA, 2002. \n[5] Tolonen, T. and Karjalainen, M., A computationally \nefficient multipitch analysis model, IEEE Transactions on \nSpeech and Audio Processing, Vol. 8, No. 6, Nov. 2000"
    },
    {
        "title": "Why not MARC?",
        "author": [
            "Harriette Hemmasi"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417491",
        "url": "https://doi.org/10.5281/zenodo.1417491",
        "ee": "https://zenodo.org/records/1417491/files/Hemmasi02.pdf",
        "abstract": "Traditional library cataloging records in the United States, based on AACR2R cataloging rules and MARC standards, constitute a solid foundation for many of the descriptive metadata elements needed for searching and retrieving works of music.  However, there are significant weaknesses associated with these records and the online environment in which they live as users seek access to digitized representations of music.  While music metadata in the library catalog records offer less than a perfect solution, they can and should have an important role in the total solution. Variations2, the Indiana University Digital Music Library, builds on the advantages of AACR2R and MARC and offers a domain- specific data model and search environment that address many of the  identified problems.",
        "zenodo_id": 1417491,
        "dblp_key": "conf/ismir/Hemmasi02",
        "keywords": [
            "AACR2R",
            "MARC standards",
            "descriptive metadata",
            "searching and retrieving",
            "music metadata",
            "domain-specific data model",
            "search environment",
            "problems",
            "Indiana University Digital Music Library",
            "traditional cataloging"
        ],
        "content": "Why Not MARC? \nWhy Not MARC? \nHarriette Hemmasi \nIndiana  Unive rsity \nBlooming ton, IN USA 4740 1 \n+1 812/855-3 403 \nhhemmas i@indiana.edu  \nABST RACT \nTradi tiona l library cataloging re cords  in th e Uni ted States, based \non AACR2R cataloging  rules and MARC sta ndards, constitute  a \nsolid foundation  for man y of the descr iptive metad ata elements \nneeded for s earching and r etrieving works of music.  However, \nthere are s ignificant weakn esses associated with these records  and \nthe on line environment in whi ch the y live as users seek access to \ndigiti zed r eprese ntations of m usic.  W hile m usic metadat a in the  \nlibrar y catalog  records offer  less than a perfect solution,  they can \nand should have an important role in the total solution .  \nVariations2, the Indiana Univer sity Digi tal Music Libr ary, build s \non the advantag es of AACR2R and MARC and offers a domain -\nspecific dat a model and s earch environment that address man y of \nthe  identified p roblem s.     \n                                                              \nFigure 1.   US MARC Bibliographic Record \n1.  MUSIC METADATA IN MARC  \nTradi tiona l libr ary cataloging r ecords in the United Stat es are \nbased on Anglo -American C ataloging Rules (AACR2R) and th e \nstandardi zed M ARC form ats for transm itting  machine r eadable \ndata.  Together , AACR2R and MARC offer a solid foundation f or many of the de scriptiv e metadata el ements that  are need ed b y \nmusic cons umers as they endeavor to discover, identify, and \nretrieve works of music [1] , [2].  Figures 1-3 are ex amples of \ntypical MARC b ibliograph ic and  authority  records  found in  librar y \nonline  catalogs. \n  ARN:   858600 \nFixed fields:                         Rec stat: c      Entered:     19830316  \nCoded descriptive and    Type:     z       U pd status:  a      Enc lvl:   n      Source: \nadministrative elements   Roman:           Ref status :  a      Mod rec:          N ame use: a     \n    Govt ag n:        Auth statu s: a      Subj:      a      Sub j use: a     \n    Series:   n      Auth/ref:    a      Geo subd:  n      Ser use:  b     \n    Ser num:  n      Name:        a      Subdiv tp: n      R ules:    c  \nLC control number                 010     n  82162690   \nCatalog ing agency                  040     D LC ‡c DLC ‡d D LC ‡d CtY-Mus  \nAdministrative  005     20010913114354.0  \nAuthoritative form    100 1   Lalo, Edou ard, ‡d  1823-1892. ‡t S ymphoni e espa gnole  \nAlternate fo rm    400 1   Lalo, Edou ard, ‡d  1823-1892. ‡t Ispanska´i`a simfoni ´i`a  \n     400 1   Lalo, ´Edouard V ictor Antoine, ‡d 1823-1 892. ‡t     \n                                                                S ymphoni e espa gnole, ‡m violin & orchestra, ‡n op. 21   \n                                                               ‡w nnaa  \n     400 1   Lalo, Edou ard, ‡d  1823-1892. ‡t S ymphon y espagnol  \nSource note                            21 (1874))                    670     New Grov e ‡b (S ymphonie esp agnole; vn,  orch., op.  \nFigure 2.  US MARC Name /Title Authority  Record  \n   OCLC:  xxxxxxxx            Rec stat:    c  \nFixed fields:  Entered:    19940309      Replaced:    19950709     Used:     \nCoded descriptive and            20010906 \nAdministrative elements Type:  j     E Lvl:  I     Sr ce:  d    Audn:         Ctrl:      Lang:   \n                                                N/A \n                          BLvl:  m     F orm:        Comp:  mu   AccM:  i      MRec:      \n    Ctry:  nyu \n    Desc:  a     FMus:  n     LTxt:       Dt St:  s      Dates : 1961,      \n \nLC number    010     ‡z  r60-1367 ‡z  r60-1368  \nCatalog ing agency    040     F NP ‡c FNP  \nCoded descriptive eleme nts 007     s ‡b d ‡d b ‡e s ‡f m ‡g  e ‡h n ‡i n ‡m u ‡ n e  \nLabel number    028 02  MS 6201 ‡b Col umbia  \nForm of composition    047     co ‡a s y  \nClassification number     090     M1012 ‡b .W 241 1961  \nHolding  librar y    049     IULA  \nMain entr y      100 1   Walton, William, ‡d 1902-  \nUniform title   240 10  Concerto s, ‡m violin, orchestra  \nTitle proper    245 00  Concerto for viol in and orchestr a / ‡c Walton. S ymphonie  \n         espagnole / Lalo ‡h [ sound recordin g].  \nPublication information 260     [ New York? ] : ‡b Columbia, ‡c [ 1961 ?]  \nPhysical des cription 300     1 sound disc : ‡b analog , 33 1/3 rpm, ster eo. ; ‡c 12 in.  \nDurations   306     002835 ‡a 002645  \nSeries title   490 0   Columbia masterworks  \nPerformers    511 0   Z ino Fran cescatti,  violin ; Phi ladelphia Orc hestra,  \n                 Eugene Orm andy, condu ctor (1st work) ; N ew Yo rk  \n                 Philharmonic, Dimitri M itropoulos, conductor (2n d work).  \nNotes    500     Durations: 28:35; 26:45.  \n    500     Prog ram notes on container.  \nSubject heading    650  0  Concertos (Violin)  \n    650  0  S ymphoni es (Vio lin with orchestra)  \nAdded person al name   700 1   F rancescatti, Z ino, ‡d 1902- ‡4 prf  \n     700 1   Ormand y, Eugene, ‡d 1899-1985. ‡4 cnd  \n     700 1   Mitropoulos, Dim itri, ‡d 1896-1960. ‡4 cnd  \nAdded author/title   700 12  Lalo, Edou ard, ‡d 1823-1892. ‡t S ympho nie espag nole.  \nAdded corpo rate nam e   710 2   Philadelphia Orc hestra. ‡4 prf  \n     710 2   New York Philha rmonic. ‡4 prf  \nAdded title     740 01  S ymphoni e espag nole.  \nURL  856   0  ‡uhttp://purl.dlib .Indian a.edu/iudl/variatio ns/sound/ \n                                                             AB D9455 \n  \nEach field and subfield of the bi bliographic reco rd is defined an d \nfram ed by a strict syntax.  Semantic contro l is imposed on code s \nfound in both the fixed and variab le fields (010, 040, etc.) an d \nstandardi zed fo rms of entr y are used for nam es, titles, ser ies, and \nsubject h eadin gs.  Authori tative lists for na mes, titles, and subjec t \nheadings  are available from  the Library of Congress and other \nauthori zed agen cies [3].  Similar to b ibliogr aphic records , name \nand s ubject authorit y records  also have a pres cribed form at and \nsyntax.   \n \nFigure 3.  US MARC Su bject Authority Record ARN:   2098799 \n Fixed fields:  Rec stat: c      Entered:     19860211 \n Coded descriptive and  Type:     z       Upd status:  a      Enc lvl:   n      Source: \n administrative elements Roman:           Ref status:  a      Mod rec:          N ame use: b     \n    Govt ag n:        Auth statu s: a      Subj:      a      Sub j use: a     \n    Series:   n      Auth/ref:    a      Geo subd:  i      Ser use:  b     \n    Ser num:  n      Name:        n      Subdiv tp: n      R ules:    n  \nLC control number  010     sh 85095326 ‡z  sh 85085902 ‡z  sh 851044 92 ‡z  sh   \n                                                               85104566‡z  sh 85145018   \nCatalog ing agency    040     D LC ‡c DLC ‡d DLC  \nAdministrative   005     20000427093003.0  \nLC class numbe r    053  0  M1000 ‡b M1075  \nAuthoritative form    150     Orchestr al music  \nSee also     360     ‡i heading s for forms and t ypes of music th at include  \n                                      \"orchestra \" and headin gs with medium of perform ance  \n                       that include \" orchestra \"; also ‡a Conce rtos [Solo  \n                       instrument(s)]  ‡i and the headin gs ‡a Overtures,  \n                              S ymphoni c poe ms, ‡i and ‡a S ymphonie s  \nAlternate fo rm    450     Orchestr a music  \nBroader term    550     Instrument al music ‡w g  \nScope note     680     ‡i Here are ent ered compositions not in a s pecific fo rm  \n                               or  of a spe cific type fo r orch estra, and co llections of  \n                               compositions in  several fo rms or t ypes fo r orchestr a.  \nNote                          681     ‡i Note under ‡a W it and h umor, Musical  \n \nAACR2R standards and the MARC bibliograp hic and authority  \nform ats are us ed b y music cataloging agen cies worldwide .  \nMillions of MARC bibliographi c and authorit y records alread y \nexist in  indiv idual library catalogs and in the w orld’s two la rgest \nonline s hared c ataloging s ystems, OCLC’s  WorldCat and RLG’s  \nUnion Catalog [ 4], [5].  When a new item is to be add ed to a \nlibrar y collection, the cataloger  first searches WorldCat , the Unio n Perm ission to make digital or  hard copies of all or  part of this \nwork for per sonal or classr oom use is gr anted without fee  \nprovided that cop ies ar e not made or  distr ibuted fo r profit or \ncommercial advantage and that copies bear this notice a nd the f ull \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pompidou Why Not MARC? \nCatalog , or both to determine if matching b ibliograph ic an d \nauthorit y record s exis t.  If s o, these records  are imported into the  \nlocal s ystem an d updated as ap propr iate.  If no records exist, the \ncataloger creates matching recor ds and, dependin g on the librar y’s \nstatus as a ca taloging agenc y, the cat aloger m ay con tribut e the  \nnew records to one or both of cataloging coop eratives. MARC  \ndata is easily  exchanged among libra ry information s ystems [6].  \nThe com bined AACR2/M ARC standa rds, as eviden ced in the \nrecords contain ed in the indiv idual and coop erative catalog ing \ndatab ases, com prise a pr edictable and r eliable model of r esource \ndescription for  works of music.     \n \nDespite th ese strengths , there are several weaknesses associated \nwith AACR2R/MARC records for digitiz ed represent ations o f \nmusi c. \n \n2.  WEAKNESS ES IN MARC FO R \nDIGITI ZED REP RESENTATIONS OF \nMUSIC  \n2.1  Lack of adequate structural an d \nadministrative me tadata \nAs demonstrated in Figur e 1, the MARC bib liographic r ecord \naccom modates  deta iled des cription of a s tatic physical ar tifact, \nsuch as a sound  recording or score, and can provide a link to th e \ndigiti zed r eprese ntation .  Bu t to adequa tely descr ibe, access, and \nfacilitate the navigation of  digitized r epresen tations  of m usic \nworks, additional structural and  administrativ e metadata is needed . \n \nStructura l metadata is used to a ssist in the display  and nav igation \nof a particular object and incl udes informatio n on the object’s \nintern al stru cture, such  as track descrip tions and time or  pag e \nnumbers.  Stru ctural metadata provides users with navigatio n \ncapab ilities wi thin a give n recording or sco re.  Administrativ e \nmetadata represents management information  for an object, \nincluding  infor mation related to the creation  of the d igital object \n(file form at, eq uipm ent used, date cre ated, et c.) and intellectual \nproperty  rights manage ment inf ormation. \n \nPrecursors of structural and admi nistrative metad ata can b e found \nin the MARC record, and the number of such attributes has \nincreased with the inclusion of electronic resour ce descrip tion in  \nAACR2R/MAR C.  Table of con tents notes, notes about duration , \nand the 856 tag used for the univers al resource locator (URL) are \nexam ples of s tructura l metadata. These f ields d escribe or provid e \naccess to the contents of th e entity being c ataloged, but they do \nnot allow the u ser to adequa tely search and n avigate the s ub-\nsections of  a digitized work.   They keep th e user  at the level of the \nsurrogate rather than lead ing the  user into the resource.   \n \nThe MARC bibliographic record also includ es administrativ e \nmetadata: cop yright date, date th e record was cr eated or updated , \nand notes about  access restric tions and file form at for electron ic \nresources .  While thes e data elements are us eful, they are limited \nin scope, especially  in terms of recording  technical, access right s, \nand preservation  elements.   \n \n2.2  Limits of the conventional online catalog \nOne of th e primary weakn esses associated  with  MARC records  is \nthe environm ent in which  they reside.  Most conv ention al online \ncatalogs under-r epresent music d ata in MARC records at th e poin t \nof sear ching and underu tilize it at the poi nt of retrieval and  \ndisplay .    \n \nThe gener ic sear ch options of online catalogs do not lead users to \nformulate music-specific quer ies.  As in Figur e 4, th e “author ” search includ es composer, perfo rmer (soloist or group), condu ctor, \neditor , and all other contribu tors w ithout the ability  to diff erentiate \namong them  at the point of searching. Sim ilarly, the “titl e” sear ch \ninclud es unifor m title, title on  contain er, alterna te title, ser ies title, \nand all other t itles.  Distinct ions between the  type or role o f \n“author” and t ype or source of “title” are cri tical refin ements for \nsearching  music.  \n \nDistinguishing the ty pes of dates associated with music resources \nis also importan t at th e point of  searching . Date of perform ance, \ndate o f cop yright, date of com position, and d ate of d igitizatio n \ncould b e of particular  interest dur ing th e search  process.   \n \nFrom the options provided on th e advanced screen shown in  \nFigure 4,  only two featur es are specific to m usic:  the limit by \nlibrar y and  the limit by form at.  However , many more options \ncould be made available if the current level o f indexing were \nmaximized or  if a sligh tly deeper level of ind exing were applied .   \n \n \n \n \nFigure 4.  Advanced search  screen of IUCAT,  Indiana \nUniversity onli ne catalog \n \nInitia l and  follo w-up searches could be im prove d by offering the \nuser alte rnative search pa ths.  Many librar y catalogs alread y make \nuse of “lead in” vocabular y found in Librar y of Congress name  \nand subject auth ority  records, an d more could be done b y further  \nexposing the rich vocabular y and syndetic structure contained in  \nthese records .  \n \nThe m anner in which s earch res ults are returned has an impact o n \nthe user’s abilit y to understand what was retriev ed and why  it was \nretrieved .  Libr ary catalogs var y widel y in how th ey display search \nresults and in how m uch the y allow th e user  to custom ize the \nresult set.  Th e “sort results by ” option in Figure 4 empowers the \nuser to sort results b y title, auth or, or date. Oth er useful sorting \noptions might in clude format, edition, uniform title, or  performer.  \nIn the followin g sections of t his paper the author explores  \naddition al probl ems that occur i n the tradit ional online ca talog’ s \ndispla y of retrieval se ts. Why Not MARC? \n2.3  Impervious pre-coor dinated, multi-faceted \nheadings \nThe nested, building-block style of creating uniform titles and \nsubject headings may be efficient for the cataloger but it is often \nimpervious to the searcher [6].  Each subfield building block \ncarries a precise mean ing that is well known by the cataloger but \nthat is often unknown and more importantly, unannounced by the \nsystem to the user.  The sample uniform title:   [‡a Sonatas, ‡m \npiano, ‡n no. 21, op. 54, ‡r C major; ‡o arr.] contains many \nsignificant details a bout the work: ‡a = title of work;  ‡m = \nmedium of performance (instr umentation); ‡n = number of \npart/section of work; ‡r = key for music; ‡o = arranged statement for music.  Most catalogs do not provide separate search options \nfor the title’s sub-parts, and users are left with the risky keyword option and a potential multitude of false hits as they try to extract \nand reconnect the critical elements  of the title. \n \nLibrary of Congress music subject headings represent a complex \nmixture of pre-coordinated, multi -faceted strings that may or may  \nnot contain subfields denoting the individual facets they embrace.  \nMulti-faceted headings without subf ields are usually comprised of \nmusical form and genre terminology accompanied by names of instruments or languages.  Examples incl ude:  Sonatas \n(Saxophone and piano); Accordion music (Jazz);  Folk Songs, \nBengali. Due to the lack of subfielding, it is impossible for a \nlibrary database to distinguish forms from instruments or forms \nfrom languages and offer a search for each separate facet.   \n \nIn other cases, Library of Congress subject headings contain separate subfields that  denote uni que facets: \n \nTopical:  Woodwind instruments ‡x Reeds \nForm:  Jazz ‡v Discographies  \nGeographical:   Composers ‡z Austria \nChronological: Chorale preludes ‡y 17th century \n \nEven in such cases, few library catalogs provide a separate search for each of the facets represented.  \n \nAdded to this tangle is the fact that many headings represent both \ntopic and musical form or genre.  An example is the heading \n“Dance music” which is applied to books about dance music and \nmusical works that are dance music.  Despite the conceptual \ndistinction between topic and form , historically all music subject \nheadings have been coded as  topical (field 650), with the \noccasional inclusion of a geographical heading (field 651).  Over the past several years a specific field for form headings (field 655) \nhas evolved, but the Library of Congress has been cautious in \nseparating topical headings for music from form headings due to \nthe overlap in terminology and the large amount of legacy data.  \nThe resulting fate for subject searching is much the same as that \nfor titles: the keys are locked up with the data, leaving the users \nlocked out. \n \n2.4   Weak relationships between fields \ndescribing separate works \nA fundamental deficiency  surfaces in bibliographic records that  \ndescribe more than one work:  the lack of established relationships \nbetween fields associated with each work.  As demonstrated in \nFigure 1, no structural relationships exist between the two works \nand their performers or conductors; nor are there any relationships \ndrawn between the works and the two time durations or the two \nsubject headings.  Instead, the connection between the performers, conductors, and works is narrated in field 511 and all other connections are implied by the order in which the fields are \nentered in the record (first duration, first 650, first 700 ‡4 cnd \nbelong to first work).    \n \nBecause essential relationships are not established among the key \naccess points (title, performer, subject heading, etc.) within a \nrecord describing more than one work, the user is unable to \neffectively communicate that only those recordings with Eugene \nOrmandy as the conductor of Symphonie espagnole are wanted.  \nLikewise, the online catalog is unable to process this distinction. \nThus the search  <Symphonie espagnole, Ormandy> will \ninvariably retrieve the bibliographi c record in Figure 1, regardless \nof the fact that Ormandy has no connection to this recorded \nperformance of Symphonie espagnole.    \n \n2.5   Insufficient links between versions of a \nwork \nTwo conceptual strengths of AACR2R/MARC that in the case of \nsearching and retrieving multiple versions of a musical work can \nbecome stumbling blocks are: (1) the distinction between \npublications of the complete work versus parts of the work, and \n(2) the distinction between various  editions and formats.  These \ndistinctions drawn by AACR2R/MARC are not the primary issues; rather, it is the lack of linking and the laborious \nmaneuvering among the many versions of a work within the \ndatabase environment that create problems for users.   \n \nUsers may want to see several printed editions of Wagner’s Ring \ndes Nibelungen .  They may want to listen to recordings of \nRheingold  or have a special interest in finding all the recordings of \nthe aria, Abendlich strahlt der Sonne Auge .  For research purposes, \nthey may want to move back and forth between the MARC \nrecords they have retrieved and perhaps also find books written \nabout the opera. \n \nTo obtain a comprehensive view of which versions of the work are \navailable, a user must either  (1) wade through many, unsorted \nentries resulting from a generic all-formats search, or (2) initiate the same search several times, each time with a different format \nlimitation.  Neither is an attractive or efficient option. The MARC \nrecord provides for the distinction between physical formats in the \nfixed field as well as in other of its descriptive fields.  Many \nonline catalogs support the distinc tion between searching scores \nand sound recordings; some cata logs dump both into a single \n“music” search; but few catalogs offer an either/or choice beyond that of searching all formats or searching only one format. \n \nThe MARC record also provides for linking of the multiple \nversions of a work, regardless of its format, edition, or \ncompleteness, through the uniform title. The construction of the \nuniform title  (Ring des Nibelunge n. Rheingold) ensures that all \ninstances of Rheingold  will be retrieved via a keyword or phrase \nsearch <Ring des Nibelungen>.    \n \nHowever, the results of this keyword or phrase search might be \noverwhelming since the retrieval se t would collect records related \nto all of the operas contained in the Ring  cycle.  If the search were \nlimited by the term Rheingold , records for publications of the Ring  \ncycle would not be retrieved un less the cataloger had entered a \ncontents note or an added entry for the individual operas of the \nRing .  Lastly, if the search were further narrowed to include the \naria title, results would almost certainly exclude publications of \nthe complete opera as well as the complete Ring .   \n Why Not MARC? \nThese exclusions occur no t only be cause of the lack of conten ts \nnotes  or add ed entries but also becaus e ther e is no structura l link \nbetween a work and its parts, de spite the semantic link embedded \nin the uniform  title. Othe r than the rep etition of the prim ary \nnode(s) of the uniform  title, there is no supporting s yndetic \nstructure to connect th e various  parts .  Each part  is catalog ed as  a \nseparate unit --  as if it were a separate wo rk -- creating a \nrelationship with  its own specific extended portio n of the uniform \ntitle rather than the whole . \n \nWhile th is appr oach supports the necessar y distinctions between \npublished par ts or arrang ements of a work, it m ay not adequately  \nfacilitate their collocation in a search res ult.  Becaus e of these \npract ices, most librar y catalogs have diff iculty in r eturning \nmeaningful retr ieval sets tha t link multiple versi ons of a work and \nin providing  direct nav igation among these man y different \nversions.   \n \n3.  ADVANTAGES OF  VARI ATIONS2  \nThe Varia tions2  project a ims to establish a digi tal m usic librar y \ntestbed s ystem containing music in a variety  of fo rmats, involving  \nresearch  and d evelopm ent in  the areas of s ystem arch itecture, \nmetadata stand ards, componen t-based applicati on architecture, \nand network ser vices.  Th is system will be used  as a fo undat ion for digit al librar y research in the areas of ins truction , usability, \nhuman-computer interaction , and intellectu al prop erty rights. \n \nVariations2 builds on the con cepts dev eloped  in the o riginal \nVariations project [7] . The or iginal Var iation s makes use o f \nstandard MARC catalog ing recor ds for sound recordings or scores \nand adds an 85 6 field for the URL (see Figur e 1).   This UR L \npoints to an inte rmediar y navigational pag e that allows the user t o \nselect th e preferr ed poin t of entry into the dig itized resource.While \nthis approa ch a llows acc ess to and navig ation of the digitized \nrepresentation , it does not addres s the m any remaining weakn esses \nof the MARC record and its lib rary catalog environment.  \nVariat ions2 provides a solution for each of the problem s outlin ed \nabove, beg inning with the estab lishment of a new da ta model.                                   Ring des Nibelungen     \u0014 bib records for complete Ring \n \n                              Ring des Nibelungen. Rheingold   \u0014  bib records for Rheingold \n \n       Ring des Nibelungen. Rheingold. Abendlich strahlt…    \u0014   bib records for Rheingold aria  \n \nRing des Nibelungen. Rheingold. Abendlich strahlt…; arr.   \u0014    bi b records for arrangements of Rheingold aria \n \nFigure 5.  Relationships Between Uniform Titles and Bibliographic Records \n \n3.1   Work-centered da ta model \nThe Var iations2 data model is sim ilar to  the sche me develop ed by \nthe International Federation of Li brary Associations (IFLA) Study  \nGroup on the Function al Requirements for Biblio graphic Records  \n[8].   \n \nWhereas  both th e Variat ions2 and IFLA m odels  are ent ity-based \nwith a focus on establishing appropriate links between indiv idual \nworks  and their  associated pro perties, the Var iations 2 des ign is \n \n \nFigu re 6.  Indiana University Variations Project: Intermediary Navigation Page \n Why Not MARC? \nspecific to th e music dom ain and is developed  in tandem with  a \nsupporting search interfa ce. While the primar y goal of Variations2 \nis the descrip tion, repr esentation , and use of  digital r epresen tation s \nof music, the basic organizatio nal princ iples a pply to all m usic \nresources, reg ardless of format. \n \nThe ov erriding  organization al principles in  AACR2R/MARC are \nthe description of and distinction between indiv idual bibliogr aphic \ncarriers rather t han works , themselves.  Thes e principl es beco me \napparen t when the c arrier cont ains more than  one work, as  in \nFigure 1.  Figur e 1 d isplay s a bibliograph ic record containing  2 \nworks, 5 perfor mers, 2 duration s, 2 subject headings, and 1 UR L  \n– none of which bears an y logical relationship to the other.  Th e \nonly existing clear-cut relationship is that all p arts belong to one \ncontainer.  \n \n \n \n \nFigure 7.  Container-center ed MARC reco rd (reflecting  2 \nwork s in 1 container) \nBy contrast, Var iations2 establis hes precise r elationships between \neach work and  its associated pro perties, as evidenced in Figure 8.  \n \nWith this m odel, the stru ctura l organiza tion of  data shifts from \ncontainer-center ed to work-centere d.  While th e physical container  \nis still th e fundam ental bind ing f or the two works and their relat ed \nentities, the co ntain er represents a single po int of connectio n rather than the o nly connection  among the man y parts.  The work  \npredom inates  all other elem ents; they are tied to i t and are pres ent \nonly to d escribe and distin guish one particular instan ce \n(performance, p ublication) o f the work from another .  Even  \nthough this same concep t of work-attr ibute relationship is true for \nAACR2R/MAR C, the stru cture of MARC does not support the \nconcep t. \n \n3.2 Variatio n2 record types: their relationship s \nand attributes \nIn Variat ions2 each record  type, or entity – contributor , work , \ninstantiation , contain er, media o bject  – is uniquely  describ ed and \nidentif ied and is  explic itly related to the other e ntities. Instead o f \nthe single-dimensional MARC record comprised of disparate \nfields with link ed (or un linked) author ity records, Variations2 \npresents a m ulti-dim ensiona l reco rd struc ture with distinct ly \nrelated attributes. The  work rep resents the abs tract conc ept o f a \nmusical composition or set of compositions.  The instan tiation \nrepres ents a manifes tation of a work as  a perform ance or a s core.  \nThe container  represents the physical item or set of item(s) on \nwhich one or more instantiation s of works can be found, e.g., a \nCD or published score.  The medi a object repr esents the digitized \nrepresentation  of music, such  as a sound file or score image.  \nContributors rep resent peop le or groups that contribute to a work , \ninstanti ation , or conta iner. \n \nEach entity is assigned approp riate des criptive, structu ral, and  \nadministrative metada ta [9] . The main purpo se of descriptiv e \nmetadat a is to a ssist users in the iden tification of various m usic \nrepresentation o bjects through search ing and bro wsing.  Structural \nmetadata is used to organize the entities into log ical and ph ysical \ndivisions, allowing for the display  and nav igation of the intern al \nstructure  of m usic repr esentations.  Admi nistrative me tadata \naccom modates  various  manage rial con cerns  regarding r ecord \ncreation and maintenan ce, proper ties of the d igital objects, access \nand intellectual property  rights. To a great exten t the ini tial \nselection of metadata properties for Variations 2 was based in part \non those elements found in th e MARC recor d.  As much as \npossible, MARC data is mapped  from corresponding bibliographic \nand authority  records into the Variations2 databas e. With the work \nat its forefron t, Variations2 facil itates linking dif ferent parts of a \nwork and m ultiple instant iations of a work in wa ys that MARC is \nunable  to do wit hin the traditional online catalog.   \n \n \nFigu re 8.  W ork-centered Variations2 Data Model (reflecting 2 works in 1 container) \n Why Not MARC? \n3.4 Future of a domain -specific environment \n Variat ions2, th e Indiana Uni versity Digit al Music Librar y, \npresents a collection of digital r epresentations  of m usical works , a \ndata model with  meaningful rela tionships established between th e \nkey ent ities of a m usical work, des criptive, structural, an d \nadministrative elem ents relating to each o f thos e entit ies, \nexpanded vo cabulary control of the ind ividua l elements, and a set \nof search op tions that ar e specific to th e music dom ain.  The \nresults are an in tegration of infor mation and a m eans of ac cess that \nwere n ever b efore possible.   \n \nTo prove the v alue of  thes e results, additional user testing is \nneeded and also a reckoning with  the practicality  of providing  \nresearch- level access to digit ized repr esentations  of m usic.  \nAdditionally , the value must ex tend b eyond th e confin es of th is \nproject.  Will it  be possible to provide on-goin g support for th e \nrich bu t complex environm ent created by Variations2?  Will there \nbe acc eptan ce by others  of the precep ts establis hed b y \nVariat ions2?  The music librar y com munity has yet to embrace  \nany form of mu sic metadata outside the MARC  record, and the \ngeneral public continue s searching the w eb with out an y regard  for \nmetadata.     \nFigure 9.  Com parison of MA RC and V2 dat a models  Can Variations2 metadata  be distributed in a simpler form, such as \nDublin Core , and not lose  its in tegrity  and usefu lness?  Numerous \ngroups and agen cies ar e creating  metadat a for d igitized works of \nmusic; m any are using the s ame elem ents and going throug h \nsimila r proce sses of ide ntifica tion and au thorization .  Can we \ncreate music metadata on ce and use it many times? While music \nmetadat a in MARC records offers less than a perfect solution , it \ncan and should have an important role in the total solution.  Many \nmusic recording s and scores have and will be cat aloged in th e \nMARC form at.  Standardi zed n ame and t itle entries have and will \nbe created for m ajor composers, performers, and compositions.  If  \nthis rich set of m etadat a were made eas ily and  openl y available, \nwe must ask: wh y not MARC –  plus so much more?   \nSuch linking r esults not only  from the stru ctural metadata \nelem ents, them selves, but als o from  the m ore inclus ive m edia \nobject-to-instantiation- to-work chai n.  In  Variations2 structural \nmetadat a may be recorded in  several loc ations with differing \nlevels of specificit y: (1) in th e conta iner stru cture with tr ack \ndescriptions, time , page, or measure numbe rs to be use d for \nnavigation; (2) in the work struct ure with  movem ents, acts, scenes, \nsections describing the work, itsel f, and (3) in the work bindings \nthat are associated with  each instantia tion.  These bindings  tie \nparticular  time o r page r anges with in the media objects of a g iven \ninstantiation  to the abstract stru cture of th e corresponding wor k \nand may  be used for the s ynchronization of scores and sound \nrecordings.   \n4.  ACKNOWLEDGE MENTS \nI would lik e to acknowledg e th e con tributions of Natalia \nMinibay eva for her help in creating the figur es th at are part of this \npublication .  Acknowledgements  also  go to Natalia Minib ayeva, \nMary Wallace D avidson, Jon Du nn, Rob Pendl eton, Jim  Hallid ay, \nand Matt Jadud of Indian a Univ ersity  for their contributions to the \ndevelopment of  the Variations 2 da ta m odel.  This m aterial is \nbased upon wor k supported b y the Nation al Science Foundation  \nunder Grant No. 9909068.    \n3.3 Vocabulary enhancements  \nNot only  has th e range and number of  descriptive, structu ral, and \nadministrative metadata increased in Variatio ns2 over what is \navailable in the MARC record, but als o vocabulary tracking  has \nsignificantly  increased.  The impr oved tracking  of vocabular y is \nthe r esult of  two chang es: (1)  the deconstructio n of some of th e \npre-coordin ated, multi-faceted  strings into single-concep t \nproperties; and ( 2) the app licatio n of vocabular y control on more \nmetadat a elem ents.    \n5. REFE RENCES \n[1]   Anglo-American cata loguing rules, seco nd edition , 199 8 \nrevision.  Prep ared under the direction of the Joint Steering \nCom mittee for Revision of AACR, a com mittee of the Am erican \nLibrar y As sociation.  Chi cago : Am erican Libr ary As sociation , \n1999-  \nWhereas in the  MARC record several fie lds conta in multiple \nconcep ts (e.g., uniform  titles, subject headi ngs, publicatio n \ninform ation) th at are tied toge ther ei ther throu gh free-t ext no te \nfields or controlled subfields, Variations2 break s out cer tain k ey \nconcep ts into dis crete elem ents.  Exam ples of the s eparat ed \nattribu tes in clude key  and lang uage of a musical work, d ates \n(birth/d eath) of  contributors , instrumentation , form or genre, \npublisher, place of publication, and  date  of publicatio n. \nVocabular y control is  imposed on ea ch of  these properti es with  the \ngoal of improving data accur acy and consistency  both at the po int \nof dat a entry and at the poin t of search ing.    \n[2]   Libr ary of Congress. MARC Standards w eb site. \nhttp://www. loc.gov/marc/  \n \n[3]  Librar y of Congress.  Prog ram for Cooperativ e Cataloging  \nweb site ( including informatio n about the n ame and subject \nauthorit y files). http://www. loc.gov/catdir/pcc/  \n  [4]   Online Co mputer Libr ary Center  (OCLC) web site. In subsequent versions of Variat ions2, vocabular y tracking will b e  \nenhanced further through the impl ementation  of an end-user \nthesaurus that will focus on m usic subject d escriptors but t hat \ncould also include works  of music and contribu tor names. http://www. oclc.org/home/  \n \n[5]   Research Libraries Group  (RLG) web site.  \nhttp://www. rlg.org/toc. html Why Not MARC? \n[6]   Uniform titles are created by catalogers according to fixed \nrules as outlined in AACR2R and ar e used as a collocating device \nfor a musical work that may be published with varying titles. For example:  “Jupiter,” “Symphony no. 41 in C Major,”  \n“Symphonie, Nr. 41, C-Dur, KV 551” are various publication \ntitles used for the same Mozart symphony.  Application of the \nuniform title, Symphonies, K.551, C major, to each cataloging \nrecord ensures that searchers will re trieve all versions of the work, \nregardless of the indivi dual publicat ion titles. \n \n[7]   Jon W. Dunn and Constan ce A. Mayer, “VARIATIONS: A \nDigital Music Library System at Indiana University ,” Proceedings of the Fourth ACM Confer ence on Digital Libraries .  Berkeley, \nCA. August 1999, pp. 12-19. \nhttp://www.dlib.indiana.edu/va riations/VARIATIONS-DL99.pdf  \n \n[8]  IFLA Study Group on the Functional Requirements for Bibliographic Records.  Functional Requirements for \nBibliographic Records.  K.G. Saur, Munich, 1998.  \nhttp://www.ifla.org/VII/s13/frbr/frbr.pdf  \n \n[9]  Indiana University. Variations2. Digital Music Library web \nsite. Metadata documents. \nhttp://dml.indiana.edu/metadata/index.html"
    },
    {
        "title": "Interactive Music Summarization based on GTTM.",
        "author": [
            "Keiji Hirata 0001",
            "Shu Matsuda"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417481",
        "url": "https://doi.org/10.5281/zenodo.1417481",
        "ee": "https://zenodo.org/records/1417481/files/HirataM02.pdf",
        "abstract": "This paper presents a music summarization system called “Papipuun” that we are developing.  Papipuun performs quick listening in a manner similar to a stylus skipping on a scratched record, but the skipping occurs correctly at punctuations of musical phrases, not arbitrarily.  First, we developed a method for representing polyphony based on time-span reduction in the generative theory of tonal music (GTTM) and the deductive object-oriented database (DOOD).  The operation, least upper bound, plays an important role in similarity checking of polyphonies represented in our method.  Next, in a preprocessing phase, a user analyzes a set piece by the time-span reduction, using a dedicated tool called TS-Editor.  For the real-time phase, the user interacts with the main system, Summarizer, to perform music summarization.  Summarizer discovers a piece structure by means of similarity checking.  When the user identifies the fragments to be skipped, Summarizer deletes them and concatenates the rest.  Papipuun can produce a music summarization of good quality, reflecting the atmosphere of an entire piece through interaction with the user.",
        "zenodo_id": 1417481,
        "dblp_key": "conf/ismir/HirataM02",
        "keywords": [
            "Music summarization system",
            "Papipuun",
            "Polyphony representation",
            "Time-span reduction",
            "Generative theory of tonal music",
            "Deductive object-oriented database",
            "Least upper bound",
            "User interaction",
            "Piece structure discovery",
            "Fragment skipping"
        ],
        "content": "Interactive Music Summarization based on GTTM \nInteractive Music Summarization based on GTTM\nKeiji Hirata \nNTT Communication Science Laboratories \n3-1 Morinosato Wakamiya, Atsugi-shi \nKanagawa 243-0198, Japan \n+81-46-240-3658 \nhirata@brl.ntt.co.jp Shu Matsuda \nDigital Art Creation \n2-9-1-506 Fuchucho, Fuchu-shi \nTokyo 183-0055, Japan \n+81-42-361-2427 \nshu@dacreation.com \nABSTRACT \nThis paper presents a music summarization system called  \n“Papipuun” that we are devel oping.  Papipuun performs quick \nlistening in a manner similar to a stylus skipping on a scratched \nrecord, but the skipping occurs correctly at punctuations of musical phrases, not arbitrarily.  First, we developed a method for representing polyphony based on time-span reduction in the generative theory of tonal music (GTTM) and the deductive object-oriented database (DOOD) .  The operation, least upper \nbound, plays an important role in similarity checking of polyphonies represented in our method.  Next, in a preprocessing phase, a user analyzes a set piece by the time-span reduction, using a dedicated tool called TS-E ditor.  For the real-time phase, \nthe user interacts with the main system, Summarizer, to perform music summarization.  Summarizer discovers a piece structure by means of similarity checking.  When the user identifies the fragments to be skipped, Summarizer deletes them and \nconcatenates the rest.  Papipuun can produce a music summarization of good quality, reflecting the atmosphere of an entire piece through interaction with the user.   \n1. INTRODUCTION \nMusic summarization is a significant and attractive task because it potentially can reconstruct the architectures of music systems and, in addition, open up many new a pplications.  Let us consider \nstandard musical tasks, such as composition, arrangement and \nperformance.  Many conventiona l music systems have been \ndeveloped for these tasks.  Since these tasks are coarse-grained and highlevel, non-experts in music likely regard them as a black box full of profound unknown experience, skill, knowledge, and talent.  However, we believe that  these tasks can be decomposed \nto finer-grained lower-level tasks, such as music summarization and music information retrieval.   If a music system provides a \nnon-expert user with middle- to low-level tasks, such as summarization and retrieval, the user may be able to just combine them to design a high-level task without having to take musical \ntrifles into account.  We think that introducing middleware-level tasks will make it easier for a non-experts in music to use a music system.  Such a framework will therefore facilitate the tendency toward making musical technologies  more oriented to non-expert \nend-users.   \nRecently, Internet and Web technologies have become widespread, \nand various kinds of knowhow and information have been distributed and accumulated.  The combination of middleware-level tasks, such as music su mmarization and retrieval, with \nInternet/Web technologies will ther efore lead to new applications, \nsuch as intelligent karaoke, ringing tones of mobilephone, and interactive musical art/business on Internet.  Some are potentially killer applications.   \nThe task of music summarization is to find the most distinctive or \nrepresentative musical excerpts (automatically).  There have been a few approaches to music summarization.  They can be split into audio-signal and symbolic approach es.  The former includes that \nby Logan and Chu [8], who asserted  that the most interesting and \nmemorable part of the song, calle d a key phrase, is that which \noccurs most frequently.  However, this is not always true because a key phrase usually depends on a us er’s preference and taste.  In \naddition, Logan and Chu’s method cannot generate a \nsummarization reflecting the atmosphere of an entire piece.  The symbolic approach is focused on a symbolic representation of \nmusic, which corresponds to the desc ription level of a score, SMF, \nand so on.  We think that the symbolic approach facilitates the use of music theory, more than the audio-signal approach.  Huron’s approach [5] is a symbolic one, though the details are not described in his paper unfortunately.   \nWe aim at achieving music summarization that reflects the \natmosphere of an entire piece with a symbolic approach.  For this purpose, it is important to c onsider that an intelligent \nsummarization requires musical know ledge and skills provided by \nmusic theory to analyze a complicated structure of a piece.   \nThis paper presents a prototype system that we are developing \nnamed “Papipuun”.  Papipuun perform s quick listening by stylus \nskipping, where the skipping occurs correctly at punctuations of musical phrases, not at arbitrary positions.  The current version of \nPapipuun can accept just piano pieces, such as “Turkish March” by Mozart or “Let It Be” by the Beatles, arranged for solo piano (i.e. the format 0 SMF of a single instrument).  We think that this restriction is a good starting point for future enhancement because it highlights the essential issues to be solved, which are music representation of polyphony, discovery of a piece’s structure by similarity checking, identification of fragments to be skipped, and concatenation of selected fragments.   \nThis paper is organized as follows.  We introduce a method of \nrepresenting polyphony in Secti on 2, present the similarity \nchecking technique based on ope ration, least upper bound, in \nSection 3, and propose a method of interactive music summarization and descri be Papipuun in Section 4.  Lastly, we \nmake concluding remarks and mention future work in Section 5.   \n2. MUSIC REPRESENTATION \nThe purpose of designing our music representation method is to \ndescribe the relationships betw een polyphonies with respect to \ntime-span reduction and to use time-span reduction for similarity checking of polyphonies.  We have designed a data structure for representing polyphony based on time-span reduction in the generative theory of tonal music (GTTM) [7] and the deductive object-oriented database (DOOD)\n 1 [11, 6].  A reharmonizer [2] \nand a performance rendering system [4] were also developed in \nthe framework of GTTM and DOOD.   Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncommercial advantage and that copies bear this notice and the full \ncitation on the first page.  \n© 2002 IRCAM – Centre Pompidou  Interactive Music Summarization based on GTTM \n2.1 Brief Introduction to GTTM and DOOD \nLerdahl and Jackendoff (1983) pr oposed GTTM as a theory for \nformalizing the listener’s intuition.   GTTM provides concepts and \nprocedures for analyzing and in terpreting Western tonal music \nwritten in common music notation, namely a score.  A score is just a surface structure of music, and GTTM derives from the surface structure the score’s hidden hierarchical structures, which make up the underlying deep structures.  Note that the music treated by GTTM is limited to hom ophony.  It is said that GTTM \nis modeled on Chomsky’s transformational grammar since it employs the framework of the surface and deep structures.  Among the many theories proposed for analyzing music, we consider GTTM the most appropriate for computer implementation since it is the most formally constructed one, although even more efforts at formalizing GTTM are needed before we can write a working program.   \nGTTM comprises a grouping structure analysis, a metrical \nstructure analysis, time-span reduction, and prolongational reduction.  The grouping structure analysis segments a homophony into shorter groups.  It seems that when we sing a long melody, we have to find the proper points for drawing a breath.  The metrical structure analysis identifies a series of tacti and beats of a homophony and the positions of strong (or weak) beats at the levels of a quarter note, half note, a measure, and so on.  It seems that a conductor moves a baton to the music played or a listener keeps time by hand cla pping.  In particular, time-span \nreduction represents the intuitive idea that if we remove grace \nnotes from a long melody, we obt ain a simple similar sounding \nmelody.  An entire piece of music can eventually be reduced to a key note or a tonic triad.  The time-span reduction is performed \nbased on the results of the grouping structure and metrical structure analyses in a bottom-up manner in the sense that parts come together to form a whole.  We do not describe prolongational reduction here because of space limitation.   \nDOOD is a knowledge representation method with a theoretical \nfoundation and is thus tractable.  DOOD is an extension of the first order logic in that it can represent the lack of attributes and type declaration [11].  From the knowledge description point of view, DOOD has almost the same functions as the feature structure [1].  Plaza (1995) claimed that the feature structure is suitable for describing a case (inc luding melody).  A prominent \nfeature of DOOD is that a deductive rule for DOOD terms to formally define any relation is available; this yields descriptive efficiency, accuracy, and expandability.   \nThe DOOD framework is motivated by the introspection that \nthings in the real world can be represented as the combination of a basic (atomic) object and a set of the attributes.  Hereafter, we identify an object term with an object itself.  We write an object \nterm as o(\n…, l : v, …), where o is an atomic symbol to stand for a \nbasic object, l : v an attribute, l an attribute name (label), and v an \nattribute value.  The most fundament al relation in the real world is \nthe “is_a” relation, and it is mode led as the subsumption relation \ndefined by the deductive rule in the DOOD framework.  That is, \nthe subsumption relation (written as ⊆) represents the relation ”a \nmore informative object ⊆ a less informative object”.  In other \nwords, it represents “an instantiated object ⊆ an abstract object” \nor “a special object ⊆ a generic object”.   \nWe assert that the counterpart of the subsumption relation in \nGTTM is the time-span reduction [3].  We think that this \ncorrespondence is the most natural.   \n 2.2 Abstracting and Instantiating Melody \nusing Time-Span Reduction \nA time-span tree in GTTM is a bina ry tree.  In this article, we \nrefer to an important branch as pr imary and the other as secondary.  \nThe left-hand side of Figure 1 de picts a simple melody and its \ntime span tree.  The time span (designated as            ) covered by a primary and secondary branches is represented by a single note, \ncalled a head, which is here designated as “C4”.   \n              Our representation method can repr esent such a time-span tree and \nthe temporal information of every note of a melody, although the temporal information is not explicitly showed here.  The temporal information is needed in order to perform the time-span reduction correctly and includes the tem poral relationships between a \nrelevant note and its surrounding notes as well as the absolute onset timing and the duration of each note.   \nThe melody on the right-hand side of Figure 1, consisting of only \nthe C4 note, can be considered more abstract than that on the left-hand side from a time-span reducti on point of view.  In other \nwords, the melody on the left-hand side is more instantiated than that on the right-hand side.  This  abstraction-instantiation relation \nof the melodies is regarded as a kind of the subsumption relation \n(the partial order) and is thus written “ ⊆”.  Note that when we \ndescribe these two melodies using our music representation \nmethod, it can be mechanically derived that this subsumption \nrelation holds by looking at the actual data structures of the melodies.  In a strict interpretation of the time-span reduction in GTTM, the right-hand side should be a half note of C4. However, giving priority to mechanization, we determine that the notes remaining after the time-span reduction have the same onset \ntimings and durations as before.   \nThe shape of a time-span tree, th e head values, and the temporal \nstructure depend on the interpretation (prior analysis) of a melody.  Once an interpretation is given a nd it is represented as a data \nstructure in our method, the data structure conforms to the subsumption relation in DOOD.  If a different interpretation of a \nmelody is supposed, then the shap es of the time-span tree, the \nhead values, and the temporal information are different, and \ndifferent subsumption relations hold.   \nLet us show another example of Alberti bass in Figure 2.   \nReference [7] lists four ways of generating a head value. In Figure 2, the head values are generated by “fusion”, while “ordinary reduction” in Figure 1.  The way of generating a head value also depends on the interpretation of a melody given for analysis.   \n _primary secondary \nFigure 1. Subsumption relation of melodies.headC4 C4\nInstantiating  Abstracting& qq q ⊆ _ Interactive Music Summarization based on GTTM \n \n        Since our representation method pr eserves the onset time and the \nduration of a relevant note upon tim e-span reduction, when a less \nimportant note is removed, the tim e span occupied by the note is \nsubstituted with an interval wit hout a note.  As a result, the \ninterval can be regarded as a rest  with the duration of the removed \nnote.  Thus, the rest marks in th e figure are put in parentheses.  \nThey may correspond to the rests in SMF.   \nHereafter, for space efficiency, we omit the G clef.   \n2.3 Reduction to Ordering Information \nThe temporal structure of our representation method was \nconceptualized by straightforwardly intuiting that abstracting the difference between onset times  should produce the ordering \ninformation between them, not absolute timings.   \nFigure 3 shows examples where th e two melodies have the same \npitch sequence but different onset timings and durations.   \n                  Upon listening to these two melodies, one may recognize \nsomething common to them: they consist of the same pitches, whose orderings are identical, i.e. “C4 F4 G4 C5” in the figure.  This observation implies that reducing a melody should yield a note sequence consisting of the same pitches with just the ordering information preserved.  No te that the original melodies \nand the abstract note sequence (temporally reduced melody) share the same time-span tree.  Moreover, our method can represent a melody that is partly reduced to the ordering and determine the subsumption relation between it and some other abstract or \ninstantiated melody as well.   Since the subsumption relation in Figure 3 holds in our \nrepresentation method, we think that our method succeeds in formalizing the tacit assumption for the temporal aspect of the time-span reduction.   \n2.4 Representing Polyphony \nThe scope of GTTM is limited to  just homophony for theoretical \nreasons.  But for high applicability, we think that a practical music system should treat polyphony, taking into account its deep structures.  Thus, we extend the time-span reduction so that it can treat polyphony.  Basically, polyphony means a texture achieved by the interweaving of several melodic lines that are independent but work together harmonically.  For our purpose, we need the following more formal definition of polyphony.  First, a homophony is defined as a melody superimposed by subordinate \nmelodies and interpreted as a si ngle melody temporally.  Next, a \npolyphony is defined inductively.  A homophony is a polyphony, and a node of a time-span tree c onsisting of two polyphonies is a \npolyphony, where these two polyphonies may be temporally overlapped.  That is, the principle of our method is to regard two possibly overlapped polyphonies just  as two branches of a time-\nspan tree’s node by coercively making the order between them with respect to the time-span reduction.   \nFigure 4 shows an example of polyphony and our extended time-\nspan reduction.   \n            On the left-hand side of the figure, each polygon of dotted lines \nshows a homophony obtained by decomposing the original \npolyphony into components until they become homophonies.  On the right-hand side, only the heads are left with the same onset timings and durations.  Note that the decomposition into \nhomophonies and the time-span tree in the figure is one way of \nanalyzing the sample polyphony.  If a different analysis result is given, another decomposition and tim e-span tree are constructed.   \n3. SIMILARITY CHECKING \nWe use the subsumption relation to construct an algorithm for \nchecking the similarity between two polyphonies, where the core \noperation is least upper bound ( lub).  Intuitively, the operation lub \ncalculates the largest common part  of two given melodies.  We \nusually define operation lub(x, y) as min({z | x ⊆ z ∧  y ⊆ z}).  For \nall x and y, x ⊆ lub(x,y) and y ⊆ lub(x,y) .  The mathematical \nmeaning of lub is well known.  If two completely different \nmelodies that share no common part are given, the calculation result is a vacancy, which means the most abstract, least \ninformative melody, denoted as \nT (called top).  Since T \ncorresponds to a vacancy, we can not always listen to the calculation result of lub.  For instance, if a note in the calculation \nresult of lub has timing and duration sufficiently instantiated yet \nT ( ){C4,G4} \nŒ ( ) ⊆ \nFigure 2. Subsumption relations of Alberti bassqq q qŒ \nq q q & ⊆ \nq( )qŒ{E4,G4} {C4} {E4,G4} {C4,E4,G4} {C4,E4,G4} {C4,E4}\n⊆ q q q q q q\nQ H Qh\nFigure 4. Time-span reduction of polyphony. q q ∑ ∑∑ Q . . \n∑ ∑ Œ Ó\ne _ q ⊆ q e _ e \nq q q \nFigure 3. Reduction of diffe rent melodies of same\npitch sequences to identical pitch ordering. ⊆C4  F4  G4 C5 \nC4  F4  G4 C5 Interactive Music Summarization based on GTTM \nfor pitch, the note cannot be translated to an actual note on a score \nbecause its pitch is not determined.   \nIn our method, the similarity ch ecking between two melodies is \nbased on lub.  Since calculating lub of two melodies results in \ntheir largest common part, the larg er the common part is, the more \nsimilar to each other the two melodies are.  Plaza [9] claimed that the feature structure [1] is suitable for describing cases and also used lub for finding cases most similar to a query, although the \nconcept of Plaza’s representation method for music is essentially different from ours.   \n3.1 Similarity based on lub \nFigure 5 shows an example of calculating lub of two simple \nmelodies, C4 G4 and C4.  The time-span trees of the melodies are shown, whereas their temporal structures are omitted for simplicity.  For this example, si nce subsumption relation “melody \nC4 G4 ⊆ melody C4” holds, the result is melody C4.  \n \n \n    \n \n   Figure 6 shows examples of more complicated melodies.  The \nmiddle notes of the two melodies are D4 and F4.   \n          Since the notes do not match each other in terms of pitch and the \ntime-span tree (accordingly, temporal structure, as well), the resulting melody does not include the middle note.   \nFigure 7 shows the same sample me lodies as in Figure 3, where \nthe two given melodies having the same pitch sequence yet \ndifferent durations.   \n         The result of lub of the two melodies is that notes C4, F4, and G4, \nwith their durations unknown, form a sequence in this order that \nends with a quarter note of C5.  That is, the resulting abstract melody is the most informative one that can be obtained from the two input melodies.  Notes C4, F4, and G4 are considered incomplete in the sense that their pitches are determined but their durations are not.   \nLet us suppose that two melodies, D4 C4 and C4 G4, both have \nnote C4 at the origin time (Figure 8).  Due to the temporal structure of our method, our met hod can align the input melodies \nand then perform lub.  In Figure 8, the alignment is shown by the \nbar lines immediately followed by C4.   \n         The temporal structure introduced enables us to treat a melody \nstarting at auftakt.   \nIn contrast, Figure 9 shows the result of lub of the same two \nmelodies as in the previous example, D4 C4 and C4 G4.  Since the \nnotes at the origin time are different however, the result is \nT.   \n         All examples above treat only monophonies for ease of \nexplanation, but the lub of our method can treat polyphony as well.   \n3.2 Similarity Measures \nIntuitively speaking, since lub calculates the largest common part \nof two input polyphonies, if the calculation result is equal to either of the two polyphonies, there is no loss of information by the calculation of lub, and the two polyphonies are considered \nidentical (most similar).  If the calculation result is, in contrast, T, \nthere is no common part, and they are considered unrelated (least similar).  Thus, our concept for measuring similarity between two \npolyphonies is to measure how  much information in the lub \ncalculation is lacking from the two polyphonies.   \nThe lacking information is concerned with the time-span tree and \nthe temporal structure, which correspond to the aspects of music formalized by our method.  To e xpress similarity parameters, we \nhere introduce mathematical notations.  Let P be a polyphony.  \nThen, |P|\nN means the number of notes in P, |P|A the total number \nof attributes of all note objects in P, and | P|T the total number of \nattributes of all timing objects in P.  Since a single note object has \ntwo attributes, pitch/duration and timing, | P|A = |P|N * 2 for well-\nformed P. Similarly, since a single timing object has four Figure 5. Simple example of lub. qq q qlub(                    ,                     ) =  \nFigure 6. Largest common part by lub. lub(                        ,                        ) =  qq q qqq qqŒ q\nFigure 8. Temporal alignment. lub(                        ,                        ) =  q qq q\nFigure 7. Abstract notes and ordering\ninformation. e _ q q q_ e q q e lub(                         ,                             ) =  C4  F4  G4 qq\nFigure 9. lub of incoherent  melodies. lub(                        ,                        ) = Tq qqInteractive Music Summarization based on GTTM \nattributes, i.e. predecessor, successor, salient note, and difference, \n|P|T = |P|N * 4 for well-formed P.  Then we introduce three \nmeasures, RN, RA, and RT, to make the lacking information \nquantitative. Let P and Q be polyphonies.  Then \nR$(P,Q)  =  |lub(P,Q)|$ \nmax(|P|$,|Q|$)   \nRN and RA are associated with the time-span tree, and RT the \ntemporal structure.   \nRN is the similarity of time-span trees at the note level, where an \nincomplete note is also regarded as a note.  RA is the similarity of \ntime-span trees at the attribute level and indicates to what extent the pitch and duration attributes of all notes in the result of lub are \ninstantiated.  Likewise, R\nT is the similarity of temporal structures \nat the attribute level and indicates to what extent the four attributes of all notes in the result of lub are instantiated.  Note \nthat all the attributes of notes and of temporal structures are assumed to be equally weighted.   \nLet us look over real values of these similarity  measures.  If P = Q, \nR\nN = RA = RT = 1.0, and if lub(P,Q) = T  in contrast, RN = RA = RT \n= 0.0.  For an interesting case, consider the similarity between P \nand Q such that P ⊆ Q.  Then since this leads to | P|$ ≤ |Q|$  for $ = \nN, A, or T, we have R$ = |P|$ / |Q|$.  For the sample in Figure 6, \nwe obtain RN = 2/3, RA = 2/3, and RT = 5/9.  For the sample in \nFigure 7, we obtain RN = 1.0, RA = 5/8, and RT = 10/13.  For the \nsample in Figure 8, we obtain RN = 1/2, RA = 1/2, and RT = 1/5.  \nHere we do not explain in depth how these values are calculated because of space limitation, and please understand that these values just give a feeling of the multiple viewpoints of polyphonic similarity. \nConventional working approaches for representing and comparing \nmusic mostly focus on surface information, not deep structures [10].  A relevant way of taking into account deep structures is to adopt the concept of reduction.  Selfridge-Field (1998) claimed \nthat the reductions suited for melodic searching and comparison may not be as concise as those used in the implication-realization model by Narmour, which was de veloped for analytic purposes, \nalthough she did not mention the time-span reduction in GTTM.  We think that our method proposed here is a practical answer to her claim.   \n4. INTERACTIVE MUSIC \nSUMMARIZATION \nAs described in Section 1, a music summarization proceeds in \nthese steps: (1) discovery of piece structure by similarity checking, (2) identification of fragments to be chosen or skipped, and (3) concatenation of chosen fragm ents.  The operations of our \nprototype system, Papipuun, are designed in accordance with these steps (Figure 10).   \nIn the preprocessing phase, a user analyzes a set piece in SMF \nusing the time-span reduction and generates the information of \ncorresponding time-span trees using a dedicated tool, TS-Editor.  \nIn the real-time phase, the user interacts with a main system, \nSummarizer, to perform music summarization (Section 4.3).  \nSummarizer first looks for similar fragments in the piece using the lub of time-span trees of the fragm ents, groups them, and displays \nthem on its window (Figure 15).  Se veral parameters are given for \nchanging the behavior of the polyphonic similarity, some of which are used as threshold values of R\nN, RA, and RT.  A user repeatedly \nchanges the similarity parameters and examines the intermediate results of grouping similar fragmen ts using Summarizer’s GUI.   \n   \n                  When the fragments to be skipped (opposite of left) are fixed, \nSummarizer deletes them, concatenates the rest, and lets a user listen to the resulting concatenation.  If the concatenation is acceptable to the user, the session is terminated, and otherwise the user may return to the step of changing the similarity parameters.  TS-Editor and Summarizer are bot h implemented in Java.   \nOur system design policy here is  that the modules underlain by \nthe music theory operate automa tically and ones not underlain by \nthe music theory are operated manually.  In this case, the method for representing polyphony is underlain by GTTM, but a method for music summarization is not underlain by any music theory.  Thus, all modules up to and in cluding clustering of similar \nfragments are done automatica lly, and modules after this, \nmanually.  To automatize the ma nual modules, such as choosing \nfragments to be concatenated, vari ous heuristics may be applied to \nthem.  However, the resulting concatenation is not always preferable and may be rather frustrating.  For the present, we think that our system design policy is proper.   \n4.1 Preprocessing by TS-Editor \nTS-Editor is a dedicated tool that allows a user can enter the time-span tree of a set piece.  The input is the SMF file of a set piece, and the output is two XML files that are the translation of the input SMF file and the data of the corresponding time-span tree.   \nFigure 11 shows the window of TS-Editor at work.  It displays a \npolyphony in the piano roll fo rmat and its corresponding time-\nspan tree (the first seven bars  of “Turkish March”).  The \nsubwindow at the upper left also displays the same time-span tree \nin the folder format for ease of pointing at intermediate nodes.  \nThe input procedure has two steps: the time-span tree and temporal structure.   \nFirst, when a SMF file is read in to TS-Editor, all notes are sorted \nchronologically and a default time-span tree is provisionally attached to them such that a relevant note is a left-branching elaboration of its chronological successor (Figure 12a).   $ is N, A and T, respectively A set piece in SMF \nAnal ysis by time-s pan reduction \nClustering by similarity checking \nDisplay and audition of a result  \nin progress  \nConcatenatin g chosen fra gments Preprocessing using  TS-Editor \nFigure 10. System operations of Papipuun Similarity Parameters\nSummarized music in SMF Processing \nusing Summarizer(1)\n(2)\n(3)Interactive Music Summarization based on GTTM \n \n \n                In the figure, each gray box stands for a note of a piano roll score.  \nSuppose a user wants to obtain the time-span tree in Figure 12b \nfrom that in Figure 12a.  A user selects the node designated by the \narrow in Figure 12a and issues the exchange command.  TS-Editor provides four operators including the exchange command to manipulate a time-span tree.  Also, TS-Editor gives a user a command to group more than one note as a chord.   \nAfter entering a time-span tree, the user inputs the temporal \nstructure information.  In Figure 12c, we assume that a relevant \nnote, enclosed by an oval, occurs between preceding and succeeding notes.  To express this temporal relation, a user attaches two double lines, one be tween the relevant and the \npreceding notes and the other between the relevant and the succeeding notes.   \nFigure 13 shows the same part as Figure 11 when a user finishes \nattaching the temporal structure to every note included in the piece\n2. \nTS-Editor greatly improves the efficiency of entering a time-span \ntree and temporal structures.  In the case of entering “Turkish March” using the current version of  TS-Editor, it takes about three \nhours for four bars on average, where a bar approximately contains 12.9 notes on average.   \n \n \n \n   \n4.2 Clustering of Similar Fragments \nFigure 14 is a startup snapshot of Summarizer’s GUI taken just \nafter loading the output files of TS-Editor.  The entire score of “Turkish March” is also displayed in the piano roll format.  According to the standard notation, a piece structure would be expressed as AABA or ABA’.  However, the GUI instead shows a piece structure by putting on a piano roll score colored rectangular slices for indicating similar fragments.   \n   \n \n \n      At the upper left, there are five sliders for controlling the \nsimilarity parameters.  The first three from the top are the threshold values for R\nN, RT, and RA, called TN, TT, and TA, \nrespectively ( T$ = 0.0~1.0 for $ = N, T, or A).  The next two are \nfragment size S and a margin for fragment size M.  Within \nSummarizer, a fragment of a piece is represented as a corresponding subtree of the time-sp an tree of the entire piece.   Figure 13. Polyphony with its time-span tree \nand its tem poral information. Figure 11. Polyphony in the piano roll fomat and \nits time-span tree on the window of TS-Editor. \nFigure 12 Editing a time-span tree and \nattaching a temporal structure. b: after exchange \noperation issued a: initial branching preceding \nnote succeeding \nnote \nc: attaching temporal \nstructure note \nFigure 14. Startup snapshot of Summarizer’s GUI. five similarity parameters operation history \nsummarization buttonplayback \nrate summarization \nratio audition buttons\nbutton for registering an \nintermediate situation into operation history AA A  BB  CC  C  part A\npart B\npart CInteractive Music Summarization based on GTTM \nGiven two fragments of a piece, P and Q, Summarizer determines \nthat P and Q are similar to each other if RN(P,Q) > TN ∧ RT(P,Q) > \nTT  ∧ RA(P,Q) > TA.  Fragment size S can be varied 2, 4, 8 or 16 \nbeats, and a margin for fragment size M can be varied from 0% to \n20%.  For instance, when S = 4 and M = 10%, the time spans of \nall the fragments for calculating the similarity measures are restricted within 3.6 (= 4 * 0.9) to  4.4 (= 4 * 1.1) beats.  Here, the \nlength of one beat is obtained from the Tempo meta event in an input SMF file.   \nWhen these similarity parameters are fixed, Summarizer pairwise \ncalculates the lub of all time-span (sub)trees of a designated size \nand makes a decision on similar ity based on the similarity \nparameters.   If fragments P and Q are similar and Q and R are too, \nthen Summarizer infers P, Q, and R are all similar and makes \nthem a cluster (e.g. named part A and colored red).  In Figure 14, \nthe width of each rectangular slice means the time span of a corresponding time-span tree and ranges from 12.8 (= 16 * 0.8) to 19.2 (= 16 * 1.2) beats.  On the other hand, the height of each rectangular slice is one-third the height of the entire piano roll score, just because there are three kinds of clusters (parts) found.  \nThe height does not make musical sense.  The intervals that are not covered by any rectangular sli ce mean that there is no similar \nfragment within the same piece.  Below the GUI window, for reference, the standard notation of  the result of clustering is shown \n(A B A C …).   \n4.3 Interaction with a User \nThe interaction with the user on the Summarizer’s GUI begins with the situation shown in Figure 14 and proceeds as follows (Figure 15).  \n                      Upon pressing the summarization butt on at Step 4, the fragments \nchosen by the user at Step 3 are deleted, the remaining parts are concatenated, and the summarization ratio is updated.  At that time, vertical black bands emerge  to indicate the deleted parts \n(Figure 16).  The 2\nnd and 3rd of part A, the 2nd of part B, and the 2nd and 3rd of part C are deleted, and the summarization ratio \ndecreases to 68.75%.   \n \n \n \n  At Step 5, the user plays back an intermediate summarization and \nmakes fine adjustments by cha nging the playback rate slider \nvaried from 0.5~2.0 (1.0 means norma l rate).  At Step 6, a current \nsituation consisting of the intermed iate result and the values of the \nsimilarity parameters is saved for backtracking.  Then, a line of the current situation is appended to the end of the operation history.  The user can save a current summarization as a SMF file any time in a session, as well as upon finishing.   \nFor instance, the user can further proceed with summarization and \nmay change the fragment size from 16 to 4 to 2 and reach the situation in Figure 17 with the summarization ratio being 41.31%.  Note that the number of lines in the operation history has increased.   \n  \n \n \n  \n5. CONCLUSION \nWe have not done an audition e xperiment with subjects, but \nseveral people who tried Papi puun reported mostly favorable \nimpressions.  However, sin ce a summarization result highly \ndepends on the skill with which Papipuun is manipulated and the Figure 16. Snapshot with some parts deleted.\nFigure 17. Snapshot of a further summarization.Step 1: ad just the similarit y parameters \nStep 2: listen to some fragments colored by\nrectangular slices \nStep 3: choose the fra gments to be delete d \nStep 4: press summarization button \nStep 5: listen to intermediate summarization \nfinishe d love it like it and \nfurther improve it \nStep 6: register  \ncurrent situation does not \nlike it \nStep 7: backtrack to \nprevious situation Step 1\nStep 1\nFigure 15. User’s operati ons on Summarizer’s GUI.Interactive Music Summarization based on GTTM \nuser’s musical sense in changing the similarity parameters and \nchoosing fragments to be skipped, it will be difficult to construct a \nproper experimental framework  for Papipuun and evaluate it \nappropriately.   \nAs mentioned in Section 1, the key issues in music summarization \nare (a) music representation of polyphony, (b) discovery of a piece’s structure by similarity checking, (c) identification of fragments to be skipped, and (d) concatenation of selected \nfragments.  Regarding (a), amalga mating other music theories into \nour framework for music repres entation may improve preciseness \nand efficiency.  The module for (b ) is currently implemented for \nmanual generation.  Automatizing this module is future work.  \nRegarding (c), it may be convenien t for a user to indicate parts \nthat are not only skipped but also concatenated, although the current GUI provides only a capability for the latter.  Automatizing this task is also future work.  For (d), since Summarizer just concatenates fragments not to be skipped, it occasionally generates unnatural concatenations.  Hence, an intelligent concatenation technique is required.   \nTS-Editor greatly reduces effort, but further improvements are \nneeded.  We think one is some  means of supporting mechanical, \nroutine manipulations of a time-span tree in TS-Editor.  A similar idea would be useful for Summarizer’s GUI as well.   \nLastly, some other music su mmarization algorithms can be \nconsidered.  One decreases the num ber of notes by performing the \ntime-span reduction and fast-forwards the reduced one.  We will try to integrate such algorithms into Papipuun in the future.   \n6. ACKNOWLEDGMENTS \nWe thank Mr. Takaki Odachi of Digital Art Creation for data entry.  We also thank Prof. Tatsuya Aoyagi of Tsuda College for discussions in the early stage of this research.    \n \n1 DOOD was originally the name of an international conference \nand a research field.  Here, it only refers to the name of a knowledge representation method. \n2 In Figure 13, red lines are the branches of the time-span tree, \nwhile green lines are the temporal  structures, although readers will \nnot be able to distinguish them on the hard-copy proceedings, unfortunately.   7. REFERENCES \n[1] Carpenter, B. The Logic of Typed Feature Structures. \nCambridge University Press. 1992.   \n[2] Hirata, K., and Aoyagi, T. Musically Intelligent Agent for \nComposition and Interactive Performance. In Proceedings of ICMC 1999. pp.167-170.   \n[3] Hirata, K., and Aoyagi, T. Representation Method and \nPrimitive Operations for a Polyphony based on Music Theory GTTM. IPSJ Journal 43, 2. pp.277-286. 2002.  In Japanese.   \n[4] Hirata, K., and Hiraga, R. Next Generation Performance \nRendering – Exploiting Controllability. In Proceedings ICMC 2000. pp.360-363.   \n[5] Huron, D. Perceptual and Cognitive Applications in Music \nInformation Retrieval. In Proceedings of ISMIR 2000.   \n[6] Kifer, M., Lausen, G., and Wu , J. Logical Foundations of \nObject-Oriented and Frame-Ba sed Languages. Journal of \nACM 42, 3. 1995.   \n[7] Lerdahl, F., and Jackendoff, R. Generative Theory of Tonal \nMusic. The MIT Press. 1983. \n[8] Logan, B., and Chu, S. Mu sic Summarization using Key \nPhrases. In Proceedings of ICASSP 2000. \n[9] Plaza, E. Cases as terms: A feature term approach to the \nstructured representation of cases. Lecture Notes in Artificial Intelligence Vol.1000. pp.265-276. Springer-Verlag. 1995. \n[10] Selfridge-Field, E. Conceptual and Representational Issues in \nMelodic Comparison.  Computi ng in Musicology 11, pp.3-64. \n1998.   \n[11] Yokota, K. Towards an Integrated Knowledge-Base \nManagement System: Overview  of R&D on Databases and \nKnowledge Bases in the FGCS Project. In Proceedings of International Conference on Fifth Generation Computer Systems 1992.  Institute fo r New Generation Computer \nTechnology. pp.89-112."
    },
    {
        "title": "Variations on the Theme of Musical Similarity.",
        "author": [
            "Douglas Hofstadter"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1180595",
        "url": "https://doi.org/10.5281/zenodo.1180595",
        "ee": "https://zenodo.org/records/1180595/files/nime2012_125.pdf",
        "abstract": "SoundStrand is a tangible music composition tool. It demonstrates a paradigm developed to enable music composition through the use of tangible interfaces. This paradigm attempts to overcome the contrast between the relatively small of amount degrees of freedom usually demonstrated by tangible interfaces and the vast number of possibilities that musical composition presents. SoundStrand is comprised of a set of physical objects called cells, each representing a musical phrase. Cells can be sequentially connected to each other to create a musical theme. Cells can also be physically manipulated to access a wide range of melodic, rhythmic and harmonic variations. The SoundStrand software assures that as the cells are manipulated, the melodic flow, harmonic transitions and rhythmic patterns of the theme remain musically plausible while preserving the user&#39;s intentions.",
        "zenodo_id": 1180595,
        "dblp_key": "conf/ismir/Hofstadter02",
        "keywords": [
            "tangible music composition tool",
            "tangible interfaces",
            "musical composition",
            "degrees of freedom",
            "physical objects",
            "melodic flow",
            "harmonic transitions",
            "rhythmic patterns",
            "musical plausible",
            "user intentions"
        ],
        "content": "SoundStrand: Composing with a Tangible Interface  \n \n \nEyal Shahar \nMIT Media Lab \n75 Amherst Street \nCambridge, MA 02139 \npersones@media.mit.edu \n \n \nABSTRACT \nSoundStrand is a tangible music composition tool. It \ndemonstrates a paradigm developed to enable music \ncomposition through the use of tangible interfaces. This \nparadigm attempts to overcome the contrast between the \nrelatively small of amount degrees of freedom usually \ndemonstrated by tangible interfaces and the vast number of \npossibilities that musical composition presents. \nSoundStrand is comprised of a set of physical objects called \ncells, each representing a musical phrase. Cells can be \nsequentially connected to each other to create a musical theme. \nCells can also be physically manipulated to access a wide range \nof melodic, rhythmic and harmonic variations. The \nSoundStrand software assures that as the cells are manipulated , \nthe melodic flow, harmonic transitions and rhythmic patterns of \nthe theme remain musically plausible while preserving the \nuser’s intentions.  \n \nKeywords  \nTangible, algorithmic, composition, computer assisted \n1. INTRODUCTION \n1.1 Motivation \nIt is well established that the benefits people get from engaging \nwith music are substantial – it arouses creativity, provides \nmeans of deep expression and builds self-confidence  [1] [7]. \nThe entry level for music composition, however, can be quite \nintimidating. Composing music requires extensive knowledge \nof music theory, developed aural skills, and often requires \nmastering a musical instrument. This prevents a large share of \nthe population to take on composing, including young children \nand mentally or physically challenged people – those who need \ntheir self-confidence supported and their means of expression \nenhanced the most. \nTherefore, it is not surprising that many endeavors have been \nmade to lower the entry level for involvement in music \ncomposition. Software systems have been designed to enable \nmusic composition using abstract graphical input, with the \nsoftware translating shapes, lines and colors to musically \nplausible compositions  [5]. On the other hand, the use of \ntangible interfaces in music is mostly restricted to exploration \nand manipulation of pre-composed music due to their limited \namount of degrees of freedom . SoundStrand demonstrates a hybrid approach that provides tangible interaction yet yields a \ncomplex, deliberate musical composition. \n1.2 Distinction from Related Work \nThe body of work in the field of tangible interfaces for music \ncreation is immense. Concentrating on interfaces for music \ncomposition, we can observe works such as the reacTable  [6] \nand AudioCubes  [8], in which objects represent musical \nelements that are constantly playing, bringing the interaction \nmore similar to a performance, improvisation or composition in \nreal time.  Other interfaces such as the Tangible Sequencer  [1] \nand Music Blocks  [10] are, indeed, sequencers; however, the \nsequenced units are sampled, unchangeable segments of music, \na fact that leaves the user highly limited in terms of composing. \nFinally, augmented reality projects, such as the Music \nTable  [2], support detailed music composition, yet the objects \nwith which the users interact take the role of commands or \ntemporary representation of content, rather than embody the \nmusical material and act as a consistent representation. \nThe SoundStrand cells not only represent their musical content \nbut also the means to control it. Their individual manipulation \ncontrols rhythm, pitch and harmony, while their assembly is a \nhigh level description of the musical content arrangement. The \nfinished strand is a mentally sustainable representation of a \ncomposed musical piece. \n2. SOUNDSTRAND \nSoundStrand is a set of cylindrical cells, each representing a \nmusical one measure long musical phrase . Cells come in \ndifferent types that represent different musical phrases. Cells \ncan be attached to one another to create a musical theme, as \nshown in Figure 1 . The skeletal mechanism of a cell allows it to \nbe stretched or shrunk to shift the rhythmic center of mass, bent \nupwards or downwards to change melodic directionality and \ntwisted along its axis to change the harmonic context. \n \n \nFigure 1. Two SoundStrand configurations, demonstrating \ndifferent manipulations of cells \nThe physical interface of SoundStrand is a set of cells. A cell is \na cylindrical object with a 3D printed skeleton and stretchable \nfabric skin. The skeleton is designed so that the cell can be \nbent, elongated and twisted. Cells also include electrical \ncircuitry that is used to digitally capture the cell’s physical state \nand to communicate with neighboring cells. Cells connect to \neach other by a set of extrusions and holes on both their ends. \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advant age and that \ncopies bear this notice and the full citation on the first page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, \nrequires prior specific permission and/or a fee.  \nNIME’12 , May 21 -23, 2012, University of Michigan , Ann Arbor.  \nCopyright remains with the author(s).  \n Electrical connectors are also located on the cells’ ends. These \nare used to carry power and data between the cells. The first \ncell in a strand connects to a computer using an FTDI cable. \nThe data that is received by the computer describes th e \ncomplete current strand configuration. \nThe computer, in turn, is running dedicated software that reads \nthe strand ’s configuration and translates the cells’ order and \nindividual cell manipulations to a musical composition. \n3. MUSICAL APPROACH \n3.1 Musical Paradigm \nCurrently SoundStrand is restricted to a single voice, single \ntimbre theme accompanied by a harmony and a bass. It is \nrecognized that a theme is the assembly of musical phrases, \noften repetitive with some modifications. It is then stressed that \nthese variations are introduced to the timing of the notes, their \npitches and the underlying harmony. Applying these variations \nusing a tangible interface requires that a user will have access \nto the wide varieties of possible variations of one of these \nproperties with a single parameter. \nFor rhythmic variations, a phrase can be viewed as having a \ncenter of rhythmical mass .  If a phrase is one musical bar long \nand it is comprised of four consecutive quarter notes, it is said \nthat its center of rhythmical mass is 0.5, i.e. in the middle of the \nmeasure. If notes are shifted towards the beginning of the \nmeasure it will have a smaller center of mass and a larger \ncenter of mass if notes are shifted towards the end. The \nsuggested application will shift the notes to accommodate the \ndesirable center of mass as expressed by the user. \nFor melodic variations, the notes comprising the phrase can be \nviewed as having directionali ty. Modifying the directionality of \na phrase will shift the notes’ pitches upwards or downwards.  \nFinally, when a harmonic variation is introduced to the phrase , \nthe system not only changes the notes of the underlying \nharmony and bass, but also the melody notes in a manner that \nwill be musically plausible. \nIt is also required that the system ensures that transitions from \none phrase to the next are musical, mainly in respect to the \nharmonic transitions and the melody line. \n3.2 Parameter Mapping \nAssignment of the various degrees of freedom of the \nSoundStrand cell to the different types of phrase variation \nattempts to be as intuitive as possible.  \nAs cells are connected sequentially to one another, it suggests \nthat time moves along the trajectory connecting the cells, i.e. \nalong the cells ’ lengths. Therefore the variation concerning the \ntiming of the notes – their rhythmic distribution – is mapped to \nchanges along that axis, which is the cell’s elongation.  \nPitch is commonly referred to as the axis perpendicular to time; \ntherefore, the cell’s bend is mapped to the phrase’s melodic \ndirectionality. \nFinally, the mapping of the cell’s twist to harmonic tension \nseems natural as the act of twisting is often paired with physical \ntension, such as springs or lids of jars. \nTable 1  shows a phrase along with one of the several results of \nthe cell’s manipulation along each degree of freedom.  \n4. DESIGN \n4.1 Mechanical Design \nThe cells are cylindrical objects about 4” in length and 2” in \ndiameter. They consist of a plastic skeleton, a skin and \nelectronic circuitry.  \n \n Table 1. Example of a manipulated cell \n \n4.1.1 Skeleton \nThe skeleton of a SoundStrand cell ( Figure 2 ) is fabricated by \n3D printing. It is designed to enable the cell to be bent, twisted \nand elongated, and to be connected to neighboring cells  [9]. \nThe center piece of the skeleton is the frame . It has a rail in \nwhich slides the rack. The rack can be moved back and forth to \nchange the elongation. The frame has a niche to which the \npinio n fits. When the rack slides back and forth, the pinion \nturns, and by measuring a potentiometer attached to the pinion, \nthe amount of elongation can be determined.  \n \n \nFigure 2 . A disassembled cell skeleton  \nThe arm serves as the bending mechanism of the cell. It \nextends from the frame to which it is connected with a pin, \nallowing the arm to turn around the pin’s axis. The pin itself \nhas an extrusion that fits into a niche in the arm, forcing them \nto move together. A potentiometer attached to the pin measures \nthe angle between the arm and the frame, determining the cell’s \nbend. \nFinally, the twisting mechanism consists of the plate  rotating \nagainst the rack’s end. Two braces locked into each other and \nto the plate encapsulate the rack’s end and hold the plate in \ncontact with it. A potentiometer attached to the plate measures \nthe amount of rotation between the plate and the rack’s end in \norder to determine the cell’s twist.  \n4.1.2 Skin \nThe cells are covered with an elastic fabric skin. The skin is \nsewn to a cylinder, and it keeps its shape with three plastic \nrings fastened to its interior. Two rubber rings are sewn to the \nends of the skin and fit to grooves on the edge of both the \nskeleton’s ends.  \n4.1.3 Connectors \nThree cross-shaped extrusions are located on the face of the \nplate. These fit into corresponding cuts in the face of the \nconnected cell’s arm . In addition, the electrical connectors , \nwhich are a 2x2, 2.54mm pitch header-connector pair, not only \nprovide power and communication but also support the \nmechanical connection between cells. \n4.2 Electrical Design \nIn the center of every SoundStrand cell is an Atmel \nATmega168 microcontroller. It measures the value of three \npotentiometers fixed to the cell ’s skeletal structure to dete rmine \nthe cell’s elongati on, bend and rotation.  \nCells connect to each other electronically as described in \nsection  4.1.3 . The first cell in a strand is connected to the \ncomputer with a USB FTDI cable. This connection allows the \ncomputer to provide a 5V power line and a ground line for the \nentire strand and receive the strand ’s configuration, encoded as \ndescribed in section  4.3.  Two of the pins are used as a shared \n5V power supply. A third pin is used to transfer data over a \nserial bus from a cell to the one preceding it. The fourth pin is \nreserved for future use. \nThe circuit features an RGB-LED that serves to indicate that \nthe cell is working properly. The color of the LED is \ndetermined by the state of the potentiometers. The light is \nclearly visible to the user as it is diffracted by the cell’s skin.  \n4.3 Communication Protocol \nBy design, data can flow between the cells in only one direction \n- from the end of the strand towards the computer. The last cell \nin the strand periodically initiates the data transfer with a \npacket that contains the cell’s type and potentiometer values, \nfollowed by an “End of Transmission” (ETX) byte. The packet \nis passed to the preceding cell which adds its own type and \npotentiometer values to the beginning of the packet before \npassing it on. This process is illustrated in Figure 3 . \n \n  \nFigure 3 . Packet formation. a) Cell A, the last in the strand, \ninitiates a packet. b) Cell B adds its data to the packet. \nWorking in tail mode, a cell assumes it is the last one in the \nstrand unless receiving a packet in its input port. While \noperating in this mode, it will initiate a data transfer every \n250ms. Once a packet is received, the cell will no longer \nconsider itself last and will enter body mode. Under this mode \nof operation, packets received will be promptly modified and \npassed on. If no packet is received for a period of 500ms, the \ncell will assume its subsequent cells have been removed and \nreturn to tail mode. The 500ms interval assures cells do not \nleave body mode prematurely. 4.4 Software \nThe software used to translate the strand’s configuration to a \nmusical theme is written in Java. When the software is started, \nit loads XML files describing the various cell types in terms of \ntheir musical content, which is the original pitches of the notes \nand a table of possible rhythmic distributions. Other XML files \ndescribing harmonic transition tables are also loaded.  \nThe software then allows the user to select various harmonic \nmodels which are implemented through their respective \ntransition table. The user can also select melodic models which \ndetermine how note pitches will be quantized, e.g. diatonic vs. \npentatonic scales.  \nThe software also features a transport bar, allowing playing, \nstopping and looping the strand or a particular cell and a piano-\nroll style visual representation of the theme. \nFinally, the software allows the user to enter simulation mode \nin which connection to a physical interface is not necessary and \ncells are added, deleted and manipulated virtually. \nTwo additional software tools allow the user to program new \nharmonic transition tables and new phrases along with their \npossible rhythmic permutation. These will be described in \nsection  5.4. \n5. ALGORITHMS DESCRIPTION  \n5.1 Rhythmic Variation \nStretching and compressing a cell result in changes in the \nrhythmic distribution of its phrase’s  notes. Currently, a phrase \nis programmed with a predetermined list of possible rhythmic \nvariations. The value of the cell’s elongation potentiometer is \nquantized by the software which in turn selects the appropriate \nrhythmic permutation from the list.  \n5.2 Harmonic Variation \nSoundStrand’s harmony system is inspired by David Cope’s \nSPEAC system  [4]. A cell can have one of five possible tension \nfunctions: Statement, Preparation, Extension, Antecedent or \nConclusion. Different states correspond to different degrees of \ntwist introduced to the cell. A harmonic transition table \ndetermines the cell’s chord based on its tension function and \nthe preceding cell’s chord. As an example, let us assume a \nSoundStrand playing in the C-Major key. If a cell is twisted to \nassume the Extension  function and its preceding cell’s harmony \nwas an F-Major chord, the harmonic transition table might \nindicate that the cell’s chord should be a D -Minor chord. \nDifferent harmonic transition tables can be applied to produce \ndifferent musical modes, e.g. Minor or Phrygian, or various \nmusical genres. \n5.3 Melodic Variation \nBending a cell alters the melodic directionality of its phrase . \nThe alteration of notes is done in several stages. First, if a cell \nis not the first one in a strand, the phrase is transposed so that \nits reference point, which is the middle C, is shifted to be the \nsame as the last note of the preceding cell. This is done in order \nto achieve a natural melodic flow from cell to cell .  \nIt is then that the direction and amount of bending are \nconsidered. Notes are transposed up or down depending on the \ndirection of bending - notes that are closer to the end of the \nmeasure are transposed more than notes that are closer to the \nbeginning. The notes ’ pitches are then quantized to pitch values \nthat are on the strand’s key, and the first and last notes are \nfurther quantized to pitch values which form the cell’s chord.  \n5.4 Content Generation Tools \nTo achieve even greater flexibility using SoundStrand, \nadvanced users can create their own content using two content \ngeneration tools. These define the melodic content, the \nrhythmic permutations and the harmonic behavior of \nSoundStrand. \n5.4.1 Cell Content Editor \nWith the Cell Content Editor, the user can program cell \nphrases, along with the possible rhythmic permutations:  in the \npiano-roll like Pitch Area  the user defines the number of notes \npresent in the phrase and their pitches when the cell is not bent . \nAlthough the timing of the events is also expressed in this \neditor, it is only for reference and for the initial setting of a new \nrhythmic pattern. The actual timing is specified in the Rhythm \nArea . The Rhythm Area allows the user to create the various \nrhythmic patterns accessible through modifying the elongation \nproperty. The pattern editing interface is a row of boxes \nrepresenting the 1/16th note intervals in the measure. The user \nedits a pattern by marking the desired onset times.  \n5.4.2 Harmonic Transition Table Editor \nThe Harmonic Transition Table Editor allows the user to create \na set of rules that determine SoundStrand’s harmonic behavior. \nA rule, as a software entity, has two fields: state, which is a \nstring expressing the harmonic function of the preceding cell; \nand transitions, which are an array of strings, each expressing \nthe next function based on the degree of tensions as conveyed \nby the cell’s twist . The user begins to create a new harmonic \ntransition by starting with an empty table. When adding a new \nrule, the software prompts the user to determine the state of that \nrule. The user can choose any degree on the scale, i.e. I to VII, \nand has a choice between a major or minor chord. The user can \nthen determine the different transitions of that rule. However, in \norder to use a harmonic function as a transition, it must first be \ndefined as a state of another rule. This scheme prevents \nSoundStrand from reaching a harmonic function for which \nharmonic transitions are not defined.  \n6. SUMMARY AND FUTURE WORK \nThis work intends to demonstrate the feasibility of composition \nwith tangible interfaces, a concept which allows users of \nvarious musical backgrounds to engage in creative musical \nactivity. Additional work to develop this concept can take many \nforms. \nEnhancement of SoundStrand interactivity can be explored in \nmany paths. At present, auditioning cells, playing, stopping, \nand toggling the loop mode are all done from the computer \nGUI. A search for ways to execute these functions through \ntangible interaction will help to concentrate the user’s \nengagement around the physical interface. Other sensors can be \nincorporated in the SoundStrand cells to detect the strand’s \nlocation and orientation in space. This, for example, can be \nused once the piece is ready and is being performed by \nhandling it in space. The research work to be done in this case \nwill include the sensing technology, but even more importantly, \nthe mapping of the data retrieved to musical parameters. \nVisual feedback can also be greatly improved, perhaps by \nincreasing the illumination of the currently playing cell or even \nthe current playing position within the cell. In addition, a \nsturdier, more robust physical structure can be developed, both \nfrom the perspective of the mechanical design and the materials \nbeing used. \nOn a wider scope, the paradigm of composing by manipulation \nof pre-composed musical fragments can be implemented i n \nother applications - the most obvious are a range of yet \nunexplored tangible interfaces for music composition that can \ntake very different form from SoundStrand, use different \nmappings, but still utilize the same paradigm and algorithms. \nFurther research can be done regarding the algorithms \nthemselves, allowing multiple harmonic changes within a measure, automatic generation of rhythmical permutations and \nkey changes. \n7. ACKNOWLEDGMENTS \nI would like to thank Yan Shen and Kelsey Brigance for their \nkey contribution in the mechanical design and Tod Machover \nfor supporting this project. \n8. REFERENCES \n[1] Bernstein, J. T., The Tangible Sequencer - a Simple \nMusical Instrument , \nhttp://murderandcreate.com/tangiblesequencer, 2005 \n[2] Berry, R.,Makino, M., Hikawa, N., and Suzuki, M. , The \nAugmented Composer Project: The Music Table, in \nProceedings of the 2003 International Symposium on \nMixed and Augmented Reality , Tokyo, Japan, 2003, pp. \n338–339. \n[3] Boulanger, A. Music, Mind and Health: How Community \nChange, Diagnosis, and Neuro-Rehabilitation can be \nTargeted during Creative Tasks . Ph.D. Thesis, \nMassachusetts Institute of Technology, 2010 \n[4] Cope, D., Computer and Musical Style, A-R Editions, \nMadison, WI, 1991 \n[5] Farbood, M., Kaufman, H., and Jennings, K. Composing \nwith Hyperscore: An Intuitive Interface for Visualizing \nMusical Structure. In Proceedings of the International \nComputer Music Conference , Copenhagen, Denmark , \n2007 \n[6] Jorda, S., Kaltenbrunner, M., Geiger, G. & Bencina, R., \nThe reacTable*. In Proceedings of the International \nComputer Music Conference , Barcelona, Spain, 2005 \n[7] Machover, T . Shaping minds musically . BT Technology \nJournal , Kluwer Academic Publishers Hingham , MA , \n2004, 22(4):171-179.  \n[8] Schiettecatte, B. and Vanderdonckt, J., \"AudioCubes: a \nDistributed Cube Tangible Interface based,\" in \nProceedings of the Second International Conference on \nTangible and Embedded Interaction , Bonn, Germany, \n2008, pp. 3-10. \n[9] Shen, Y. Sound Strand Design: Designing Mechanical \nJoints to Facilitate User Interaction within a Physical \nRepresentation of Digital Music.  B.S. Thesis. \nMassachusetts Institute of Technology, Cambridge, MA, \n2011 \n[10] Sosoka, J., Abercrombie, B., Emerson, B. and Gerstein, \nA., Educational Music Instrument for Children , 6,353,168, \nMarch 5, 2002.  \n[11] Weinberg G. Playpens, Fireflies, and Squeezables – New \nMusical Instruments for Bridging the Thoughtful and the \nJoyful Leonardo Music Journal , MIT Press, 2003. Vol. 12, \npp. 43-51"
    },
    {
        "title": "Indexing Hidden Markov Models for Music Retrieval.",
        "author": [
            "Hui Jin",
            "H. V. Jagadish"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418259",
        "url": "https://doi.org/10.5281/zenodo.1418259",
        "ee": "https://zenodo.org/records/1418259/files/JinJ02.pdf",
        "abstract": "/\u00040213134658789;:\u0010@7@=A1346BDCFEG/H7\"7@CJI\u0006K.9\u0002>\u00124HL\u0018464#58CNMPOQO\b46CSR\u00104\u00021\"9;CT9;5 4#U$4#V#R\u00100D>\u00124WR\u001046V)KP5P0YXZMP4WR\u0010=F:\u001046[3:\u001046CN4#5AR]\\FMPCN0DV\b^\u0011_T0Y>\b465\"9`V6=QBYBD46VJR\u00100Y=Q5%=Qa \\FMPCN0DV\u00029QBb[30D46V646C6cd4\u00029QV)Ke:\u00104#[3:\u001046CN465ZR\u00104\u00021%LZf%0gR\u0010CW/H7\"7\u000fc\u001d9Q5P1\u000f9hXZMP4#:NfZc R\u0010K34\u001b:\u00104#RN:\u00100D46>Q9QB\u0012R)9;CN\u00124 OQ465P4#:)9?R\u00104\u00021\"R\u0010KP4FXAM34#:NfZ^\u0004n\u0011KP4F\\oMPCN0DV\u00029;Bm[P0D46V#4F:\u001046[p:\u001046CN465ZR\u00104\u00021\"LZf@R\u0010KP0DC /W7\"7q0DCFa2:\u00104\u0002XZMP465ZR\u0010Bgf+R\u0010K34r=Q5P4s:\u0010465.1p4#:\u00104\u00021,LZfeR\u0010KP4rMPCN4#:\u0002ct[\u0018=QCNCN0YL3Bgf 0D\\l[\u00184#:Nau46V#R\u0010BgfZ^ n\u0011K30YCb\\l4JR\u0010KP=A1H\\l0DO\bKZRmL\u00184 0Y534#vlV60D465ZRb0ga3R\u0010KP4#:\u00104 0YCt9w>\u00124#:NfWB29?:\u0010O\b4x\\FMPCN0DV 139;R)9QLP9QCN4\bcPCN0D5PV64H4\u00029;VJK@/H7\"7yR\u0010=`L\u00184TR\u001046CSR\u0010461h:\u00104\u0002XZMP0g:\u001046CxR\u0010KP4T46>\b9;BDM.9;z R\u00100D=\b5l=;ai9H1pfp5.9;\\l0DVx[3:\u0010=\bO;:)9Q\\l\\l0D5POT9QBDO\b=;:\u00100gR\u0010KP\\8^t{\u000b5FR\u0010KP0DC [P9Q[\u00184#:\u0002c\b|\u00114 [p:\u0010=\b[\u0018=\bCN4W9Q5@0D5P134#}p0D5POF\\l46VJKP9Q530YCN\\kR\u0010K.9?RxV\u00029Q5@9QOQOQ:\u001046CNCN0D>\u001246Bgf@[3:\u0010MP534 R\u0010K34lCN4#R~=QawV\u00029;5.130Y139;R\u00104s/H7\"7@CR\u0010=8L\u00184l46>Q9QBDM.9?R\u00104\u00021,0D5+:\u001046CN[\u0018=Q5PCN4lR\u0010= 9\"XZMP4J:NfZ^%TM3:~4#}p[\u00184#:\u00100D\\l465ZR\u0010C=\b598\\FMPCN0DVs1P9?R)9QLP9QCN4rCNKP=\u0002|\u00114\u000219;5 9\u0002>\u00124#:)9;O\b4H=;a\u001b9oCN46>\u0012465pza=\bBY1@CN[\u00184#4\u00021hM3[h|w0gR\u0010K85P=FaG9QBDCN4~130DCN\\l0DCNC\u00109QBDC6^",
        "zenodo_id": 1418259,
        "dblp_key": "conf/ismir/JinJ02",
        "keywords": [
            "abstract",
            "article",
            "key aspects",
            "Qwen",
            "helpful assistant",
            "Qwen",
            "Alibaba Cloud",
            "capturing",
            "key aspects",
            "Qwen"
        ],
        "content": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0001\u0006\f\u000e\r\u000f\t\u0010\u0003\u0011\u0003\u0011\u0005\u0012\u0001\u0014\u0013\u0016\u0015\u0018\u0017\b\u0019\u001b\u001a\u001d\u001c\u001e\u0013\u0016\u001a\u001b\u0003\u0011\u0005\u0012\u001f\u000b \"!#\u001a$\u0017%\u0013'&\u001d \b\t)(+*,\u0005$-)\u0017\b\t)\u0005.\u001c.\u0015$\u001f\nIndexing HiddenMark ovModels forMusic Retrie val\nHuiJin\nDepar tment ofElectr icalEngineer ingand\nComputer Science\nTheUniversity ofMichigan\n3334 EECS Building 1301 Beal Ave\nAnnArbor ,MI49109­2122\n(10)(734)763­5243\njinh@eecs .umich.eduH.V.Jagadish\nDepar tment ofElectr icalEngineer ingand\nComputer Science\nTheUniversity ofMichigan\n2238 EECS Building 1301 Beal Ave\nAnnArbor ,MI48109­2122\n(10)(734)763­4079\njag@eecs .umich.edu\nABSTRA CT/\u00040213134658789;:\u0010<\u0012=?>@7@=A1346BDCFEG/H7\"7@CJI\u0006K.9\u0002>\u00124HL\u0018464#58CNMPOQO\b46CSR\u00104\u00021\"9;CT9;54#U$4#V#R\u00100D>\u00124WR\u001046V)KP5P0YXZMP4WR\u0010=F:\u001046[3:\u001046CN4#5AR]\\FMPCN0DV\b^\u0011_T0Y>\b465\"9`V6=QBYBD46VJR\u00100Y=Q5%=Qa\\FMPCN0DV\u00029QBb[30D46V646C6cd4\u00029QV)Ke:\u00104#[3:\u001046CN465ZR\u00104\u00021%LZf%0gR\u0010CW/H7\"7\u000fc\u001d9Q5P1\u000f9hXZMP4#:NfZcR\u0010K34\u001b:\u00104#RN:\u00100D46>Q9QB\u0012R)9;CN<W:\u00104\u000213M3V646CiR\u0010=]jP5.130D53O]/W7\"7k\\l=QCSRmBD0D<\u001246BgfHR\u0010=\u0004KP9\u0002>\u00124OQ465P4#:)9?R\u00104\u00021\"R\u0010KP4FXAM34#:NfZ^\u0004n\u0011KP4F\\oMPCN0DV\u00029;Bm[P0D46V#4F:\u001046[p:\u001046CN465ZR\u00104\u00021\"LZf@R\u0010KP0DC/W7\"7q0DCFa2:\u00104\u0002XZMP465ZR\u0010Bgf+R\u0010K34r=Q5P4s:\u0010465.1p4#:\u00104\u00021,LZfeR\u0010KP4rMPCN4#:\u0002ct[\u0018=QCNCN0YL3Bgf0D\\l[\u00184#:Nau46V#R\u0010BgfZ^n\u0011K30YCb\\l4JR\u0010KP=A1H\\l0DO\bKZRmL\u00184\n0Y534#vlV60D465ZRb0ga3R\u0010KP4#:\u00104\n0YCt9w>\u00124#:NfWB29?:\u0010O\b4x\\FMPCN0DV139;R)9QLP9QCN4\bcPCN0D5PV64H4\u00029;VJK@/H7\"7yR\u0010=`L\u00184TR\u001046CSR\u0010461h:\u00104\u0002XZMP0g:\u001046CxR\u0010KP4T46>\b9;BDM.9;zR\u00100D=\b5l=;ai9H1pfp5.9;\\l0DVx[3:\u0010=\bO;:)9Q\\l\\l0D5POT9QBDO\b=;:\u00100gR\u0010KP\\8^t{\u000b5FR\u0010KP0DC\n[P9Q[\u00184#:\u0002c\b|\u00114[p:\u0010=\b[\u0018=\bCN4W9Q5@0D5P134#}p0D5POF\\l46VJKP9Q530YCN\\kR\u0010K.9?RxV\u00029Q5@9QOQOQ:\u001046CNCN0D>\u001246Bgf@[3:\u0010MP534R\u0010K34lCN4#R~=QawV\u00029;5.130Y139;R\u00104s/H7\"7@CR\u0010=8L\u00184l46>Q9QBDM.9?R\u00104\u00021,0D5+:\u001046CN[\u0018=Q5PCN4lR\u0010=9\"XZMP4J:NfZ^%TM3:~4#}p[\u00184#:\u00100D\\l465ZR\u0010C=\b598\\FMPCN0DVs1P9?R)9QLP9QCN4rCNKP=\u0002|\u00114\u000219;59\u0002>\u00124#:)9;O\b4H=;a\u001b9oCN46>\u0012465pza=\bBY1@CN[\u00184#4\u00021hM3[h|w0gR\u0010K85P=FaG9QBDCN4~130DCN\\l0DCNC\u00109QBDC6^\n1. INTR ODUCTION7@M3CN0YVh:\u00104#RN:\u00100D46>\b9;Bw0YClR\u0010K34@[3:\u0010=ZV#46CNCl=QaW:\u00104JRN:\u00100Y4#>p0D53O+4#}p[\u001846V#R\u0010461CN=Q5PO\bCa2:\u0010=\b\\9F\\FMPCN0DV\u00041P9;R)9;L.9;CN4\b^tKP465h9FV#=\b\\l[\u0018=\bCN4J:wV#:\u00104\u00029;R\u00104#C\u00049~\\l46BD=A1pfZcCNK34r\\l0DOQKAR|\u00119Q5ZRFR\u0010=\"jP5.1+CN0D\\l0DBY9;:F=\b5P4#CoV#:\u00104\u00029?R\u00104\u00021,L\u00184#au=Q:\u00104\b^+7@MPCN0DVCSR\u0010=;:\u001046Cl9Q5P1\\oMPCN0DVsBD0DL3:)9;:\u00100Y9;5PCs9;:\u00104r=Qa2R\u00104659QCN<\u00124\u00021,R\u0010=\"j.5P19e[30D46V64=;a\u0006\\FMPCN0DVL.9QCN461%=Q5e9lau4#|CNM35POs=Q:TKpM3\\l\\l4\u00021h53=QR\u001046C6^Wn\u0011KP0DCTR)9;CN<$c\\FMPCN0DV\n:\u00104#RN:\u00100D46>Q9QBpLZfV6=\b5ZR\u0010465ZR\u0002cQ0DCbVJKP9QBDBD465POQ0Y53OWV6=Q5PCN0Y134#:\u00100D53OWR\u0010KP4\na9QV#RR\u0010KP9;R\u001bR\u0010KP4#:\u00104]0DC\u00119HBY9;:\u0010O\b4T9Q\\l=QMP5ZR\u0006=;ai\\oMPCN0DV\b^bP=Q:\u00064#}39Q\\l[PBD4\bc\bR\u0010KP4wF^ $^i0YLp:)9;:Nf=QaF\u0006=\b53OQ:\u001046CNC@KP=\bBY1pCh=?>\u00124#:hCN0g}\\l0DBDBD0Y=Q5[P0D46V64#Ch=;aFCNK3464#R\\FMPCN0DV]9Q5.1`R\u0010K34]T9;R\u00100D=\b5P9QB$i0YLp:)9;:Nfl=;a\u001d3:)9Q53V64\u0004KP9QCxQ\bA)\b\bKP=QM3:\u0010C=;aWCN=\bM35.1:\u001046V6=Q:)1p0D5PO3cx\b\u0012=QaW0gRs\\FMPCN0DV+ ?^nd=+\\s9;<\u00124@0gRs46>\u00124#5\\l=;:\u00104o1p0Dv`V6MPBgR\u0002ci\\oM3CN0YVF0DCH9Q5\u000f9;:NRWau=Q:\u0010\\8ciKP=\u0002|CN0D\\l0DBY9;:HR|\u0011=r[P0D46V646C=;at\\oMPCN0DVH0DCTCNM3LAS46V#R\u00100D>\u00124\b^\u0011n\u0011KAMPC6c\u0018\\FMPCN0DVW:\u00104#RN:\u00100D46>Q9QBdKP9QC]L\u001846465\"9`K3=QR:\u001046CN469;:\u0010V)K@R\u0010=\b[30YVW:\u00104#V6465ZR\u0010BgfZ^n\u0011K34TR\u001046V)KP5P0YXZMP4#C\u00119Q[3[PBD0D4\u00021h0D5r\\oM3CN0YVw:\u00104#RN:\u00100D46>Q9QB$CNKP=QMPBY1hL\u00184\u0004L\u0018=QR\u0010Kr4#auzau46V#R\u00100D>\u00124x9;5.1F4JvlV60D465ZR\u0002^bbU$46V#R\u00100D>\u00124\u0011\\l469Q5PCmR\u0010KP9;RmR\u0010K34\u0006:\u001046CNM3BDRb:\u00104JR\u0010M3:\u00105P461LZfe9hCSfpCSR\u001046\\yCNKP=\bM3BY1\u000fL\u00184oR\u0010K34`=\b534l9hM3CN4#:~4#}p[\u001846V#R\u0010C6c\u001d0^ 4\b^oR\u0010KP4`=Q5P4\\l=QCSRwCN0Y\\l0DBY9;:xR\u0010=~R\u0010KP4HXZM34#:NfZ^tbvlV60D465PVJfh\\l469Q5PC\nR\u0010K.9;RxR\u0010K34WCSfpCSR\u001046\\CNK3=\bMPBY15P=;RbBD4#R\u001b9\u0004M3CN4#:m|x9;0gRbR\u0010=Z=TBD=\b53O\u0004a=;:bR\u0010KP4\u001b:\u001046CNMPBgR\u0002^\u001bA46>\u00124#:)9;BZ[3:\u0010=Qz[\u0018=QC\u00109QBDCx0D5sR\u0010K34wBY0gR\u00104#:)9?R\u0010M3:\u00104\u0004K.9\u0002>\u00124]4#}p[PBD=Q:\u00104\u00021l/\u0004021313465r789?:\u0010<\u0012=?>s7@=A134#BEG/W7\"7%IxR\u0010=laG9;V60DBY0gR)9?R\u00104o\\oM3CN0DV:\u00104JRN:\u00100Y4#>\b9;B^Tn\u0011KP4#CN4FR\u001046V)KP5302XZM346C\u0004KP9\u0002>\u00124L\u0018464#5+CNKP=\u0002|w5+R\u0010=hL\u00184`4#U$46VJR\u00100Y>\b4\"D6 ;^r/T=\u0002|\u00114#>\u00124#:\u0002ciR\u0010KP46CN4`R\u001046V)KP530YXAM346C9?:\u00104r53=QR~>\u00124#:Nf\u000f4#vlV#0Y4#5AR\u0002^hTCN0D53O8R\u0010KP46CN4`R\u001046V)KP530YXAM346C6ct98XZM34#:Nf+KP9QCR\u0010=~L\u00184TV6=Q\\l[.9?:\u00104\u00021`|w0DR\u0010Kr4\u00029;VJKr[P0D46V64\u0004=Qad/H7\"7\u000f^pn\u0011KP4J:\u00104W0DCx9~53464\u00021lR\u0010=0D5.1p4#}@R\u0010K34/H7\"7@C\u0004R\u0010=sCN[\u0018464618MP[8R\u0010KP4H:\u00104#RN:\u00100D46>Q9QB\u001d|wKP4658a9QV64\u00021\"|w0gR\u0010KBY9;:\u0010OQ4\\oMPCN0DVH139;R)9;L.9QCN4#C6^+4@[3:\u0010=Q[\u0018=\bCN4@9\u000fCN0D\\l[PBD4hf\u00124#R`4#v`V60D465ZRl\\l46V)K.9Q530DCN\\¡R\u0010=e0Y5P134#}+R\u0010KP4/W7\"7@C6^\n{5lR\u0010KP0DC\u0011CN[\u001846V60gjPVT/H7\"7¢|\u00064TV6=Q5PCN0Y134J:\u0002c3469QV)KhCSR)9?R\u00104W0DC\u0011:\u001046[3z\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro­\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom­\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.V\n£;\b\u0012o{¤]\u0011¥W7\u0016¦h\u00064#5ARN:\u00104§m=Q\\l[P0Y13=QM\n:\u00104#CN465ZR\u00104\u00021\"LAf89Q5e0D5AR\u00104J:\u0010>\b9;Bb9Q5P1%0D5ZR\u00104#:T=\b5PCN4JRH0D5AR\u00104J:\u0010>\b9;Bd:)9;R\u00100D=3^~\u001b9QV)KRN:)9;5PCN0gR\u00100D=\b5s0D5r9Q5l/H7\"70DC\nR\u0010M3:\u0010534\u00021`0D5ZR\u0010=9Hau=\bMp:x130D\\l4653CN0D=\b5.9;B.L\u0018=\u0002}$^\u0004R\u0010K34#:m/W7\"7¨1p4#j.530DR\u00100D=Q5~V69Q5L\u00184\nKP9Q5P13BD4\u00021HCN0D\\l0YBY9?:\u0010BDfZ^mn\u0011KP4\nL\u0018=?}T=\bLpzR)9;0D5P4\u00021%\\s96f8K.9\u0002>\u00124~\\l=Q:\u00104R\u0010K.9;5\"a=\bMp:~130D\\l4653CN0Y=Q5PC6^¥]BDBmR\u0010KP4~L\u0018=?}p46C9?:\u00104F0D53CN4#:NR\u00104\u00021\"0D5AR\u0010=l=Q5P4~¤H©6zRN:\u0010464sD\u0002ª?ci9Q5%0D5.1p4#}p0D5POlCSRN:\u0010MPVJR\u0010M3:\u00104Ha=;:\\FMPBgR\u00100gz130D\\l465PCN0D=Q5.9QB\u00061P9?R)9p^+KP4659%XZMP4#:Nf+0YCoCNMPLP\\l0gRNR\u00104\u00021$ct4\u00029QV)KRN:)9;5PCN0gR\u00100D=\b5\u000f=Qa\u001bR\u0010KP4`XZM34#:Nf\"0DCTjP:\u0010CSRWV6=\b5A>\u00124#:NR\u0010461%0D5ZR\u0010=h9sau=\bM3:~1p0D\\l4653zCN0D=Q5.9QBi[\u0018=\b0D5ZR]9Q5P1rR\u0010K34WL\u0018=\u0002}34#Cx0Y5rR\u0010KP4\u0004¤H©6z«RN:\u00104#4TR\u0010K.9?RwV6=\b5ZR)9Q0D5rR\u0010KP46CN4[\u0018=Q0D5AR\u0010CT9;:\u00104R\u0010KP465\"au=\bMP5P1i^Tn\u0011KP4~/H7\"7@CT9;:\u00104F:)9;5P<\u00124618LAfhR\u0010KP45AMP\\`zL\u00184J:H=Qa\nR\u0010KP=\bCN4oL\u0018=?}p46CW9Q5.18R\u0010K3460g:V6=Q:N:\u001046CN[\u0018=Q5.1p0Y53O@CN=Q5PO\bC9?:\u00104oR)9Q<\b4659;C\nCN0D\\l0DBY9;:\u0006CN=\b5POQC6^\u001b¥]a2R\u00104#:tR\u0010KP4wV\u00029Q5P130Y1P9?R\u00104w/H7\"7@C\n9;:\u00104wCN46BD46V#R\u00104\u00021$cZ|\u00114M3CN4lV6=\b\\l\\l=Q5eR\u00104#VJK35P=\bBD=QOQfZcdR\u0010K34`a=;:N|x9;:)19;BDO\b=Q:\u00100gR\u0010K3\\8cdR\u0010=80Y134#5AR\u00100ga2fR\u0010K34W\\l=\bCSRwCN0D\\l0DBY9;:\u0004CN=\b53O3^+4\u0011K.9\u0002>\u00124\u00060Y\\l[3BD46\\l465ZR\u00104\u00021WR\u0010KP0DC\u001b1346CN0DOQ5l9Q5P1o13=Q5P4x4#}p[\u00184#:\u00100D\\l465ZR\u0010Cm=Q5l9\\FMPCN0DV\u0011139;R)9;L.9QCN4xKP9\u0002>p0D53OW; \u0012ªQW[30D46V646Cb=;a\u0018/H7\"7¬1p4#:\u00100D>\u00124\u00021Hau:\u0010=\b\\kQ­\b­\\FMPCN0DV\u00029;B\u001b[30D46V646C6^hn\u0011KP4`:\u00104#CNMPBgR\u0010C~9;:\u00104l4653V6=\bMp:)9QO\b0D53O3^@n\u0011KP4`0D5.1p4#}p0D5POR\u00104#VJK35P0YXZMP4~[3:\u0010MP534\u00021\"=\bMpRW\\l=\bCSRW=Qa\u001bR\u0010KP4`1p0DCNCN0Y\\l0DBY9;:CN=Q5POQC6cm9;5.18R\u0010KP4CSfpCSR\u001046\\®MPCN461W=\b5PBgfT=\b5P4\u001bCN46>\u00124#5AR\u0010KT=QapR\u0010KP4bR\u00100D\\l4tau=Q:m9Q59\u0002>\u00124#:)9;O\b4\u0006\\FMPCN0DVXZM34#:NfZ^n\u0011K34`:\u001046CSR~=QaxR\u0010K34l[.9Q[\u00184J:F0DC=Q:\u0010O\b9Q530Y¯#4\u000219QCa=QBYBD=\u0002|wC6^8+4l1346CNVJ:\u00100YL\u00184R\u0010K34wL.9QV)<AOQ:\u0010=QMP5.1`=;ad\\FMPCN0DVx:\u00104#RN:\u00100D46>Q9QB\u00189;5.1`KP0Y1313465r789?:\u0010<\u0012=?>o\\l=A134#B0D5+p46VJR\u00100Y=Q5Z^`{\u000b5\u000fA46V#R\u00100D=\b5+r|\u00114o[3:\u0010=?>A021p4FR\u0010K34F:\u001046[p:\u001046CN465ZR)9;R\u00100D=\b5\u000f=Qa\\FMPCN0DV@LZfK3021313465789;:\u0010<\u0012=?>\\l=A134#B^n\u0011KP4\"1p46CN0DO\b5=;aWR\u0010KP4@0Y5P134#}CSRN:\u0010M3V#R\u0010M3:\u001040DCWOQ0Y>\b465\"0D5eCN4#V#R\u00100D=\b5e°3^],4~1346CNVJ:\u00100YL\u00184F=\bMp:T4#}p[\u00184#:\u00100D\\l465ZR1p46CN0DO\b5+9Q5.1%:\u001046[\u0018=Q:NRHR\u0010K34F:\u001046CNM3BgR\u0010CH0Y5,p46VJR\u00100Y=Q5ªr9Q5.1%|\u00114`V#=\b5PV#BYMP1340D5%A46V#R\u00100D=\b5@ p^\n2.BACKGR OUNDn\u0011K34\nj3:\u0010CSRbCSR\u001046[~0Y5~\\oM3CN0DV\u001b:\u00104#RN:\u00100D46>Q9QBA0DCdR\u0010=\u0004[3:\u0010=?>A021p4\u00069]CNMP0gR)9QL3BD4\u0006\\l=A134#BR\u0010=8:\u00104#[3:\u001046CN465ZRF4\u00029QV)K[P0D46V#4h=;a\u0004\\FMPCN0DV\b^%¥¬CNM30gR)9QLPBD4s\\l=A134#B\u0006CNKP=QMPBY153=QR\n=\b53BDfoL\u00184wBY=\u0002fA9;B3R\u0010=H=;:\u00100YOQ0D5.9QB$139;R)9WL3M3R\u00069QBDCN=HaG9QV#0YBD0gR)9;R\u00104]:\u00104#RN:\u00100D46>Q9QBG^/\u0004=\u0002|\u0014R\u0010=e\\l=A1346B\u00069eCN=Q5POe0Y59\"\\oM3CN0DV@139;R)9QLP9QCN4rR\u0010=%a9QV60DBD0gR)9;R\u00104@:\u00104JzRN:\u00100D46>Q9QB2±e7@=\bCSR]R\u001046V)KP5P0YXZMP4#C][p:\u0010=\b[\u0018=QCN4\u00021@0D5@R\u0010KP4HBD0gR\u00104#:)9?R\u0010M3:\u00104\\l=A1346B\u001d9CN=Q5POe9QCo9\"CSRN:\u00100Y53O3c\n9Q5P1+R\u0010KP465ic\u001b9Q[3[PBgf+RN:)9Q130gR\u00100D=\b5.9;B\u0011R\u00104#}AR:\u00104#RN:\u00100D46>\b9;BR\u00104#VJK35P0YXZMP46CH ;\u0006 ?\n  ?\u0011 ª?^_TK30Y9QC\u00064JR\u00119;B^w ?\u0018MPCN4\u00021`=Q5PBgfoR\u0010Kp:\u0010464wBD4#RNR\u00104#:\u0010C\u0006=;a\u001d9QBD[PKP9QL\u00184#R\u001bR\u0010=VJKP9;:)9QVJzR\u00104J:\u00100Y¯#4`R\u0010KP4`:\u001046BY9;R\u00100D=Q5PCNKP0D[L\u00184#R|\u0011464#5+V6=Q5PCN46V6MpR\u00100D>\u00124l[P0gR\u0010V)KP46C6²`³ F´Dc ³ µo´9;5.1r³g$´DcQ:\u001046[3:\u001046CN4#5AR\u00100D53OR\u0010K34xCN0gR\u0010M.9;R\u00100D=Q5s|wK34#:\u00104]9H5P=QR\u00104w0DC\n9QL\u0018=?>\u00124\bcZL\u00184JzBD=\u0002|=;:\nR\u0010K34wC\u00109Q\\l4]9QCtR\u0010KP4w[3:\u00104#>p0D=QMPC\u001b5P=QR\u00104#C6^t\n9;VJKs=;a\u0018R\u0010KP4H\u0002¶QCN=Q5POQC0D5R\u0010KP4h1P9?R)9QLP9QCN489;5.1XZMP4#:\u00100D46Co0Y5R\u0010K3460g:`4#}p[\u00184#:\u00100D\\l465ZR~|\u00114#:\u00104hV6=\b5pz>\b4#:NR\u00104\u00021l0D5AR\u0010=F9CSRN:\u00100Y53OFV#=\b\\l[\u0018=\bCN461r=;adR\u0010KP4wR\u0010K3:\u00104#4TBD4#RNR\u00104#:\u0010C6^\u001bnd=~CN4\u00029?:\u0010VJKR\u0010K34\u000f\\l=QCSR8CN0D\\l0DBY9;:\"CN=Q5PO3c\u0004R\u0010KP4#f46\\l[PBD=\u0002fZ4\u000219;[P[3:\u0010=\u0002}p0D\\s9;R\u00104%CSRN:\u00100D5PO\\s9?R\u0010VJK30D5POH[3:\u0010=Q[\u0018=\bCN4\u00021s0D5%D6;cA|wKP0DV)Kh9;BDBY=\u0002|\u00064\u00021sau=Q:x53=QR\u00104wRN:)9Q53CN[\u0018=\bCN0gzR\u00100D=Q5\u001dc\u001d1346BD4#R\u00100D=Q5\u001dc\u001d9Q5P1%0D53CN4#:NR\u00100D=\b5\u001d^FA4\u00029;:\u0010V)KeCSRN:\u00100Y53OhV69Q5eL\u00184F\\s9?R\u0010VJK34\u00021|w0gR\u0010K%9;5Af@[\u0018=Q:NR\u00100D=\b5\"=;atR\u0010KP4\\l46BD=A1pfZcp:)9;R\u0010K34#:\u0004R\u0010KP9Q5oSMPCSR]R\u0010KP4L\u001846O\b0D5pz530D5PO3^7@V#W9QL4#Rl9QBG^\u001eg#~ ?\u00049QBDCN=+4#\\l[PBD=\u0002fZ4\u000219Q59Q[3[3:\u0010=\u0002}p0D\\s9;R\u00104CSRN:\u00100D53Ol\\s9;R\u0010V)KP0D5PO`R\u001046V)KP530YXAM34\b^xn\u0011K34#fh0Y53V6BDM.1p4\u00021@R\u001046\\l[\u0018=3c.13Mp:)9;R\u00100D=\b5ic9;C@9;5\\s9;R\u0010V)KP0D53O+=\b[pR\u00100D=\b5au=Q:@9\b13>Q9Q53V64\u00021XAM34#:\u00100D46C6^\u000e¥\u0004Cr9Q[3[3:\u0010=\u0002}p0Dz\\s9?R\u00104oCSRN:\u00100D5PO@\\s9;R\u0010V)KP0D5POr0DCF9rCSR)9Q5P1P9;:)1\u000f9Q[3[PBD0DV\u00029;R\u00100D=\b5+=Qaw1Af35P9Q\\l0DV[p:\u0010=\bO;:)9Q\\l\\l0D5POpc.R\u0010KP4HR\u00100D\\l4HR)9Q<\u0012465\"0DCT9s\\s9?RNR\u00104#:]a=Q:TCN=\b\\l4V6=Q5PV64#:\u00105i^\u0000\u0002\u0001\u0004\u0003\u0011\u0005Q\u0007\n\t\u000b\u0001\u0006\f\u000e\r\u000f\t\u0010\u0003\u0011\u0003\u0011\u0005Z\u0001¨\u0013\u0016\u0015$\u0017Q\u0019\u001b\u001a\u001d\u001c\u001e\u0013\u0016\u001a\u001b\u0003\u0011\u0005Z\u001f \"!#\u001a$\u0017%\u0013'&d Q\t)(+*,\u0005$-J\u0017Q\t)\u0005.\u001c\u0018\u0015\u0018\u001fn\u0011K34h9;[P[p:\u0010=?}p0D\\s9;R\u00104s\\s9?R\u0010VJK30Y53O8=\b5;°\u0012QeCN=Q5PO\bC~R\u0010=Z=\b<ZsCN46V6=Q5.13C0D5R\u0010KP4#0D:sR\u001046CSR\u0002^¥yaG9QCSR\u00104J:@CSR)9?R\u00104\u000f\\s9?R\u0010VJK30Y53O9QBDOQ=Q:\u00100gR\u0010KP\\·g\u0002;T|x9QC0D\\l[PBD46\\l4#5AR\u0010461ic]LPMpR@0gR%1p=Z46C85P=;R\"1p0YCNVJ:\u00100Y\\l0D5P9;R\u00104,9;C@|\u00114#BYB9;C\"1pfAz5P9Q\\l0DV@[p:\u0010=\bOQ:)9;\\l\\l0D5PO\u000f13MP4rR\u0010=\u000fR\u0010K348130gU$4#:\u00104#5ARl130DCSR)9Q53V648\\l4\u00029;CNM3:\u00104=;a\u0006R\u0010KP4`9QBDO\b=;:\u00100DR\u0010K3\\8^ln\u0011CN4653O+ ­?\u0004  ;\n[3:\u0010=Q[\u0018=\bCN4\u00021\u000f9h[P0gR\u0010V)K\u000f[p:\u0010=QjPBY4o4653zV6=A1p0D5PO\u000fau=Q:hXZMP4#:\u00100D46C`9Q5.19Q553z«5P=;R\u00104@0D5.1p4#}p0D5POe\\l4#R\u0010KP=A1a=;:r9Q[3z[p:\u0010=?}p0D\\s9;R\u00104~\\s9;R\u0010V)KP0D53Oh0D5eCNMPL3z«BD0D5P4\u00029?:WR\u00100D\\l4\b^~n\u0011KP4FCNKP=;:NR\u00104#:53z«5P=;R\u00104|w0DBDB\u0018\\s9;R\u0010V)KoR\u0010KP4w0D5P[3M3R\u0006XZMP4#:NfF0D5sR\u0010K34w[3:\u001046CN4653V64]=Qa$:)9Q5P13=\b\\k4#:N:\u0010=;:\u0010C6c|wK30YBD4~R\u0010KP4~BY=Q5POQ4#:H53z«53=QR\u00104H|w0DBYBba9?>\b=Q:WR\u0010K34~=Q5P46C\u0004|w0DR\u0010K%\\l=;:\u00104o9;V6V6M3z:)9?R\u001048\\l46BD=A130D46C6^µW=\u0002|w5P0D484JRs9QBG^¸ °;~ ª\u0002][3:\u0010=\b[\u0018=QCN4\u00021R\u0010=+MPCN4h=\b53BDf0D5ZR\u00104#:\u0010>Q9QBd0D5pa=Q:\u0010\\s9?R\u00100D=\b5%=QatR\u0010KP4CN=Q5PO\bC]R\u0010=`aG9;V60DBY0gR)9?R\u00104o:\u00104#RN:\u00100D46>Q9QBG^\u0004n\u0011KP4CN=Q5PO\bCw|\u00114J:\u00104~V#=\b5A>\u00124#:NR\u00104\u00021sR\u0010=l9Q5@0Y5ZR\u00104#:\u0010>Q9QB$:\u001046[p:\u001046CN465ZR)9;R\u00100D=\b5@=Qat\\l=\b53=Qz[3KP=\b530DVo\\l46BD=A130D46C9;5.18R\u0010KP4#5ea2:)9QO\b\\l4#5AR\u0010461e0D5ZR\u0010=hBD4653OQR\u0010Kpz5%CNM3LPCN46V#zR\u00100D=\b53C\nV69QBDBD4\u00021l53z«OQ:)9;\\lC6^bTCN0D5POH5pz«OQ:)9Q\\lC\u001b9;C¹\u0010\\FMPCN0DV\u00029;B3|\u0011=;:)13CNºpcZR\u0010KP49;M3R\u0010KP=;:\u0010C8V6=Q5PCSRN:\u0010MPVJR\u00104\u00021®\\FMPCN0DV\u000f1P9;R)9;L.9;CN4+MPCN0D53OR\u0010K34eR\u00104#}ARNz«L.9;CN4\u00021ic378¥\u0004¤\u001bn¬0D53a=;:\u0010\\s9;R\u00100D=\b5:\u00104JRN:\u00100Y4#>\b9;B\u0004CSfpCSR\u001046\\8^¸n\u0011KP4Jf9;5.9QBgfp¯64\u00021R\u0010KP4BD4653OQR\u0010K8=;ab53z«OQ:)9;\\»9Q5P1hR\u0010KP4W1346O;:\u0010464H=Qat9;V6V6Mp:)9QV#fh0D5@:\u001046[3:\u00104#CN465ZR\u00100Y53OR\u0010K34x0Y5ZR\u00104#:\u0010>Q9QBDCtLZf~CN0Y\\FMPBY9;R\u0010461oXZMP4#:\u00100D46C\u001bMPCN0D53OW5P=;:\u0010\\s9QBD0D¯64\u00021o[p:\u001046V60DCN0D=\b59;5.1h53=Q:\u0010\\s9;BY0D¯6461h:\u001046V\u00029;BYBG^nd=hVJKP9;:)9;V#R\u00104#:\u00100D¯64`R\u0010KP4F4#:N:\u0010=Q:~=;a\u0011KAMP\\l\\l4\u00021853=QR\u001046C6ci:\u001046CN4\u00029?:\u0010VJK34#:\u0010C~9;:\u0010453=?|®RN:Nfp0D5PO`R\u0010=l\\l=A134#BiR\u0010K346CN40Y5%9`[p:\u0010=\bLP9QLP0DBD0DCSR\u00100DVF\\s9;5P5P4J:\u0002^x§b:\u0010=\bL3z9;LP0DBD0YCSR\u00100DV\"\\l=A1p46BDCoa=;:sCN0Y\\l0DBY9;:\u00100gRfCN4\u00029;:\u0010V)K0DCoj3:\u0010CSRl4#}p[\u0018=\bM35.13461LZf§m=Q5AR\u001044#RT9QBG^H ;;t9;5.1@R\u0010KP4#5\"9Q[3[PBD0D4\u00021@R\u0010=l\\oM3CN0YVHLZfh§t0YV)<\u00124653CF Z#^¥/T0Y1P1p465F789;:\u0010<\u0012=?>~7@=A1p46B$EG/H7\"7%I\u001d1p46CNV#:\u00100DL\u001846Ct9]13=\bM3LPBgfHCSR\u0010=ZV)K.9;CSzR\u00100DVW[p:\u0010=ZV646CNCw0D5h|wKP0DV)K@=Q5PBgfsR\u0010KP4WV6Mp:N:\u0010465ZRxCSR)9;R\u001049?U$46V#R\u0010CxR\u0010KP4\u0004VJK3=\b0DV64=;aTR\u0010KP4h5P4#}ARlCSR)9?R\u00104\b^n\u0011KP4#f9?:\u001048CSR)9;R\u00100DCSR\u00100DV\u00029QB]\\l=A134#BYC`au=Q:sCN4\u0002XZM34653zR\u00100Y9QBm1P9;R)9oR\u0010KP9;R\u0004K.9\u0002>\u00124L\u001846465\"M3CN4\u000218CNMPV#V646CNCSaM3BDBDf80Y5\"\\s9;5Zf@\\s9;VJK30Y534BD4\u00029?:\u00105P0D5PO\u000f9Q[3[PBD0DV\u00029;R\u00100D=\b53C6c\u00064#CN[\u001846V60Y9QBDBgfa=;:`CN[\u00184646V)K:\u001046V6=QO\b5P0gR\u00100D=\b5®g\bJg\u0002?^½¼\u000646V\u00029;MPCN4e=Qa0gR\u0010Cs[\u0018=QR\u0010465ZR\u00100Y9QBT9QL30YBD0gR\u000bfR\u0010=V\u00029Q[pR\u0010M3:\u001048R\u0010K34\"V6=\b\\`z\\l=Q5P5P4#CNC\u0011=QamRN:)9;0D5P0D5POFCN4#R]9Q5P1s|\u00119;:)1h=;U%5P=Q0YCN4QcP0gRwK.9;CxL\u001846465sRN:\u00100D4\u000210D5+\\FMPCN0DVF:\u00104JRN:\u00100Y4#>\b9;Bt:\u00104#V6465ZR\u0010Bgfg\u0002?\u0004D6 ;^s{\u000b5g?\u0002«cdR\u0010KP4oR\u001046V)KP530YXAM346C9?:\u00104h9Q1P9;[3R\u00104\u00021\u000fa2:\u0010=\b\\9QMpR\u0010=\b\\s9?R\u00100YVlCN[\u00184#46VJK\u000f:\u001046V6=\bOQ5P0gR\u00100D=\b5,R\u0010=\"V#:\u0010469;R\u00104r9\\l46BD=A1AfCN[\u0018=QRNR\u00100D53OCSfpCSR\u001046\\q0D5R\u0010KP4e\\oM3CN0YV69QBW1p=\b\\s9;0Y5i^¨n\u0011KP4\u000f9QM3zR\u0010K3=Q:\u0010Cr9QBDCN=[\u0018=\b0D5ZR\u00104\u00021=QM3RsR\u0010K.9?Rs=QR\u0010K34#:sR\u0010K.9Q51pfp5.9;\\l0YVh[3:\u0010=QOQ:)9Q\\`z\\l0D5POra=;:o\\l46BD=A1p0YVh7@{¤~c\u001d=QR\u0010K34#:~V6=\b\\l[P9;:\u00100DCN=\b5+R\u00104#VJK35P0YXZMP46CH|wKP0DV)KKP9\b1@L\u0018464#58CNMPV6V#46CNCSaM3BYBgf89Q[P[3BD0Y461hR\u0010=l=QR\u0010K34#:wa=Q:\u0010\\lCw=;a\u001b9QM.1p0D=lCN0YOQ5.9;BM35.134J:\u0010CSR)9Q5.1p0D5PO3c\n[.9?:NR\u00100DV6MPBY9;:\u0010BgfR\u0010KP=QCN4@R\u0010=Z=\bBDC`a=;:rCN[\u00184646V)K:\u001046V#=\bO\b530gzR\u00100D=\b5icxCNKP=QMPBY19;BDCN=L\u0018484#}p[PBD=Q:\u00104610D5\u001e7@{\u000b¤:\u001046CN4\u00029?:\u0010VJKi^¸pK30Da2:\u00100D54#R9;B^®g\u0002 ?]\\l=A1346BDCoR\u0010K34rR\u0010K346\\l46Co=;aWCN=Q5PO\bCs9;C`/H7\"7@C6c\u00119Q5P19;[P[3BDf9\"a=;:N|x9;:)19QBDO\b=;:\u00100DR\u0010K3\\¾R\u0010=+\\s9?R\u0010VJK9eXZMP4#:Nf9;5.1/H7\"7@C6^n\u0011KP4#0D:R\u001046V)KP530YXAM34W0DCwCNKP=\u0002|w5hR\u0010=`L\u00184H>\b4#:Nfs4#U$46V#R\u00100D>\u00124H0gamR\u0010K34WKAMP\\l\\l4\u00021l53=QR\u001046CKP9?>\b4WO\b=Z=A1\"XZM.9;BD0DRfZ^\n3.REPRESENT ATION OFTHEMES BYHID­\nDEN MARK OVMODELS{5\u000fR\u0010KP0DCHCN46VJR\u00100Y=Q5\u001dc\u001d|\u00114s134#CNV#:\u00100DL\u00184lKP=\u0002|R\u0010=@:\u001046[p:\u001046CN465ZR~9rR\u0010K346\\l4oLZf\u000f9/\u00040213134658789;:\u0010<\u0012=?>h7@=A134#B^{5`9H¿%À\bÁ\u0010ÂQÃQÄ`ÅdÆpÀ\bÇÈ3cQ|\u00064xK.9\u0002>\u00124x9wj.530DR\u00104\u0006V6=\bBDBD46V#R\u00100D=\b5rÉ\"=QadÊJËGÀ\bËÌ#Ê)^\n¥\u0011R4\u00029;VJK8R\u00100D\\l4WCSR\u00104#[\u001dcPR\u0010K34CSf3CSR\u00104#\\»460gR\u0010KP4#:\u0004CSR)96f3C]0D5@R\u0010KP4HC\u00109;\\l4CSR)9;R\u00104=Q:V)K.9;5PO\b4#CtR\u0010=W9W130gU$4#:\u0010465ZRtCSR)9?R\u00104\b^\u001bn\u0011KP4x[p:\u0010=\bL.9;LP0DBD0gR\u000bf~=Qa\u0018R\u0010K34\u0011CSfpCSR\u001046\\½9;R9wR\u00100Y\\l4\u001bV)K.9Q53O\b0D5PO]au:\u0010=Q\\Í=\b534\u0006CSR)9;R\u00104\nR\u0010=T9;5P=QR\u0010K34#:tCSR)9;R\u00104Qc\b=;:tCSR)9\u0002fp0D5PO]0D5R\u0010K34\nC\u00109;\\l4\nCSR)9?R\u00104\bc;V\u00029Q5L\u00184\n:\u00104#[3:\u001046CN465ZR\u00104\u00021WLZfW9wRN:)9;5PCN0gR\u00100D=\b5~[3:\u0010=\bLP9QL30YBD0gR\u000bf\\s9?RN:\u00100D}~Îo^mn\u0011KP4\n[3:\u0010=\bLP9QL30YBD0gR\u000bf~=;a.O\b=Q0D5PO\u0004a2:\u0010=\b\\=Q5P4\u0006CSR)9?R\u00104\u0006R\u0010=W9;5P=;R\u0010KP4#:CSR)9?R\u00104l13=Z46CT5P=QRH1346[\u0018465P1\"=\b5%R\u00100Y\\l4Qc\u001d53=Q:H=Q5e|wK34#:\u00104oR\u0010K34FCSfpCSR\u001046\\¢0YC[p:\u001046>A0Y=QMPCwR\u0010=lR\u0010KP4V6M3:N:\u00104#5AR\u0004CSR)9;R\u00104Q^Tn\u0011K30DCT0DCT<A5P=\u0002|w5%9;C\u0004R\u0010KP4o¿eÀQÁ\u0010ÂQÃ\bÄÏ\u0011ÁSÃJÐ3Ì#ÁJËÑ;^n\u0011K34l130g:\u001046V#R\u0010461eO;:)9Q[PK\u000f0D5,m0DO\bMp:\u00104@o:\u001046[p:\u001046CN465ZR\u0010C98789?:\u0010<\u0012=?>\"\\l=A1346B=;ax9hCNV69QBY9;:~[.9;CNC\u00109QOQ4l=Qax9lR\u0010KP4#\\l4\b^oAR)9;R\u00104#CH9;:\u00104`53=QR\u00104oRN:)9;5PCN0gR\u00100D=\b53C6c4\u00029;VJK@=Qab|wKP0DV)K80DCx:\u001046[3:\u001046CN4#5AR\u0010461@LZf@9;580D5ZR\u00104#:\u0010>\b9;Bd9;5.1h9Q58{ST{:)9;R\u00100D=pcÒ\n0D5ZR\u00104#:\u0010>Q9QBGci{NT{:)9?R\u00100D=3ÓH^F{\u000b5ZR\u00104#:\u0010>Q9QBm0DCWR\u0010KP4o130gU$4#:\u00104653V64o0D5\u000f[P0gR\u0010VJK%L\u00184#zR|\u001146465HR|\u0011=]V6=\b53CN46V6M3R\u00100D>\u00124\n5P=;R\u001046C6^bn\u0011K34\u0004ÇÈPËÌ#ÁxÃ\bÈPÊJÌ6Ë$ÇÈ.ËÌ#Á)Ä?À\u0012Ô\u0012EÕZÖHÕ#×.I0DCdR\u0010K34\u0011130gU$4#:\u00104#5PV64\u001bL\u00184#R\u000b|\u000646465R\u0010KP4\n=\b5PCN4JRb=Qa35P=;R\u001046CmØs9Q5P1~ØbÙs\bc;|wKP0DV)K0DC\u0011R\u0010KP4W13M3:)9?R\u00100D=\b5r=Qab53=QR\u00104WØbc39Q5P1@ÕZÖHÕ\u0012Ú;ÛpÜÝÞ~0DC\u0011R\u0010K34\u0004:)9;R\u00100D=`L\u00184JR\u000b|\u00114#465S10.6\n1.00.2\n0.6\n0.2\n0.20.8S2S4S3\nS50.4ßwàá\u0018âdã?ä,å.æçéè\u000eê3ã;ë\u0018ìpíè\u000eìiîdäpïR|\u0011=HV6=Q5PCN46V#M3R\u00100D>\u00124wÕZÖHÕP^b{5lm0DO\bM3:\u00104HQc\u0012R\u0010KP4x5AMP\\l4J:\u00100YV69QBp>\b9;BYM34xL\u001846BD=\u0002|469QV)K1p0D:\u00104#V#R\u00104\u000214\u000213OQ4@0D5.1p0DV\u00029;R\u001046CFRN:)9Q5PCN0gR\u00100D=\b5[3:\u0010=QL.9;LP0DBD0DR\u00100D46C6^T53BDfRN:)9;5PCN0gR\u00100D=\b53C\u0004|w0gR\u0010K%5P=\b5pz«¯64#:\u0010=l[3:\u0010=QL.9;LP0DBD0DR\u00100D46CW9;:\u00104~CNKP=\u0002|w5\"0D58R\u0010KP4Hj.O;zMp:\u00104\b^KP0DBD4F4\u00029;VJK\"\\FMPCN0DV60Y9Q5\"CN0D53O\bCW9`CN=Q5POsCNBD0DO\bKZR\u0010Bgf%130gU$4#:\u0010465ZR\u0010BgfZc.V69QCNM.9;BCN0D53O\b4#:\u0010CH\\s9;<\u00124~R\u0010K34`1p0DU$4J:\u0010465PV64F46>\u0012465%\\l=;:\u00104o=\bLA>A0D=\bM3C6^~+4o9;CNCNMP\\l4R\u0010KP9;RiR\u0010KP4\u0006789;:\u0010<\u0012=?>\u0004VJKP9Q0D5V#:\u00104\u00029;R\u0010461~1p0g:\u001046V#R\u0010BgfWau:\u0010=Q\\\u000e9x\\FMPCN0DVbR\u0010KP46\\l4wE9CSR)9;5.139;:)1oR\u0010KP4#\\l4;Ib0DC\u00069/\u00040Y1P134#5s789;:\u0010<\u0012=?>s7@=A1p46BE«ðT¿@¿8I)cA9Q5P1oR\u0010KP4CSR)9?R\u001046C\u00064J}pRN:)9;V#R\u00104\u00021`au:\u0010=Q\\\u00149HKAMP\\l\\l461F5P=;R\u001046C\u00119?:\u00104\u0004=QLPCN4#:\u0010>Q9QL3BD4\u0004CSR)9?R\u001046C6^n\u0011K34x[3:\u0010=QL.9QL30DBY0gRfo1p0DCSRN:\u00100YL3M3R\u00100D=\b5F=Qa.R\u0010K34\u0011=\bL3CN4#:\u0010>Q9QLPBD4xCSR)9?R\u001046Ct=;>\b4#:bR\u0010KP4K30Y1P134#5+CSR)9;R\u00104#C~0DCL.9;CN4\u00021+=Q5\u000fR\u000b|\u0006=8KP4#M3:\u00100DCSR\u00100DV`R)9QLPBD46C~9;5.1+L3MP0DBgR=\b5R\u0010K34xñPfhD6 ;^\u001b{\u000b5ZR\u00104#:\u0010>Q9QBp0DC\n:\u001046[p:\u001046CN465ZR\u00104\u00021`LZf`9;5l0D5AR\u00104#O\b4#:\u00119QCtR\u0010KP4#:\u00104]9;:\u00104\u0002hCN46\\l0gzR\u0010=\b5346CW|w0gR\u0010KP0D59Q5+=ZV#R)9\u0002>\u00124\b^s{NT{«:)9;R\u00100D=80DCH:\u001046V6=;:)134\u00021,9;C~9Q5ñP=\u00129?R\u0004>Q9QBDM34\b^\n4.ANINDEX MECHANISM BASED ONR*­\nTREE¥]C@L\u0018=;R\u0010K\u001eR\u0010K34%/H7\"7@C\"9;5.1R\u0010KP4eXZMP4#:\u00100D46C@9;:\u00104\u000fVJKP9;:)9QVJR\u00104#:\u00100D¯64\u00021\u001eLAfCSR)9?R\u00104\u0004RN:)9;5PCN0gR\u00100D=\b5PC6c30gR\u00110DCx0D5ZR\u0010MP0gR\u00100D>\u00124]R\u0010KP9;R\nR\u0010KP4]\\l=Q:\u00104\u0004R\u0010K34]RN:)9;5PCN0gR\u00100D=\b53C=;aW9%XZMP4J:\u00100Y4#Co\\s9;R\u0010V)K,R\u0010KP4sRN:)9Q53CN0gR\u00100Y=Q5PC`=;aW9;5/H7\"7\u000fcmR\u0010K34hK30YOQKP4#:[p:\u0010=\bLP9QLP0DBD0gR\u000bf,R\u0010KP9;RFR\u0010KP0DCF/W7\"7òO\b4#5P4#:)9?R\u001046CoR\u0010KP4rXZMP4J:NfZ^\u000f/\u0004=\u0002|\u001146>\u00124J:\u0002c9;CT=QLPCN4#:\u0010>Q9QL3BY4~CSR)9;R\u00104#CT0D5e9rXZM34#:Nf@0D53V6BDM.134R\u0010KP44#:N:\u0010=;:W\\s9Q134LZf\"9CN0D53O\b4#:\u0002c.9FKP0Y1313465hCSR)9;R\u00104H0D589;5@/H7\"7'V\u00029;5P53=QR]130g:\u001046VJR\u0010BDfh\\s9;R\u0010V)Kh9Q5=QLPCN4#:\u0010>Q9QL3BD4@CSR)9;R\u00104Q^¼\n4#a=;:\u001048\\s9Q<A0D5PO%R\u0010KP4r\\s9;R\u0010V)K\u001dc\u001bKP0Y1313465CSR)9;R\u00104#CCNK3=\bM3B21L\u00184rV6=\b5A>\u00124J:NR\u00104\u00021,0D5AR\u0010=\u000f98a=;:\u0010\\s9;RoR\u0010KP9;Ro0Y53V6BDM.1p46CFR\u0010K34h4#:N:\u0010=;:\u0002^T534WV6=QMPBY1h460gR\u0010K34#:wa=\bBY1hR\u0010K30DCx4#:N:\u0010=Q:]0D5ZR\u0010=FR\u0010KP4W1P9;R)9:\u001046[3:\u00104#CN465ZR)9;R\u00100D=\b5=;:b0D5ZR\u0010=]R\u0010K34\u0006XZMP4J:NfZ^\u001d+4\nV)KP=Z=QCN4\u0006R\u0010KP4\u001bjP:\u0010CSR\u0002c?a=;:t4\u00029QCN4\n=Qa.V#=A130D5POp^tn\u0011KP4BY9?RNR\u00104#:~\\s9\u0002f%L\u00184o[3:\u00104#au4#:N:\u00104\u00021\u000f0ga\u0011|\u00064l4#}p[\u001846V#R1p0gU$4#:\u0010465ZR~XZMP4#:Nf%V6BY9;CNCN46C|w0gR\u0010K\"1p0gU$4#:\u0010465ZRw4#:N:\u0010=Q:][p:\u0010=\bL.9;LP0DBD0gR\u00100Y4#C6^\n4.1 TheRepr esentation ofaTransition inHMMs7@V#W9QL4#R`9QBG^ \u0002w5P=;R\u00104\u00021+R\u0010KP4#:\u00104s|\u00114J:\u00104rau=\bM3:oR\u000bfp[\u001846C~=Qa]4#:N:\u0010=Q:\u0010Co=\b50D5ZR\u00104#:\u0010>Q9QBi0D5%9`KpM3\\l\\l4\u00021s5P=QR\u00104#C6²xó\u001bô)ÐpÀ\bÈPÊ\u0010ÇGÃQÈpõÅtÃ\bö]Ð\u0018ÁNÌ#Ê\u0010Ê\u0010ÇGÃ\bÈAõ\n÷wÌÐ$øÌ6ËÇËÇGÃ\bÈF9Q5P1+ù\u0011öoÇÊ\u0010Ê\u0010ÇGÃ\bÈ3^tb}3[P9Q53CN0Y=Q5l4#:N:\u0010=Q:\u0006=ZV6V6Mp:\u0010C\u0006|wKP465`R\u0010K34wCNMPL3zS46V#R\u0010CWCN0D53OrR\u0010K34FCN\\s9;BYBd0Y5ZR\u00104#:\u0010>Q9QBDC]R\u0010K.9;R\u0004aG9;BYBb|w0gR\u0010KP0D5úw°sR\u0010=r°p^AMPL3zS46V#R\u0010CHR\u0010465P13CWR\u0010=84#}p[P9Q5.1\"R\u0010KP4#CN4`0D5AR\u00104J:\u0010>\b9;BYCCNBD0DO\bKZR\u0010BgfZ^r\u0006=\b\\l[p:\u001046CNCN0D=\b54J:N:\u0010=Q:x=ZV6V#M3:\u0010C\u0011|wKP4#5rR\u0010K34\u0004CNM3LAS46V#RwCN0D5POR\u0010KP4\u0004B29?:\u0010O\b4W0D5ZR\u00104#:\u0010>Q9QBDC6cpBY9;:\u0010O\b4J:R\u0010KP9Q5\u000f°\"=Q:~BD46CNC~R\u0010KP9Q5úw°3^8pMPLZN4#V#R\u0010CHR\u0010465.1%R\u0010=8V6=\b\\l[p:\u001046CNCHR\u0010KP4`0D5pzR\u00104J:\u0010>\b9;BYClCNBD0DO\bKZR\u0010BgfZ^¤\u000446[\u00184JR\u00100DR\u00100D=Q54#:N:\u0010=;:r=ZV6V6M3:\u0010C`|wK3465CNMPLZN46V#R\u0010Cl0D5pzV#=Q:N:\u001046V#R\u0010Bgfs:\u001046[\u00184\u00029?Rw5P=;R\u001046C6^\nT\\l0DCNCN0Y=Q5@4#:N:\u0010=;:w=ZV6V6Mp:\u0010C\u0011|wKP465rCNMPLZN46VJR\u0010C=Q\\l0gR\u00069W53=QR\u00104\b^m{R\n0YC\n130gvlV#MPBgRtR\u0010=H021p465ZR\u00100gaufo:\u00104#[\u00184#R\u00100gR\u00100Y=Q5s9Q5P1o=\b\\l0DCNCN0D=\b54J:N:\u0010=Q:\u0010Ctau:\u0010=\b\\R\u0010K34xKpM3\\l\\l4\u000215P=QR\u00104#C6^tnd=HV\u00029;[3R\u0010M3:\u00104\u0011R\u0010K34x4#}p[.9;5PCN0D=\b5`4J:Nz:\u0010=;:W9;5.18V6=Q\\l[3:\u001046CNCN0D=Q5e4J:N:\u0010=Q:\u0010C6c$9lKP0Y131346580D5ZR\u00104#:\u0010>Q9QB\u001d0YCw:\u001046[p:\u001046CN465ZR\u00104\u00021LZf89Q5%0D5AR\u00104J:\u0010>\b9;Bd:)9Q53O\b4\b^T{\u000b5\"=\bMp:W4#}p[\u00184#:\u00100D\\l465ZR\u0010C6c.|\u00064F=QLPCN4#:\u0010>\b4\u000218R\u0010K.9;R{ST{:)9?R\u00100Y=e9;BYCN=%R\u0010465P13CR\u0010=\"L\u00184hV6=Q\\l[3:\u001046CNCN461,=Q:`4J}3[P9Q5P134\u00021+LZf+CNMPL3zS46V#R\u0010C6^\nn\u0011K.9?Rx0DC6cpCN0Y53O\b4#:\u0010Cx\\s9;<\u00124]4#:N:\u0010=Q:\u0010Cx53=QRx=Q5PBgfl0D5h[30gR\u0010VJKrLPMpRx9QBDCN=0D5\"1pM3:)9?R\u00100Y=Q5\u001d^\u0006nd=`V\u00029;[3R\u0010Mp:\u00104WR\u0010KP0DCw4#:N:\u0010=Q:\u0002c.9`KP0Y1P1p465@{ST{:)9;R\u00100D=`0DCw:\u001046[3z:\u00104#CN465ZR\u00104\u00021FLZf9;5o{NT{«:)9;R\u00100D=\u0004:)9;5POQ4\b^t¥KP0Y1P1p465FCSR)9?R\u00104\u00110DCtCN=T4#}p[.9;5.1p4\u00021R\u0010=`9Fa=;:\u0010\\s9;R\u00049QCûxû8üFý;þ ÿ\u0001\u0000\u0003\u0002\u0005\u0004\u0007\u0006\t\b?ý\u000b\n\u0005\fSü~ÿ\u0001\u0000 ÿ\r\u0000\u0003\u0002\u0005\u0004\u000e\u0006\u000e\b;ý\u000f\n\u0011\u0010\u0012\fû%ü~ýQþ \u0013\u0015\u0014\u0012\u0013\u0016\u0006#ý\u000f\u0002Gÿ\u0001\u0017\u000f\fü~ÿ\u0001\u0000 \u0013\u0015\u0014\u0012\u0013\u0018\u00066ý\u0015\u0002Gÿ\u0001\u0017\u0019\u0010\u0012\u0010|wK34#:\u00104~\\s9;} 0D5ZR\u00104#:\u0010>Q9QBd9Q5.1\"\\l0D5 0D5ZR\u00104#:\u0010>Q9QBb9?:\u00104~R\u0010K34F\\s9?}30D\\FMP\\»9;5.1\\l0D530Y\\FMP\\ 0D5ZR\u00104#:\u0010>Q9QBDCsR\u0010K.9;R@9;:\u00104eV6=Q\\l\\l=\b5PBgf=\bL3CN4#:\u0010>\b9;LPBD4%0D5AR\u00104J:\u0010>\b9;B1p46>A0Y9;R\u00104\u00021\"au:\u0010=Q\\é9QV6V6Mp:)9;R\u00104`0D5ZR\u00104#:\u0010>Q9QBGciR\u0010KP4oK30Y1P134#5e0D5ZR\u00104#:\u0010>Q9QBGc\u001dCN=89;:\u00104\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0001\u0006\f\u000e\r\u000f\t\u0010\u0003\u0011\u0003\u0011\u0005\u0012\u0001\u0014\u0013\u0016\u0015\u0018\u0017\b\u0019\u001b\u001a\u001d\u001c\u001e\u0013\u0016\u001a\u001b\u0003\u0011\u0005\u0012\u001f\u000b \"!#\u001a$\u0017%\u0013'&\u001d \b\t)(+*,\u0005$-)\u0017\b\t)\u0005.\u001c.\u0015$\u001f\\s9?} {NT{«:)9;R\u00100D=9;5.1Í\\l0D5 {NT{«:)9;R\u00100D=3^ ¼\nf\u000e4J}3[P9Q5P130D5PO9K30Y1P134#5CSR)9?R\u00104\u000fR\u0010=R\u000b|\u0006=:)9;5PO\b4#C6c9,RN:)9Q5PCN0gR\u00100D=\b5®0D5®9Q5®/H7\"7·V69Q5®L\u00184\u000f4#}Az[P9Q5.1p4\u00021rR\u0010=Fa=\bMp:w:)9Q5POQ46C6²\u001a\n\u0002Gý\u000f\u0006\t\u0002 ûxû\"ü~ý;þ ÿ\r\u0000\u0003\u0002\u0005\u0004\u000e\u0006\u000e\b;ý\u000f\n\u0005\fSü~ÿ\u0001\u0000 ÿ\u0001\u0000\u001b\u0002\u0005\u0004\u000e\u0006\t\b?ý\u000f\n\u0011\u0010\u0012\fû%üFý;þ \u0013\u000f\u0014\u0012\u0013\u0018\u00066ý\u0015\u0002Gÿ\u0001\u0017\u000b\f«ü~ÿ\u0001\u0000 \u0013\u0015\u0014\u0012\u0013\u0016\u0006#ý\u000f\u0002Gÿ\u0001\u0017\u0019\u0010\u0012\u0010\u0004\u000e\u0000\u001b\u001c ûxû8üFý;þ ÿ\r\u0000\u0003\u0002\u0005\u0004\u000e\u0006\u000e\b;ý\u000f\n\u0005\f\u000bü~ÿ\u0001\u0000 ÿ\u0001\u0000\u001b\u0002\u0005\u0004\u000e\u0006\t\b?ý\u000f\n\u001d\u0010\u0012\fû%üFý;þ \u0013\u000f\u0014\u0012\u0013\u0018\u00066ý\u0015\u0002Gÿ\u0001\u0017\u000b\f«ü~ÿ\u0001\u0000 \u0013\u0015\u0014\u0012\u0013\u0016\u0006#ý\u000f\u0002Gÿ\u0001\u0017\u0019\u0010\u0012\u0010n\u0011K346CN4Ha=\bMp:\u0004:)9;5POQ46C\u0004V69Q5\"L\u00184W:\u001046[p:\u001046CN465ZR\u00104\u00021@LZfh9oa=QM3:T130D\\l465PCN0D=Q5.9QBL\u0018=\u0002}$^\n4.2 The Repr esentation ofaTransition ina\nQuery¥]C@9;5®=QLPCN4#:\u0010>Q9QL3BD4\u000fCSR)9?R\u00104\u000f0DC81p46[P0DV#R\u0010461LAf913M3[PBD4\bcÒ\n0D5AR\u00104J:\u0010>\b9;Bc{ST{:)9;R\u00100D=pÓHc\u00189oRN:)9Q53CN0gR\u00100Y=Q5\"0D5\"9lXZMP4#:NfZcP9Q58=QLPCN4#:\u0010>Q9QL3BD4RN:)9;5PCN0gR\u00100D=\b5ic0DCx:\u001046[3:\u00104#CN465ZR\u00104\u00021@LZf\u001a\n\u0002Gý\u000f\u0006\t\u0002 û\"ÿ\r\u0000\u0003\u0002\u0005\u0004\u000e\u0006\u000e\b;ý\u000f\n\u0005\f\u001e\u0013\u0015\u0014\u0012\u0013\u0016\u0006#ý\u000f\u0002Gÿ\u0001\u0017\u001f\u0010\u0012\f\u0004\u000e\u0000\u001b\u001c û%ÿ\r\u0000\u0003\u0002\u0005\u0004\u000e\u0006\u000e\b;ý\u000f\n\u0005\f \u0013\u0015\u0014\u0012\u0013\u0018\u00066ý\u000f\u0002Gÿ\r\u0017\u0019\u0010|wK30YV)K,V\u00029;5,L\u00184`:\u001046[p:\u001046CN465ZR\u00104\u00021+LZf\u000f98[\u0018=\b0D5ZR~0Y5,9ha=QM3:o130D\\l465PCN0D=Q5.9QBCN[P9QV64\b^\n4.3 Create aTreeandSear chintheTree{«ad9[\u0018=\b0D5ZR\n=Qaia=\bMp:w130D\\l4653CN0D=\b5l0DC\u00110D5r9Hau=\bMp:w130D\\l4653CN0Y=Q5.9;B.L\u0018=\u0002}ic\bR\u0010KP4V6=;:N:\u001046CN[\u0018=\b5P130D5POF=\bL3CN4#:\u0010>\b9;LPBD4W53=QR\u00104TRN:)9;5PCN0gR\u00100D=\b5@V69Q5@L\u00184]:\u001046[3:\u00104#CN465ZR\u00104\u00021LZf8R\u0010K34FK3021313465\"RN:)9;5PCN0gR\u00100D=\b5\u000f|w0gR\u0010KeKP0DO\bKe[3:\u0010=QL.9;LP0DBD0DRfZ^H,4~C\u001096f\"R\u0010KP4=QLPCN4#:\u0010>Q9QL3BY4WRN:)9;5PCN0gR\u00100D=\b58\\s9?R\u0010VJK346C\u0011R\u0010KP4HK30Y1P134#5rRN:)9;5PCN0gR\u00100D=\b5\u001d^nd=0D5P134#}\\oMPBgR\u00100gz130D\\l4653CN0Y=Q5.9;BT139;R)9pcwR\u0010K34e\\l=QCSRr[\u0018=\b[PM3BY9;:80D5P134#}R\u001046V)KP53=\bBD=\bOQ0D46C80D5\u001e1P9?R)9QL.9;CN46C\"9?:\u00104%a2:\u0010=\b\\qR\u0010KP4%¤]zRN:\u0010464g#°QHaG9;\\l0DBDfZ^/\u00044#:\u00104@|\u00114@MPCN48¤W©\u0002zRN:\u0010464\u000fD\u0002ª?\u0004L\u001846V\u00029;MPCN4\"=QaT0gR\u0010ClKP0DO\bK4#vlV60D4653V#fa=Q:CN4\u00029?:\u0010VJK30D5PO3^b¥\u0004BDBZR\u0010KP4\nRN:)9;5PCN0gR\u00100D=\b5FL\u0018=?}p46Cd=Qa.469QV)KF/W7\"7¬9;:\u00104\u00110D5PCN4J:NR\u00104\u000210D5ZR\u0010=9WCN0D5PO\bBD4]¤H©6zRN:\u0010464w=Qa$a=QM3:\u0006130D\\l465PCN0D=Q5PC6^tn\u0011K34wau=\bBDBD=\u0002|w0Y53Oo0DCtR\u0010KP4\n:\u00104\u00029?R\u00104 n\u001d:\u00104#4¥\u0004BDO\b=;:\u00100DR\u0010K3\\¬R\u0010=`V#:\u00104\u00029?R\u00104WR\u0010KP4W0D5P134#}$^!#\"\u0007$\u000f%'&\u0007$ ()\"\u000e$\u0015$*,+\u001e-.+0/\t1*,+0/\u0018243 56/\t587:9;9*,+\u001e-.+0/\t1<\u0018+\u001e1=/\u0019>'3?2\u000b2'+\u001e5;1,*@/\t5BA@3 1C3?D\t5E>'/\t5B<\u0018+\u00123 1=3 5\u000f1@DF/#GHD\tIB*@J\u00052\u000f3?KL+\u001e5BA@3 D\t5B/\tMON.D\u0007P3 5'A,+0*@1Q1@>'+:N\u001bDRPS3 5\u000f1@DFTVU\u0007J\u00011,*@+0+I'5\u00151@3 MO5BDLKLD\u0018*@+:>'3?2'2\u000b+0581,*@/\t5BA@3 1C3?D\t5;3 581@>B+\u00127V9;9I'5\u00151@3 MO5BDLKLD\u0018*@+\u00127:9;9K3465¸9XZMP4#:Nf\u001e0DC%CNMPL3\\l0gRNR\u00104\u00021icT0DR%0DC\"V)K.9;5PO\b461®R\u0010=9CN4\u0002XZMP4653V64=;awCSR)9;R\u00104lRN:)9Q53CN0DR\u00100D=Q5PCF9Q5.1\u000f4\u00029QV)K,RN:)9Q53CN0DR\u00100D=Q50YCH:\u001046[p:\u001046CN465ZR\u00104\u00021+LZf+9au=\bM3:\u00041p0Y\\l4#5PCN0D=\b5P9QB$[\u0018=\b0D5ZR\u0002^\nn\u0011KpM3Cx4\u00029QV)K8XZM34#:Nfr0YCx:\u001046[p:\u001046CN465ZR\u00104\u00021h9QC]9CN4#R~=;ax[\u0018=\b0D5ZR\u0010C6^sn\u0011KP4#CN4l[\u0018=\b0D5ZR\u0010CF9?:\u00104lMPCN4\u00021%R\u0010=@CN4\u00029?:\u0010VJK\u000fR\u0010KP4`¤W©\u0002zRN:\u0010464\b^n\u0011K34T5AMP\\FL\u00184#:\u0006=;am4\u00029;VJKh/H7\"7\u000f´ CxL\u0018=\u0002}p46C\u0011V6=Q5AR)9;0D5P0D5POR\u0010KP4\u0004[\u0018=\b0D5ZR\u0010Cx9;:\u00104V6=QMP5ZR\u00104\u00021$^n\u0011KP4s:\u00104#R\u0010Mp:\u00105P4\u00021+:\u001046CNM3BgRl0DCoR\u0010KP4l:)9Q53<\u00124\u00021/H7\"7@C6c\u001b|wKP0DV)KKP9?>\b4\u001bR\u0010K34\u0006KP0DOQK~[p:\u0010=\bLP9QLP0DBD0gR\u000bfHR\u0010=]L\u00184\nR\u0010K34\n4#}p[\u001846VJR\u00104\u00021H/H7\"7@C6^tn\u0011K34\na=QBgzBD=\u0002|w0D5PO]0YC$R\u0010KP4\nA4\u00029;:\u0010V)K ¤T9;5P<\u0004¥\u0004BDO\b=;:\u00100DR\u0010K3\\8^W\u0003$\u0015%X\"\tY\u0018Z [F%.\\O]*,+0/\u0018243 581C>B+_^\u0015I'+R*C`4/\t5B2\nE>'/\t5B<\u0018+\u00123 1=3 5\u000f1@DF/LA@1@*@3 5B<LD\tG^\u0016IB+0*@`S-.D\t3 5\u000f1@A*,+\u001e-.+0/\t1<\u0018+\u001e1QD\t5B+\u0012-\u001bD\t3 5\u000f1A@I\u000bNBK#3 1a1C>B+\u0012-\u001bD\t3 5\u000f1Q1,D\u00191@>B+_TVU\u0007J\u00011@*,+0+*,+\nED\u0018*,241@>'+\u00127:9;96A)1@>'/\t1Q>'/\u0018A=/\u0018AaM?+0/\u0018A@1=/#N.D\u0007PED\t5\u00151,/\t3 5\u000b3 5B<S1@>'3 A=-.D\t3 5\u00151I'5\u00151@3 MO5BDLKLD\u0018*@+:-.D\t3 5\u000f1@A=3 581C>B+_^\u0015IB+0*C`ED\tI'5\u00151=1C>B+:5\u0015IBK\u001fN.+0*QD\tGbK#/\t1\nE>'+RA=D\tG\u001d+0/\nE>;7:9;9*,/\t5\u000bc41C>B+0A,+\u00127V9;96A¥wauR\u00104#:\u001bR\u0010KP4x:)9Q53<\u00124\u00021o/W7\"7@C\u00069?:\u00104]:\u00104JR\u0010M3:\u00105P461ic\bR\u0010K34\u0011a=;:N|x9;:)1r9QBDOQ=Q:\u00100gR\u0010KP\\0DCm46\\l[3BD=?f\u00124\u00021TR\u0010=T1p=w[\u0018=\bCSRNz«[3:\u0010=ZV64#CNCN0Y53O\u0004R\u0010=w:\u00104#RN:\u00100D46>\u00124tR\u0010KP4\u001b\\l=\bCSRdCN0Y\\l0DBY9;:\n=Q5P46C6^@¥\u0004C\\l=QCSR/H7\"7@CF9;:\u00104s[3:\u0010M35P4\u00021%=\bMpRF0D5+R\u0010K34rp4\u00029?:\u0010VJK ¤\u00049Q53<$cR\u0010K34~au=Q:N|\u00119;:)1e9QBDO\b=;:\u00100DR\u0010K3\\y[p:\u0010=ZV646CNCN46CWau4#|\u00114#:W/H7\"7@CH9;5.1@R\u0010KP4R\u0010=QR)9;BV#=\bCSR]0DC\u0004O;:\u00104\u00029;R\u0010Bgfr:\u00104\u000213M3V64\u00021i^\n5.EXPERIMENT{5\"R\u0010KP0DCHCN46V#R\u00100D=\b5ic\u001d|\u00064`1p46CNV#:\u00100DL\u00184`=\bMp:H4#}p[\u00184#:\u00100D\\l465ZRH1p46CN0DO\b5\u000f9Q5P1%R\u001046CSR:\u00104#CNMPBgR\u0010C6^\n5.1 Experiment setup{5h=QM3:xR\u001046CSR]139;R)9;L.9QCN4Qc3R\u0010K34#:\u00104T|\u00114J:\u00104H9FV6=;:\u0010[PM3Cx=Qa\u001bQ­\b­FCN=Q5PO\bCHEGM3CN4\u000210D5D6 ;I)^ln\u0011K346\\l46C6ciR\u0010KP4F\\oMPCN0DV\u00029;Bb4\u0002XZMP0D>Q9QBD465ZRH=Qa\u0011<\u00124#fA|\u0011=;:)13C6c$|\u00114#:\u001044J}pRN:)9;V#R\u00104\u000218au:\u0010=Q\\y4\u00029;VJK%[30Y4#V64o=;a\u0006\\FMPCN0DVLAf\"7\"78kg6¶;^¥\u00045e9\u0002>\u00124#:Nz9;O\b4s=Qa]A^ ªQ¶@R\u0010KP4#\\l46C[\u00184#:~[P0D46V64l=;awCN=\b5PO8|\u00064#:\u00104la=QMP5.1\u000fLAf+7\"78xc:\u00104#CNMPBgR\u00100D5POl0D589l139;R)9QLP9QCN4H=;a\nQ \u0012ª;l\\l=\b5P=Q[PK3=\b5P0DVTR\u0010K346\\l46C6^t/W7\"7@C=;a\u001dR\u0010K34]R\u0010K346\\l46Ct|\u00114#:\u00104]R\u0010KP465lOQ465P4J:)9;R\u00104\u00021r9QMpR\u0010=\b\\s9;R\u00100DV\u00029;BDBDfr9;5.1l[PBY9;V64\u000210D5@R\u0010K34H1P9?R)9QLP9QCN4\b^+4\u00069;BDCN=\u0004M3CN4\u00021WR\u0010KP4\nXAM34#:Nf\u0004V6=Q:\u0010[3MPCmM3CN4\u00021W0D5rg6 ;^t°wCNMPLZN46V#R\u0010CdCNMP5POw0D5 [P0D46V#46Cx=Qad\\oMPCN0DV]4\u00029QV)K\u001dcAR\u0010=;R)9QBDBgf8?°`XZMP4J:\u00100Y4#C6^tTCN0D5POR\u0010KP4]a=Q:N|\u00119;:)19;BDO\b=Q:\u00100gR\u0010K3\\8c.\u0002H=Qa$R\u0010K34wXAM34#:\u00100D46Ct|\u00114#:\u00104w:\u00104#R\u0010M3:\u0010534\u00021F|w0gR\u0010K`R\u0010KP4w4#}p[\u001846VJR\u00104\u00021CN=Q5PO]9;R\u001dR\u0010KP4bR\u0010=\b[=;apR\u0010K34b:)9Q5P<\b4\u00021T:\u001046CNMPBgR\u0010C6^m+4\u001b134653=QR\u00104bR\u0010KP46CN4\nXZMP4#:\u00100D46C9;C%¹NO\b=Z=A13ºeXZMP4J:\u00100Y4#C6^eP=Q:~R\u0010KP4s=QR\u0010K34#:oXAM34#:\u00100D46C6cbR\u0010KP4lR\u0010=Q[CN=\b5PO8:\u00104JzR\u0010Mp:\u00105P4\u00021`|\u00119QC\u001153=QR\u0011R\u0010K34\u0004=Q5P4T4J}3[\u00184#V#R\u00104\u00021i^\u001b3=Q:\u0011\\l=QCSRx=QaiR\u0010KP4#CN4\u0004aG9;0DBYMp:\u001046C6c0gR\u0004|x9QCHL\u00184#V\u00029QM3CN4FR\u0010K.9?RTR\u0010K34FKAMP\\l\\l461h5P=;R\u001046C\u0004|\u00064#:\u00104o=;a\u0011LP9\b1%XZM.9QBD0gR\u000bfZca9;:sau:\u0010=Q\\ R\u0010KP484#}p[\u001846VJR\u00104\u00021=Q5P4\b^\u000ep=Q\\l4@aG9;0DBYMp:\u001046Cs|\u00114#:\u00104\"V\u00029;MPCN4\u00021LAfR\u0010K34\"[\u0018=Z=Q:\u0010Bgf4J}pRN:)9;V#R\u00104\u000215P=QR\u00104#C6^n\u0011KP4\"[30gR\u0010VJKRN:)9;VJ<\b4#:rCN<p0D[3[\u00184\u00021CN=\\s9;5Zfh53=QR\u001046CwR\u0010K.9?RwR\u0010KP4H4#}ARN:)9QVJR\u00104\u00021@5P=;R\u001046Cw|\u00114#:\u00104~130gvlV#MPBgR\u0004R\u0010=F:\u001046V6=\bO;z530D¯64r|wK3465[PBY96fZ4\u000219QC`7@{\u000bµW{)^t¥\u0004\\l=Q5PO8R\u0010K346CN4e#°+XZM34#:\u00100D46C6ctR\u0010KP4#:\u00104|\u00064#:\u00104o°rXZMP4J:\u00100Y4#C\u0004|wKP0DV)K\"R\u0010KP4~4#}p[\u001846V#R\u00104\u00021\"CN=\b53Ol|x9QCW9Q\\l=\b53OlR\u0010KP4R\u0010=\b[6%:\u00104JR\u0010M3:\u00105P461,CN=\b5POQC`CN4\u00029?:\u0010VJK34\u00021LAf\u000fR\u0010KP4la=;:N|x9?:)19QBDOQ=Q:\u00100gR\u0010KP\\8^e,41p465P=;R\u00104rR\u0010K346CN4@°\u000fXZMP4#:\u00100D46Co9QCe¹\u00101346V#465ZR\u0010º3^n\u0011K34@=QR\u0010K34#:@6\u000fXZM34#:\u00100D46C6c|wK30DVJKsR\u0010KP4\u00044#}p[\u001846V#R\u00104\u00021lCN=\b53OF|\u00119QCx5P=;R\u00110D5sR\u0010KP4\u0004R\u0010=Q[e6~:\u00104#R\u0010Mp:\u00105P4\u00021$cA9;:\u001041p465P=;R\u00104\u0002189;Cl¹NL.9\b1pº3^\u0006{\u000b58=QM3:\u00044#}p[\u00184#:\u00100D\\l465ZR\u0010C6c3|\u00064HMPCN4\u00021@R\u0010K34Wa=Q:N|\u00119;:)19;BDO\b=Q:\u00100gR\u0010K3\\'R\u0010=@1p=h[\u0018=QCSR[3:\u0010=ZV646CNCN0D53O89;a2R\u00104#:HR\u0010KP4FV\u00029Q5P130Y1P9?R\u00104o/W7\"7@C|\u00064#:\u00104HCN46BD46V#R\u00104618LAflR\u0010K34H¤H©6zRN:\u0010464\b^¥]CiR\u0010K34ba=Q:N|\u00119;:)19QBDOQ=Q:\u00100gR\u0010KP\\®|x9QC\u001d46\\l[PBD=\u0002fZ4\u00021wR\u0010=]13=x[\u0018=\bCSRNz«[p:\u0010=ZV646CNCN0D5POpc¹NL.9Q13ºHXZMP4J:\u00100Y4#C6´Q4J}3[\u00184#V#R\u00104\u00021o9;5PCS|\u00064#:\u001bV\u00029Q5F5P=QRtL\u00184\n:\u00104#R\u0010M3:\u0010534\u00021ic;|\u00114\u0011M3CN4\u00021=Q5PBgflR\u0010KP46°\"¹\u0010OQ=Z=A13º`9;5.1+¹)1p46V6465ZR\u0010ºFXAM34#:\u00100D46C\u00119;Cx=\bMp:\u0011R\u001046CSRxXZM34#:\u00100D46C6c{«Ro0DC~0D5AR\u0010M30gR\u00100Y>\b4sR\u0010KP9;RF0gaxR\u0010KP4l:)9Q53O\b46CF=QawR\u0010KP4sL\u0018=\u0002}p46Co9?:\u00104sCN4#RoR\u0010=\"L\u00184BY9?:\u0010O\b4\bc\u001dBY9;:\u0010O\b4o5pM3\\oL\u00184#:\u0004=Qa\u0006V\u00029Q5P130Y1P9?R\u00104o/W7\"7@CW|\u0011=\bM3BY1eL\u00184~:\u00104#R\u0010M3:\u0010534\u00021LZf\"¤W©\u0002zRN:\u0010464Q^~7@MPV)K\"R\u00100Y\\l4~CNKP=QMPBY1%L\u00184FCN[\u0018465ZRH0D5%[\u0018=\bCSRW[3:\u0010=ZV646CNCN0D53O3^{5,V6=Q5ARN:)9;CSR\u0002c\u001b0DawR\u0010K34`:)9Q5POQ46Co=;awR\u0010KP4sL\u0018=\u0002}34#Co9?:\u00104hCN4#R~R\u0010=\"L\u00184sCN\\s9QBDBGcau4#|\u0016V\u00029Q5P130Y1P9?R\u001048/H7\"7@C`|\u0006=\bM3B21L\u001848CN46BD46V#R\u0010461LZf¤H©6z«RN:\u00104#4\b^n\u0011K30DC|\u0006=\bM3B21h\\s9Q<\u00124TR\u0010K34WCSfpCSR\u001046\\¬aG9;CSR\u0002^\u0006/\u0004=\u0002|\u001146>\u00124J:\u0002cPR\u0010=Z=`CN\\s9;BYBiL\u0018=\u0002}p46CxR\u0010K.9;RR\u0010=QBD4#:)9;R\u00104BD0gRNR\u0010BY4H4J:N:\u0010=Q:]\\l0DO\bKZRw[3:\u0010M35P4T4J}3[\u00184#V#R\u00104\u00021h/W7\"7@C\u0004=QM3R\u0002^\nA=3c30DR0DCw0D\\l[\u0018=Q:NR)9;5ARxR\u0010=oVJK3=Z=\bCN4~9oO\b=Z=A1hL.9QBY9;5PV64\b^A0D5PV64H0D5ZR\u00104#:\u0010>Q9QB$>\b9?:\u00100D46CwLAfsR\u0010K34WCSR\u001046[8=;a\u0011\bcp|\u00114CN4#Rd\u001eÛXe ÝØ$Ü f\u0002Ú\u000fgZÛ.h\u001dikjPÝmlBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u000fgZÛ.hAÙon9;5.1d\u001eÝØ ÝØ$Ü f\u0002Ú\u0015gZÛ\u001bh\u001dikjPÝmlBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u0015gZÛ\u001bhPúpn¥]C\u0011ÕZÖHÕ\u0012Ú;ÛAÜ\u000bÝÞW0DC\nR\u0010K34]:)9?R\u00100Y=~=Qa\u001dV6=Q5PCN46V6MpR\u00100D>\u00124T1pM3:)9?R\u00100Y=Q5PC6cZ|\u00064\u0004CN4#R\nR\u0010KP4:)9;5POQ4H=Qa\u001bÕZÖHÕ\u0012Ú;ÛpÜÝÞ~LZfd\u001eÛXe ÕZÖHÕ\u0012ÚQÛAÜÝ«Þ#iqjPÝmlBlXf?Ø ÕZÖHÕ\u0012ÚQÛAÜÝ«Þ\u0012r=s9;5.1dÝ«Ø ÕZÖHÕ\u0012ÚQÛAÜ\u000bÝÞLiqj3ÝClBlXf\u0002Ø ÕZÖHÕ\u0012Ú;ÛAÜ\u000bÝÞ\u000bt\u0016s{5\"=\bMp:W4#}p[\u00184#:\u00100D\\l465ZR\u0010C6cP|\u00064FCN4JRLn9Q5P1us+|w0DR\u0010Ke1p0DU$4J:\u0010465ZRT>Q9QBDMP4#C\u0004R\u0010=jP5.1sR\u0010KP4WCNM30DR)9;LPBD4H=\b5346C6^TMp:tR\u001046CSR\n|\u00119QC\u001b:\u0010MP5`=Q5s97@0YVJ:\u0010=\bCN=Qa2R\u00110D5.1p=\u0002|wCxQQ\bAcp§m465ZR\u00100YM3\\sÕZÕZÕ\\s9;VJK30D5P4\b^\u0000\u0002\u0001\u0004\u0003\u0011\u0005Q\u0007\n\t\u000b\u0001\u0006\f\u000e\r\u000f\t\u0010\u0003\u0011\u0003\u0011\u0005Z\u0001¨\u0013\u0016\u0015$\u0017Q\u0019\u001b\u001a\u001d\u001c\u001e\u0013\u0016\u001a\u001b\u0003\u0011\u0005Z\u001f \"!#\u001a$\u0017%\u0013'&d Q\t)(+*,\u0005$-J\u0017Q\t)\u0005.\u001c\u0018\u0015\u0018\u001f\n5.2 Experiment Results and Sensiti vity Anal­\nysis{5\u000fR\u0010KP4FjP:\u0010CSRCN4#R~=Qa\u0006R\u001046CSR\u0010C6cm|\u00114lCN4JRvnwi¬@9;5.1psxi\u0016\u000fyYQ?\u000by @9Q5P1\u000fy A^\n¥\u0004BgR\u0010KP=QMPO\bKFR\u0010KP4w[3:\u0010=ZV64#CNCN0Y53O~R\u00100D\\l4wMPCN0D53OR\u0010K34w0Y5P134#}~|x9;C\u0011\\oM3VJKBD46CNCsR\u0010K.9;5R\u0010KP4@R\u00100D\\l4\"M3CN0Y53O,=\b53BgfR\u0010K348a=Q:N|\u00119;:)19;BYOQ=Q:\u00100gR\u0010KP\\8cwCN4#>Az4#:)9;B\u0011XZMP4J:\u00100Y4#CF1p0Y1+5P=;RFOQ4#R~R\u0010KP4`4#}p[\u001846VJR\u00104\u00021,9Q53CS|\u00114#:\u0002^8n\u0011KP4s9\u0002>\u00124#:)9;O\b4:\u001046CN[\u0018=Q5PCN4lR\u00100D\\l4r9Q5.1eR\u0010KP4l5AMP\\FL\u00184#:=Qa\u0004XZMP4#:\u00100D46C~\\l0DCNCN0D5PO\"4#}p[\u001846V#R\u0010461/W7\"7é0D5%130gU$4#:\u00104#5AR\u0004CN4#RNR\u00100D5POQCT9?:\u00104FOQ0D>\u0012465\"0D5%nm9QL3BD4sQ^]n\u0011KP4H:\u00104\u00029;CN=\b5z\nê\u001b{bïäkåPæ}|lä.~\ttì~6ä6à\u0005,ä¸ê\u0003mîà\u0005~\u0018~\u0002à\u0005\u001dá\u001dâ\u001däpã;àä.~là\u0001\u0018îmà\r\näAã?ä\u001b\u001d4nêOmîsD\u0018*@/\u0018*,2 QM?<\u0018D\u0018*@3 1C>BK o\u0015  o\u0015  o\u0015 =\u0016+0*@/\u0018<\u0018+*@+0A@-.D\t5BA,+1@3?K#+SHA\u001e \u000b  \t \u000b \u0018¡ \u000b \u0018¢ \u000b \u0018£¤I'K\u001fN.+0*D\tG^\u0015IB+0*C3?+0AK#3?A@A@3 5'<+\u001eP\u0015-\u001b+\nE1@+R27V9;9    au=Q:tR\u0010KP0DCb[\u0018=Z=Q:\u001b[\u00184#:Nau=Q:\u0010\\s9;5PV64\u00110DCbR\u0010K.9?R)n8|x9;CtCN4#RbR\u0010=TL\u00184xAc\u00124#}p[\u001846VJR\u00100Y53OR\u0010K34#:\u001040YC\u00045P=l4#:N:\u0010=;:W0D5\"0D5ZR\u00104#:\u0010>\b9;BG^\u0004p=3cPa=Q:\u0004R\u0010KP=QCN4oXZMP4J:\u00100Y4#C]R\u0010KP9;R]|\u00114#:\u0010453=QRx=;adK30YOQK@XZMP9QBD0gR\u000bfZc\u0012R\u0010K34\u00044#}p[\u001846VJR\u00104\u00021l/H7\"7@C\u0011|\u00114J:\u00104T[3:\u0010M35P4\u00021`=QM3RxLZf0D5.1p4#}$^{5rR\u0010K34WCN46V6=Q5.1hCN4JRw=QamR\u001046CSR\u0010C6c\u001bn+|\u00119QCwCN4#RxR\u0010=oL\u00184FH9Q5P1s¥i\u000e\u000fyYQ\u0002\u000by 9;5.1e\u000fy p^\n¥\u0004BDB$R\u0010KP4o#°lXZMP4#:\u00100D46C\u0011|\u00064#:\u00104W:\u00104#R\u0010M3:\u0010534\u00021r|w0gR\u0010K@R\u0010K34W4#}p[\u001846V#R\u0010461/W7\"7\u000f^in\u0011KP4`9\u0002>\u00124J:)9QO\b4F:\u001046CN[\u0018=\b53CN4FR\u00100D\\l4F0DCWBD0DCSR\u00104\u00021e0Y5enm9;LPBD4sZ^~n\u0011KP4z\nê\u001b{bïä§¦iæWçsí\u0018äAã?êPá.äsã?ä.~\t\u001bìO~6ä¨\u0002à\u0005,älà\u0001\tîmà\r\u0006äAã?ä.\u0011#nê\u0003mîsD\u0018*@)/\u0018*@2 =M <\u0018D\u0018*@3 1C>BK ©\u0015  o\u0015  o\u0015 =\u0016+0*@/\u0018<\u0018+*@+RAC-.D\t5BA,+1C3?KL+FHA\u001e \u000b  \t \u000b ª\u0016£ \u000b «\u0016  \u000b  \u0018 L\u001846CSR~V\u00029;CN4l0DCR\u0010KP9;RH|wK3465©nkis9Q5P1¥swi\u0016\u000byD\bcdR\u0010KP4l9\u0002>\u00124#:)9;O\b4`:\u00104#zCN[\u0018=Q5PCN4HR\u00100D\\l4W0DC\u0004Xy °\u0012lCN46V6=Q5.13Cw|wKP0DBD4WR\u0010KP49\u0002>\u00124J:)9QO\b4H:\u001046CN[\u0018=Q5PCN4HR\u00100D\\l40DCoXy ­Q%CN46V6=Q5.13C~LZf\u000fR\u0010K34la=Q:N|\u00119;:)19QBDOQ=Q:\u00100gR\u0010KP\\8^%n\u0011K34@9\u0002>\u00124J:)9QO\b4l:\u00104#zCN[\u0018=Q5PCN4R\u00100Y\\l40D5PVJ:\u00104\u00029QCN46C\u0004|w0DR\u0010K\"R\u0010K34~0D53V#:\u00104\u00029QCN0D53Oh=;aQs\u001b^w0gR\u0010K%B29?:\u0010O\b4#:s\u001bcA\\l=Q:\u00104wV\u00029;5.130Y139;R\u00104]/H7\"7@C\n|\u0006=\bM3B21lL\u00184w:\u00104#R\u0010M3:\u0010534\u00021`LZf`¤W©\u0002zRN:\u0010464]9Q5P1R\u0010K34TR\u00100D\\l4WMPCN4\u00021h0Y5h[\u0018=\bCSR][3:\u0010=ZV#46CNCN0D5PO`|x9;C\u00040D53V#:\u00104\u00029QCN461i^nd=\"R)9;<\u00124`R\u0010KP4sV6=Q\\l\\l=\b5,4#:N:\u0010=;:\u0010Co\\s9Q134lLZf+R\u0010K34lCNMPLZN46V#Ro0D5AR\u0010=\"V#=\b53zCN0Y134J:)9;R\u00100D=\b5ic\u00069;C`1p46CNV#:\u00100DL\u00184\u00021,0D5CN46V#R\u00100D=\b5°p^YQct|\u00114rCN4#R8n¸130gU$4#:\u00104#5AR\u0010Bgf9;5.11p0219;5P=;R\u0010KP4#:rCN4#Rh=QaR\u001046CSR\u0002^ÍKP465kj3ÝClBlXf\u0002Ø Ý«Ø$Ü,f?Ú\u0015gZÛ.hhÓ °pcdÛXe ÝØ$Ü f\u0002Ú\u0015gZÛ.h\u001diqj3ÝClBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u0015gZÛ.hm9Q5P1¬dÝ«Ø ÝØ$Ü f\u0002Ú\u0015gZÛ.h\u001dijPÝmlBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u0015gZÛ\u001bh«úH\b^\u001d{apÒ\njPÝmlBlXf\u0002Ø ÝØiÜ,f\u0002Ú\u000fgZÛ.hÒ\n°3c\u000bdÛ.e ÝØ$Ü f\u0002Ú\u0015gZÛ\u001bhikjPÝmlBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u000fgZÛ.hNÙh\u00069Q5P18dÝØ ÝØ$Ü f\u0002Ú\u000fgZÛ.h\u001dikjPÝmlBlXf\u0002Ø ÝØ$Ü f\u0002Ú\u000fgZÛ.h^K3465pjPÝmlBlXf?Ø ÝØiÜ,f\u0002Ú\u000fgZÛ.h=ikr=Q:H°pcdCN4#RSnxi¬\b^`Afp\\l\\l4#RN:\u00100DV\u00029QBDBgf|w0gR\u0010KlR\u0010KP4]5P46O\b9;R\u00100D>\u00124\u00040Y5ZR\u00104#:\u0010>Q9QBDC6^\u001b¥]BYB3R\u0010KP46°FXAM34#:\u00100D46C\n|\u00064#:\u00104\u0004:\u00104JR\u0010M3:\u00105P461|w0gR\u0010KFR\u0010K34\u00064#}p[\u001846VJR\u00104\u00021H/H7\"7\u000f^Qn\u0011KP4\u0006:\u00104#CNMPBgR\u0010Ct9;:\u00104\u0006:\u00104#[\u0018=Q:NR\u00104\u000210D5onm9;LPBD4xA^\u0006=Q\\l[.9;:\u0010461sR\u0010=~R\u0010KP4T[p:\u001046>A0D=\bMPC\u0011R\u00104#CSR\u0002cpR\u0010K34H9\u0002>\u00124#:)9;O\b4T:\u001046CN[\u0018=Q5PCN4TR\u00100D\\l4\u00040YCz\nê\u001b{bïä®­iæ\u000eçrí\u0018äAã?êPá.äã?ä.~\ttì~6ä®\u0002à\u0005,äklà\u0001\u0018¯~6äX\t6à\u0005dán»êO~îdä.~\u0018°\bã;à\u0001{täpîA,+\u001e1¬/\u0018A\nD\u0018*C)/\u0018*,22'+0A\nE*C3 N.+02 =M?<\u0018D\u0018*C3 1@>'K o\u0015  o\u0015  o\u0015 Q\u0016+R*@/\u0018<\u0018+*,+0A@-.D\t5BA@+1@3 KL+F\rA  \u000b  \t \u000b \u0018« \u000b ª\u0016« \u000b ¢\u0018\n4#>\u0012465+CNK3=Q:NR\u00104#:\u0002^h¼\u00064#V\u00029QM3CN4sR\u0010K30DCHCN4#RNR\u0010BD4r5P9;:N:\u0010=\u0002|¨13=\u0002|w5+R\u0010K34`0D5AR\u00104J:\u0010>\b9;B:)9;5POQ4\bc.R\u0010KP4H5AM3\\oL\u00184#:w=;atV\u00029Q5P130Y1P9?R\u00104~/W7\"7@C\u0004:\u00104#R\u0010Mp:\u00105P4\u00021hLAfr¤W©\u0002zRN:\u0010464|\u00119QCw:\u00104\u000213M3V64\u00021$c3CN=F|x9QCwR\u0010KP4\u0004R\u00100Y\\l4T=Qat[\u0018=\bCSR][p:\u0010=ZV646CNCN0D5POp^3=Q:wR\u0010KP4HCN4#RNR\u00100D53Ol=Qa)spiÍ\u000byD\bc\u0018469QV)K80D5.130D>A0Y13MP9QB\u001dXAM34#:\u00100D46C6´P:\u00104#CN[\u0018=\b5PCN4R\u00100D\\l4]0DCw130DCN[PBY96fZ4\u00021s0D5@m0DO\bMp:\u00104A^\u001bp:\u0010=\b\\¬m0DOQM3:\u00104Zc30gRxV69Q5rL\u00184TCN46465ic\n0123456\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14Response Time (s)\nQuerySearch R*-tree\nPost process\nSearch R*-tree+Post process\nForward Algorithmßwàá\u0018â\u001dã;ä8¦iæz\ndäFã?ä.~\ttì~6äv\u0002à\u0005,äìO±mäAêO°\u0015\"à\u0005mîmàí\u001dàGîmâdêPï\u001dâdäAã\u0016²au=Q:wR\u0010KP4l¹\u0010OQ=Z=A13ºs9Q5P1¹\u00101346V64#5AR\u0010ºoXZMP4#:\u00100D46C6cP9QBDB$R\u0010KP4TV#=Q:N:\u001046V#R\u00049;5PCS|\u00114J:\u0010C|\u00064#:\u00104@:\u00104#R\u0010Mp:\u00105P4\u00021,|w0gR\u0010K\\oMPV)KCNKP=;:NR\u00104#:`:\u001046CN[\u0018=Q5PCN4@R\u00100D\\l4s|wKP4#:\u00104rR\u0010KP40D5P134#}%|x9;CoM3CN4\u00021i^rµW0gU$4#:\u0010465ZRFXZMP4#:\u00100D46CV6=QCSR`1p0gU$4#:\u0010465ZRHR\u00100D\\l4\b^@{«R0DCL\u00184#V\u00029QM3CN4`130gU$4#:\u00104#5ARTXZMP4#:\u00100D46C\u0004K.9Q1e1p0gU$4#:\u0010465ZRW5AMP\\FL\u00184#:]=Qa\n5P=;R\u001046CW9;5.1RN:)9;5PCN0gR\u00100D=\b53C6^~n\u0011KP4oBY9?:\u0010O\b4#:HR\u0010K34F5AMP\\FL\u00184#:]=Qa\n5P=;R\u001046CW0D5+9sXZMP4J:NfZc.R\u0010KP4BD=Q5PO\b4J:\u0004R\u00100D\\l4H0gRwR)9Q<\u001246CxR\u0010=lCN4\u00029?:\u0010VJKhR\u0010KP4H¤W©\u0002zRN:\u00104649Q5P1@V\u00029?:N:Nf@=QM3RwR\u0010KP4au=Q:N|\u00119;:)1%9QBDO\b=;:\u00100gR\u0010KP\\8^\u0006n\u0011KP4~9\u0002>\u00124#:)9QOQ4H:\u001046CN[\u0018=\b53CN4R\u00100D\\l4HM3CN0Y53OoR\u0010KP4H0D5pz1p4#}h\\l46VJKP9Q530YCN\\»0DC\u0004.y \b lCN46V#=\b5.1pC\u0004[\u00184J:WXZMP4J:NfZcp|wK30DBY4R\u0010KP49\u0002>\u00124J:)9QO\b4:\u00104#CN[\u0018=\b5PCN4~R\u00100D\\l4FM3CN0Y53Oh=Q5PBgf\"R\u0010KP4a=;:N|x9?:)1+9QBDOQ=Q:\u00100gR\u0010KP\\'0DC~By ­;hCN46VJz=Q5.1pC~[\u00184J:~XAM34#:NfZ^`+4l9QV)KP0D46>\u0012461+9\u0002>\u00124#:)9;O\b4r­rau=\bBY1,CN[\u0018464\u00021pMP[+M3CN0D5POR\u0010K34W0D5.1p4#}r\\l46VJKP9Q530YCN\\8^3=Q:mR\u0010K34¹\u0010LP9\b1pºWXZMP4#:\u00100D46C6c?|\u00064\u0011V6=\bM3BY1~53=QRbOQ4#RbR\u0010KP4\u001b4#}p[\u001846V#R\u00104\u00021CN=\b53O\bCb0D5R\u0010K34\u0004V69Q5.1p0Y1P9;R\u00104\u0004/H7\"7@C\u0006:\u00104JR\u0010M3:\u00105P461`LAfoR\u0010KP4]0Y5P134#}$^b{«R\u00110DC\u0011L\u001846V69QMPCN4\u0004=QaR\u0010K34WBD=\u0002|ÍXZM.9;BD0DRfr=QamR\u0010K34XAM34#:\u00100D46C6^\u001bn\u0011KP4H4J:N:\u0010=Q:]0D5@R\u0010KP4HXZM34#:\u00100D46Cw0YCwCN=BY9?:\u0010O\b4FR\u0010KP9;R\u0004R\u0010KP4[\u0018=\b0D5ZRT:\u001046[p:\u001046CN465ZR\u00100D5POr9`RN:)9Q53CN0DR\u00100D=Q5\u000f0DCT53=QRTBY=ZV69;R\u00104\u000210D59eRN:)9Q53CN0gR\u00100Y=Q5L\u0018=\u0002}4#}p[\u001846V#R\u00104\u00021$^A=3c\u0011R\u0010K346CN4\"XZMP4#:\u00100D46C`jP5.1CN=\b\\l4CN=Q5POQC\u0004=;R\u0010KP4#:xR\u0010KP9Q5@R\u0010K34W=\b5346CwCNMP[P[\u0018=QCN4\u00021i^\n6.CONCLUSION{5`R\u0010KP0DCt|\u0011=Q:\u0010<\u0018c\u0012|\u00064\u0004[p:\u0010=\b[\u0018=QCN4\u00021l9H5P=?>\u001246B3021p4\u00029WR\u0010=HMPCN4wCNKP=;:NR\u0006:)9;5PO\b4#C\nR\u0010=R\u0010=QBD4#:)9;R\u00104~4#:N:\u0010=Q:\u0010CT0Y5\"KAM3\\l\\l4\u00021r53=QR\u001046C6^\u0004\u001b9QV)K8RN:)9Q53CN0DR\u00100D=Q5e0D5\"/W7\"7@C0DC\u0011RN:)9;5PCSa=;:\u0010\\l4\u00021h0D5ZR\u0010=`9a=QM3:Nz130D\\l4653CN0Y=Q5.9;BiL\u0018=\u0002}h9Q5P1r0D5P134#}p4\u00021s0D589¤W©\u0002zRN:\u0010464Q^xn\u001d:)9;5PCSa=;:\u0010\\l0D5POrXZMP4#:\u00100D46C]0D5AR\u0010=Fa=\bMp:Nz130D\\l465PCN0D=Q5.9QB\u001d[\u0018=\b0D5ZR\u0010C6cV69Q5.1p0Y1P9;R\u00104@CN=\b5POQCl|w0DR\u0010KKP0DO\bK[3:\u0010=QL.9QL30DBY0gRfV69Q5L\u00184@CN46BD46V#R\u00104\u00021=\bM3RXZM30YV)<ABgfZ^w¼\nf\"9;CNCN0DO\b5P0D53OrR\u0010K34~[P9;:)9;\\l4#R\u00104#:\u0004=QatR\u0010KP4H:)9Q53O\b46C\u0004[3:\u0010=Q[\u00184#:\u0010BgfZc|\u00064r9QV)KP0D46>\u00124\u00021+9@CN46>\u0012465pza=\bBY1+CN[\u00184#4\u00021+MP[+MPCN0D5PO@R\u0010K30DCF0D5.1p4#}e\\l46V)K.9;z530DCN\\¨R\u0010K.9;5lR\u0010KP4]=Q:\u00100DO\b0D5.9;Biau=Q:N|\u00119;:)1@9;BDO\b=Q:\u00100gR\u0010K3\\¬0D5r=QM3:x4J}3[\u00184J:\u00100Y\\l4#5AR\u0002^3=Q:sBY9;:\u0010OQ4#:8139;R)9;CN4#R\u0010C6c\u0011|\u00114\"4J}3[\u00184#V#R`R\u0010=,L\u00184\"9;LPBD4@R\u0010=,CNK3=\u0002|\u00169+BY9;:\u0010O\b4CN[\u00184#4\u00021@MP[i^\n7.ACKNO WLEDGMENT+4OQ:)9;R\u00104JaMPBDBgfe9QV)<A5P=\u0002|wBD4\u000213OQ4FR\u0010K34~CNM3[P[\u0018=;:NRW=QatR\u0010K34FT9;R\u00100D=\b5P9QBtAV60gz4#5PV64P=QMP5P1P9;R\u00100D=Q5@MP5P134#:\u0011OQ:)9Q5P1@{N{NZz\b\b¶\bªQ;°ZªAc.9Q5P18n\u0011KP4W\u00045P0D>\u00124#:NzCN0gRf=Qa~7@0DV)KP0DO\u00129;5\u0006=\bBDBD46OQ4e=;aHt5POQ0Y53464#:\u00100D5PO\u000fCN464\u00021OQ:)9Q5P1R\u0010=%R\u0010KP47@M3C\u0010\u001b5%[p:\u0010=?S46V#R\u0002^Tn\u0011KP4~=\b[P0D530Y=Q5PCW0D58R\u0010K30YC\u0004[.9;[\u00184#:H9?:\u00104FCN=QBY4#BDf\"R\u0010K3=\bCN4=;abR\u0010KP4~9QMpR\u0010KP=;:\u0010C\u00049;5.1\"1p=`5P=;R\u00045346V646CNC\u00109?:\u00100DBDf@:\u00104#ñ.46VJR\u0004R\u0010K34H=\b[P0D530Y=Q5PC]=QaR\u0010K34TaM35.1p0Y53Ol9QOQ465PV#0Y4#C6^\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0001\u0006\f\u000e\r\u000f\t\u0010\u0003\u0011\u0003\u0011\u0005\u0012\u0001\u0014\u0013\u0016\u0015\u0018\u0017\b\u0019\u001b\u001a\u001d\u001c\u001e\u0013\u0016\u001a\u001b\u0003\u0011\u0005\u0012\u001f\u000b \"!#\u001a$\u0017%\u0013'&\u001d \b\t)(+*,\u0005$-)\u0017\b\t)\u0005.\u001c.\u0015$\u001f+4e|w0DCNK\u001eR\u0010=R\u0010KP9Q5P<pKP0ga2:\u00100Y5au=Q:%[3:\u0010=?>A0Y130D5POM3C@R\u0010KP4%R\u001046CSR\"139;R)9A^+4h9?:\u0010489;BYCN=eOQ:)9?R\u00104#aM3B\u0011a=;:l0D5PCN0DO\bKZRNaM3Bw130DCNV6M3CNCN0Y=Q5PCo|w0gR\u0010K§b:\u0010=Qa4#CSzCN=;:]0YBDBD0Y9Q\\¢¼\u00060g:\u0010\\l0D5PO\bKP9Q\\»9;5.1@K30DC]CSR\u0010MP13465ZR\u0010C6^\n8.REFERENCESDJ¤~^ ³P^$7@V6W9;L\u001dci\u001b^ ¥^ip\\l0gR\u0010K\u001dcP{J^ /F^30gRNR\u0010465\u001dc\u001d\u0004^ t^\u0018/T465P134#:\u0010CN=Q59Q5P1\u0018^ ³3^x\u0006M35P5P0D53O\bK.9;\\8^\u001bnd=\u0002|x9?:)13C`R\u0010KP4@µW0DO\b0gR)9QBT7@MPCN0DV\"i0DzL3:)9?:Nfi²~n\u001dMP5P4o¤\u00044#RN:\u00100D46>Q9QBtau:\u0010=Q\\¥\u0004V6=QMPCSR\u00100DVl{\u000b53[PM3R\u0002^i{5+µW0DO\b0gR)9;B\u001d0DL3:)9?:\u00100D46C~´ Q p²\n§t:\u0010=ZV#464\u0002130D53O\bC]=QabR\u0010KP4W¥\u0004x7'µH+\u0006=\b5pa4#:\u00104653V64\b^ \u0002¤~^ ³P^\u001d7@V#W9QLici\u001b^ ¥~^\u001dA\\l0gR\u0010K\u001dc\u0018µo^\u0018¼\u00119Q0D5AL3:\u00100Y13OQ4l9Q5P1\"{J^ /F^.0DRNzR\u0010465\u001d^\u0012n\u0011KP4wT4J|k´\u00184\u00029QBY9;5.1`µW0DO\b0gR)9;B\u0018\u001d0DL3:)9?:Nfs78\u001b\u001d=A1Af`0D5PµHaµF^{\u000b5hµTz\u001d0DL\"789;O\u00129Q¯#0Y534\bc$7896fe\u0002Q\u0012­A^ ?~¥~^x_TKP0Y9QC6c\u0019³P^xi=\bO\b9Q5\u001dc\u0004µo^x\u0006K.9Q\\FL\u00184#:\u0010BD0D5\u001e9Q5.1¼]^ \u0004^wp\\l0gR\u0010K\u001d^¶MP4#:Nf8¼\nf8/\u0004MP\\l\\l0D5POp²w7@M3CN0YV69QBb{\u000b5pa=;:\u0010\\s9;R\u00100D=\b5%¤]4#RN:\u00100D46>\b9;Bm0D5¥\u00045@¥\u0004M.1p0D=oµH9?R)9QL.9;CN4\b^.{58¥]x7é7@MPBgR\u00100D\\l4\u0002130Y9Ac\u001d6\b\u0012ªZ^ °;S³P^ $^3µW=\u0002|w5P0D4\b^.7@MPCN0DVH¤]4#RN:\u00100D46>\b9;Bd9;C]nd4#}ARx¤]4#RN:\u00100D46>\b9;BG²\u0011A0D\\l[PBD4·\u00064JR]tU$46VJR\u00100Y>\b4\b^P{5\"A{N_T{\u000b¤c\u0018\u0002Q\bA^ ª\u0002S³P^ $^AµW=\u0002|w530Y4\u00049Q5P1r7\u000f^AT46BDCN=\b5i^At>\b9;BYMP9;R\u00100D=\b5r=Qad9~p0D\\l[PBD4]9Q5P1tU$4#V#R\u00100D>\u00124e7@MPCN0DV%{\u000b5pa=Q:\u0010\\s9?R\u00100D=\b5¤]4#RN:\u00100D46>\b9;BW7@4JR\u0010KP=A1i^\u0006{\u000b5\u001eA{z_T{\u000b¤cPQ\bQp^  ?F·\u0006M34653z«/\u0004CN0Y4#5n\u0011CN4653O\u0006=\b5ZR\u0010465ZRNz¼\u00119QCN4\u00021¤]4#RN:\u00100D46>Q9QB]a=Q:87@MPCN0DV\u0006=\bBDBD46V#R\u00100D=\b53C6^\u0018{\u000b58p{S_T{\u000b¤ci6\bQp^ ­\u0002F·\u0006M34653z«/\u0004CN0Y4#5ln\u0011CN465POo\n:NfpCSR)9QBG²\u001b¥\u001e\u0006=\b5ZR\u0010465ZRNz«L.9;CN4\u00021r7@MPCN0DVT¤]4#zRN:\u00100D46>\b9;BmZfpCSR\u001046\\8^A{\u000b58p{N_T{¤~c$6\b\bA^ ¶?~¥~^Z\u00040gR)13465AL\u0018=\bOQ4#:)1s9Q5P1³P^\u001b´\u0018=\bL\u00184#B^37@4#BY=A1p0DV789;R\u0010V)KP0D5POond46V)K3z5P0YXZMP46C\u001da=Q:\n\u001d9;:\u0010O\b4]7@M3CN0DVxµH9;R)9;L.9QCN4#C6^\u0012{\u000b5F¥]x77@MPBgR\u00100D\\l4\u0002130Y9pc\u0002Q\bp^ ?S³\bfpK3z\u000bpK30D5PO¤]=\b53O\b4#:¥³Z9;5POpco/\u0004=\b5PO;z«¤\u0004M\u000e\u001d4649;5.1\u0006K3029?z«/TMP0·\u00064#K\u001d^\n¶MP4J:NflLZfrnm9Q[3[P0D5POp²\u001b¥®\u00044#|\u001e§b9;:)9Q130DO\b\\¬au=Q:T\u0006=\b5ZR\u0010465ZRNz¼\u00119QCN4617@MPCN0DV\"¤]4#RN:\u00100D46>\b9;Bxau:\u0010=\b\\ ¥]V6=\bM3CSR\u00100DV\"{\u000b5P[3M3R\u0002^t{5{S\u001b\u001b§b9QV60gjPVH¤\u00040D\\\u0016\u0006=\b5pa4#:\u00104#5PV64=\b587@MPBgR\u00100D\\l4\u0002130Y9Ac\u0018;\bpQ^g\u0002?F·o^t¼\n465POQ0Y=p^\u0006789;:\u0010<\u0012=?>A0Y9Q57@=A1346BDCoau=Q:hp46XAM3465ZR\u00100Y9QB\u0011µH9?R)9p^t{5AR)9?R\u00100YCSR\u00100DV\u00029;BbAV60D465PV64Qc.\b­A^\ngQ#~\u001b^t¤^\u001b¤T9;LP0D5P4J:\u0002^\u001b¥¢ndMpR\u0010=Q:\u00100Y9QBw=Q5/T0Y1313465789;:\u0010<\b=;>7@=A1p46BDC9Q5P1p4#BY4#V#R\u00104\u00021½¥\u0004[3[PBD0DV\u00029;R\u00100D=\b53C\u000f0D5kA[\u00184646V)KÍ¤\u00044#V6=\bOQ5P0gR\u00100D=\b5\u001d^{5§b:\u0010=ZV6464\u00021p0Y53O\bC]=QamR\u0010K34H{S\u001b\nxci\u0002\b¶Qp^g\u0002?~¥~^ \u0018^\bµWMp:\u00104#fo9Q5.1`7\u000f^ ¥^A\u0006BD46\\l465ZR\u0010C6^Z7@46BD=A1pfsA[\u0018=QRNR\u00100D5POH\u0004CN0D5PO/T0Y1313465@789;:\u0010<\u0012=?>@7@=A1346BDC6^P{\u000b5\"7@{¤~c.QQpg6;F$^t+M9;5.1F^\u0006789Q5AL\u00184#:\u0002^\u0006.9QCSRrn\u001d4#}ARrp4\u00029?:\u0010VJK30D5PO¥]BYBD=\u0002|w0D5POb:N:\u0010=Q:\u0010C6^.{5\"\u0006=Q\\l\\oMP530DV\u00029;R\u00100D=\b5h=Qat¥]x7\u000fc\u001b¸\n=QBG^\u001d6pc$;\bAg#°Q~¥~^m_TM3RNR\u0010\\s9;5\u001d^i¤wz«RN:\u00104#46C6²~¥\u0014µTfp5.9;\\l0DVo{\u000b5P134#}+ZRN:\u0010MPV#R\u0010Mp:\u00104oa=;:p[P9;R\u00100Y9QBop469;:\u0010V)KP0D5POp^T{5\u000e§b:\u0010=ZV6464\u00021p0D5PO\bCe=Qal¥\u0004x7 A{N_H7\"Tµ\u0006=\b5pa4#:\u00104#5PV64\bc\u001d\u0002Q¶Q°3^g\u0002ª?F^Q¼\u00064#VJ<A\\s9Q535\u001dc;/F^ §m^\u0015¹W:\u00100D46O\b46BGcZ¤^\bpV)KP53460Y134#:t9;5.1F¼\u0004^ZA4646OQ4#:\u0002^n\u0011KP4l¤W©\u0002zRN:\u0010464Q²o¥]5,bvlV60D465ZRF9Q5P1\u000f¤]=\bLPM3CSR~¥\u0004V6V#46CNC`7@4JR\u0010KP=A1a=;:x§m=Q0Y5ZR\u0010C\n9Q5P1`¤\u000446VJR)9Q5POQBD46C6^Z{\u000b5`§t:\u0010=ZV64#4\u0002130D5POQC\u0011=Qa\u001d¥]x7\u0016p{N_\u0004z7\"Tµ½\u0006=Q53a4J:\u0010465PV64Qcd\u0002Q\bA^g6 ;S³P^WpK30Da2:\u00100D5\u001dcF¼\u0004^T§b9;:)13=pco\u0004^7@4646<®9;5.1¸Í^\u0004¼\u00060g:\u0010\\l0D5POQK.9;\\8^/H7\"7hz¼\u00119;CN4\u00021\"7@M3CN0YV69QB\n¶MP4#:Nfr¤]4#RN:\u00100D46>\b9;BG^3{5¨³\u0012=Q0D5AR\u0004\u0006=Q53au4#:Nz4653V64H=\b5hµW0DO\b0gR)9QBd\u001d0DL3:)9?:\u00100Y4#CW;\b\u0012Z^g\u0002­?HÍ^ §m^m¼\n0D:\u0010\\l0D53O\bK.9;\\8cm¤^ µo^dµW9Q5P53465AL\u00184#:\u0010O3cb_F^ /F^i9Q<\b4#j.46BY1$c7\u000f^w¼\u00119?:NR\u0010CNVJKicWµo^x¼\nfp<\u0012=\u0002|wCN<p0Gc\u0004µo^]789;¯6¯6=\b530Gc\u0004^\u00047@4646<\u0018cW7\u000f^7@46BDBD=A1pfZcAÍ^A¤\u00049Q5P1i^p7@M3C\u0010¥]:NR\u0002²\u001b7@M3CN0YV\u0004¤\u00044JRN:\u00100Y4#>\b9;B¸T0Y9F¥]M3:)9;B¶MP4J:\u00100Y4#C6^P{58§b:\u0010=ZV6464\u00021p0Y53Ol=Qab{SP7@{¤W;\bpQ^g6¶;F\u0004^\n7@4646<9;5.1Í^ §m^\u001b¼\u00060g:\u0010\\l0Y53O\bKP9Q\\8^tn\u0011K346\\s9;R\u00100DV8b}ARN:)9QV#R\u0010=;:\u0002^{\u000b5@§t:\u0010=ZV#464\u0002130D53Ol=Qab{SP7@{\u000b¤TQQpQ^g6;¤^ ¥^m¼\u00069Q46¯\u00029?zm·\u00119?R\u001046C`9;5.1\u0004^ /F^m§m4#:\u0010BD46L\u00184#:\u0010O3^\u001bP9QCSRo9Q5.1§b:)9QVJzR\u00100DV\u00029QB\u0004¥\u0004[3[3:\u0010=\u0002}p0D\\s9;R\u00104%ZRN:\u00100Y53O789?R\u0010VJK30Y53O3^\u0006{\u000b5\u0006=Q\\oL30Y5P9;R\u0010=;:\u0010029;B§b9;RNR\u00104J:\u00105%789;R\u0010V)KP0D5POpc.n\u0011K30g:)18¥\u0004535pMP9QB\u001dAfp\\l[\u0018=QCN0YM3\\8c.\u0002Q\u0012Z^ ;;S³P^ 7\u000f^3§m=\b5ZR\u00104H9;5.1lÍ^ ¼\u0004^P\n:\u0010=QauR\u0002^P¥\u001ed9;5PO\bMP9QOQ4~7@=A1346BD0D5POo¥\u0004[3z[3:\u0010=\b9QV)K@R\u0010=o{53au=Q:\u0010\\s9;R\u00100D=Q58¤\u00044JRN:\u00100Y4#>\b9;B^3{\u000b58p{S_T{\u000b¤ci6\bQ¶p^ Z#S³P^\n§t0YV)<\u00124653C6^\n¥'\u0006=\b\\l[P9;:\u00100DCN=\b5=QaWd9Q53O\bMP9QO\b4e7@=A1p46BD0D5PO9;5.1§b:\u0010=\bL.9;LP0DBD0DCSR\u00100YV\u000end4#}AR{\u000b5pa=Q:\u0010\\s9?R\u00100D=\b5»¤]4#RN:\u00100D46>\b9;B8¥][P[3:\u0010=\b9QV)KP46CR\u0010=7@=Q5P=\b[3KP=Q5P0DV\u000f7@MPCN0DV%¤\u00044#RN:\u00100D46>Q9QBG^\n{\u000b5§b:\u0010=ZV64646130D5PO=;a{NAz7@{\u000b¤TQQ\bA^"
    },
    {
        "title": "Voice Separation - A Local Optimization Approach.",
        "author": [
            "Jürgen Kilian",
            "Holger H. Hoos"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417645",
        "url": "https://doi.org/10.5281/zenodo.1417645",
        "ee": "https://zenodo.org/records/1417645/files/KilianH02.pdf",
        "abstract": "Voice separation, along with tempo detection and quantisation, is one of the basic problems of computer-based transcription of music. An adequate separation of notes into different voices is crucial for ob- taining readable and usable scores from performances of polyphonic music recorded on keyboard (or other polyphonic) instruments; for improving quantisation results within a transcription system; and in the context of music retrieval systems that primarily support mono- phonic queries. In this paper we propose a new voice separation algorithm based on a stochastic local search method. Different from many previous approaches, our algorithm allows chords in the in- dividual voices; its behaviour is controlled by a small number of intuitive and musically motivated parameters; and it is fast enough to allow interactive optimisation of the result by adjusting the pa- rameters in real-time. We demonstrate that compared to existing approaches, our new algorithm generates better solutions for a num- ber of typical voice separation problems. We also show how by changing its parameters it is possible to create score output suitable for different needs, \u0001 \u0002 \u0003 \u0002 , piano-style \u0004 \u0005 \u0002 orchestral scores.",
        "zenodo_id": 1417645,
        "dblp_key": "conf/ismir/KilianH02",
        "keywords": [
            "voice separation",
            "tempo detection",
            "quantization",
            "polyphonic music",
            "transcription",
            "score output",
            "music retrieval",
            "polyphonic instruments",
            "keyboard instruments",
            "stochastic local search"
        ],
        "content": "Voice Separ ation -ALocal Optimisation Appr oach\nVoice Separation —ALocal Optimisation Appr oach\nJ¨urgen Kilian\nDepar tment ofComputer Science\nDarmstadt Univ ersity ofTechnology\nWilhelminenstr .7\n64283 Darmstadt, German y\n+49 6151 166184\nkilian@salier i.orgHolger H.Hoos\n\u0000\nDepar tment ofComputer Science\nUniv ersity ofBritish Columbia\n2366 Main Mall\nVancouv er,BC,V6T 1Z4, Canada\n+1604 822 1964\nhoos@cs .ubc.ca\nABSTRA CT\nVoiceseparation, along with tempo detection andquantisation, isone\nofthebasic problems ofcomputer -based transcription ofmusic. An\nadequate separation ofnotes intodifferent voices iscrucial forob-\ntaining readable andusable scores from performances ofpolyphonic\nmusic recorded onkeyboard (orother polyphonic) instruments; for\nimpro ving quantisation results within atranscription system; andin\ntheconte xtofmusic retrie valsystems thatprimarily support mono-\nphonic queries. Inthispaper wepropose anewvoice separation\nalgorithm based onastochastic local search method. Different from\nmanyprevious approaches, ouralgorithm allowschords inthein-\ndividual voices; itsbehaviour iscontrolled byasmall number of\nintuiti veandmusically motivated parameters; anditisfastenough\ntoallowinteracti veoptimisation oftheresult byadjusting thepa-\nrameters inreal-time. Wedemonstrate that compared toexisting\napproaches, ournewalgorithm generates better solutions foranum-\nberoftypical voice separation problems. Wealso showhowby\nchanging itsparameters itispossible tocreate score output suitable\nfordifferent needs,\n\u0001 \u0002 \u0003 \u0002,piano-style\u0004 \u0005\n\u0002orchestral scores.\n1.INTR ODUCTION\nForthetranscription oflow-levelmusical data intoscore notation,\nthree major tasks need tobeperformed: tempo-detection, quantisa-\ntion, and, inthecase ofnon-monophonic input, aseparation ofthe\nnotes intodifferent voices which canpossibly contain chords. Voice\nseparation isalsoessential foranyMIR system based onmonophonic\ntechniques (see,\n\u0001 \u0002 \u0003 \u0002,[5]). Finally ,voice separation isofinterest in\ntheconte xtsofmusical analysis andmusic cognition [2,9].\nInthiswork,ourfocus isonthecreation ofvoiceseparations forvar-\nious needs, particularly asarising inscore generation andnotation\ntasks aswell asintheconte xtofmusic information retrie val.Hence,\nourgoal isnottoﬁnd ‘the correct’ voice separation, which could\nhardly bedeﬁned without making restricti veassumptions about mu-\nsical style, andwould bedifﬁcult tocapture accurately eveninthe\npresence ofsuch assumptions. Rather ,wepursue themore pragmatic\ngoal ofcreating anadequate algorithm thatiscapable ofﬁnding a\nrange ofvoice separations thatcanbeseen asreasonable solutions\nintheconte xtofdifferent types ofscore notation (\n\u0001 \u0002 \u0003 \u0002,only mono-\nphonic voices, only onevoice including chords, multiple voices and\nchords; seeFigure 1).The underlying idea istoallowauser by\ncontrolling asmall number ofintuiti veparameters ofthealgorithm\ninreal-time tointeracti velyﬁndavoice separation foragivenpiece\nthat suits herspeciﬁc butnotnecessarily explicitly knownneeds,\nandrequires only minimal manual modiﬁcations inorder toobtain a\nsatisfying result.\u0000 \u0006\b\u0007 \t \n\f\u000b \r\u000e\u0007 \u000f \u000b \u0010 \u0011 \u0012\u0014\u0013\b\u000f \u0010 \u0015\u0016\u0010 \u0015 \u0011\u0018\u0017\u0019\u0011 \u001a \u000b \u001b \u0010 \u001c\u000e\u0011 \u001d \u0010\u001e\n \u001f\u001e !\n \u001c\u0018\u001a \" \u0010 \u0011 \u001b$# % \u000f \u0011 \u001d % \u0011 &\u0017$\u000b \u001b \u001c\u0018\t \u0010 \u000b \u0012 \u0010('\u0019\u001d \u000f ) \u0011 \u001b \t \u000f \u0010 *+\n \u001f-, \u0011 % \u0015 \u001d \n \u0007 \n . * &-/-\u000f \u0007 \u0015 \u0011 \u0007 \u001c\u0018\u000f \u001d \u0011 \u001d \t \u0010 \u001b 0\f1 &2 3 4 5 6\n\u0017\u001e\u000b \u001b \u001c\u0018\t \u0010 \u000b \u0012 \u0010 & 7\u0019\u0011 \u001b \u001c8\u000b \u001d * 0\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feeprovided\nthat copies arenotmade ordistrib uted forprofitorcommercial\nadvantage andthatcopies bear thisnotice andthefullcitation on\nthefirstpage.\n%9\n2002 IRCAM -Centre PompidouOurapproach splits acomplete piece intosmall slices ofoverlapping\nnotes. These slices areprocessed iterati vely,assigning each note of\naslice toavoice. During thisprocess, chords canbecreated byas-\nsigning multiple notes from aslice tothesame voice. Arandomised\nlocal search algorithm isused forﬁnding assignments thatminimise\naparametric cost function which isused toassess thequality of\npartial voice separations. This cost function includes components\nthatreﬂect therelationship between thenotes within aslice (possible\nchord groupings) aswell asbetween aslice andthepartial voice\nassignment ofpreviously processed slices (voice leading).\nThe task ofassigning thenotes ofaslice toanumber ofexisting\nvoices becomes challenging ifthenumber ofnotes differsfrom the\nnumber ofvoices available atthat point. Ifthere aremore notes\nwithin theslice than voices, chords havetobeintroduced inoneor\nmore voices, ornewvoices need tobecreated. Ourapproach works\nwith amaximum number ofvoices thatcaneither bespeciﬁed by\ntheuser orderivedfrom themaximum number ofnotes overlapping\natanypoint intime. (Itmay benoted thatwepreprocess theinput\ndataprior toperforming voiceseparation inorder todealwith certain\ntypes ofinaccuracies andnoise; thiswillbedescribed inmore detail\ninSection 5.2.) However,voices areonly used when required and–\ndepending ontheparameter setting forthecostfunction –theresult\nofthevoice separation process willnotalwaysutilise themaximum\nnumber ofvoices.\nAnempirical evaluation ofouralgorithm showsthatbyusing differ-\nent(user accessible) settings fortheparameters ofthecostfunctions,\narange ofreasonable voice separations canbeobtained foragiven\ninput. Inmanycases thatareknowntobeproblematic forother\nvoice separation methods, such asstatic split-point separation orthe\napproach described byTemperle y[9],ouralgorithm achie vesgood\nresults. Inother cases, including randomly generated music, pieces\nwith ahigh degree ofpitch overlaps between voices, orpieces with-\noutanyvoice structure, ourmethod encounters similar problems as\nother approaches.\nThe remainder ofthispaper isstructured asfollows. Inthenext\nsection, webrieﬂy describe previous workonvoice separation and\nknownalgorithmic approaches. Wethen introduce some formal\nconcepts thatarerequired foraprecise description ofthevoice sep-\naration problem addressed inthisworkandouralgorithmic solution\ntothisproblem. Thebasic voiceseparation algorithm ispresented in\nSection 4,andSection 5describes important aspects ofourcurrent\nimplementation. Finally ,wepresent anddiscuss arange ofsample\nresults obtained from ournewvoice separation algorithm. Weend\nwith some conclusions anddirections forfuture work.\n2.EXISTING APPR OACHES\nVarious approaches forﬁnding good voice separations ofagiven\ninput piece havebeen proposed intheliterature and/or areused in\npractice. Most commercial sequencer softw areproducts implement\ntheextremely simple split point separation technique, while more\ncomple xapproaches appear tobeonly implemented inacademic\nsystems, such asTemperle yandSleator’ sMelisma Music Analyzer\n[10].Voice Separ ation -ALocal Optimisation Appr oach\u000b \u0000\n \u0001 \u0000\n % \u0000\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\n\t \u000b\r\f\u000e\u0003 \u000f\u0010\b \u0007 \b \u0011 \u0012\u0014\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\u0019\u0013\u0014\u0018 \u001a\u001b\u0012 \u001c \u0007 \b \b\u001d\u0011\u0019\u0018 \u0012 \b \u0013\u0014\u001e\u001b\u0003 \u0012 \u001c\b \u001f \u0006 \u0016  \u000e\u0018 \u0011\u0019\u0013 \b \u0012\u001d\u0012 \u0003 !\u0014\b \u0013\u001d\u0016 \u0011\u0017\"#\"\u0017\u0006 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\u0019\u0013 \u000b$\u0016\u0019%\r\u0013 \u0003 \u0011\u0019\u0005  \b'& \u001c\u0019\u0018 \u0007 \"\u0017()*%+\u0012 \u001c\u0019\u0007 \b \b\u0014, \u0018 \u0003 & \b \u0013 (\u0010& %-\u0012 \u001e.\u0018/, \u0018 \u0003 & \b \u0013.\u001e\u001b\u0003 \u0012 \u001c\u001d& \u001c\u0019\u0018 \u0007 \"\u00170\u000b \u0000\n\u0001 \u0000\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\n1 \u000b32-\u0018 \u0003 & \b\n\u0013 \b \u0015\u0019\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011'\u001a \u0018 \u00074\u0016\n\u0015\u0017\u0003 \u0016 \u0011 \u00185\u0013 \u0012 6\u0019 \b\u001d\u0015\u0010\u0003 \b & \b \u000b\u0016\u0019%-\u001e\u001b\u0003 \u0012 \u001c\r\u0016\u00147\u00178 \b \"\r\u0013 \u0015\u0017 \u0003 \u0012\u001b\u0015\u0017\u0018 \u0003 \u0011 \u0012\u001b\u0016 \u0015\u0017\u0015\u0019\u0007 \u0018 \u0016 & \u001c\r\u0016 \u0011\u0019\"\r)\u0010%.\u001e\u001b\u0003 \u0012 \u001c\r\u0018 \u0006 \u0007\u0011\u0019\b \u001e$\u0016 \u0015\u0019\u0015\u0017\u0007 \u0018 \u0016 & \u001c\u001709 \u0015\u0017 \u0003 \u0012\u000e:\u0004\u0018 \u0003 \u0011 \u0012\u000e9 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\nOne ofthesimplest methods forvoice separation istosplit therange\nofallpossible pitches intoanumber ofdisjoint interv alsandtoassign\nnotes tovoices depending onwhich pitch range theyfallinto. For\ntwovoices, thisisachie vedbyﬁxing onepitch asasplit point and\nbyassigning allnotes with equal orhigher pitch totheﬁrst voice,\nwhile allnotes with alowerpitch areassigned tothesecond voice.\nIngeneral, forseparating apiece into;voices,;+<\u0014= split points are\nused todeﬁne the ;respecti vepitch ranges.\nThis approach iseasy toimplement andisused inmost commercial\nsequencer softw arepackages; however,itworks correctly only for\npieces inthere arenooverlaps intheranges ofdifferent voices and\nwillproduce errors ifthiscondition isnotsatisﬁed (seeFigure 2a).>\u001b\u0006\u0019 \b/?-\u0016 \u0013 \b \"\r@A\u0015\u0017\u0015\u0019\u0007 \u0018 \u0016 & \u001c\u0019\b \u0013\nBecause composers/arrangers aretypically using voice-leading rules\nwhen composing orarranging musical pieces, itispossible touse\nthesame rules fortheinverse task ofseparating notes intoseparate\nvoices. Studies ofhuman perception ofmusic showthatifmelodies\narefollowing such rules theyareeasier thepercei vebyhuman lis-\nteners [2]. Examples ofvoice leading rules include thefollowing:\n(1)prefer small interv alsbetween succeeding notes\n(2)keeprange ofavoice small\n(3)useasmall number ofvoices\n(4)avoidcrossings ofvoicesBecause there aremanysuch rules, including ‘weak’ rules which\ndepend onthemusical conte xt,thisapproach willleadtomanyerrors\ninpieces with more than twovoices orwith achanging number of\nvoices. Animplementation which mostly uses the‘nearest path\nrule’ (1)isdescribed byCambouropoulos [1]. The nearest path\nrule implementation inFERMA TA[6]showed thattheresults are\nmostly correct iftheinput data canbecleanly separated intoaﬁxed\nnumber ofcontinuous voices throughout thepiece. If,however,the\nnumber ofactivevoices changes ortheranges oftwovoices show\nmajor overlaps, thisapproach often produces erroneous separation.\nImpro vedperformance canbeachie vedbysegmenting thegiven\npiece intofragments during each ofwhich thenumber ofactivevoices\nremains constant, butunfortunately ,ﬁnding such segmentations can\nbedifﬁcult.\nAsthenumber ofrules increases, ﬁnding optimal voice separations\nwith respect toagivenrulesystem becomes anon-tri vialtask. Tem-\nperle yandSleator solvedthisproblem byusing adynamic program-\nming approach [9,10]. Their Melisma Music Analyzer includes a\ncontrapuntal analysis module which isbased onapreference rule\nsystem. Different from ourapproach presented here, their focus is\nmore onacorrect analysis than oncreating reasonable andﬂexible\nscore-notation. One major difference toourapproach isthefactthat\ntheir system does notdetect chords.\n3.PRELIMIN ARIES\nForthepurposes ofthiswork, weassume thatthepiece ofmusic\nforwhich voice separation istobeperformed isgivenintheform\nofalistofnotes sorted bythetime positions oftheir respecti ve\nonset times. TheBthnote inthislistisrepresented byafeature\nvector CED\u0019FHG I D J K D J L D\n\u0000,where I D, K D,and L Ddenote theonset time,\nduration, andpitch ofCED.WealsouseM N \u0005\n\u0001 OG CED\n\u0000,P Q R S\nO TM N*G CED\n\u0000,\nand U\nT O V WG CED\n\u0000torefer to I D, K D,and L Dforagivennote CED.Likewise,\nweassociate twointegervaluesX DandY Dwith each noteCEDto\nrepresent thevoice andchord CEDiscurrently associated with; we\nalsorefer tothese values as\u0004 M\nT V \u0001G CED\n\u0000and\nV WM R P\u0019G CED\n\u0000,respecti vely.\nFurthermore, wedeﬁne M Z \u0005\n\u0001 OG CED\n\u0000F$I D\u0019[\u001dK D,which effectively\nindicates theendpoint ofnoteB.\nNext,wedeﬁne some relations between notes thatareneeded inthe\nfollowing:CED\u0019\\\rC\u001b]_^ `aM N \u0005\n\u0001 OG CED\n\u0000\\3M N \u0005\n\u0001 OG C\u001b]\n\u0000CED F\u001dC\u001b]_^ `aM N \u0005\n\u0001 OG CED\n\u0000F\nM N \u0005\n\u0001 OG C\u001b]\n\u0000\nFurthermore, thefunctionM \u0004\n\u0001R b S U\u0010G CED J C\u001b]\n\u0000indicates whether notesCEDand C\u001b]overlap intime: M \u0004\n\u0001R b S U\u0010G CED J C\u001b]\n\u0000istrue ifandonly\nifCED.\\$C\u001b] andM Z!\u0005\n\u0001 OG CED\n\u0000\u001bcM N \u0005\n\u0001 OG C\u001b]\n\u0000orifCED\ncC\u001b]andM Z \u0005\n\u0001 OG C\u001b]\n\u0000\u0010cM N \u0005\n\u0001 OG CED\n\u0000.\nUsing these deﬁnitions, theinput ofouralgorithm canbeformally\nwritten as deFfG C\u0014g J h h h J CEi\n\u0000such that j B\u000ek'l = J h h h J m\u0017<\n= n4^CED\u0017\\4CED o\u0017g,\nT \u0002 \u0001 \u0002,asalistofnotes sorted inascending order oftheir\nrespecti veonset times.\nIntheoutput ofourvoice separation algorithm, weallowtwoor\nmore notes with thesame onset time only tobeassigned tothesame\nchord iftheyform achord:CED\u0019F\rC\u001b]_p \u0004 M\nT V \u0001G CED\n\u0000+qF \u0004 M\nT V \u0001G C\u001b]\n\u0000 rV WM R P\u0019G CED\n\u0000F\nV WM R P\u0019G C\u001b]\n\u0000\nForquantised input data, werestrict chords toconsist only ofnotes\nwith equal onset times. Inthecase ofunquantised input data, onset\ntimes may beimprecise andhence wehavetoallowcombining over-\nlapping notes with different onset times intothesame chord. Inour\nimplementation, werecognise andeliminate small overlaps andother\ninaccuracies during apreprocessing phase; thiswillbeexplained in\nmore detail inSection 5.Asaresult ofthese constraints, each voice\ngenerated byouralgorithm isasequence ofnon-o verlapping notes\nandchords.\nBefore assigning thenotes oftheinput piecedinto voices, weVoice Separ ation -ALocal Optimisation Appr oach\npitch\ntimey1y2y3\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u0001\u0000 \u000b\u0003\u0002\u00048 \u0016 !\u0014\u0015\u0017 \b\n\u001a \u0018 \u0007\u001d\u0015\u0019\u0016 \u0007 \u0012 \u0003 \u0012 \u0003 \u0018 \u0011\u0019\u0003 \u0011\u0019\u0005'\u00165\u0013 \u0003 !\u0014\u0015\u0010 \b\n\u0015\u0017\u0003 \b & \b\u0003 \u0011 \u0012 \u0018\r\u0013  \u0003 & \b \u0013 0\nwill partition dinto slices \u0004 Dofconsecuti veoverlapping notesG C\u0006\u0005 J h h h J C\u0006\u0005o \u0007\n\u0000such thatthere isanoverlap between anypairof\nnotes within each slice andthatbetween anytwoconsecuti veslices,\u0004 Dand\u0004 D o\u0019gthere areatleast twonotes thatdonotoverlap. This\nimplies thatallnotes from \u0004 Dexcept fortheonewith thesmallest\noffsettime may overlap with notes in\u0004 D o\u0017g(see Figure 3foran\nexample).\nMathematically ,thepartitioning ofdintoslices canbemodelled as\nfollows.First, wedeﬁne theset \bofallindices ofnotes in dthat\nbecome theﬁrstnotes oftheslices\u0004 g J h h h J \u0004 \t:\b ^ F l\u000b\n g J h h h J \n \t\r\f j B\u0017k/l = J h h h J \u000e\u0017n-^ \n D\u0017k\r\u000f\u0011\u0010\n g*F\n=\u0012\u0010\u0013\n \t\u001b\\\u001dm \u0010G j \u0014 J \u0015.k\u0014l \n D J h h h J \n D o\u0019g\u0010<4= n-^M \u0004\n\u0001R b S U\u0017G C\r\u0016 J C\u0006\u0017\n\u0000 \u0000\u0010G \u0018\u001a\u0019 \u001b\u001d\u001c\u001e\n D o\u0017g\u0004^ j \u0014.k/l \n D J h h h J \n D o\u0017g\u0010<4= n-^M \u0004\n\u0001R b S U\u0017G C\r\u0016 J C\u0006\u001f\n\u0000n\nBased on\b,wecannowdeﬁne thesetofallslices\u0004 D: ^ F l \u0004 g\u0019h h h \u0004 \t\r\f \u0004 D FHG C\u0006! \" J h h h J C\u0006! \" # $ %g\nn\nTheinduces thefollowing partitioning ofdintoslices\u0004 D:d F\nG C\u0014g J h h h J C\n! & %g' ( ) *+$\nJ C\n! &J h h h J C\n! , %g' ( ) *+&\nJ h h h J C\n! -J h h h J CEi' ( ) *+-\n\u0000\nWedenote avoice separation foraslice \u0004 D\u0017F5G C\n!\" J h h h J C\n!\" # $\n%g\n\u0000\nby.\u0004G \u0004 D\n\u0000F G \u001b D g J h h h J \u001b D \u0007\n\u0000where the\u001b D \u0016\u001dF G \u0004 M\nT V \u0001G C\u0006! \"o \u0016\n%g\n\u0000JV WM R P\u0019G C\n!\" o \u0016\n%g\n\u0000 \u0000represent thevoice andchord thatthe \u0014thnote\nofslice\u0004 Disassigned tounder separation.\u0004G \u0004 D\n\u0000.Asanabbre viation,\nwilluse .\u0019D\u0017^ F/.*G \u0004 D\n\u0000.Furthermore, weuse .\n\u0000Dtodenote thesetofall\npossible voice separations.*G \u0004 D\n\u0000forslice\u0004 D.Anyvoice separation.forthecomplete input piece correspond tothecombinations of\nvoice separations forallslices\u0004 D,\nT \u0002 \u0001 \u0002.\u001dF'G .\u0017g J h h h J .0\t\n\u0000,andthe\nsetofallpossible voice separations of disdenoted .\n\u0000.\nThenumber ofdifferent voice separations forasingle slice\u0004 D.\nT \u0002 \u0001 \u0002,\nthesize oftheset .\n\u0000D,depends onthenumber ofnotes in \u0004 D, \f \u0004 D \f,\nandonthemaximum number ofvoices inthedesired output ofour\nseparation algorithm, \u000e0132 B Y 4 \u001b.More precisely ,intheworstcase,\ninwhich anysubset ofnotes in\u0004 Dcanbecombined into achord,\nthere areatleast\u000e0132 B Y 4 \u001b 5\n+\"5possible voice separations ofslice\u0004 D.\nHence, intheworstcase, thenumber ofpossible voice separations\nofagiveninput piecedisexponential inthenumber ofnotes ind.\nThis suggests thatinorder toﬁndvoice separations thatareoptimal\nwith respect toagivensetofcriteria (which willbemore precisely\ndeﬁned inthenextsection), thenaivemethod ofenumerating allpos-\nsible separations andselecting thebestofthese canbeprohibiti vely\nexpensi ve,especially when thegoal istoallowtheuser tointerac-\ntivelytune theparameters voiceseparation process inorder toobtain\nadesired output. Consequently ,ourvoice separation algorithm uses\nasubstantially more efﬁcient, heuristically guided process foritera-\ntivelyconstructing voice separations using astochastic local search\nprocedure foroptimising theseparation ofindividual slices.\n\u0015\u0017\u0007 \u0018\u0019& \b \"\u0019\u0006\u0019\u0007 \b\n) \n \u000f % \u0011 # \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001dG d\u001dJ ;\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \n \u001b \u0010 \u0011 \u0012\u0018\u0007 \u000f \t \u0010\b\n \u001f \u001d \n \u0010 \u0011 \td\u001c\u0018\u000b 6 \u000f \u001c8\u000b \u0007 \u001d \" \u001cA\u0001 \u0011 \u001b \n \u001f ) \n \u000f % \u0011 \t;\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\t \u0011 . \u001c\u000e\u0011 \u001d \u0010d\n\u000f \u001d \u0010 \n\u000e\t \u0007 \u000f % \u0011 \t\u0004 g J h h h J \u0004 \t./^ FHG\n\u0000\u001a \u0018 \u0007B\u0004^ FH=\n\u0012 \u0018\u000e. D*^ F \u0005\n\u0001U S R S\nO \u0001 7b\nT V \u0001G \u0004 D J .\n\u0000.\r^ F8.E[9. D\u0011 \u0007 \u000f \u001c\u000e\u000f \u001d \u000b \u0010 \u0011\u001e\n ) \u0011 \u001b \u0007 \u000b \u001a \t\b\u0013 \u000f \u0010 \u0015 \u000f \u001d8) \n \u000f % \u0011 \t\b\n \u001f.\u000b \u001d \u0012\u0018\u001b \u0011 . \" \u0007 \u000b \u001b \u000f \t \u0011$% \u0015 \n \u001b \u0012 \t \u0013\b\u0015 \u0011 \u001b \u0011\u001e\u001b \u0011 : \" \u000f \u001b \u0011 \u0012\b \u0011\u0017\"\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b<;\u0019\u000b\r=A\u0006 \u0012  \u0003 \u0011\u0019\b\u0014\u0018 \u001a+\u0018 \u0006\u0019\u0007\u000e, \u0018 \u0003 & \b\u0014\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\r\u0016  \u0005 \u0018 \u0007 \u0003 \u0012 \u001c\u0019!9>\u001a \u0018 \u0007/\u0006\u0019\u0011\u0017\u001f \u0006\u0019\u0016 \u0011 \u0012 \u0003 \u0013 \b \"3\u0003 \u0011\u0019\u0015\u0017\u0006\u0019\u0012\u0014\" \u0016 \u0012 \u0016 (\u0004\u0012 \u001c\u0017\u0003 \u0013\u000e\u0015\u0019\u0007 \u0018\u0019& \b \"\u0017\u0006 \u0007 \b\r\u0003 \u0013E& \u0016   \b \"\u0016 \u001a \u0012 \b \u0007\r\u0007 \b !\u0014\u0018 ,\u0019\u0003 \u0011\u0019\u0005H\u0003 \u0011 \u0016 & & \u0006 \u0007 \u0016 & \u0003 \b \u00134\u0016 \u0011\u0017\"'\u0011 \u0018 \u0003 \u0013 \b\n\u0003 \u00115\u0012 \u001c\u0019\b\n\u0003 \u0011 \u0015\u0010\u0006 \u0012\u0015\u0017\u0003 \b & \bd\n0\n4.OUR HEURISTIC ALGORITHM\nThemain idea underlying ouralgorithm istoconstruct avoice sepa-\nration forthegiveninput piecedfrom locally optimised separations\nfortheslices of d.This local optimisation isbased onaparametric\ncost function ?thatassesses thequality ofthevoice separation of\nagivenslice,.\u0019D,giventheseparations ofallprevious slices,\nT \u0002 \u0001 \u0002,G .\u0019g J h h h J . D\n%g\n\u0000.The deﬁnition ofthiscost function will begiven\nbelow.\nGivenaninput piecedandamaximal number ofvoices\u000e0132 B Y 4 \u001b,\nourvoice separation algorithm works asfollows: After segment-\ningdintoslices\u0004 g J h h h J \u0004 \t(asdescribed above),acost-optimised\nvoice separation fortheﬁrstslice, \u0004 g,iscomputed. Then, thisvoice\nseparation isiterati velyextended bycost-optimised separations for\u0004 @ J h h h J \u0004 \t,resulting inacomplete voice separation ford.How-\never,particularly inthecase ofunquantised input data, thisvoice\nseparation might contain chords with notes that slightly differin\ntheir onset times ordurations aswell asoverlapping notes within\nthesame voice. Therefore, everytime after thevoice separation is\nextended byaslice separation, these situations areresolv edbyad-\njusting thedurations oronset times. This guarantees thatintheﬁnal\nresult there arenooverlaps between notes within anyofthevoices\nandallnotes within anychord havethesame onset time andduration.\nInthefollowing, wedescribe thecostfunction andthecost-op timising\nvoice separation ofslices inmore detail.\n4.1 The Cost Function\nThecostfunction?used forassessing andoptimising thequality of\navoice separation .\u0019Dofaslice \u0004 D,givenseparations .\u0019g J h h h J . D\n%g\nforallprevious slices, isaweighted sum ofterms that penalise\nindividual, undesirable features:?AG . D J .\n\u0000F L \u0007 D A B C ?\u000b\u0007 D A B C G .\u0019D J .\n\u0000[EL D E \u0007 ?\u000bD E \u0007 G . D J .\n\u0000[L B C F \u0017 G ?\u0012B C F \u0017 G G .\u0019D\n\u0000[\u0014L F H i ?\u000bF H i G .\u0019D J .\n\u0000\nHere,.denotes thepartial voice separationG .\u0019g J h h h J . D\n%g\n\u0000.Intu-\nitively, ?\u000b\u0007 D A B Cand ?\u0012D E \u0007penalise largepitch interv alsandgaps (rests)\nbetween successi venotes inavoice, respecti vely;?B C F \u0017 Gpenalises\nchords with alargepitch interv albetween thehighest andthelowest\nnote, aswell asirregular chords containing notes with different onset\ntimes ordurations; and ?\u0012F H ipenalises overlaps between successi ve\nnotes inthesame voice. Byadjusting theweights ofthese terms,\ndifferent trade-of fsbetween these features canbeachie ved,leading\ntoqualitati velydifferent voice separations (chordal, single voices,\u0001 O V \u0002;seeFigures 1and5).Inthefollowing, wedescribe indetail\nhowthefour penalty terms arecalculated.:-\u0003 \u0012 & \u001c\u001d\f\u000e\u0003 \u0013 \u0012 \u0016 \u0011\u0017& \b/:\u0004\b \u0011\u0019\u0016  \u0012 6?\u0012\u0007 D A B C\nThe segregation ofmultiple melodic lines byhuman listeners de-Voice Separ ation -ALocal Optimisation Appr oach\u000b=\n\u0000\n \u000b4\n\u0000\n\u0001=\n\u0000\n \u00014\n\u0000\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u0001\u0000 \u000bH\f\u000e\u0003 \u000f\u0010\b \u0007 \b \u0011 \u0012\r\u0013 \b \u0015\u0019\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\u0017\u0013/\u0018 \u001a\u0014\u0011 \u0018 \u00115\u0018 , \b \u0007  \u0016 \u0015\u0019\u0015\u0010\u0003 \u0011 \u0005\u0011\u0019\u0018 \u0012 \b \u0013 \u000b\r\u0016\u0019\t %\u000e\u0015\u0003\u0002.\u001c \u0018 \u0007 \"\nc-c\u0015\u0017:-\u0003 \u0012 & \u001c\u0017(A\u0016 1 %E\u0015\u0017:-\u0003 \u0012 & \u001c\nc-c\u0015\u0003\u0002.\u001c \u0018 \u0007 \"\u0010()*\t %+\u0015\u0017:-\u0003 \u0012 & \u001c\nc-c\u0015\u0003\u0004A\u0016 \u0015\u0010(\u0017)\u00171 %+\u0015\u0005\u0004\u001b\u0016 \u0015\nc-c\u0015\u0017:-\u0003 \u0012 & \u001c\u001a>\u000b\u0002\u00048\u0019\u0016 !\u0014\u0015\u0010 \b\u001b)\u0010%\u0003 \u0013+\u0016  \u0013 \u0018\u0014\"\u0017\u0003 \u0013 & \u0006\u0019\u0013 \u0013 \b \"\u0014)\u00196/>\u000e0\u001a=\u001b0\u0006\u0004\b\u0007 \b \u0007 \"\u0017\u0003 \u0011 \u0005 \b \u0011\n\t \u0000 \u000b ( \u0015\u001000\u0000 \u0000 \f (\u0017\u0002\u0004\u0003 \u0005 \r\u0006\u0019\u0007 \b4\t \u000e 0\npends verystrongly onthefrequenc ydistrib ution andseparation of\nthemelodies [2]. Consequently ,itmakessense tousesimilar fea-\ntures forintheconte xtofautomatic voice separation. The pitch\ndistance penalty increases with theinterv alsize between twosuc-\nceeding notes inavoice. Fortheﬁrstnote ofavoice, aﬁxedpenalty\nisimposed forstarting anewvoice. Insome cases (melodies includ-\ningshort sequences oflargeinterv als), a‘lookback mechanism’ can\nbeadvantageous bywhich thepitch attheendofanexisting voice is\ncalculated from theaverage pitch ofthelast\u000enotes intherespecti ve\nvoice. This mechanism behavessome what similar totheapproach\nofGjerdingen [3]inwhich themotion-tracking system moveswith\nsome delay from acurrent pitch tothepitch ofanincoming note.\nOnly iftheincoming note islong enough, themotion track erreaches\ntheexact pitch levelofthatnote andstays there.\nIfanoteC\u001b]isassigned toachord, wedeﬁne thepitch-distance\n(interv alsize) between note C\u001b]andapitch L iasthepitch-distance\ntothenote ofthechord which isclosest inpitch toLi.Therefore we\ndeﬁne afunction Y \u000f+B \u0010 Y \u0011\u0019G C\u001b] J L i\n\u0000which returns L B \u0010 Y \u0011\u0019G C\u001b]\n\u0000if C\u001b]isa\nnon-chord noteandreturns thepitch ofthechordnotewhich isclosest\nto L iotherwise. Ifnopitch lookback isused, thepitch ofanexisting\nvoiceX–when comparing topitchLi–isdeﬁned as\nV \u0012*T O V WG X J Li\n\u0000\n=\nV \u0012*T O V WG b \u0013\u0010N\u0010G X\n\u0000J L i\n\u0000,where b \u0013*N\u0017G X\n\u0000denotes thelatest onset time\ninavoiceX,\nT \u0002 \u0001 \u0002,b \u0013\u0010N\u0017G X\n\u0000FAM N \u0005\n\u0001 O(C\u001b])where\u0014isthelargest value\nsuch that X 2 B Y 4 G C\u001b]\n\u0000F Xin ..Ifpitch lookback isused, the\npitch ofvoiceXiscalculated asshowninFigure 6;U R\n\u0001\u0004 G C\u001b] J Li\n\u0000\ndenotes thenote directly preceeding note C\u001b]inthesame voice\nasC\u001b]andnotassigned tothesame chord asC\u001b].Ifthedirectly\npreceeding note isassigned toachord, thechord note which isclosest\ntopitchLireturned instead\nT \u0002 \u0001 \u0002,L \u0015 4 X G C\u001b] J Li\n\u0000^ F'C\u0006\u0005 where;is\nthelargestvalue such that C\n\u0005\u0016\u0015C\u001b]with \u0004 M\nT V \u0001G C\n\u0005\n\u0000F \u0004 M\nT V \u0001G C\u001b]\n\u0000\nand\nV WM R P\u0019G C\u0006\u0005\n\u0000+qF\nV WM R P\u0019G C\u001b]\n\u0000.Theweighting of0.8forthecurrent\npitch and0.2fortheprevious pitch thatisused inthiscalculation\nwasfound empirically togivegood results forthetested input data.\nThe pitch distance penalty ?\u000b\u0007 D A B Cforasingle voice canthen be\ncalculated asshowninFigure 7.Based onthe?\u0007 D A B Cvalues for\neach voice X,theoverall pitch distance penalty foracomplete slice\nseparation. Dcanbecalculated asshowninFigure 8.\u0004\u001b\u0016 \u0015\u001d\f\u000e\u0003 \u0013 \u0012 \u0016 \u0011\u0017& \b/:\u0004\b \u0011\u0019\u0016  \u0012 6?\u000bD E \u0007\nStudies haveshownthatmelodies with fewandshort rests areper-\nceivedmore easily asacoherent melodic linebyahuman listener\nthan melodies with manylong rests [2]. The structure ofmany\nmelodic lines inWestern music isconsistent with thisobserv ation.\nTherefore, weimpose agappenalty ifadding anote from thecurrent\n\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011\n% \u0017 \u000f \u0010 % \u0015G X\n&m\n&L ]\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b) \n \u000f % \u0011\u001e\u000f \u001d \u0012 \u0011 6X\n& \u0007 \n \n \u0018 \u0001 \u000b % \u00188\t \u000f \u0019 \u0011m\u001a \u000f \u0010 % \u0015L ]\n\n \u001f \u001d \n \u0010 \u0011\u0014\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\u000b ) \u0011 \u001b \u000b . \u0011\u001e\u001a \u000f \u0010 % \u0015L\n\n \u001f ) \n \u000f % \u0011X\u001f \n \u001b\b% \n \u001c\u0018\u001a \u000b \u001b \u000f \t \n \u001d8\u0010 \nL ]L \u0015 4 X \u001a\u00132 \u0010 4.^ F\nU R\n\u0001\u0004 G b \u0013*N\u0017G X\n\u0000J L ]\n\u0000L\u0014^ F\nV \u0012*T O V WG b \u0013*N\u0017G X\n\u0000J L ]\n\u0000B*^ F5=\u001e\u001b\u001c\u0019\u0003  \bB\u0017\\\u001dmL\u0014^ F\u0001\u001b h\n5\b\u001cL.[\u001d\u001b h\n4\b\u001cV \u0012*T O V WG L \u0015 4 X \u001a\r2 \u0010 4 J L ]\n\u0000L \u0015 4 X \u001a\u00132 \u0010 4.^ F3U R\n\u0001\u0004\u0019G L \u0015 4 X \u001a\u00132 \u0010 4\n\u0000B\u0004^ F\nB [\r=\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G L\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u001e\u000e \u000b :-\u0003 \u0012 & \u001c & \u0016  & \u0006\u0017 \u0016 \u0012 \u0003 \u0018 \u0011 \u001a \u0018 \u0007 , \u0018 \u0003 & \bX\n\u001e\u001b\u0003 \u0012 \u001cL B \u0010 Y \u0011 m 2 2 ; \n I Y ;\nc\u001b\n>\u0017\u0015\u0010\u0003 \u0012 & \u001c\u001d\u0003 \u0013A!\u0014\b \u0016 \u0013 \u0006 \u0007 \b \"\r\u0003 \u0011\r\u0013 \b !/\u0003 \u0012 \u0018 \u0011\u0019\b \u0013 0\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?\u000b\u0007 D A B C G . D J .\u0019J X\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t) \n \u000f % \u0011\u001e\u000f \u001d \u0012 \u0011 6X\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\u001a \u000f \u0010 % \u0015 \u0012 \u000f \t \u0010 \u000b \u001d % \u0011L X \u001fL \u0015 4 X \u001a\u00132 \u0010 4.^ F\nU R\n\u0001\u0004 G X J U\nT O V WG C\u001b]\n\u0000 \u0000L X \u001f$^ F\u0001\u001b  \u001a \u0018 \u0007\u001b\u0016   C\u001b]-k\u0006. D\n\u001e\u001b\u0003 \u0012 \u001c\u0004 M\nT V \u0001G C\u001b]\n\u0000F\rX\n\"\u0019\u0018L \u001fAB \u001b \u0010\u0004^ F\u0003\f\nV \u0012*T O V WG L \u0015 4 X \u001a\u00132 \u0010 4 J U\nT O V WG C\u001b]\n\u0000 \u0000<U\nT O V WG C\u001b]\n\u0000 \u0000\f ! =\n4 5L X \u001f#^ F\u001dL X \u001f3[3G =*<EL X \u001f\n\u0000\u001cL \u001f.B \u001b \u0010\u0003 \u001aV WM R P\u0019G L \u0015 4 X \u001a\u00132 \u0010 4\n\u0000+qF\nV WM R P\u0019G C\u001b]\n\u0000\u0012 \u001c \b \u0011L \u0015 4 X \u001a\u00132 \u0010 4.^ F\nC\u001b]\b \u0011\u0017\"\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G L X \u001f\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b#\" \u000b#\u0002-\u0016  & \u0006\u0019 \u0016 \u0012 \u0003 \u0018 \u0011'\u0018 \u001a?\u0012\u0007 D A B C\n\u001a \u0018 \u0007\r\u0016H\u0013 \u0003 \u0011 \u0005  \b\n, \u0018 \u0003 & \bX\u0003 \u0011.\u0019D\n>-\u0012 \u001c\u0019\b\u001d\"\u0019\u0003 ,\u0017\u0003 \u0013 \u0003 \u0018 \u0011H) 65\t 1 $3\u0003 \u0013E)\u0019\u0016 \u0013 \b \"\n\u0018 \u0011\n\u0012 \u001c \b\r\u001a \u0016 & \u0012E\u0012 \u001c\u0019\u0016 \u0012\u001e\u001b\u001c \b \u00115\u0006\u0019\u0013 \u0003 \u0011 \u0005\u0001%'& \f(&\u0014\"\u0019\u0016 \u0012 \u0016\u0019(\u001b\t 1 $\n\u0003 \u0013\u0014\u0012 \u001c\u0019\b3!\u0014\u0016 8\u0019\u0003 !\u0014\u0006\u0017! \u0015\u0017\u0003 \u0012 & \u001c\"\u0019\u0003 \u000f\u0010\b \u0007 \b \u0011\u0017& \b 0\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?\u000b\u0007 D A B C G . D J .\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\u001a \u000f \u0010 % \u0015 \u0012 \u000f \t \u0010 \u000b \u001d % \u0011\u001e\u001a \u0011 \u001d \u000b \u0007 \u0010 *L \u001fL \u001f$^ F\u0001\u001b  \u001a \u0018 \u0007\u001b\u0016   X\n\u0006\u0019\u0013 \b \"\r\u0003 \u0011. D\n\"\u0019\u0018L \u001f#^ F\u001dL \u001f3[3G =*<EL \u001f\n\u0000\u001c?\u0007 D A B C\nG . D J .\u0019J X\n\u0000\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G L K\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b)$ \u000b\u0016\u0002-\u0016  & \u0006\u0019 \u0016 \u0012 \u0003 \u0018 \u00114\u0018 \u001a*\u0015\u0010\u0003 \u0012 & \u001c4\"\u0017\u0003 \u0013 \u0012 \u0016 \u0011\u0019& \b\u000e\u0015\u0010\b \u0011\u0019\u0016  \u0012 6?\u0012\u0007 D A B C\u001a \u0018 \u0007A\u0013  \u0003 & \b\u000e\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011. D\n\u0005 \u0003 , \b \u0011\u0014\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011.\n\u001a \u0018 \u0007.\u0015\u0019\u0007 \b ,\u0019\u0003 \u0018 \u0006\u0017\u0013\u0013  \u0003 & \b \u0013 0Voice Separ ation -ALocal Optimisation Appr oach\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?\u000bD E \u0007 G . D J .\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b. \u000b \u001a \u0012 \u000f \t \u0010 \u000b \u001d % \u0011\u001e\u001a \u0011 \u001d \u000b \u0007 \u0010 *\u0001\u0000\u001f\u0000\u001f$^ F\u0001\u001bY \u001a\r2 \u0010 4 \u001bA^ F\u0001\u001b\u001a \u0018 \u0007\u001b\u0016   \n) \n \u000f % \u0011 \tX\n\u0006\u0019\u0013 \b \"\r\u0003 \u0011.\u0019DC ^ F\n\u0011 \u000b \u001b \u0007 \u000f \u0011 \t \u0010\b\u001d \n \u0010 \u0011\u001e\u000f \u001d.\u0019D\n\u0013 \u000f \u0010 \u0015\u0004 M\nT V \u0001G C\u001b]\n\u0000F\u001dX\u0000\u001f$^ F\n\u0000\u001f3[\u001e?\u000bD E \u0007 G C\u0014J X\n\u0000Y \u000e 2 \u0010 4 \u001b.^ F\nY \u000e 2 \u0010 4 \u001b\u0010[\u001d=\b \u0011\u0017\"\u0000\u001f$^ F\n\u0000\u001f ! Y \u000e 2 \u0010 4 \u001b\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G\n\u0000\u001f\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u0003\u0002 \u000b\u0001\u0002-\u0016  & \u0006\u0019 \u0016 \u0012 \u0003 \u0018 \u0011'\u0018 \u001a\u001b\u0005 \u0016 \u0015'\"\u0017\u0003 \u0013 \u0012 \u0016 \u0011\u0019& \b3\u0015\u0017\b \u0011 \u0016  \u0012 6?\u0012D E \u0007\u001a \u0018 \u0007A\u0013  \u0003 & \b\u000e\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011. D\n\u0005 \u0003 , \b \u0011\u0014\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011.\n\u001a \u0018 \u0007.\u0015\u0019\u0007 \b ,\u0019\u0003 \u0018 \u0006\u0017\u0013\u0013  \u0003 & \b \u0013 0\nslice toavoice introduces arest; furthermore, thepenalty increases\nwith theduration oftherest. Iftheadded note Cistheﬁrst of\ntherespecti vevoice, thetime difference between time position zero,T \u0002 \u0001 \u0002,theonset time oftheﬁrstnote in d,andtheonset time of Cis\npenalised. Because allnotes inaslice\u0004 Dareoverlapping each other ,\ngaps between notes within \u0004 Dcannot occur .\nThegapdistance penalty?\u000bD E \u0007forasingle noteC\u001b]andavoiceXis\ndeﬁned asthelength ofthegapintroduced invoice Xbyadding C\u000e]\ndivided bythemaximal gaplength introduced byaddingC\u001b]toany\nvoice in .;thisresults ingappenalty values thatarealwaysbetween\nzero andone. Based onthismeasure, theoverall gapdistance penalty\nfor .\u0019Dand .canbecalculated asshowninFigure 9.\u0002.\u001c \u0018 \u0007 \"\u001d\f\u000e\u0003 \u0013 \u0012 \u0016 \u0011\u0017& \b/:\u0004\b \u0011\u0019\u0016  \u0012 6?B C F \u0017 G\nBoth, thelimitations ofhuman physiology inplaying chords with\nverylargeranges,\nT \u0002 \u0001 \u0002,pitch differences between thehighest and\nlowest note, aswell ascompositional practice suggest that when\ncombining notes into chords, chords with small ranges should be\npreferred overchords with largeranges. Furthermore, inmost cases,\nwewould expect allnotes belonging tothesame chord tohave\nidentical orverysimilar onset times anddurations. (Note thatwe\nallowthegrouping ofnotes ofwith different onset times into the\nsame chord only forunquantised input data.) Hence, weuseachord\ndistance penalty thatincreases with therange ofachord, with the\ndifferences indurations ofitsnotes, andwith thedistance between\ntherespecti veonset times (inthecase ofunquantised data).\nBased onthese considerations, thefollowing penalty terms areused\nascomponents oftheoverall chord distance penalty foragivenslice\nseparation . D:\nTherange penaltyL \u0004+I \u000e\n\u00004forachordYisdeﬁned asL \u0004\u0004I \u000e\n\u00004 G Y\n\u0000F\u001c\u0018\u000f \u001dl R S N\n\u0003 \u0001G Y\n\u0000!\n4 3J = n,where R S N\n\u0003 \u0001G Y\n\u0000isthepitch difference be-\ntween thelowest andthehighest note inY(measured insemitones).\nNote thataccording tothisdeﬁnition, therange penalty isalways\navalue between zero andone, andallchords with arange oftwo\noctavesormore recei vethesame maximum penalty value ofone.\nAnalogous totherange penalty ,theduration penaltyL \u001f\u0006\u0005 \u0015 I \u0010 B 2 \u000efor\nachord Ydepends ontherelation between shortest andlongest note\ninY;itisdeﬁned asL \u001f\u0006\u0005 \u0015 I \u0010 B 2 \u000e\u0010G Y\n\u0000F\n=*<<\u001b K \u0005 \u0015 G Y\n\u0000! m K \u0005 \u0015 G Y\n\u0000,where\u001b K \u0005 \u0015 G Y\n\u0000and m K \u0005 \u0015 G Y\n\u0000arethedurations oftheshortest andlongest\nnote inY,respecti vely.Note thataccording tothisdeﬁnition, the\nduration penalty isalwaysavaluebetween zero andone, andarange\npenalty ofzero isobtained ifandonly ifallnotes ofchordYhave\nequal duration.\nFinally ,theonset time penaltyL \u0007\u001d\u000eofagivenchordYisdeﬁned\nas L \u0007\u001d\u000e\u0010G Y\n\u0000FfG m \u0007\u001d\u000e\u0010G Y\n\u0000</4 \u0007\u001d\u000e\u0010G Y\n\u0000 \u0000! m \u001f\u0006\u0005 \u0015 G Y\n\u0000,where m \u0007\u001d\u000e\u0010G Y\n\u0000and4 \u0007\u001d\u000e\u0010G Y\n\u0000aretheonset times ofthelatest andtheearliest note inY(with\n\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?\u000bB C F \u0017 G G . D\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b% \u0015 \n \u001b \u00128\u0012 \u000f \t \u0010 \u000b \u001d % \u0011\u001e\u001a \u0011 \u001d \u000b \u0007 \u0010 *Y \u001fY \u001f#^ F\u0001\u001b  \u001a \u0018 \u0007\u001b\u0016   \n% \u0015 \n \u001b \u0012 \tY\n\u0003 \u0011. D\n\"\u0019\u0018L\u0014^ F\nU \b+Q R S\nO TM N\u0010G Y\n\u0000[G =\u0004<\u0014U \b+Q R S\nO TM N*G Y\n\u0000 \u0000\u001cU \t*S N\n\u0003 \u0001G Y\n\u0000L\u0014^ F\u001dL.[\u001dG =\u0004<\u000eL\n\u0000\u001cU\u0006\u0013\u0010N\u0010G Y\n\u0000Y \u001f#^ F\nY \u001f3[\u001dG =\u0004</Y \u001f\n\u0000\u001cL\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G Y \u001f\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b \t \f \u000b \u0002-\u0016  & \u0006\u0017 \u0016 \u0012 \u0003 \u0018 \u0011 \u0018 \u001a\u001d& \u001c \u0018 \u0007 \" \"\u0017\u0003 \u0013 \u0012 \u0016 \u0011\u0019& \b$\u0015\u0010\b \u0011\u0019\u0016  \u0012 6?B C F \u0017 G\n\u001a \u0018 \u0007\u000e\u0013  \u0003 & \b/\u0013 \b \u0015\u0019\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011.\u0019D\n0\nrespect totheir respecti veonset times), while m \u001f\n\u0005 \u0015 G Y\n\u0000istheduration\nofthelongest note inY.Note thattheonset time penalty forchords\ninwhich allnotes haveequal onset times iszero; furthermore, L \u0007\u001d\u000e\nvalues canneverbelargerthan one, because non-o verlapping notes\ncannot bepartofthesame slice andhence willneverbecombined\nintothesame chord.\nBased onthese three penalty terms forindividual chords, theoverall\nchord distance penalty foracomplete slice separation.\u0019Discalcu-\nlated asshowninFigure 10.This particular wayofcombining the\npenalty terms ischosen toensure thatifoneoftheterms islarge,the\noverall chord penalty islargeaswell; note thatbyusing a(weighted)\narithmetic average ofthethree penalty terms, thisproperty cannot\nbeguaranteed.=., \b \u0007  \u0016 \u00153\f\u000e\u0003 \u0013 \u0012 \u0016 \u0011\u0019& \b\u0014:\u0004\b \u0011 \u0016  \u0012 6?F H i\nAlthough notes within avoicegenerally should notoverlap, depend-\ningontheinstrument andstyle ofmusic, there arecases inwhich the\nnotes ofasingle melodic lineareplayed with substantial overlaps\nthatcannot beremo vedreliably bypreprocessing, asillustrated in\ntheofﬁngered pedal example showninFigure 11.Therefore, we\nallowoverlapping notes tobeassigned tothesame voice without\ncombining them into achord, butimpose apenalty thatincreases\nwith theamount ofoverlap. (Note thatsuch overlaps areultimately\neliminated inouralgorithm byshortening theduration oftheearlier\nnote.)\nConsequently ,wedeﬁne theoverlap distance penalty fortwosuc-\ncessi venotes C\u001b]and C\n\u0005within thesame voiceas ?\u0012F H i G C\u001b] J C\n\u0005\n\u0000F=\u0004<\rG M N \u0005\n\u0001 OG C\u0006\u0005\n\u0000<\rM N \u0005\n\u0001 OG C\u001b]\n\u0000 \u0000! P Q R S\nO TM N\u0010G C\u001b]\n\u0000ifC\u001b]andC\u0006\u0005over-\nlap,\nT \u0002 \u0001 \u0002,if M \u0004\n\u0001R b S U\u0017G C\u001b] J C\n\u0005\n\u0000,and ?\u0012F H i G C\u001b] J C\n\u0005\n\u0000F \u001botherwise.\nAccording tothisdeﬁnition, theoverlap distance penalty between\nnotes ?\u0012F H i G C\u001b] J C\n\u0005\n\u0000isalwaysavalue between zero andone. The\noverlap distance penalty forasingle voiceXused inaslice separation. Dandtheoverall overlap distance foraslice separation .\u0019Darethen\ncalculated asshowninFigures 12and13.\n4.2 Cost-Optimised Slice Separation\nBased onthecost function ?deﬁned aboveandgivenaseparation.ofslices\u0004 g J h h h J \u0004 D\n%g,weuseastochastic local search approach\nforﬁnding acost-optimised voiceseparation . Dforslice \u0004 D:Starting\nwith aninitial separation.\u0019D\u0017^ F/.\f\u000bD,aseries ofrandomised iterati ve\nimpro vement steps isperformed during each ofwhich onenote is\nreassigned toadifferent voice. Whene versuch step results inan\nassignment with lower cost than thebest assignment seen sofar,\nthisassignment anditscost arememorised. This search process\nisterminated when nosuch impro vement hasbeen achie vedfora\nﬁxednumber\rofsteps. Inthecurrent implementation, weuse\r-F\n6\b\u001c\f \u0004 D \f\n\u001c\u000e0132 B Y 4 \u001b.Figure 14givesapseudo-code speciﬁcation\nofthisrandomised iterati veimpro vement procedure.\nAninitial separation .\f\u000bDforthegivenslice . Disobtained byassigning\nallnotes of\u0004 Dtotheﬁrstvoice. During thisprocess, notes with equalVoice Separ ation -ALocal Optimisation Appr oach\u000b \u0000\n\u0001 \u0000\n % \u0000\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u0014\t \t \u000b-\f\u000e\u0003 \u000f\u0010\b \u0007 \b \u0011 \u0012-\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\u0019\u0013-\u0018 \u001a*\u0012 \u001c \u0007 \b \b\u001b\u0018 , \b \u0007  \u0016 \u0015\u0019\u0015\u0010\u0003 \u0011 \u0005\u0011\u0019\u0018 \u0012 \b \u0013 \u000b-\u0016\u0019%-\u0003 \u0011\u0019\u0015\u0017\u0006\u0019\u0012\u001b\" \u0016 \u0012 \u0016\u0014)\u0010%-\u0016   \u0010\u0018 , \b \u0007  \u0016 \u0015\u0017\u0013.\u0007 \b !\u0014\u0018 , \b \"/\u0012 \u0018/\u001a \u0018 \u0007 !\u0013 \u0003 \u0011\u0019\u0005  \b\u0014, \u0018 \u0003 & \b (\u0010& %.\u0013 \u0015\u0010 \u0003 \u0012\u001b\u0003 \u0011 \u0012 \u0018/\u0012 \u001c \u0007 \b \bE, \u0018 \u0003 & \b \u0013 0\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?\u000bF H i G . D J .\u0017J X\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t) \n \u000f % \u0011 \u0000 \u0001 \u0017X\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\n ) \u0011 \u001b \u0007 \u000b \u001a \u0012 \u000f \t \u0010 \u000b \u001d % \u0011\u001e\u001a \u0011 \u001d \u000b \u0007 \u0010 *2 X \u001fL \u0015 4 X \u001a\u00132 \u0010 4.^ F'b \u0013*N\u0017G X\n\u00002 X \u001f#^ F\u0001\u001b\u001a \u0018 \u0007\u001b\u0016   C\u001b]\n\u000f \u001d\u0004 D\n\u0013 \u000f \u0010 \u0015\u0004 M\nT V \u0001G C\u001b]\n\u0000F\rX\n\"\u0019\u00182 \u001fAB \u001b \u0010+^ F\u0001?\u0012F H i G L \u0015 4 X \u001a\r2 \u0010 4 J C\u001b]\n\u00002 X \u001f#^ F82 X \u001f3[\u001dG =\u0004<<2 X \u001f\n\u0000\u001c2 \u001f.B \u001b \u0010\u0003 \u001aV WM R P\u0019G C\u001b]\n\u0000+qF\nV WM R P\u0019G L \u0015 4 X \u001a\r2 \u0010 4\n\u0000\u0012 \u001c \b \u0011L \u0015 4 X \u001a\u00132 \u0010 4.^ F\nC\u001b]\b \u0011\u0017\"\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G 2 X \u001f\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b5\t 1 \u000b#\u0002-\u0016  & \u0006\u0019 \u0016 \u0012 \u0003 \u0018 \u0011'\u0018 \u001a\u000e\u0018 , \b \u0007  \u0016 \u0015$\"\u0019\u0003 \u0013 \u0012 \u0016 \u0011\u0017& \b\n\u0015\u0010\b \u0011\u0019\u0016  \u0012 6\u001a \u0018 \u0007E\u0013 \u0003 \u0011\u0019\u0005  \bE, \u0018 \u0003 & \b 0\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011?F H i\nG . D J .\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\u0019D) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\n ) \u0011 \u001b \u0007 \u000b \u001a \u0012 \u000f \t \u0010 \u000b \u001d % \u0011\u001e\u001a \u0011 \u001d \u000b \u0007 \u0010 *2 \u001f2 \u001f#^ F\u0001\u001b\u001a \u0018 \u0007\u001b\u0016   X\n\" \t \u0011 \u0012\u0018\u000f \u001d.\u0019D2 \u001fAB \u001b \u0010+^ F\u0001?\u0012F H i G .\u0019D J .\u0019J X\n\u00002 \u001f#^ F82 \u001f3[\u001dG =\u0004<<2 \u001f\n\u0000\u001c2 \u001f.B \u001b \u0010\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G 2 \u001f\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b5\t \u0000 \u000b#\u0002-\u0016  & \u0006\u0019 \u0016 \u0012 \u0003 \u0018 \u0011'\u0018 \u001a\u000e\u0018 , \b \u0007  \u0016 \u0015$\"\u0019\u0003 \u0013 \u0012 \u0016 \u0011\u0017& \b\n\u0015\u0010\b \u0011\u0019\u0016  \u0012 6?F H i\n\u001a \u0018 \u0007A\u0013  \u0003 & \b\u000e\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011.\u0019D\n\u0005 \u0003 , \b \u0011/\u0013 \b \u0015\u0019\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011.\n\u001a \u0018 \u0007.\u0015\u0019\u0007 \b \r,\u0017\u0003 \u0018 \u0006\u0017\u0013A\u0013  \u0003 & \b \u0013 0\n\u001a \u0006\u0019\u0011\u0017& \u0012 \u0003 \u0018 \u0011\n\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u0011 # \u0007 \u000f % \u0011G \u0004 D J .\n\u0000\u0003 \u0011 \u0015\u0010\u0006 \u0012 \u000b\t \u0007 \u000f % \u0011\u0004 D) \n \u000f % \u0011\u001e\t \u0011 \u001a \u000b \u001b \u000b \u0010 \u000f \n \u001d.\n\u001f \n \u001b\u0019\u001a \u001b \u0011 ) \u000f \n \" \t\b\t \u0007 \u000f % \u0011 \t\u0018 \u0006 \u0012 \u0015\u0010\u0006 \u0012 \u000b\n \u001a \u0010 \u000f \u001c\u000e\u000f \t \u0011 \u0012\u0018\t \u0011 \u0007 \u0011 % \u0010 \u000f \n \u001d.\nF \u0007 AD\n \u0001 \u0010 \u000b \u000f \u001d. D\n\u0001 *\u0018\t \u0011 \u0010 \u0010 \u000f \u001d .\u0018\u000b \u0007 \u0007 \u001d \n \u0010 \u0011 \t\b\n \u001f\u0004 D\n\u0010 \n\u000e) \n \u000f % \u0011\u001b\u000b \u001d \u0012\u0018% \n \u001cA\u0001 \u000f \u001d \u000f \u001d .\u0018\u000b \u0007 \u0007 \u001d \n \u0010 \u0011 \t\b\u0013 \u000f \u0010 \u00158\u0011 : \" \u000b \u0007 \n \u001d \t \u0011 \u0010\b\u0010 \u000f \u001c\u0018\u0011 \t\u000f \u001d \u0010 \n$% \u0015 \n \u001b \u0012 \t.\nF \u0007 AD\n^ F8.\u0019D\u000e 2 \u0002 CAL \u0015A^ F\u0001\u001b\u001e\u001b\u001c\u0019\u0003  \b\u000e 2 \u0002 CAL \u0015\n\u0015\f \u0004 D \f \u0003\u001a\u000e01 2 B Y 4 \u001b\u0004\u0003\n6\u001e\u001b\u0003 \u0012 \u001c4\u0015\u0017\u0007 \u0018 )\u0019\u0016 )\u0017\u0003  \u0003 \u0012 6\u001b h\n5\n\" \u0018. D*^ F\n\u001d \u0011 \u000f . \u0015 \u0001 \n \" \u001b.\u0006\u0005D\n\n \u001f.\u0019D\n\u0013 \u000f \u0010 \u0015\u001c\u0018\u000f \u001d \u000f \u001c8\u000b \u0007 % \n \t \u0010?AG .\u0006\u0005D\nJ .\n\u0000\u0018 \u0012 \u001c \b \u0007 \u001e\u001b\u0003 \u0013 \b. D*^ F\n\u001b \u000b \u001d \u0012 \n \u001c\u000e\u0007 *\u000e\t \u0011 \u0007 \u0011 % \u0010 \u0011 \u00128\u001d \u0011 \u000f . \u0015 \u0001 \n \" \u001b\b\n \u001f. D\b \u0011\u0017\"\u0003 \u001a?AG .\u0019D J .\n\u0000\u0015?AG .\nF \u0007 AD\nJ .\n\u0000\u0012 \u001c \b \u0011.\nF \u0007 AD\n^ F/. D\u000e 2 \u0002 CAL \u0015\u001b^ F\u0001\u001b\b  \u0013 \b\u000e 2 \u0002 CAL \u0015\u001b^ F8\u000e 2 \u0002 CAL \u0015*[\u001d=\b \u0011\u0017\"\b \u0011\u0017\"\u0007 \b \u0012 \u0006\u0019\u0007 \u0011G .\nF \u0007 AD\n\u0000\b \u0011\u0019\"\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\r\t ;\u0019\u000b\u000e>A\u0016 \u0011\u0019\"\u0019\u0018 !/\u0003 \u0013 \b \"3\u0003 \u0012 \b \u0007 \u0016 \u0012 \u0003 , \b4\u0003 !\u0014\u0015\u0017\u0007 \u0018 , \b !\u0014\b \u0011 \u0012A\u0016  \u0005 \u0018 \r\u0007 \u0003 \u0012 \u001c\u0019! \u001a \u0018 \u000747\u0010\u0011\u0019\"\u0017\u0003 \u0011 \u0005H\u0016H& \u0018 \u0013 \u0012 \r \u0018 \u0015\u0019\u0012 \u0003 !/\u0003 \u0013 \b \"$\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u00115\u001a \u0018 \u00074\u0016\u0013 \u0003 \u0011\u0019\u0005  \b/\u0013  \u0003 & \b\u0004 D\n0\nonset times arecombined intochords. Another natural choice forthe\ninitial separation istodistrib uteallnotes in \u0004 Dintovoices such that\noverlaps andchords areavoided asfaraspossible. Empirical tests\n(not reported here) suggested thattheformer initialisation method\nleads tobetter results than thelatter approach.\nSubsequently ,ineach local search step wemovefrom thecurrent\nseparation of\u0004 Dtoaneighbouring separation; twoseparations. Dand.\u0006\u0005Dareneighbours ifandonly if .\u0019Dand .\u0007\u0005Dareboth validseparations\nof\u0004 Dthatdifferinthevoice and/or chord assignment ofexactly one\nnote in \u0004 D.Aseparation isvalidifandonly ifanynotes with identical\nonset times thatareassigned tothesame voice arealso combined\nintoachord.\nThe selection oftheactual search step tobeperformed isbased\nonarandomised greedy choice: Withacertain probability (inour\nimplementation, weused avalue 0.8forwhich weobtained good\nempirical results), theneighbouring separation with minimal costis\nselected, otherwise, aneighbour ofthecurrent assignment isselected\nuniformly atrandom. Therandomisation preventsthesearch process\nfrom getting stuck inlocal minima ofthecostfunction?.\nWhile there isnotheoretical guarantee thatthisrandomised iterati ve\nimpro vement algorithm willﬁndtheglobally optimal separation for\nthegivenslice, itﬁnds optimal orclose-to-optimal separations very\nefﬁciently inpractice. Similar stochastic local search strate gieshave\nbeen verysuccessfully applied tomanyprominent combinatorial\nproblems (see[4]).\n5.IMPLEMENT ATION\nOurvoiceseparation algorithm isimplemented inthecurrent version\nof \b\n\t \u000b \t \f \r \b\u000f\u000e,alargerprogramme forconverting MIDI ﬁles into\u0010\u000f\u0011 \u0012 \u0013\u0006\u0014Music Notation. Allparameters forthevoice separation can\nbespeciﬁed inaninitialisation ﬁle(fermata.ini). Thecode iswritten\ninANSI C++ andhasbeen successfully compiled andtested under\nWindows,Linux andMac OS.\nThe input data canbequantised orunquantised low-levelmusical\ndata. Unquantised data (where thetempo might also beunkno wn)\nrequires preprocessing toremo veinaccuracies andnoise thatcouldVoice Separ ation -ALocal Optimisation Appr oach\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u000e\t \u0000 \u000b\u001d\u0002+\u0011\u0017\"\u0019\u0003 \u0011\u0019\u0005\u001b\u0018 \u001a\u0017\u0012 \u001c \bA& \u001c \u0018 \u0007 \u0016  \u0001\u0000 %\n\u0003 \u0012 \u0012 \b \u0011E\u001e\u001b\u0003 \u0007-\u0003 !\u0003\u0002\u0010\b )\u0010\b \u0011\u0013 \u0003 \u0011\u0017\"\u0005\u0004.) 6\u0007\u0006\u00170 9\u00190.?.\u0016 & \u001c\u0017(\u001b?\t\b$2 \u0000 $ \u0000 (\u001b\u0013 \b \u0015\u0019\u0016 \u0007 \u0016 \u0012 \b \"H\u0016 \u0013\u0014\u00163\u001a \u0018 \u0006 \u0007, \u0018 \u0003 & \b\u0014\u0013 & \u0018 \u0007 \b 0\nhavedetrimental effects ontheperformance ofthevoice separation\nalgorithm.\nOur preprocessing remo vessmall overlaps between notes asmay\narise,\n\u0001 \u0002 \u0003 \u0002,when playing legato passages onakeyboard instrument.\nWeconsider anoverlap small, ifthetime difference between the\noffset oftheearlier note and theonset time ofthelater note is\nsmall compared tothedurations ofthetwonotes. Such overlaps are\neliminated byshortening theduration oftheearlier note\nFurthermore, minor differences intheonset times oftwonotes that\nresult from imprecise playing canberesolv edbyreplacing both on-\nsettimes with their average ifthedurational overlap ofthetwonotes\nislargecompared totheonset time distance andiftheonset time\ndistance isverysmall.\n\u0000Notes with alonger duration orgreater inten-\nsitycanhavea‘higher inﬂuence’ onthecalculation oftheresulting\naverage onset time. This onset time correction forces ourvoice sep-\naration algorithm tocombine therespecti venotes intochords ifthey\ngetassigned tothesame voice andhence facilitates therecognition\nofchords.\nOur implementation uses MIDI ﬁles asinput data; these canbe\nobtained from notation softw are,byrecording ahuman performance\nusing aMIDI sequencer ,from awavetoMIDI converter,orasthe\nresult ofconverting musical data from other formats.\nThefour penalty parameters andthepitch lookback parameter can\nbedeﬁned bytheuser inaninitialisation ﬁle. Parameters forwhich\nnovalueisspeciﬁed aresettodefaultvalues predeﬁned inourimple-\nmentation. Themaximum number ofvoices tobeused intheﬁnal\nseparation ofthegivenpiece canalso bespeciﬁed intheinitialisa-\ntionﬁle. Ifthisparameter isnotspeciﬁed bytheuser,themaximum\nnumber ofvoices issettothemaximum number ofoverlapping notes\natanytime position oftheinput piece.\n6.RESUL TSAND DISCUSSION\nWetested ourapproach ondifferent types ofmusic: most ofthe\ninventions byJ.S.Bach, some chorals bythesame composer ,awaltz\nbyF.Chopin, parts of‘Mikrok osmos’ byB.Bartok, andseveralother\npieces. Asinput data weused quantised MIDI ﬁles with allevents\nmergedintoasingle track.\nWiththecorrect parameter settings, thetested Bach chorals and\ninventions were separated almost entirely correctly (see Figures 15\nand16). Only inafewcases where thevoices nearly meet atthesame\npitch, sometimes localised errors occurred. Withdifferent parameter\nsettings, itwaspossible toseparate thechorals into single voices,\nintoatwostaffpiano score, ortocollect them aschords inonevoice.\nFigure 17showsacorrect separation ofapartofawaltzbyChopin\nobtained from ourvoice separation algorithm. When using higher\nchord penalties andloweroverlap penalties, however,thesame input\ndata is(incorrectly) separated asshowninFigure 18.\u0000\n\u0001 \u0010\u0019\u0015 \u000b \t-\u0001 \u0011 \u0011 \u001d\u0014\t \u0015 \n \u0013\b\u001d\u0014\u0011 6 \u001a \u0011 \u001b \u000f \u001c\u0018\u0011 \u001d \u0010 \u000b \u0007 \u0007 *\f\u0010 \u0015 \u000b \u0010\u001e\n \u001d \t \u0011 \u0010\u001e\u0010 \u000f \u001c\u000e\u0011 \t\u001e\u0013\b\u000f \u0010 \u0015-\u000b\u0012 \u000f \t \u0010 \u000b \u001d % \u0011$\n \u001f \u0007 \u0011 \t \t\u0019\u0010 \u0015 \u000b \u001d\f\u000b \u001a \u001a \u001b \n 6 03\u001b\n\u001c\u0018\t\u0019\u000b \u001b \u0011$\u001a \u0011 \u001b % \u0011 \u000f ) \u0011 \u0012 \u000b \t\u0019\t \u000f \u001c$\" \u0007 \u0010 \u000b \u0000\u001d \u0011 \n \" \t\u0004\u0001 *\u0018\u0015 \" \u001c8\u000b \u001d\u0018\u0007 \u000f \t \u0010 \u0011 \u001d \u0011 \u001b \tG\n\t \u0011 \u0011 &\u0001 \u0002 \u0003 \u0002\n&\u0001\n4 \u000b\n\u0000 0\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u000e\t \u000e \u000b\u001d\u0002+\u0011\u0017\"\u0019\u0003 \u0011\u0019\u0005\u001b\u0018 \u001a\u0017\u0012 \u001c \bA& \u001c \u0018 \u0007 \u0016  \u0001\u0000 %\n\u0003 \u0012 \u0012 \b \u0011E\u001e\u001b\u0003 \u0007-\u0003 !\u0003\u0002\u0010\b )\u0010\b \u0011\u0013 \u0003 \u0011\u0017\"\u0005\u0004*) 6\f\u0006\u00170 9\u00170\u0010?.\u0016 & \u001c\u0017(*?\r\b'2 \u0000 $ \u0000 (*\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \b \"\r\u0016 \u0013A\u0016/\u0015\u0017\u0003 \u0016 \u0011 \u0018 \r\u0013 \u0012 6\u0017 \b\u0014\u0013 & \u0018 \u0007 \b 0\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u001d\t \" \u000b \u0002.\u001c \u0018 \u0015\u0010\u0003 \u0011\u0017(\u00042-\u0016  \u0013 \b (\u001d=.\u0015\u0010\u0006\u0019\u0013(\u000e ;\u000f\u000e\u001b\u0007 0+\t (\u0004!/!40+\t \f \t \r\t \f ;\u00190\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b\u001d\t $ \u000b \u0002.\u001c \u0018 \u0015\u0010\u0003 \u0011\u0017(\u00042-\u0016  \u0013 \b (\u001d=.\u0015\u0010\u0006\u0019\u0013(\u000e ;\u000f\u000e\u001b\u0007 0+\t (\u0004!/!40+\t \f \t \r\t \f ;\u0019(\u0010\u0003 \u0011\u0017& \u0018 \u0007 \u0007 \b & \u0012\u000e\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\u00170Voice Separ ation -ALocal Optimisation Appr oach\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b \t \u0002 \u000b \u0006\u00170 9\u00170$?-\u0016 & \u001c\u0010( \b\n\b   \r\u0001\n\b !\u0014\u0015\u0017\b \u0007 \b \" \u0002. \u0016 ,\u0017\u0003 \b \u0007 (?.\u0018 \u0018 \u0002 & (*\u0002\u0019\u0006\u0019\u0005 \u0006 \b \u000e\u001b\u0018\u00190\u0004\tE\u0003 \u0011 \u0002 %3\u0016 \u0007 \u0018 \u0007 (\u0010!/!40*\t \f \r \t \t 0\u000b \u0000\n\u0001 \u0000\n\u0002\u0004\u0003 \u0005 \u0006 \u0007 \b 1 \f \u000b \u0006\u00170 9\u00170$?-\u0016 & \u001c\u0010( \b\n\b   \r\u0001\n\b !\u0014\u0015\u0017\b \u0007 \b \" \u0002. \u0016 ,\u0017\u0003 \b \u0007 (?.\u0018 \u0018 \u0002\u001d& (\u0017\u0002\u0017\u0006 \u0005 \u0006\u0019\b \u000e\u001b\u0018\u00190\u001a\u0000\u0014\u0003 \u0011\n\u0002\u0004\u0003 %3\u0016 \u0007 \u0018 \u0007 (\u0017!/!40\u0010\t \r \u0000 0-\u0016\u0019%\u0016\u0002-\u0018 \u0007 \r\u0007 \b & \u0012A\u0013 \b \u0015\u0017\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u00114\u001e\u001b\u0003 \u0012 \u001cL \u0005+I L\ncL \u000f+B \u0010 Y \u0011\n0+)*% & \u0011\u0019& \u0018 \u0007 \u0007 \b & \u0012\u001b\u0013 \b \u0015 \r\u0016 \u0007 \u0016 \u0012 \u0003 \u0018 \u0011\r\u001e\u001b\u0003 \u0012 \u001cL \u000f+B \u0010 Y \u0011\ncL \u0005+I L\n0\nSituations inwhich avoice continues with alargeinterv alstepafter\narestcanlead toincorrect voice separations. Intheexample of\nFigure 19,thealtoandtenor voices pause inthemiddle ofmeasure\n10. Intheoriginal score, thealto voice continues attheendof\nmeasure 10andthetenor voice hasstill tacet. Because theﬁrst\nnote ofthecontinuing motive(d1) hasasmaller pitch distance anda\nsmaller gapdistance tothetenor voice(g0)than tothealtovoice(b1)\nandbecause there areonly three notes toseparate intofour voices,\nthealgorithm assigns thisfragment incorrectly tothetenor voice.\nThe same example isalso discussed byTemperle y[9],whose ap-\nproach encountered thesame problems. Wecould notimpro vethis\nresult bychanging parameter settings. Itseems thatwhen only con-\nsidering thenotes, without anyknowledge about thecomposer and\nstyle ofthepiece, there isnoreason why another separation should\nbepreferred. This case showsthat there existscores orcompo-\nsitions where atsome points theintention ofthecomposer differs\nfrom theresults obtained byapplying standard voice-leading rules\norcost functions. Inmore comple xpieces (\n\u0001 \u0002 \u0003 \u0002,Mikrok osmos 153\nbyBartok), thesame effectoccurs atsome positions. Inanother\nexample (Figure 20), wecould avoidtheproblems which Temper -\nleydiscusses inhisbook byusing ouralgorithm with appropriately\nchosen parameter settings.7.CONCLUSIONS AND FUTURE WORK\nWepresented anapproach forvoice separation onlow-levelmusical\ndata using stochastic local search forcalculating anoptimal voice\nstructure. Different from most other existing approaches which are\nrestricted toseparating apiece intomultiple monophonic lines, our\nsystem candetect chords. Itisalso capable ofproducing different\ntypes ofvoice separations, based onthesettings ofasmall setof\nparameters thatareaccessible totheuser.These parameters control\ntherelati veinﬂuence ofvarious criteria forgood voice separations\ninanintuiti veway.\nOur algorithm canbeapplied toquantised input data aswell asto\nunquantised input data with ourwithout tempo information. For\nunquantised input data, weapply preprocessing inorder toeliminate\ninaccuracies andnoise prior torunning ourvoice separation algo-\nrithm. Itmay benoted thatcorrectly separating thevoices within\nunquantised input datacanincrease thequality ofasubsequent quan-\ntisation process.\nWebelie vethatthequality oftheresults obtained from ouralgo-\nrithm canbefurther impro vedbyﬁne-tuning thecost function. For\nexample, ourlatest tests indicate thatusing anexponential function\nforpenalising thechord range may givebetter voice separations.\nWearecurrently investigating possibilities forconnecting ourcurrent\nimplementation (midi2gmn) totheonline\n\u0010\u000f\u0011 \u0012 \u0013\u0006\u0014NoteServ er[8]to\nprovide anonline MIDI-to-Score service. Inthisconte xt,weare\nstudying further optimisations ofourimplementation with respect to\nitsrun-time.\nInthefuture, weplan todevelop agraphical interf aceforourvoice\nseparation programme thatallowstheuser tochange thepenalty pa-\nrameters andseethecorresponding results inreal-time. Furthermore,\ninsome cases itwould bebeneﬁcial tosupport theuseofdifferent\nparameter settings fordifferent parts ofagivenpiece. This could be\naccommodated byallowing theuser toselect individual fragments\nofagivenpiece andtoperform voiceseparation only fortheselected\nfragments.\n8.REFERENCES\n[1]E.Cambouropoulos. From MIDI toTraditional Musical Nota-\ntion; In\n\u0012R M\nV \u0002M \u0006\nO W \u0001\b\u0007\b\u0007\t\u0007\b\n\f\u000bM R \r \u0005\nWM UEM N\n\u0007R\nO T \u000e\u0010V TS b\n\nN\nO \u0001b b\nT \u000f\u0003 \u0001N\nV \u0001S N P\u0011\u0010\u000eQ \u0005\nT V:Towards Formal Models forComposition,\nPerformance andAnalysis. Austin (TX), USA, 2000.\n[2]A.S. Bregman. Auditory Scene Analysis. TheMIT Press, Cam-\nbridge (MA), USA, 1990.\n[3]R.O. Gjerdingen. Apparent motion inmusic? Music Percep-\ntion, Vol.11,pp.335–370, 1994\n[4]H.H. Hoos. Stochastic Local Search –Methods, Models, Ap-\nplications. Inﬁx Verlag, Sankt Augustin, German y,1999.\n[5]H.H. Hoos, K.Renz, andM.G¨org.GUIDO/MIR —anExperi-\nmental Musical Information Retrie valSystem based onGUIDO\nMusic Notation. In\n\u0012R M\nV \u0002M \u0006\nO W \u0001\u0013\u0012N P\n\nN\nO \u0001R N S\nO TM N S b\n7 \u0014 \u0015\u0011\u000fU M \u0005\nTQ\n\u0015M N\u0016\u0010EQ \u0005\nT V\u0004\nN \u0006 M R\n\u0015S\nO TM N \t\n\u0001 OR\nT \u0001\u0004 S b\b\u0017\n\n 7\u0010\n\n\t\n\u0012 \u0018 \u0018 \u0019 \u001a,\nIndiana University ,Bloomington (IN), USA, 2001.\n[6]J.Kilian. FERMA TA–Flexible Quantisierung vonMusik-\nst¨uckenimMIDI-Dateiformat. M.Sc. thesis (inGerman),\nDarmstadt University ofTechnology ,German y,1996.\n[7]S.L.McCabe andM.J.Denham. Amodel ofauditory stream-\ning.Journal oftheAcoustical Society ofAmerica, Vol.101(3),\npp.1611–21, 1997.\n[8]K.Renz and H.H.Hoos. AWeb-based Approach to\nMusic Notation Using GUIDO. In\n\u0012R M\nV \u0002M \u0006\nO W \u0001\u001b\nN\nO \u0001R\n\u000fN S\nO TM N S b\u001d\u001c\u0017M\n\u0015U Q\nO \u0001R\u001e\u0010EQ \u0005\nT V\u001c\u0017M N \u0006\n\u0001R\n\u0001N\nV \u0001\u001f\u0019   !,pp.455–\n458, ICMA, San Francisco (CA), USA, 1998. See also:\nhttp://www .noteserv er.org.\n[9]D.Temperle y.TheCognition ofBasic Musical Structures. The\nMIT Press, Cambridge (MA), USA, 2001.\n[10] D.Temperle yandDaniel Sleator .The Melisma Music Ana-\nlyzer .http://www .links.cs.cmu.edu/music-analysis."
    },
    {
        "title": "Categories of Music Description and Search Terms and Phrases Used by Non-Music Experts.",
        "author": [
            "Ja-Young Kim",
            "Nicholas J. Belkin"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417763",
        "url": "https://doi.org/10.5281/zenodo.1417763",
        "ee": "https://zenodo.org/records/1417763/files/KimB02.pdf",
        "abstract": "Previous research has demonstrated that people listen to music for various reasons. The purpose of this study was to investigate people’s perception of music, and thus their music information needs. These ideas were examined by presenting 22 participants with 7 classical musical pieces, asking one-half of them to write words descriptive of each piece, and the other half words they would use if searching for each piece. All the words used by all subjects in both tasks were classified into 7 categories. The two most frequently appearing categories were emotions and occasions or filmed events regardless of the task type. These subjects, none of whom had formal training in music, almost never used words related to formal features of music, rather using words indicating other features, most of which have not been considered in existing or proposed music IR systems. These results suggest that music IR research should be extended to consider needs other than finding known items, or items identified by formal characteristics, and that understanding music information needs of users should be prioritized to design more sophisticated music IR systems.",
        "zenodo_id": 1417763,
        "dblp_key": "conf/ismir/KimB02",
        "keywords": [
            "music",
            "perception",
            "music information needs",
            "classical musical pieces",
            "words descriptive",
            "words they would use",
            "music IR systems",
            "formal features of music",
            "emotions",
            "occasions or filmed events"
        ],
        "content": "Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \nCategories of Music Description and Search  \nTerms and Phrases Used by Non -Music Experts  \nJa-Young Kim  and Nicholas J. Belkin  \nSchool of Communication, Information and Library Studies  \nRutgers University  \n4 Huntington Street  \nNew Brunswick, NJ 08901 -1071, USA  \n+1 732 932 7500 ext 8270  \njaykim@scils.rutgers.edu  nick@belkin.rutgers.edu  \n \nABSTRACT  \nPrevious research has demonstrated that people listen to music for \nvarious reasons. The purpose of this study was to investigate \npeople’s perception of music, and thus th eir music information \nneeds. These ideas were examined by presenting 22 participants \nwith 7 classical musical pieces, asking one -half of them to write \nwords descriptive of each piece, and the other half words they \nwould use if searching for each piece. All  the words used by all \nsubjects in both tasks were classified into 7 categories. The two \nmost frequently appearing categories were emotions  and occasions \nor filmed events  regardless of the task type. These subjects, none \nof whom had formal training in musi c, almost never used words \nrelated to formal features of music, rather using words indicating \nother features, most of which have not been considered in existing \nor proposed music IR systems. These results suggest that music \nIR research should be extended t o consider needs other than \nfinding known items, or items identified by formal characteristics, \nand that understanding music information needs of users should be \nprioritized to design more sophisticated music IR systems.  \n1. INTRODUCTION  \nMusic information ret rieval has flourished in recent years, and \ninterested researchers from various fields have devoted efforts to \ndesigning a range of music IR systems. Most such efforts have \nbeen focused on known -item retrieval, best represented as sound -\nbased music IR syste ms. Such systems are certainly important, \nsince they address a long sought goal of a wide range of users, \nranging from music librarians to ordinary music lovers. However, \nwe can also think about other musical information needs of those   \nwho cannot, or do not wish to represent their music information \nneeds in musical terms. More specifically, there may be other \ninformation needs for searching music than just by known items or \nthe formally specified features such as title, composer, genre, or \nperformer. As m usic IR systems have progressed in satisfying one  \ntype of music information need , it may be a good time to speculate \nand question if other types of music information needs have been \nunidentified.  \nPrevious research has demonstrated that people listen to music for \nvarious reasons, and that they may be involved  in many mental activities when listening to a piece of music. From the point of \nview of music IR, these complicated mental activities are one of \nthe central barriers in designing user -oriented music IR systems, \nsince there is no explicit way to explain these cognitive structures \nand processes. Further, it is even more intricate to integrate \nfindings about them into design of music IR systems . People can \nexpress, convey, and experience relationships to anything definite \nor indefinite, tangible or conceptual through the medium of music. \nWhen it comes to “music as information,” we do not have full \nunderstanding of how people might want such perceptions  of \nmusic to be understood and responded to by music IR systems.  \nIt is not the purpose of the present study to explore the “nature” \nof human cognitive structure and processes, rather, the purpose of \nthis study is to find out if there are some other needs in music \ninformation without too much speculation about these mental \nactivities, and to make future suggestions about what we can do \nwith these unidentified needs. We thus go somewhat further than \nthe few earlier exploratory studies in this area (e.g. [7]) .   \n2. RELATED WORK  \nWilson [14] states that some of the difficulties with identifying \n‘information needs’ lie with the troublesome concept, information. \nTherefore, we believe that the concept of music as information \nshould be understood as among the most impo rtant of the issues \nfacing music IR. The work of McLane [10] on the concept of \n“music as information” is an exceptional contribution to the \nliterature of this problem. Among his three views of musical \nwork —subjective, objective, and interpretive views, the  \ninterpretive view is the one that we concentrate on here. According \nto McLane, a significant characteristic of this view is its formal \nindependence from the document it addresses, and this view offers \na means to search for noncontiguous relationships. Alt hough the \nconcepts of “subject” and “aboutness” are difficult in text IR, \nMcLane points out that they are even more uncertain in music, \nand this is certainly one of the problems with music IR as \ndiscussed in Byrd & Crawford [3]. McLane (p. 240) concludes \nthat, “[b]oth the choice of view for a representation of music and \nthe degree of completeness of a work’s representation depend on \nthe user’s information needs,” and this convinces us that studying \nand understanding such needs is of primary importance for m usic \nIR.  \nAccording to Wilson [14], the central questions of ‘information \nneed (preferably ‘information -seeking towards the satisfaction of \nneeds’ by the author ’) should be : why does the user decide  to seek Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and \nthe full citation on the first page.  \n© 2002 IRCAM – Centre Pompidou  Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \ninformation; what purpose does the user believe i t will serve; and \nto what use is it actually put when found . He also notes that the \nstudy of information -seeking behavior can stand on its own as an \narea of applied research where the motive for the investigation is \npragmatically related to system design a nd development. \nFurthermore, he asserts that the study of information -seeking \nbehavior should be considered as an area of basic research and, \nalthough the resulting knowledge may have practical applications, \nthere is no necessity  that it should. Three basi c ‘human needs’ \nadopted by Wilson (p. 7) from psychological research are as \nfollow:  \n• Physiological needs , such as the need for food, water, shelter, \netc.; \n• Affective needs  (sometimes called psychological or emotional \nneeds) such as the need for attainment, f or domination etc.;  \n• Cognitive needs , such as the need to plan, to learn a skill, etc.  \nThese three categories of basic human needs and Wilson’s \napproach are certainly of interest for the purpose of music IR. \nWilson continues that most of the practical appro aches taken by \ninformation scientists are more or less concerned with only a single \nfacet of human needs, that is, cognitive needs.  For example, he \ncites Belkin (1978) and Wersig (1971). The former notes that \n“[the] concept of an information recipient’s ‘ anomalous state of \nknowledge’ leads to ‘an explicitly cognitive view of the situation \nwith which information science is concerned’ (p. 80)”. The latter’s \n(1971, cited by Belkin) view of information can be summarized as \nreduction in the uncertainty involved  in problematic situations, \nwhich similarly connotes cognitive changes in the recipient of a \ncommunication. However, as Wilson notes, “because the \nsituations in which information is sought and used are social \nsituations, however, purely cognitive conceptio ns of information \nneed are probably adequate for some research purposes in \ninformation science, but not for all. Information may also satisfy \naffective needs... (p. 9).” Even though Wilson further provides \nsome examples of how far affective needs “may” be applied to \nsome extent in information science in principle, the present study \nneeds to further examine how these two different concepts of \nhuman needs —cognitive needs and affective needs —may be \napplied for the purpose of music IR. This issue will be explor ed in \nfurther detail in the next section.  \nIt has been argued by many researchers that music in an aesthetic \nor philosophical sense, as well as othe r forms of arts such as \nliterature, visual and plastic arts , can have “meaning.” That music \ncan be regarded as an effective means of communication —\ndelivering “meaning” —from composers to performers or listeners, \nor from performers to listeners, has certainly been a critical issue. \nHowever, whether music can have “meanings” as an effective \ncommunication means is n ot, and cannot be a primary concern of \nmusic IR research. In other words, it is not such a critical issue if \nthe original purpose of a composer —for example, sadness —can be \ndelivered to listeners as originally intended by the composer. The \nprimary concern o f this study is: do people really consider music \nas having an affective meaning  from this point of view?  But we do \nnot mean to suggest that this is the only kind of meaning that \nmusic can have.      Byrd & Crawford [3] recently wrote an extensive review o n \nproblems of music information retrieval. In this study, they not \nonly provide a comprehensive literature review on music IR \nresearch, but also examine explicit and implicit reasons why music \nIR research is inherently different and more complex  compared t o \nresearch on text retrieval. One of the most important statements \nmade by Byrd & Crawford (p. 260) for the purpose of the present \nstudy is that, “there is simply no predictable association of \nmusical entities with meanings. And even if music has words, in  \nmany cases, experts will not agree on where the boundaries are, \nand a few musical techniques do have conventional associations \nwith emotional states: the use of the minor mode to express \n“sadness,” for example. But, such associations are notoriously \nunrel iable and inconsistent.”  \nIn contrast, some researchers assert [4] that music has assertoric \nmeaning in the way that declarative sentences have assertoric \nmeaning; music differs from natural languages only in that its field \nof reference is restricted to th e world of emotions . Another \nexample is found in Osborne [11], who states, “music has always \nbeen regarded as the most evocative of the arts, and throughout the \nworld music has been revered for its extraordinary power to move \nthe emotions (p. 15).”  \nByrd &  Crawford’s statement about the subjectivity of music’s \nemotional functions quoted above is certainly reasonable. \nHowever, for the purpose of music IR, it may be proper that we \nraise broader questions: how much we know about users’ musical \ninformation need s; how far users studies in music IR have been \ndone; and if affective uses of music information have been ignored \nregardless of users’ needs due to their seeming  subjectivity.  \n3. RESEARCH QUESTIONS  \nThe purpose of this study is neither to detect precise simil arities \nbetween a composer’s intention and a listener’s interpretation, nor \nto expect the regularities to occur in different listeners’ reactions. \nThe premise of this study is, that despite the seeming subjectivity \nin relating descriptions of the affect an d function of music to \nspecific musical works, it may still be possible to discover and \nrelate categories of such terms of description. However, these \nterms or descriptions as representation of music should be \nconsidered as only “means” for listeners to ex press their \ninformation “needs”.  \nIn this study, we set the subjects two tasks: a “description” task \nand a “searching” task. The purpose of the description task is to \nlearn about how people perceive music —how do they recognize \nand describe music. The purpo se of the searching task is to find \nout how people might want their perception of music as \n“information” to be understood by an ideal music IR system. \nFurthermore, we wish to consider people who are not experts in \nmusic, but rather just music listeners. Th is leads us to the \nfollowing research questions.  \n1. How do users who do not have musical backgrounds in effect \nperceive and describe music that they hear?  \n2. How do such users think they would go about searching for \nmusic that they have heard, and in particular what words or \ndescriptions would they use for such purposes?  Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \n3. To what extent can the answers to questions 1 and 2 inform \nus of people’s various music information needs?  \n4. METHODOLOGY  \nThe methodology used in this study was adopted from \nJörgensen’s 1998 study [ 8] of “Attributes of images in describing \ntasks.” In this study, she asked participants to write individual \ndescriptions of six projected color images while viewing them one \nat a time in a classroom setting. However, unlike the study \nreported here, she use d three groups of people for each task: \nsimple description; search term description; description from \nmemory. For the purpose of this study, the 3 rd task, description \nfrom memory, was excluded since it was believed that \nreperception memory for music —especi ally unfamiliar music —is \ngenerally worse than that of images. Furthermore, it was thought \nthat it would be extremely difficult for participants to remember \nseveral unfamiliar musical pieces from the same genre over time.  \nTo the researchers’ knowledge, the re have been no studies \nconducted using this methodology in music IR. However, as \nmentioned above, the purpose of the study reported here is \ncertainly different  from that of Jörgensen’s —while this study \ninvestigates music information needs as expressed in “texts as a \nmeans,” her study is rather an attempt to relate texts with images.  \nThe data analyzed in this study came from volunteer participants \nat Rutgers University in spring 2002:  nine master s students in \nLibrary & Information Science; fourteen Ph.D. students in LIS and \nCommunication ; two faculty; and one undergraduate student \nparticipated. Four of the participants were excluded from analysis \non the grounds  that they were music “experts”. Participants were \nrandomly assigned to one of two groups: those performing the \ndescription task, and those performing the searching task. Both \ngroups of participants were asked to listen to the same seven \nmusical pieces. The description group was asked to write three or \nmore words which they believed described the musi cal piece, and \nthe searching group was asked to write down words that they \nwould use when searching for the musical piece using their “ideal” \nmusic IR system. The first task was designed to elicit \nunconstrained descriptions of music while the second task w as \ndesigned more specifically to investigate what categories of words \nusers would relate with the chosen music in searching. Participants \nwere asked to imagine that their words would, in effect, express \ntheir “music information needs” to represent that spe cific musical \npiece within any music information retrieval system. The two \nquestions used for each task are as following:  \n• Please write down three or more words which you believe \ndescribe  this musical piece. They could be verbs, adjectives, \nnouns, or even s entences —any form of word is perfectly \nacceptable.  \n \n• How would you want to find this musical piece? Suppose \nthat you’re using your “ IDEAL ” music information retrieval \nsystem —which means that your words do not necessarily \nneed to be confined within some of existing music IR system. \nPlease list 3 or more  words. For example, what words would \nyou use in following question?  \n “Find me a music on, about, from, or for …………………………”  Each musical piece was played for approximately three minutes \nfrom the beginning, and  if the piece exceeded three minutes, it was \nstopped in the middle of playing. There was a pause between each \npiece so that the participants would have enough time to do the \ntasks. Before and after the experiment, participants were asked to \nindicate: minim um demographic information such as gender, \nacademic background; their ability and degree of playing any \nmusical instruments; their experiences in searching for music \ninformation; their overall familiarity with the musical pieces; and \nthe title or composer of any recognizable piece. All the \nmeasurement questions used a 5 -point Likert scale (1=not at all, \n5=extremely).  \nThe musical pieces that were used in this experiment are: \n1. Handel, Arrival of the queen Sheba  \n2. Debussy, Claire de lune  \n3. Rimsky -Korsakov , Flight of bumblebee  \n4. Mussorgsky, The great gate of Kiev from Pictures at an \nExhibition  \n5. Mozart, Concerto No. 1 for flute & orchestra, K. 313, 3rd mov.  \n6. Saint -Saens, Le Carnaval des animaux, No 8. Aquarium  \n7. Addinsell, Warsaw concerto  \n5. DATA ANALYSIS  \nThis study attempted to identify categories  into which the words, \nterms and phrases that were used by the subjects in both tasks \ncould be placed. This was done by grouping the terms into classes \ninitially by the first author of this study, then checked by  the \nsecond author, and then regrouped, in an iterative cycle. First, \nwords from the description task for each musical piece were \nanalyzed separately across the participants in order to characterize \nthe statements generated by participants. This analysis p roduced a \nrange of words (n=3 terms per question; 11 non -music expert \nparticipants; 7 musical pieces which resulted in [3 x 11 x 7] = \n231), which were grouped conceptually into seven classes or \ncategories. These categories are as follows: emotions; musical  \nfeatures; movements; occasions or filmed events; objects; nature; \nand concepts . Second, the words gathered from the searching task \nfrom each question for each musical piece were analyzed using the \nsame method. Although analyzed separately, they also group ed \ninto the same seven categories. Third, basic descriptive statistical \nanalyses were performed to estimate the proportion of each \ncategory appearing in both descriptive words and searching words. \nThis analysis produced the frequency of each category obser ved \nacross all the musical pieces; and the category’s frequency \ndifference between descriptive words and searching words.  \nThe conceptualization into seven categories was done on the basis \nof the literature both in music perception and psychology. \nSpecifica lly, the category, emotions,  was applied only when the \nwords explicitly fall into the emotion categories defined by Shaver \net al. [12]. The category, occasions or filmed events  was derived \nfrom research in music perception in which congruence between \nmusic  and visual images, such as film and videos has long been \nidentified [1] [2]. Also, the category, movements , is based on the \nfinding that music can be congruent with such body movements as \ndance [9]. However, it should be clarified that the category, Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \nmovem ents here adopts broader criteria as it also includes words \nexplicitly describing any movements or activities. The category, \nmusical features , includes all words indicating any of seven musical \nfacets defined by Downie [5]. The other three categories —natur e, \nobjects , and concepts —are preliminary ones, and more detailed \nexplanations and definitions are provided in the following section.       \n6. RESULTS AND DISCUSSION  \nThe responses from the pre-questionnaire and post -questionnaire \nare analyzed to indicate parti cipants’ background information \nincluding their previous music knowledge. As shown in Table 1, \nmost participants came from the Library and Information Science  \nmajor. More than 50% of the participants had experience in \nsearching for any kind of music inform ation. However, only 31% \nof the participants defined themselves as frequent searchers of \nmusic information in the Internet. The mean for the familiarity \nwith the musical pieces is 2.1, which is low, as expected. Even \nthough these non -music experts indicate d that some of the music \nwas familiar, only one participant out of 22 provided one correct \nanswer for the question asking the composer or the title.  Table 1. Participant Information  \n \nQuestion  Responses  \nMajor  Communication: 5 \nLibrary & Info.: 16 \nOther: 1  \nGender  M: 9/ F: 13  \nHave you ever sought out music \ninformation?  Yes: 15/ No: 7  \nAre you a frequent searcher of \nmusic info from the Internet?  Yes: 7 /No: 15  \nCan you play any musical \ninstrument? (1=beginner, 5=expert)  Yes: 9 /No: 13 \n(Mean=2.4)  \n \nFamiliarit y with the musical pieces \nchosen for this study (1=not at all \nfamiliar, 5=extremely familiar)  Mean=2.1  \nNumber of correct answers for \ncomposers or titles of musical \npieces chosen for this study  1 (n=208)  \n \n \nTable 2. Definition and frequency of categori es, with example terms and phrases  \n \nFrequency  \n(n=231 for each task)  Categories  Explanation  \nDescription \ntask Searching \ntask Examples  \nMovements  Words related to specific \nmovements  9 (4%)  8 (3%)  Running away; Flying; Sprint  \nNeutral \nconcepts  Words that are evaluatively \nambiguous or neutral  38 (16%)  45 (19%)  Ambivalence; Transformation; Simplicity; \nRealization  \nEmotions  Words explicitly indicating \nemotional status  70 (31%)  55 (24%)  Happy; Joyful; Sad; Threat; Cheerful  \nNature  Words indicating nature -\nrelated phenomena  39 (17%)  22 (10%)  Nature; Trees; Flowers blooming; Bees; \nButterflies  \nObjects  Words indicating concrete \nmaterials other than nature  12 (5%)  4 (2%)  Spy; Europe; Wizard; Queen Elizabeth  \nOccasions or \nfilmed events  Words describing specific \noccasions or events —also \nreferring to filmed events  54 (23%)  67 (29%)  For celebration; For Baroque party; Grand arrival \nor entry; Song for exploring forest; Saturday at \nthe Art gallery  \nMusical \nfeatures  Words indicating musical \nfeatures  9 (4%)  30 (13%)  Violin ; Slow -tempo; Orchestra; Rondo; Strings; \nSymphony  \nThe results with respect to the nature of the terms and phrases \nused by the subjects were far more disperse across the musical \npieces than expected. For example, while some of the participants \nwrote brie f descriptions (e.g. happy), some wrote more detailed \ndescriptions in a sentence or even a story (e.g. Children running \nand playing happily in the field), and this was more frequently found in the description task. As mentioned above, the data were \ngrouped  into seven higher -level categories using content analysis. \nThe examples and definitions of each category are shown in Table \n2. There, the other categories than those based on the literature \nare clearly defined. Many participants frequently related music \nwith some objects including nature -related phenomena, and Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \nmetaphors. Since it was unclear and impossible to identify what \nthey wanted to express through those words, certain categories \nsuch as nature, neutral concepts , and objects  were applied \nrigorously fo r those words. The symbolic or metaphorical \ncategories in concrete forms such as nature , and objects  were found more in the description task. The proportions of category \nmovements  in each task were respectively similar. The \ndifferences of the proportions o f each category appearing in the \ntwo tasks are more clearly depicted in Figure 1.  \n \n \n0 20 40 60 80ObjectsMovementsNatureMusical FeaturesNeutral ConceptsEmotionsOccasions & Events\nFrequency of words (n=231)Searching\nDescription\n \n                                           Figure 1. Comparison between terms appeared in Searching task & Description task  \nThere are several points that need to be further discussed. First, \nalthough individual participants used different language to express \ntheir interpretation or perception of a given piece, a relatively \nsmall set of themes consistently emerged. The seven categories \nidentified  in the present study may not be exclusive or \nexhaustive; rather inclusive and preliminary in a rigorous sense. \nThere is no doubt that some aspects of these categories have been \nstudied and identified in other related fields as well. The \ndescription task f irmly supports the previous findings from these \nfields in that music in effect is perceived as in an affective \nrelationship to anything perceptible in association with, but at \nthe same time in some way structurally distinguishable from, the \nstrictly ‘music al’ structures [13]. Participants recurrently seemed \nto find affective relationships with the music, and further created \nimplications for past or future events in that such categories as \nemotions  and occasions or filmed events  ranked high in the \ndescriptio n task. The frequency of other categories as nature, \nobjects , and neutral concepts  indicates that music has connotative \nfunctions as well. These different categories of words also seemed \nto be occurring consistently in the searching task.  \nThe analysis of the searching task confirms that people want to \nfind music information for, about, or on certain occasions, events, \nor specific activities as much as, or even more than they expect to \nfind the information in accordance with certain emotions. More \nspecifica lly, we may infer from these results that their musical \ninformation needs are often related with certain uses of music \nsuch as for a party, relaxation at day’s end, ceremonies, and \ndancing. These functional needs seemed to extend to help them \nremember some  scene -specific events, and even create a “story.” \nFor example, words like “for chasing scene/ background music \nwhen Tom chases Jerry in Tom & Jerry cartoon” for musical \npiece 3, and “for children’s movie/ background music for fairytale \nstory” for piece nu mber 7 appeared. However, the relatively high ranking of a category such as neutral concepts  in the searching \ntask reinforces how complicated it is to understand people’s \nneeds in the music IR task. For instance, words like “acceptance, \ncreation, or contin uation” are evaluatively ambiguous. It may be \npossible that the subjects have expressed their emotions, or even \nsome past occasions by these ambiguous or neutral words which \nare identifiable and sensible only for themselves. This may also \nindicate the limi tations of this study, and suggest the need for \nmore rigorous methodology in future studies.  \n7. CONCLUSIONS  \nThe aim of the present study was to investigate how people —\nparticularly non -music experts —perceive music, and if their \nperception and interpretation of  the music is also observable and \ncan be classifiable in music IR tasks. These ideas were examined \nby presenting 22 participants with 7 classical musical pieces, and \nasking them to write description words or searching words for a \ngiven piece. The analyses for the description task and searching \ntask have focused on identifying primarily non -music experts’ \ninformation needs.  These analyses generated 7 categories: \nmovements; neutral concepts; emotions; nature; objects; \noccasions or filmed events ; and  musical features . Even though \nthese categories are preliminary, they are certainly valuable to \nunderstand music information needs of those who cannot, or do \nnot wish to express their needs in musical terms.  In fact, a very \nsmall portion of the participants used w ords indicating the formal \nfeatures of music information; further, none of them wanted to \nfind the music by specific tunes even though a lot of them said \nthat they had heard several of the pieces before. One possible \ninterpretation is that there might have  been some biases due to \nthe example given with the searching task question.  \nCertainly, there are limitations in generalizing the findings from \nthis study. First, the musical pieces chosen for this study are Categories of Music Description and Search Terms and Phrases Used by Non -Music Experts  \nvery limited. Most of them are somehow “descrip tive,” and all \nthe musical pieces are classical. Second, studying a mere 22 non -\nmusic experts and their responses to 7 chosen musical pieces, \ndoes not allow drawing any kind of statistically reliable \nconclusions. However, this study contributes in two ways . First, \nto the researchers’ knowledge, it is the first study in the music \ninformation needs of non -music experts, at least from the point of \nview of music IR. Also, this study attempts to identify \nfrequently appearing categories of music information needs  of \nnon-music experts. As existing or proposed sound -based music \nIR systems whereby music can be represented as musical terms \ncannot fulfill the whole variety of music information needs, \nneither does this study. It may be more appropriate to say that \nthis study serves as a “complementary” groundwork for any \nmusic IR systems designed to do more than known item searching \nor searching on formal characteristics. If music IR is to embrace \nissues in developing more multi -purpose systems which can \nappeal to more u sers by reflecting their perceptual, and other \nneeds as well as the current research agenda, and if those systems \nare something that enable users to access via any access points, \nresearch in this direction should be continued and extended. \nFuture studies m ay need to employ more rigorous methodology \nto identify and categorize music information needs; and to further \naddress more tangible suggestions for their practical application.    \n8. ACKNOWLEDGMENTS  \nOur thanks to our wonderful subjects, all volunteers.  \n9. REFER ENCES  \n[1] Bolivar, V. J., Cohen, A. J., & Fentress, J. C. (1994). \nSemantic and formal congruency in music and motion \npictures: Effects on the interpretation of visual action. \nPsychomusicology , 13, 28-59.  \n[2] Boltz, M. G., Schulkind, M., & Kantra, S. (1991). Effec ts \nof background music on the remembering of filmed \nevents. Memory & Cognition, 19, 593 -606.  \n[3] Byrd, D. & Crawford, T. (2002). Problems of music \ninformation retrieval in the real world. Information \nProcessing and Management, 38 , 249-272. [4] Cooke, D. (1959). The Language of Music . Oxford, UK: \nOxford University Press.  \n[5] Downie, J. S. (1999). Evaluating a simple approach to \nmusic information retrieval: Conceiving melodic N -\ngrams as text . Unpublished doctoral dissertation, The \nUniversity of Western Ontario, London,  Ontario.  \n[6] Huron, D. (2000). Perceptual and cognitive  \napplications in music information retrieval. Paper  \npresented at the meeting of the International \nSymposium on Music Information Retrieval, Plymouth, \nMA. URL: http://ciir.cs.umass.edu/music2000/  \n[7] Itoh, M. (2000). Subject search for music: Quantitative \nanalysis of access point selection. Poster presented at \nthe meeting of ISMIR 2000, Plymouth, MA.  \n[8] Jörgensen, C. (1998). Attributes of images in \ndescribing tasks. Information Processing & \nManagement, 34 (2/3), 161-174. \n[9] Mitchell, R. W. & Gallaher, M. C. (2001). Embodying \nmusic: Matching music and dance in memory. Music \nPerception, 19 (1), 65 -85.  \n[10] McLane, A. (1996). Music as information. Annual \nReview of Information Science and Technology, 31, \n225-262. \n[11] Osborne, H.  (1984). The language metaphor in art. \nJournal of Aesthetic Education, 18  (1), 9 -20. \n[12] Shaver, P. Schwartz, J. Kirson, D. & O’Connor, C. \n(1987). Emotion knowledge: Further exploration of a \nprototype approach. Journal of Personality and \nSocial Psychology, 52 (6), 1061 -1986.  \n[13] Tagg, P. (1982). Nature as a music mood category. \nIASPM Norden’s working paper series. Institute of \nMusicology, University of Göteborg. Retrieved April \n25, 2002 from http://www.theblackbook.net/acad/tagg \n/articles/nature.pdf  \n[14] Wilson, T. D. (1981). On user studies and information \nneeds. Journal of Documentation, 37 (1), 3 -15."
    },
    {
        "title": "Singer Identification in Popular Music using Warped Linear Prediction.",
        "author": [
            "Youngmoo E. Kim",
            "Brian Whitman"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416954",
        "url": "https://doi.org/10.5281/zenodo.1416954",
        "ee": "https://zenodo.org/records/1416954/files/KimW02.pdf",
        "abstract": "In most popular music, the vocals sung by the lead singer are the focal point of the song. The unique qualities of a singer’s voice make it relatively easy for us to identify a song as belonging to that particular artist. With little training, if one is familiar with a particular singer’s voice one can usually recognize that voice in other pieces, even when hearing a song for the first time. The research presented in this paper attempts to automatically establish the identity of a singer using acoustic features extracted from songs in a database of popular music. As a first step, an untrained algorithm for automatically extracting vocal segments from within songs is presented. Once these vocal segments are identified, they are presented to a singer identification system that has been trained on data taken from other songs by the same artists in the database.",
        "zenodo_id": 1416954,
        "dblp_key": "conf/ismir/KimW02",
        "keywords": [
            "vocals",
            "lead singer",
            "unique qualities",
            "singers voice",
            "recognize voice",
            "acoustic features",
            "database of popular music",
            "singer identification system",
            "trained on data",
            "untrained algorithm"
        ],
        "content": "Singer Identification in Popular Music Recordings Using Voice Coding Features\nSinger Identification in Popular Music Recordings\nUsing Voice Coding Features\nYoungmoo E. Kim\nMIT Media Lab\nCambridge, MA 02139\n+01 617 253 0619\nmoo@media.mit.eduBrian Whitman\nMIT Media Lab\nCambridge, MA 02139\n+01 617 253 0112\nbwhitman@media.mit.edu\nABSTRACT\nIn most popular music, the vocals sung by the lead singer are\nthe focal point of the song. The unique qualities of a singer’s\nvoice make it relatively easy for us to identify a song as\nbelonging to that particular artist. With little training, if one is\nfamiliar with a particular singer’s voice one can usually\nrecognize that voice in other pieces, even when hearing a song\nfor the first time. The research presented in this paper attempts\nto automatically establish the identity of a singer using\nacoustic features extracted from songs in a database of popular\nmusic. As a first step, an untrained algorithm for automatically\nextracting vocal segments from within songs is presented.\nOnce these vocal segments are identified, they are presented to\na singer identification system that has been trained on data\ntaken from other songs by the same artists in the database.\n1. INTRODUCTION\nThe singing voice is the oldest musical instrument and one\nwith which almost everyone has a great deal of familiarity.\nGiven the importance and usefulness of vocal communication,\nit is not surprising that our auditory physiology and\nperceptual apparatus has evolved to a high level of sensitivity\nto the human voice. Once we are exposed to the sound of a\nparticular person’s speaking voice, it is relatively easy to\nidentify that voice, even with very little training. For the most\npart the same holds true with regards to the singing voice.\nOnce we become familiar with the sound of a particular singer’s\nvoice, we can usually identify the voice, even when hearing a\npiece for the first time.\nNot only is the voice the oldest musical instrument, it is also\none of the most complex from an acoustic standpoint. This is\nprimarily due to the rapid acoustic variation involved in the\nsinging process. In order to pronounce different words, a\nsinger must move their jaw, tongue, teeth, etc., changing the\nshape and thus the acoustic properties of their vocal tract. No\nother instrument exhibits the amount of physical variation of\nthe human voice. This complexity has affected research in both\nanalysis and synthesis of singing [1].\nIn spite of this complexity, voice identification is almost\neffortless to us. But perhaps what is more remarkable is that\neven in the presence of interfering sounds, such as instruments\nor background noise, we can still identify the voice of a\nfamiliar singer. Thus, our process of identification most likely\ndepends on features invariant to these environmental\nvariations. As will be discussed later, the search for such\ninvariant features that can be used for robust automatic\nidentification is no easy task.2. BACKGROUND\nA significant amount of research has been performed on\nspeaker (talker) identification from digitized speech for\napplications such as verification of identity. These systems for\nthe most part use features similar to those used in speech\nrecognition. Many of these systems are trained on pristine data\n(without background noise) and performance tends to degrade\nin noisy environments. And since they are trained on spoken\ndata, they perform poorly to singing voice input. For more on\ntalker identification systems, see [2].\nIn the realm of music information retrieval, there is a\nburgeoning amount of interest and work on automatic song\nand artist identification from acoustic data. Such systems\nwould obviously be useful for anyone attempting to ascertain\nthe title or performing artist of a new piece of music and could\nalso aid preference-based searches for music. Another area\nwhere this research has generated a great deal of interest is\ncopyright protection and enforcement. Most of these systems\nutilize frequency domain features extracted from recordings,\nwhich are then used to train a classifier built using one of\nmany machine learning techniques. Robust song identification\nfrom acoustic parameters has proven to be very successful\n(with accuracy greater than 99% in some cases) in identifying\nsongs included in the database [3]. Artist identification is a\nmuch more difficult task, and not as well-defined as individual\nsong identification. A recent example of an artist identification\nsystem is [4], which reports accuracies of approximately 50%\nin artist identification on a database of about 250 songs.\nAlso relevant to the task of singer identification is work in\nmusical instrument identification. Our ability to distinguish\ndifferent voices (even when singing or speaking the same\nphrase) is akin to our ability to distinguish different\ninstruments (even when playing the same notes). Thus, it is\nlikely that many of the features used in automatic instrument\nidentification systems will be useful for singer identification\nas well. Work by Martin [5] on solo instrument identification\ndemonstrates the importance of both spectral and temporal\nfeatures and highlights the difficulty in building machine\nlistening systems that generalize beyond a limited set of\ntraining conditions.\nObviously, singer identification and artist identification can\namount to the same thing in many situations. In [6],\nBerenzweig and Ellis use vocal music as an input to a speech\nrecognition system, achieving a success rate of 80% in\nisolating vocal regions. In [7], Berenzweig, Ellis, and Lawrence\nuse a neural network trained on radio recordings to similarly\nsegment songs into vocal and non-vocal regions. By focusing\non voice regions alone, they were able to improve artist\nidentification by 15%.\nThe system presented here also attempts to perform\nsegmentation of vocal regions prior to singer identification.\nAfter segmentation, the classifier uses features drawn from\nvoice coding based on Linear Predictive Coding (LPC),\nalthough with some modification. LPC is particularly good atPermission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit orcommercial advantage and that copies bear this notice and thefull citation on the first page.© 2002 IRCAM – Centre PompidouSinger Identification in Popular Music Recordings Using Voice Coding Features\nhighlighting formant locations (regions of resonance, which\nhave been shown to be especially significant perceptually [8]).\nMuch research has been performed on speech coding via LPC,\nand its uses are ubiquitous today. For example, all digital\ncellular phones use LPC-based voice coders and maintain\nfairly good sound quality, even at low information rates. Since\nthese models were designed to primarily carry the human voice\naccurately, it seems logical that they could be useful for singer\nidentification as well.\n3. DETECTION OF VOCAL REGIONS\nBefore trying to establish who is singing a particular piece, it\nis obviously important to identify the sections of that piece in\nwhich singing actually occurs. In this section, we present a\ntechnique for automatically detecting these regions of singing\nwithin recordings.\n3.1 Vocal frequency regions\nThe majority of energy in the singing voice falls between 200\nHz and 2000 Hz (with some variation depending on the singer).\nThis is a region that the human auditory system is particularly\nsensitive to. Classic experiments on a wide variety of human\nlisteners have established general equal-loudness curves [9]\nwhich establish that signals within this mid-range of\nfrequencies are perceived as louder than signals of equivalent\nabsolute amplitude at other frequencies, higher or lower.\nAnother perceptual effect that predominates in this region is\nmasking, in which energy in one frequency band will obscure\nor “mask” lesser energies in adjacent frequency bands. In most\nrecorded vocal music, the tendency is to isolate other\ninstruments away from the voice so as not to mask or be\nmasked by the voice. One notable exception is singing with a\nsymphony orchestra. Many orchestral instruments fall in the\nsame frequency range as the voice, and the sheer number of\ninstruments is more than enough to mask the untrained voice.\nClassically trained singers, however, are taught to produce\nextra resonance at a higher frequency (often referred to as the\nsinger’s formant , located around 2500 Hz), which allows the\nvoice to be perceived against the overwhelming numbers [10].\nBut since the majority of popular recorded music is not in this\nstyle, we can restrict our range of interest to lower frequencies.\nSince we are interested in detecting regions of singing, a\nstraightforward method would be to detect energy within the\nfrequencies bounded by the range of vocal energy. A very\nsimple approach is to filter the audio signal with a band-pass\nfilter which allows the vocal range to pass through while\nattenuating other frequency regions. For this, we use a simple\nChebychev infinite-impulse response (IIR) digital filter of\norder 12. The frequency response of this filter is shown in\nFigure 1.\nFigure 1: Vocal enhancement filter frequency response.\nThis filter has the musical effect of attenuating other\ninstruments that fall outside of this frequency region, such as\nbass and cymbals. But even in popular music, the voice is not\nthe only instrument producing energy in this region. Drums,\nfor example, disperse energy over a wide range of frequencies, a\nsignificant amount of which falls in our range of interest. Soanother measure is needed to discriminate the voice from these\nother sources.\n3.2 Detection via harmonicity\nSinging primarily consists of sounds generated by phonation,\nthe rapid vibration of the vocal folds resulting in utterances\nreferred to as voiced by speech researchers. This is as opposed\nto unvoiced  sounds which are generated by the turbulence of\nair against the lips or tongue, such as the consonants [f] or [s].\nSinging is >90% voiced, whereas speech is only ~60% voiced\n[11]. Because of this, the singing voice is highly harmonic\n(energy exists at integer multiples of the fundamental\nfrequency, or pitch). Other high energy sounds in this region,\ndrums in particular, are not as harmonic and distribute their\nenergy more widely in frequency.\nTo exploit this difference, we use an inverse comb filterbank to\ndetect high amounts of harmonic energy. The block diagram\nand frequency response of a simple inverse comb filter is\nshown in Figure 2. By passing the previously filtered signal\n(as described above) through a bank of inverse comb filters\nwith varying delays, we can find the fundamental frequency\nwhich the signal is most attenuated. By taking the ratio of the\ntotal signal energy to the maximally harmonically attenuated\nsignal, we have a measure of harmonicity , or how harmonic the\nsignal is within the analysis frame.† H=Eoriginal\nmin\niEfiltered ,i( )(1)\nBy thresholding the harmonicity against a fixed value, we have\na detector for harmonic sounds. The hypothesis is that most of\nthese correspond to regions of singing. Results using this\ntechnique are presented in Section 5.\nFigure 2: Block diagram of the simple inverse comb filter\n(top). The frequency response of an inverse comb filter, tuned\nto ~400 Hz (bottom). The spacing of the attenuated\nfrequencies is determined by the delay parameter N.\n4. SINGER IDENTIFICATION\nAlthough relatively easy for humans, robust singer\nidentification is an extremely difficult task for a machine\nlistening system. Even with clean signals (with no other\ninstruments or background noise), simple frequency or time\ndomain features do not lead easily to a unique “voiceprint”.\nAnd in most performances or recordings where the voice is\namidst a mixture of other sounds, the problem becomes even\nmore complex. This section discusses the features, extraction\nmethods, and classification techniques used in the singer ID\nsystem.z-N+–x[n]y[n]Singer Identification in Popular Music Recordings Using Voice Coding Features\n4.1 Features from Speech Coding\nMuch research (primarily dealing with speech) has focused on\nvoice coding using an analysis/synthesis approach. In this\napproach a source signal is analyzed and re-synthesized\naccording to a source-filter model of the human voice. This is\nthe general principle behind Linear Predictive Coding (LPC).\nThe primary advantage of this technique has been its utility in\ncompressing speech data resulting in the low-bitrate speech\ncoders used in many applications today.\n4.1.1  Traditional LPC\nThe goal of linear predictive analysis is to establish an\nestimate, ˜ s n[] to the source signal sn[], using a linear\ncombination of p past samples of the input signal:˜ s n[]= ak\nk=1p\nÂsn-k[ ](2)\nThe coefficient values akin Equation (2) are determined by\nminimizing the mean squared prediction error, which is the\ndifference between the source signal and the predicted signal:Em= smn[]-˜ s mn[]( )2\nmÂ(3)\nThe transfer function relating the source signal and the signal\nestimate is shown [12] to be an all-pole filter:Hz[]=G\nAz[](4)\nwhere the denominator is defined as follows:Az[]=1- akz-k\nk=1p\nÂ(5)\nThis demonstrates how linear predictive analysis is equivalent\nto a source-filter model, where the vocal tract response is\nmodeled using a time-varying all-pole filter function of order\np. The calculated coefficients can be factored to determine the\npole locations, which generally correspond to the formants\n(resonances) of the vocal tract. An example of an LPC filter\nresponse is shown in Figure 3.\nFigure 3: A vocal segment (top) and its spectrum (bottom).\nThe line in the bottom figure shows the 12-pole LPC estimate.\nThe peaks indicate the formant locations.\nAs regions of high energy within a frequency range of\nenhanced sensitivity, vocal formants are of special perceptual\nsignificance. The general pattern of formant locations\ndetermines our perception of phonemes, and thus language. Itis also believed that an individual’s particular formant\npatterns are a key feature for speaker identification [8]. Due to\nincreased harmonic energy in singing, formants are enhanced,\nso it is reasonable to believe that they would be important for\nsinger identification as well. Formant frequencies and\nmagnitudes extracted via LPC (with 12 poles) are used as key\nfeatures in the singer classifier.\nA common technique for minimizing the prediction error\n(Equation 3) uses the autocorrelation matrix (hence its name,\nthe autocorrelation method), which can be calculated from the\npower spectrum of the signal. This will be useful in the next\nsection, which discusses warping of the power spectrum to\nbetter fit established theory on auditory perception.\n4.1.2  Warped Linear Prediction\nOne disadvantage of standard LPC is that it treats all\nfrequencies equally on a linear scale. However, the human ear is\nnot equally sensitive to all frequencies linearly. In fact our\nfrequency sensitivity is very close to logarithmic. As a result,\nLPC systems sometimes place poles at higher frequencies\nwhere the ear is less sensitive and miss closely spaced resonant\npeaks at lower frequencies where the ear is more sensitive.\nUsing a higher order LPC is one way of compensating for this,\nbut increasing the number of poles makes it difficult to track\ncorrelations between analysis frames.\nInstead, we use a Warped Linear Prediction  model by pre-\nwarping the power spectrum of each frame [13], [14]. The\nwarping function can be made to closely approximate the Bark\nscale, which approximates the frequency sensitivity of human\nhearing. Warping is achieved through the following relation:† ˆ w =w+2tan-1asinw\n1-acoswÊ \nË Á ˆ \n¯ ˜ (6)\nA parameter value of a=0.47 closely matches the Bark scale.\nThis allows us to keep the order of the analysis low (and thus\nmore easily track formants from frame to frame) while more\naccurately capturing formant locations, especially at lower\nfrequencies. The additional emphasis on lower frequencies has\nthe added benefit of being able to pick out individual low\nharmonics. Standard LPC does not have the resolution at low\nfrequencies to detect individual harmonics, but warped LPC is\noftentimes able to identify the lowest harmonic, which usually\ncorresponds to the pitch, which can be a useful feature in\nsinger identification. Figure 4 shows an example of warped LP.\nFigure 4: The same vocal segment analyzed via warped\nLinear Prediction. The warped frequency scale is on top and\nthe unwarped scale on the bottom. Note the particularly good\nfit at lower frequencies.Singer Identification in Popular Music Recordings Using Voice Coding Features\n4.2 Classification Techniques\nTwo different classifiers were trained using established pattern\nrecognition algorithms. A brief description of the two\nclassifiers implemented follows. In this task, each “class”\nrepresents an individual singer.\n4.2.1  Gaussian Mixture Model (GMM)\nThe Gaussian mixture model uses multiple weighted Gaussians\nto attempt to capture the behavior of each class of training\ndata. The use of multiple Gaussians is particularly beneficial\nwhen analyzing data that has a distribution not well modeled\nby a single cluster. It is a very flexible model that can adapt to\nencompass almost any distribution of data. Test points are\nclassified by a maximum likelihood discriminant function,\ncalculated by their distances from the multiple Gaussians of\nthe class distributions [15].\nTo determine the parameters of the Gaussians that best model\neach class, we use the well-known technique of Expectation\nMaximization  (EM). EM is an iterative algorithm that\nconverges on parameters that are locally optimal according to\nthe log-likelihood function. Thus, it is sensitive to initial\nconditions, and usually several runs are performed to ensure\nthat the derived fit is a relatively good one. It is also useful to\nperform Principle Components Analysis  (PCA) prior to EM.\nPCA is a multi-dimensional rotation of the data onto the axes\nof maximal variance. It also has the added benefit of\nnormalizing the data variances, which avoids highly different\nscaling among the dimensions, which is problematic for EM.\nThe number of Gaussians used is generally a user-defined\nparameter, and some experimentation is usually required to\nfind a reasonable number for a given data set.\n4.2.2  Support Vector Machine\nA Support Vector Machine (SVM) is based on statistical error\nminimization techniques applied to a machine learning\ndomain [16]. SVMs work by computing an optimal hyperplane\nthat can linearly separate (in one case) two classes of data.\nThese hyperplanes simplify to a set of Lagrange multipliers for\neach training case, and the set of points within the dimensional\nvectors fed for training that have non-zero Lagrangians are the\nsupport vectors . The machine saves these support vectors and\napplies them to new data in the form of the test set for further\non-line classification.\nWe used an SVM with a Gaussian kernel. Our C (maximum\nlagrangian value) was set to 10.  Each class was trained as a\nseparate SVM with all the positive examples from that class\nalong with the same amount of randomly chosen negative\nexamples. After training each individual SVM, we applied the\nconfidence thresholding metric discussed in [4] to remove\nuncertain frame classifications and chose the SVM with the\nhighest confidence to classify frames in the test set [17].\n5. EXPERIMENT AND RESULTS\nSeveral different experiments were conducted using the\nfeatures and algorithms described in sections above. First, the\ndata set used for training and testing is described. Then the\nspecifics regarding the experimental configuration and\nmethods are presented, followed by the reporting of\nexperimental results.\n5.1 The Data Set\nThe data sets used in this experiment are various subsets of the\nNECI Minnowmatch testbed [4] with some minor additions.\nThe entire Minnowmatch testbed consists of >250 songs from\nthe albums of more than 20 distinct artists/groups. Some of\nthese songs, however, do not contain singing, and were\neliminated from consideration. Additionally, on occasiondifferent individuals sing some of the songs performed by a\nsingle group. Care was taken to classify each song by the\nactual singer on the recording. After these considerations, the\nresulting testbed included 17 different solo singers and\nslightly more than 200 songs. All songs were downsampled to\n11.025 kHz (from the CD sampling rate of 44.1 kHz) to reduce\nthe data storage and processing requirements. Even at the lower\nsampling rate, most vocal energy falls well below the Nyquist\nrate (half the sampling-rate) and is preserved for our analysis.\n5.2 Experimental Procedure and Results\n5.2.1  Detection of Vocal Regions\nTo test the accuracy of the vocal segment detector, a subset (20\nsongs, or approximately 10%) of the database was segmented\nmanually “by ear” into regions of singing and non-singing to\nprovide a set of “ground truth” data. The relatively small size\nof the ground truth data set is because of the tedious and\nsomewhat ill-defined nature of this task. Establishing exactly\nwhere a vocal segment begins and ends with certainty is\nproblematic. Low-level background vocals that tend to fade in\nand out in some songs add further complications. The\nsegmentation on this set was as accurate as can be, given the\ndifficulties.\nThis set of 20 songs was then analyzed using the vocal\ndetection system described in Section 3. Analysis was\nperformed using frame size of 1024 samples (~100 msec) and\nframes were taken every 512 samples (~50 msec). The initial\nthreshold for vocal classification was a harmoncity value of\nH>2. As can be seen from the results in Table 1, the classifier is\nnot very accurate, though it does perform better than chance\n(two classes = 50%). As a front-end for the singer identification\nsystem, however, we would like to avoid false positives\n(identifying regions of non-vocal music as having vocals),\nsince the identity classifier would attempt to place these non-\nvocal segments with a singer anyway. On the other hand, false\nnegatives (classifying regions of vocals as being instrumental\nonly) are more acceptable since they simply reduce the amount\nof data fed to the singer ID system. Given this tradeoff, we can\ngreatly reduce the number of false positives by raising the\nthreshold of H, at the expense of more false negatives. As\nshown in Table 3, raising the threshold to H=2.6 reduced the\nerror rate among non-vocal frames to ~20% while retaining\n~30% of the vocal frames. This value was used to automatically\nsegment the data in the singer identification system.\nTable 1: Performance of vocal detector at multiple\nharmonicity thresholds.H\nThresholdVocal\nSegmentsNon-vocal\nSegmentsAll\nSegments2.055.4%53.1%55.4%2.340.5%69.2%55.1%2.630.7%79.3%54.9%5.2.2  Singer Identification\nFor singer classification, we used approximately half of the\ndatabase (the odd numbered songs from albums) to train the\nclassifier and the remaining songs to evaluate the performance\nof our classifier. We conducted two sets of experiments: In the\nfirst set, LPC features were extracted from entire songs and\nused for classification. The second set of experiments used\nonly features from regions classified as containing vocals. In\nboth experiments, analysis frames were again 1024 samples\ncalculated at 512 sample intervals. A 12-pole LP analysis was\nperformed on both linear and warped scales. The frequencies\nand magnitudes of the pole locations were used as inputs to\nthe classifiers.Singer Identification in Popular Music Recordings Using Voice Coding Features\nThree different feature sets (linear scale data, warped scale data,\nand both linear and warped data) were tested, and two different\nclassifiers (GMM and SVM) were used in each case. In the SVM\nclassifier, only every tenth data frame was used because of\ncomputer memory constraints. The results from these\nexperiments are summarized in Table 2. The highest\nperforming number of Gaussians was used as the GMM result.\nTable 2: Singer classification results. Results are listed as\npercentages of songs correctly classified (followed by\npercentages of individual frames correctly classified).\nExperiment 1: Entire song dataFeaturesGMMSVMLinear frequency features32.1 (16.6)39.6 (30.7)Warped frequency features31.3 (17.1)35.0 (30.4)Linear and warped features33.4 (16.5)45.3 (29.6)Experiment 2: Only song segments classified as vocalsFeaturesGMMSVMLinear frequency features36.7 (18.1)35.8 (17.6)Warped frequency features33.0 (17.4)34.0 (26.8)Linear and warped features38.5 (16.6)41.5 (28.8)On the whole, the classification results are far greater than\nchance (17 classes = ~6%), but still fall well short of expected\nhuman performance. In general, the linear frequency features\ntend to outperform the warped frequency features when each is\nused alone, but using them together does benefit performance.\nStrangely, song and frame accuracy increases when using only\nvocal segments in the GMM, but decreases in the SVM using\nthe same segments.\n6. DISCUSSION AND FUTURE WORK\nAs shown in the results, the raw accuracy of the vocal detector\ncould use some improvement. But given the uncertainties in\nthe defining the exact boundaries of vocal and instrumental\nsegments, the raw output numbers have some uncertainty\nattached as well. It should be noted that the singing detector\npresented here is an untrained system (it possesses no prior\nknowledge). Other systems ([6] and [7]) have achieved higher\nvocal detection accuracy (as well as [18] for speech vs. music),\nbut have been trained on ground-truth databases. It would be\npossible to combine features from both systems to achieve\ngreater accuracy. A better perceptual model (than the static\nfilter used here) may be of substantial benefit as well. We are\ncurrently investigating these possible improvements.\nQualitative listening to the detected regions demonstrates\nsome interesting points. Many of the extended mislabeled\nregions occur at the beginning of songs or during instrumental\nbridges where producers have highlighted instruments in the\nabsence of vocals. Also, the beginnings and ends of phrases\ntend to be cut off, since they contain less harmonic content.\nRegions of extended vowels (such as held notes) are\nparticularly well detected. A simple extension to the system\nwould be to pad each section with extra time on either side. But\nwhether this would aid in singer identification is an entirely\ndifferent question. It is conceivable that higher-level musical\nknowledge could be added to the system in an attempt to\nidentify song structure, such as the location of verses and\nchoruses, from patterns in the segmentation data. The\nprobability of vocals in those sections could be weighted more\nstrongly than in others, which would reduce the problem of\nfalsely classifying strong solo instrumental passages as\nvocals.The singer classification results are notable in a few ways. Both\nclassification techniques found something discriminatory\nwithin the features. With the GMM, accuracy increased as\nexpected when the classifier was trained and tested with the\nvoice-classified frames. That the performance of the SVM\ndecreased is a bit puzzling. It is likely that the SVM is finding\naspects of the features that are not specifically related to the\nvoice. The fewer number of frames in the second experiment\nmight account for the decrease in accuracy. There is also a great\ndeal of uncertainty about the accuracy of the voice labeling,\nand that is likely factor as well.\nThe better performance of the linear frequency scale features vs.\nthe warped frequency features probably indicates that the\nmachine finds the increased accuracy of the linear scale at\nhigher frequencies useful. Though this is contrary to human\nauditory perception, it is not surprising that there is\ndiscriminatory information there, though it is probably not\ncorrelated with the voice. The increased performance in using\nboth linear and warped features indicates that the analyses are\nnot completely redundant.\nGiven the relatively low frame accuracy reported in the singer\nidentification experiments, the overall frame confusion matrix\n(Figure 5) is not surprising. There is not a high amount of\nintensity along the diagonal, except for a few particular artists.\nIt is as yet unclear why these particular singers are easily\ndetected. There may have been particular qualities to these\nvoices or other parts of the songs, which may have been\nhighlighted by the linear predictive analysis. This remains an\nongoing investigation in our research.\nFigure 5: Confusion matrix for all voiced data frames, using\nboth linear and warped features.\nOthers (e.g. [4] and [7]) have achieved higher artist\nclassification accuracy on the same test data, but have not\nattempted to identify the individual singers. In [7], using only\nvocal segments improved accuracy in a neural net classifier\nusing Mel-frequency Cepstral Coefficients (MFCCs). But it is\nunclear in that study (and in the research presented here as\nwell) whether the classifier is actually training on vocal\nfeatures or is using some other aspect of the recordings, even\nthough both MFCCs and linear prediction have proven useful\nin speech applications. Their system also uses the differences\nbetween MFCCs as features, indicating that using differential\nmagnitudes (in addition to or instead of raw magnitudes) may\nbe beneficial.\nAnother possibility for improving the singer classifier is to\nincorporate more time-varying information. While almost all\naudio analysis is conducted at a fixed rate, the actual\ninformation rate of audio varies widely. For this reason, a state-\nbased classifier, such as a Hidden Markov Model, may improveSinger Identification in Popular Music Recordings Using Voice Coding Features\nclassifier performance. This will be explored in our research at\na later date.\n7. ACKNOWLEDGEMENTS\nOur thanks to Dr. Paris Smaragdis for his help in enlightening\nus to some of the more obscure aspects of pattern recognition.\nAnd thanks to Prof. Barry Vercoe for his support of our\nresearch.\n8. REFERENCES\n[1] Y. E. Kim, “Excitation Codebook Design for Coding of the\nSinging Voice,” Submitted to the 2001 IEEE Workshop on\nthe Applications of Signal Processing to Audio and\nAcoustics , 2001.\n[2] R. Mammone, X. Zhang, and R. P. Ramachandran, “Robust\nspeaker recognition: A feature-based approach,” IEEE\nSignal Processing Magazine , vol. 13, pp. 58-71, 1996.\n[3] J. Herre, E. Allamanche, and O. Hullmuth, “Robust\nMatching of Aduio Signals Using Spectreal Flatness\nFeatures,” presented at IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , New Paltz, NY,\n2001.\n[4] B. Whitman, G. Flake, and S. Lawrence, “Artist detection in\nmusic with Minnowmatch,” presented at IEEE Workshop\non Neural Networks for Signal Processing , Falmouth,\nMA, 2001.\n[5] K. Martin, Sound-Source Recognition: A Theory and\nComputational Model . Ph.D. Thesis. Massachusetts\nInstitute of Technology, Cambridge, MA, 1999.\n[6] A. L. Berenzweig and D. P. W. Ellis, “Locating Singing\nVoice Segments Within Music Signals,” presented at IEEE\nWorkshop on Applications of Signal Processing to Audio\nand Acoustics , New Paltz, NY, 2001.\n[7] A. Berenzweig, D. P. W. Ellis, and S. Lawrence, “Using\nVoice Segments to Improve Artist Classification of\nMusic,” In press, 2002.[8] R. Brown, “An experimental study of the relative\nimportance of acoustic parameters for auditory speaker\nrecognition,” Language and Speech , vol. 24, pp. 295-310,\n1981.\n[9] H. Fletcher, “Auditory patterns,” Review of  Modern\nPhysics , vol. 12, pp. 47-65, 1940.\n[10] J. Sundberg, The Science of the Singing Voice . Dekalb, IL:\nNorthern Illinois University Press, 1987.\n[11] P. R. Cook, Identification of Control Parameters in an\nArticulatory Vocal Tract Model, with Applications to the\nSynthesis of Singing . Ph.D. Thesis. Stanford University,\nStanford, CA, 1990.\n[12] L. R. Rabiner and R. W. Schafer, Digital Processing of\nSpeech Signals . Englewood Cliffs, NJ: Prentice-Hall,\n1978.\n[13] H. W. Strube, “Linear prediction on a warped frequency\nscale,” Journal of the Acoustical Society of America , vol.\n68, pp. 1071-1076, 1980.\n[14] A. Härmä, “A Comparison of Warped and Conventional\nLinear Predictive Coding,” IEEE Transactions on Speech\nand Audio Processing , vol. 9, pp. 579-588, 2001.\n[15] R. Duda, P. Hart, and D. Stork, Pattern Classification , 2nd\ned. New York: John Wiley & Sons, 2000.\n[16] C. J. C. Burges, “A tutorial on support vector machines for\npattern recognition,” Data Mining and Knowledge\nDiscovery , vol. 2, pp. 955-974, 1998.\n[17] G. Flake, “NODElib,”.: NEC Research Institute.\n[18] E. D. Scheirer and M. Slaney, “Construction and\nevaluation of a robust multifeature speech/music\ndiscriminator,” presented at IEEE Conference on\nAcoustics, Speech, and Signal Processing (ICASSP),\nMunich, Germany, 1997."
    },
    {
        "title": "Integrating Pattern Matching into an Analogy-Oriented Pattern Discovery Framework.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417048",
        "url": "https://doi.org/10.5281/zenodo.1417048",
        "ee": "https://zenodo.org/records/1417048/files/Lartillot02.pdf",
        "abstract": "We claim that the core mechanism of a sufficiently general MIR system should be expressed in symbolic terms. We defend the idea that music database should be pre-analyzed before being scanned for MIR queries. We suggest a new vision of automated pattern analysis that generalizes the multiple viewpoint approach by adding a new paradigm based on analogy and temporal approach of musical scores. Through a chronological scanning of the score, analogies are inferred between local relationships — namely, notes and intervals — and global structures — namely, patterns — whose paradigms mechanisms for inference of new patterns are described. The same pattern-matching algorithm used for pattern discovery during pre-analysis of musical works is reused during MIR applications. Such an elastic vision of music enables a generalized understanding of its plastic expression. This project, in an early stage, introduces a broader paradigm of automated music analysis.",
        "zenodo_id": 1417048,
        "dblp_key": "conf/ismir/Lartillot02",
        "keywords": [
            "symbolic terms",
            "pre-analyzed",
            "automated pattern analysis",
            "analogies",
            "temporal approach",
            "chronological scanning",
            "global structures",
            "paradigm of automated music analysis",
            "plastic expression",
            "generalized understanding"
        ],
        "content": "Integrating Pattern Matching into an Analogy-Oriented Pattern Discovery Framework\nIntegrating Pattern Matching\ninto an Analogy-Oriented Pattern Discovery Framework\nOlivier Lartillot\nIrcam – Centre Pompidou\nPlace Igor-Stravinsky\nF - 75004 PARIS\n+33 1 44 78 13 94\nOlivier.Lartillot@ircam.fr\nABSTRACT\nWe claim that the core mechanism of a sufficiently general MIR\nsystem should be expressed in symbolic terms. We defend the\nidea that music database should be pre-analyzed before being\nscanned for MIR queries. We suggest a new vision of\nautomated pattern analysis that generalizes the multiple\nviewpoint approach by adding a new paradigm based on\nanalogy and temporal approach of musical scores. Through a\nchronological scanning of the score, analogies are inferred\nbetween local relationships — namely, notes and intervals —\nand global structures — namely, patterns — whose paradigms\nare stored inside an abstract pattern trie (APT). Basic\nmechanisms for inference of new patterns are described. The\nsame pattern-matching algorithm used for pattern discovery\nduring pre-analysis of musical works is reused during MIR\napplications. Such an elastic vision of music enables a\ngeneralized understanding of its plastic expression. This\nproject, in an early stage, introduces a broader paradigm of\nautomated music analysis.\n1. INTRODUCTION\nMusical shaping, stemming from a multitude of elementary\nconstructive rules, may be transformed in numerous ways\nwithout altering its basic structure. If a dialog has to be\nestablished between a human user and a machine in musical\nterms, it has to take into account the specificity of music as a\nsymbolic system. That is why it seems that the heart of a\nmature MIR system should be expressed in symbolic terms. Of\ncourse, the signal reality of music may be taken into account as\nthe basic material and the medium of musical phenomenon.\nThe general research domain of Information Retrieval (IR)\naccepts a specific discrimination between two types of\napproaches, where user query is compared either to the rough\ndocuments themselves, or to a synthetic representation of the\ndocuments, resulting from an analysis of them.\nThe second approach features several interesting properties, in\nparticular: reduction of computational cost, pertinent\nanswering of queries, cognitive modeling. The pre-analytical\napproach is even more relevant in the music domain, since the\nelementary articulations of the discourse are much more\nhidden in musical surface than in natural language.\nA general approach of musical pattern detection would have to\nbe able to detect approximate repetitions of sets of notes\ninside the score multidimensional space. Each elementary\nsimilarity that defines the whole approximate repetition may\nbe expressed along different musical parameters. And the\npertinence of such a repetition has to be carefully formalized\nand justified in order to obtain musically pertinent patternsand also in order to avoid computationally excessive\ncomplexity.\nGenerally, musical pattern discovery methods directly\nconsider whole patterns. Such a global point of view enables a\nfast detection of patterns, as long as they are similar enough.\nAs soon as musical plasticity is taken into consideration, this\ntraditional approach is no more adequate.\n2. kanthume  APPROACH\n2.1 Analogy\nWe suggest another method [2], highly developed in\ncontemporary cognitive sciences and of high interest here,\nbased on analogy. Indeed, “analogy-making lies at the heart of\npattern perception and extrapolation” [1]. Analogy means that\nthe similarity of two entities is hypothesized as soon as one or\nseveral resemblances of particular relevant aspects of them are\ndetected. Human understanding heavily relies on such a\nmechanism since it progressively constructs a representation\nof phenomenon. The cognitive system has to take some risks,\nto induce , that is, to infer knowledge that is not directly\npresent in the phenomenon, and, then, to check the good\nfulfillment of this hypothesis.\nAn analogy-based vision of understanding is even more\naccurate in a musical context. Indeed, music cannot be\napprehended as a single object: at each step of music hearing,\nonly a local aspect is presented. Thus the analogy hypothesis\nof music understanding means that the global music structure\nis inferred through induction of hypotheses from partial — in\nparticular timely local — point of view. The temporal\ncharacteristic of music implies another important aspect:\nnamely, a chronological order [3].\nkanthume  project aims at building a musical pattern discovery\nsystem based on cognitive-oriented modeling of induction of\nanalogies and of temporal perception of music. In a first\napproximation, we limit our scope to pitches, durations, and\ntime onsets. The score is scanned in a temporal order and, for a\nsame time onset, in a growing order of pitch.\n2.2 Musical representations\nAs we limit music as a score, that is a set of notes, at each\ninstant of music listening, this local viewpoint consists in fact\nof elementary notes n, and relations (concerning time and\npitch) between notes, that is: intervals . As we define interval\nwithin a local framework, temporal constraints are applied:\nintervals cannot exceed a certain temporal extension.\nPattern has been defined as a set of notes that is repeated\n(exactly or varied) in the score. All these repetitions, including\nthe original first pattern, are considered as occurrences — or\nactual patterns  — of a single abstract pattern .\nBy existence of relation order in each dimension of the score\nspace (namely pitch and time onset), notes inside a set of notes\nmay be ordered through an induced relation order, that would\nconsist of a main temporal relation order — which possessesPermission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit orcommercial advantage and that copies bear this notice and thefull citation on the first page.© 2002 IRCAM – Centre PompidouIntegrating Pattern Matching into an Analogy-Oriented Pattern Discovery Framework\nactual cognitive significance —, composed with a secondary\n(arbitrary) pitch order. In this way, set of notes may be\ndisplayed as strings.\nAs pattern may be transposed, it is preferable to consider\nrelative than absolute dispositions: notes should be consider\nnot only along their absolute coordinates, but also along their\nreciprocal relations. As we showed that relations between notes\ncould be decomposed into relations between couples, or\nintervals, then sets of notes (figure 1a) may be displayed as\nstring of intervals between each successive note (figure 1b).\nThis is called the minimal interval representation .\nCollection of patterns may be represented as a trie (figure 1c),\nwhere each node is a note and where, for each branch, the\nordered set of notes from the root to the leaf, is a pattern.\nThanks to this kind of representation, two patterns that have a\nsame prefix will share a same part of branch. This has cognitive\nground since patterns cannot be discriminated when only their\ncommon prefix is heard. That is why the collection of abstract\npatterns — in minimal interval representation  — related to a\nscore will be represented inside such a trie called abstract\npattern trie (APT).\n    \nFigure 1(a-c). Polyphonic pattern, its corresponding minimal\ninterval representation, and its corresponding branch (with\ngrey nodes) in an APT.\nFor each node of the APT is associated a value, which indicates\nthe number of actual observed occurrences of the pattern\nrepresented by the branch from the root to this node. Each local\nviewpoint note, in the score, that terminates one or several\nactual patterns is linked to its corresponding abstract patterns\nin the APT. And reversely, to each node of APT is associated its\ncorresponding actual patterns in the score.\n2.3 Pattern discovery\nEach parameter of each element of local viewpoints — pitch,\nduration and onset of current notes and intervals — may be a\ncharacteristic property that, if retrieved later, will recall the\nconsidered element. Such a remembering mechanism will be\npossible only if there exist associative memory that links all\nelements sharing a same characteristic. In our framework, this\nmay be formalized by considering parallel partitions of the set\nof notes of the score. Here memorizing a local viewpoint would\nsimply mean adding current notes and interval in the\nequivalence class corresponding to each current value of pitch,\nduration, onset, etc.\nWe will now describe how similarity between current interval\nor note and memorized intervals and notes triggers analogy of\nmotives. Let (n1,n2) be one of current intervals. Several cases\nare considered in this order of preference:\n1. If the left element n1 of current interval is linked to an\nabstract pattern of the APT, and if its corresponding node n is\nnot a leaf of the APT, current interval (n1,n2) is compared to all\nintervals between n and its children m. If one link is considered\nas similar to (n1,n2), then the number of occurrences relative to\nthe abstract motive from the root of the APT to m is\nincremented (see figure 2a).\n2. In a second step, (n1,n2) is compared to all possible\nmemorized intervals, from the last element of each associated\nactual patterns of current abstract pattern, that are stored in\ninterval partitions. If similitude is found between one of these\nintervals and current interval (n1,n2), a new child is added to n,\nits link featuring this new interval (see figure 2b).3. In a last step, current interval is compared to every possible\nsimilar interval in the score, through a simple look at their\nequivalence classes in interval partitions, and also neighbor\nequivalence classes — that is, classes whose parameter value is\nsimilar to the value of current interval. If (n1,n2) is considered\nas similar to one or several previous intervals, a new node is\nadded to the root of the APT (see figure 2c), initiating a new\npattern.\n  \n    \nFigure 2(a-c). The three main methods for growing the APT.\nPertinent candidates have to be selected with the help of some\nheuristics: similitude between current and abstract (or past)\nintervals; length of the pattern and number of occurrences\n(dissimilitude between intervals may be more easily tolerated\nif these intervals belong to bigger or recurrent structures), etc.\nNow, patterns may be considered themselves as elementary\nobjects, and patterns of patterns may be researched in a\nrecursive way. General heuristics, analogous to those\ndeveloped in the realm of semantic networks, are necessary for\nthis recursive mechanism to offer pertinent results [2].\nContrary to notes, patterns cannot be directly compared one\nwith each other, since the concept of interval of patterns is\nundefined. However, such a comparison may be envisaged by\ncomparing all the information stemming from each pattern and\nselecting the relevant part.\nHarmonic progression is implicitly taken into account in this\nframework, since chords may be considered as a degenerated\ncase of pattern. Moreover, a transformed pattern may itself be\nconsidered as a new pattern if its new structure appears several\ntimes in the score. In this way, motivic transformation is made\nexplicit.\n3. kanthume  APPLIED TO MIR\nFor each musical piece in the database, kanthume  generates an\nAPT following the pattern discovery  framework showed in\nprevious part. The MIR application will consist of a pattern\nmatching  between the query and all the APTs. The pattern\ndiscovery  mechanism of kanthume , based itself on a pattern\nmatching  idea, will thus be directly applied to MIR.\nThe method that has been formalized in the last paragraph is\nnow in process of implementation as a library of Open Music\nsoftware. Such a project, still in a very embryonic aspect,\nsuggests an ambitious utopia, where not only pattern but also\nany musical structure such as harmony or form, may be\nanalyzed and taken into account for MIR.\n4. ACKNOWLEDGMENTS\nThis project is carried out in the context of my PhD, directed\nby Emmanuel Saint-James (LIP6, Paris VI) and Gérard Assayag\n(Musical Representations, Ircam). Some ideas arise from\ndiscussions with Benoit Meudic and Carlos Agon.\n5. REFERENCES\n[1] Hofstadter, D. Fluid Concepts and Creative Analogies:\nComputer Models of the Fundamental Mechanisms of\nThought. Basic Books (1995).\n[2] Lartillot, O. Generalized Musical Pattern Discovery by\nAnalogy from Local Viewpoints, in Proceedings of DS ’02\n(Germany, November 2002), LNCS/LNAI, Springer-Verlag.\n[3] Meyer, L. Emotion and Meaning in Music. The University\nof Chicago Press (1956)."
    },
    {
        "title": "Representing Traditional Korean Music Notation in XML.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie",
            "Allen Renear"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418277",
        "url": "https://doi.org/10.5281/zenodo.1418277",
        "ee": "https://zenodo.org/records/1418277/files/LeeDR02.pdf",
        "abstract": "XML promises to provide a powerful interoperable general framework for the development of music representation systems. Unfortunately current XML encoding systems for music focus almost exclusively on Western music from the 17th century onwards, and on the Western notation system, Common Music Notation (CMN). This is regrettably limiting, with cultural, theoretical, and practical consequences for MIR. In order to ensure that music information retrieval (MIR) systems have full theoretic generality, and wide practical application, we have begun a project to explore the representation, in XML, of a genre of traditional Korean music which has a distinctive notation system called Chôngganbo. Our project takes seriously the specific notational expression of musical intention and intends to ultimately contribute to the analysis of theoretical issues in music representation, as well as to the improvement of methods for representing Korean music specifically.",
        "zenodo_id": 1418277,
        "dblp_key": "conf/ismir/LeeDR02",
        "keywords": [
            "XML",
            "interoperable general framework",
            "music representation systems",
            "Western music",
            "CMN",
            "music information retrieval (MIR)",
            "notational expression",
            "theoretical issues",
            "Korean music",
            "Chôngganbo"
        ],
        "content": "Representing Traditional Korean Music Notation in XML\nRepresenting Korean Traditional Musical Notation in XML\nJin Ha Lee , J. Stephen Downie, Allen Renear\nGraduate School of Library and Information Science\nUniversity of Illinois at Urbana-Champaign\n501 East Daniel St.\nChampaign, IL 61820\n1-217-333-7197\n{jinlee1, jdownie, renear}@uiuc.edu\nABSTRACT\nXML promises to provide a powerful interoperable general\nframework for the development of music representation systems.\nUnfortunately current XML encoding systems for music focus\nalmost exclusively on Western music from the 17th century\nonwards, and on the Western notation system, Common Music\nNotation (CMN). This is regrettably limiting, with cultural,\ntheoretical, and practical consequences for MIR. In order to\nensure that music information retrieval (MIR) systems have full\ntheoretic generality, and wide practical application, we have\nbegun a project to explore the representation, in XML, of a genre\nof traditional Korean music which has a distinctive notation\nsystem called Chôngganbo. Our project takes seriously the\nspecific notational expression of musical intention and intends to\nultimately contribute to the analysis of theoretical issues in music\nrepresentation, as well as to the improvement of methods for\nrepresenting Korean music specifically.\n1. INTRODUCTION\nOur research objectives are to ensure that ( i) the full spectrum of\nthe world's musical culture is made available in the emerging\nelectronic delivery systems, (ii) theories of music representation\nare tested by the a wide range of diverse data, and (iii) the\nretrieval techniques developed by the MIR community are\nbroadly applicable, we have initiated a project to explore the\nencoding of traditional Korean music and notation.\n2. WHAT DOES OUR ENCODING\nREPRESENT?\n2.1 Conceptual Preliminaries\nAny proposed music encoding project should explicitly declare\nexactly it intends to be an encoding of; that is, precisely, what\nsort of objects and relationships it is representing. We use three\ndomains of the Standard Music Description Language (SMDL)\n(ISO 1995) to characterize the possibilities.\n2.1.1 The Logical Domain (Musical intention)\n\"The composer's intentions with respect to pitches, rhythms,\nharmonies, dynamics, tempi, articulations, accents, etc.\" (ISO\n1995). It is the pure abstract musical content and as such it is\ndeliberately and systematically indifferent to the features of\nparticular scoring systems. SMDL focuses on this domain.\n2.1.2 The Visual Domain (Scoring)\nScoring is an expression of musical intention, typically in atraditional language designed to communicate to performers.\nLanguages for typesetting or formatting of musical scores are\nconcerned with this domain.\n2.1.3 The Gestural Domain (Performance)\nAn acoustic musical event, actual or possible, may also be taken\nas the primary object of representation and encoding. MIDI is an\nexample of a music representation language that seems focused\non representing actual performances ( Selfridge-Field 1997).\nThe current project undertakes to develop an XML-based\nencoding system that is comparable to MusicXML, MusiXML,\nMuseData, GUIDO, etc. in having as an objective the\nrepresentation of musical content , and which deliberately and\nexplicitly maintains a fidelity to particular scores, in a particular\nnotation system (here, Chôngganbo). Once the intimacy and\ncomplexity of the connection between score and intention is\nadmitted our encoding focus is nicely caught in Walter Hewlett's\napt phrase: \"the logical content of musical scores\" (Hewlett\n1997).\nThe theoretical issues are difficult and subtle, and it is not our\nintention to directly address them in this project, but rather to\nposition our work to contribute to a larger discussion later.\n3. TRADITIONAL KOREAN MUSIC\nNOTATIONS\nAsian traditional music and notation are in many ways quite\nunlike western music and notation There are many notation\nsystems that have been  used for Korean traditional music,\nhowever, today Osûnbo  and Chôngganbo predominate. Osûnbo is\nWestern CMN which was adopted after the Western\nmodernization of Korea . Chôngganbo is a type of mensural\nnotation developed by King Sejong in the 15th Century. It was the\nfirst mensural notation in East Asia (Park 2000). There have\nbeen several studies done on Chôngganbo and how best to\ninterpret the notation, and some of these studies are in conflict\nwith each other. For this paper, we will be adopting the most\nwidely accepted perspective, that of Kim Kisu (Provine 2002).\n3.1 Osûnbo vs. Chôngganbo\nSince Osûnbo (CMN) is also widely used to write traditional\nKorean music, one might doubt the necessity of using\nChôngganbo rather than Osûnbo — if Chôngganbo and Osûnbo\nare merely notational variants expressing the same musical\nintentions, then building a specific DTD for Chôngganbo, as\nopposed to Osûnbo (which could be more easily represented in\nexisting XML music representation systems), wouldn't seem to\nbe necessary.\nFigures 1 and 2 present an example traditional Korean song in\nboth its Chôngganbo and Osûnbo representations, respectively.Permission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on the\nfirst page.\n© 2002 IRCAM – Centre PompidouRepresenting Traditional Korean Music Notation in XML\nAs a first caution against adopting this\napproach we observe that, as described\nabove, the invention of Chôngganbo has a\ndeep cultural and political significance for\nKorea. But apart from the cultural\nmeaning of Chôngganbo, we believe there\nmay be distinctive theoretical and\nscientific reasons for making it the focus\nof representation, rather than Osûnbo, or\nignoring notation altogether. At this point\nit seems quite possible to us that there are\nsome things that can be represented in\nChôngganbo, but not in Osûnbo. For\nexample, at slow speeds, a Korean \"beat\"\nis a malleable event that can vary in length\nby up to about 300% ( Provine 2002). This\nquality of flexible length is a presumed\nassociation with a box in Chôngganbo. A\nWestern quarter note is not as flexible,\nand we generally assume, even in\npassages of rubato, that quarter notes\nroughly equal in length.   Of course one\ncan use Western quarter notes and\nexplicitly qualify their durational\ncharacteristics — and when Koreans use\nWestern notation, they in fact assume that\nelements of the notation, like quarter\nnotes, share the characteristics of\ncorresponding elements, like flexible\ndurations, of Chôngganbo ( Provine 2002).\n       Figure 1. Jung-yeongsan in Chôngganbo (Kim 1979)\nFigure 2. Jung-yeongsan in Osûnbo (Kim 1979)\nAnother possible area of incommensurability is the relative\nefficiency of particular notation schemes. Due to the limited size\nof a Chônggan, Chôngganbo employs its symbols to simplify the\nrepresentation of melodies. This space limitation has generated a\nnotational scheme noteworthy for its being simultaneously\nsuccinct and yet semantically rich. Tone colour and note shaping\n(bending of pitches in certain pre-determined ways), for example,\nare two areas where Chôngganbo is especially efficient.\nTraditional Korean music prizes the “natural harmony of a note”.\nThe perceived effects between the “tensions” and “relaxations”\nof the string are another integral part of this music  ( Chon 1993).\nThis is why the instrumental techniques of string performance are\nsuch an important part of both traditional Korean music in\ngeneral and Chôngganbo in particular. It is important to note that\nwhen traditional Korean music is written in Osûnbo, extra\nsymbols are needed to approximate  these features that are\notherwise intrinsic to Chôngganbo.Finally traditional Korean music is not polyphonic (National\nCenter  2002); it is rather, a part of the heterophonic tradition of\nperformance that characterizes much of East Asian and Middle-\nEastern music. In heterophonic musics, melodic richness is\nenhanced not through harmonic accompaniment, but through the\nsimultaneous performance of the principal melody by the\nperforming instruments with each instrument having a relatively\nindependent flexibility with regard to ornamentation, rhythmic\ndeviations and tone colour. Again, when traditional Korean music\nis notated in Osûnbo, only one set of the many possible\nmanifestations of the heterophonic performance is “fixed”,\nimplying a rigidity of interpretation not otherwise inherent in the\nintention of the composer.\nThe preceding observations lead us to our working hypothesis\nthat “representing traditional Korean music in Western notation\nimposes presumptions of style and sound that are only\nappropriate to Western music” ( Provine 2002).\n4. FUTURE RESEARCH\nTraditional Asian music, and traditional notation like Korean\nChôngganbo, are quite different from their Western counterparts,\nand the preceding preliminary analysis suggests that until\ndefinitely shown otherwise we cannot assume that traditional\nKorean music can be adequately represented by XML-encoding\nsystems designed for Western music and Western notation. We\nwill continue to extend and verify our analysis to determine\nwhether the representational framework we are constructing\nmatches the distinctive expressive features of Chôngganbo,\nparticularly with respect to such things as granularity, syntactic\nconstraints, hierarchical structures, determinacy, overloading, and\ncognitive processing. Eventually we intend to harmonize our\nperfected analysis with ongoing work in CMN XML\nrepresentation projects, either through extension of existing\nsystems, or, if that is not possible, through the development of an\noverarching framework that coordinates the disparate specialized\nmusic XML systems.\n5. ACKNOWLEDGEMENT\nThe authors are indebted to Dr. Robert Provine, University of\nMaryland, for his expert advice on traditional Korean music.\nRemaining errors are of course ours alone.\n6. REFERENCES\nChon, In-Pyung. (1993). Gugak Gamsang. Jungang University\nPress.\nHewlett, Walter B. (1997). “The MuseData  Representation of\nMusical Information” in E. Selfridge-Field (editor), Beyond\nMIDI: The Handbook of Musical Codes , Cambridge, MA: MIT\nPress.\nInternational Organization for Standardization (ISO). (1995).\nISO/IEC DIS 10743:1995. Information Technology - Standard\nMusic Description Language (SMDL) . Geneva.\nKim, Kisu. Danso Yulbo. (1979). Seoul. Eunha Publishing Co.\nProvine, Robert. (2002). Email correspondence. 16 March 2002.\nSelfridge-Field E. (editor). (1997). Beyond MIDI: The Handbook\nof Musical Codes , Cambridge, MA: MIT Press.\nThe National Center For Korean Traditional Performing Arts.\nKorean Music.  (2001). 6 May. 2002. Available:\nhttp://210.95.200.104/english_version/html/ma0h000101.html .Figure 1 Jung -\nyeongsan in \nChôngganbo \n(Kim \n1979)Figure 2 \nJung-yeongsan \nin Chôngganbo \n(Kim 1979)"
    },
    {
        "title": "Content-Based Playlist Generation: Exploratory Experiments.",
        "author": [
            "Beth Logan"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418061",
        "url": "https://doi.org/10.5281/zenodo.1418061",
        "ee": "https://zenodo.org/records/1418061/files/Logan02.pdf",
        "abstract": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007 \u0005\b\u000e\u000f\u0001\u0010\u0007\u000f\u0003\u0011\u0001\u0013\u0012\u0015\u0014\u0017\u0016\u0018\u0012\u001a\u0019\u001b\u0005\b\u0001\u0018\u0019\u001b\u0005\u0011\u001c\u001e\u001d\u000f\u001f\u001a\u0003\u0011\u0001 \t!\u001f\u0015\"\u000f\"$#\b\u0012%\u001f\u001a\u0016&\u000e\u000f\u0001\u0018\u0003'\u0005\b\u0012(\u0014)\u0012\u0015#\b*+\"\u000f,-\u001f\u0018 ,/./\u0003\u0006\u0005\b\u00030\u00141#\b\u0012\u001a*2\u001f43\u001a.-5%\u0001\u0018\u00196\u0003\u0011\u0001\u0018\u0001 \t6\u0003\u0011\u0012%\u0019$3$798\u0004\u0007$#:\u0005\b\u0001\u0018\u0016;\u000e$\u0019\u000f.-\u0012%\u0007?# \"?#\b\u0001\u00185\f.-\u0012\u001a\u0007\u000f\u0003\u0011,@\u000b(\"$#\b\u0001\u0018\u0003\u0011\u0001A\u0019\f\u0005\b\u0001\u0018\tB\u001f\u0015\u0007 $./\u0012\u0010\u0003\u0011.-*C./,-\u001f\u0015#\b.@\u0005\u001e\u000bD*C\u0001 \u001f\u001a\u0003\u0011\u0007?#\b\u0001%79E'\u000e$.-\u00039*C\u0001\u0018\u001f\u001a\u0003\u0011\u0007$#\b\u0001 \u0016\u0018\u0012\u001a*C\" F#\b\u0001\u0018\u00036\u0003\u0011\u0012%\u0019$3%\u0003(\u001f\u001a\u0016\u0018\u0016A\u0012\u001a#&\t$./\u0019$3G\u0005\b\u0012H\u0005\b\u000e$\u0001I\u0019\u000f\u0012F5J\u0001A,/\u0005\u001e\u000bK\u0012\u001a\u0014L\u0005\b\u000e\u000f\u0001\u0018.@#6\u00141#\b\u0001\u0018\u0012\u00155%\u0001A#4h\u001ai%i%iD\u0003\u0011\u0012%\u0019$3%\u0003\u00187j\u0000g\u00014a\u000f\u0019 I\u0005\b\u000e F\u0005cQR\u000e$\u0001\u0018\u0019!.-\u0019?\u0014)\u0012\u0015#\b*!\u001f\u0015\u0005\b./\u0012%\u0019 \u0015\u001dM\u0012%\u0007$\u00059\u0005\b\u000e$\u0001c\u0003\u0011\u0012\u001a\u0019\u000f3%\u0003\u0018k\u00153\u001a\u0001\u0018\u0019$#\b\u0001'./\u0003j\u001f\u001a\t\u000f\t$\u0001\u0018\t:b\u0015./*C\"$#\b\u0012F5J\u0001\u0018*C\u0001\u0018\u0019\u001b\u0005\b\u0003:\u0012F5J\u0001A#\u0017\u0005\b\u000e$\u0001c \u0015\u0003\u0011./\u0016 ?.-\u0003\u0006\u0005&\u001f\u0015\u0019\u000f\u0016\u0018\u0001I*C\u0001 \u001f\u001a\u0003\u0011\u0007?#\b\u0001\r\u001f\u0015#\b\u0001B\u0012%\u001d?\u0005&\u001f\u001a./\u0019\u000f\u0001 \tlb:\u0003\u0011\u0007\u000f3%3\u001a\u0001\u0018\u0003\u0006\u0005\b./\u0019\u000f3_\u001dM\u0012\u0015\u0005\b\u000e\u0002\u001f\u0015\"\u000f\"?#\b\u0012J\u001f\u001a\u0016&\u000e\u000f\u0001A\u0003 F#\b\u0001m\u0003\u0011\u0007$.@\u0005&\u001f\u001a\u001d\u000f,/\u00014\u0014)\u0012\u0015#'./\u0019\u000f\u0016A\u0012\u001a#\b\"M\u0012\u001a#&\u001fF\u0005\b./\u0019\u000f3D\u0007\u000f\u0003\u0011\u0001A#=./\u0019\u000f\"$\u0007$\u0005=\u0012\u001a#L,-\u001f\u001a\u001dM\u0001\u0018,/./\u0019\u000f36./\u0019?\u0014)\u0012\u001a#\b*!\u001fF \u0005\b./\u0012%\u0019G./\u0014=\u001f 5\u001a\u001f\u001a./,-\u001f\u001a\u001d$,-\u0001\u001a7",
        "zenodo_id": 1418061,
        "dblp_key": "conf/ismir/Logan02",
        "keywords": [
            "abstract",
            "key aspects",
            "article",
            "captures",
            "Qwen",
            "helpful assistant",
            "captures",
            "key aspects",
            "article",
            "captures"
        ],
        "content": "Content-Based Playlist Gener ation: Explor atory Experiments\nContent-Based Playlist Generation: Explorator y\nExperiments\nBeth Logan\nHewlett-P ackard Labs\nOne Cambr idge Center\nCambr idge Massachusetts 02142 United States\n+16175517600\nBeth.Logan@hp .com\nABSTRACT\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\r\u0005\b\u000e\u000f\u0001\u0010\u0007\u000f\u0003\u0011\u0001\u0013\u0012\u0015\u0014\u0017\u0016\u0018\u0012\u001a\u0019\u001b\u0005\b\u0001\u0018\u0019\u001b\u0005\u0011\u001c\u001e\u001d\u000f\u001f\u001a\u0003\u0011\u0001 \t!\u001f\u0015\"\u000f\"$#\b\u0012%\u001f\u001a\u0016&\u000e\u000f\u0001\u0018\u0003'\u0005\b\u0012(\u0014)\u0012\u0015#\b*+\"\u000f,-\u001f\u0018\u000b\f\u001c,/./\u0003\u0006\u0005\b\u00030\u00141#\b\u0012\u001a*2\u001f43\u001a.-5%\u0001\u0018\u00196\u0003\u0011\u0001\u0018\u0001 \t6\u0003\u0011\u0012%\u0019$3$798\u0004\u0007$#:\u0005\b\u0001\u0018\u0016;\u000e$\u0019\u000f.-<\u001b\u0007\u000f\u0001\u0018\u0003:\u0007\u000f\u0003\u0011\u0001=\u001f\u0015\u0003\u0017\u001f4\u001d\n\u001f\u0015\u0003\u0011.-\u0003>\u0012%\u0007?#\"?#\b\u0001\u00185\f.-\u0012\u001a\u0007\u000f\u0003\u0011,@\u000b(\"$#\b\u0001\u0018\u0003\u0011\u0001A\u0019\f\u0005\b\u0001\u0018\tB\u001f\u0015\u0007\n\t$./\u0012\u0010\u0003\u0011.-*C./,-\u001f\u0015#\b.@\u0005\u001e\u000bD*C\u0001 \u001f\u001a\u0003\u0011\u0007?#\b\u0001%79E'\u000e$.-\u00039*C\u0001\u0018\u001f\u001a\u0003\u0011\u0007$#\b\u0001\u0016\u0018\u0012\u001a*C\"\n\u001fF#\b\u0001\u0018\u00036\u0003\u0011\u0012%\u0019$3%\u0003(\u001f\u001a\u0016\u0018\u0016A\u0012\u001a#&\t$./\u0019$3G\u0005\b\u0012H\u0005\b\u000e$\u0001I\u0019\u000f\u0012F5J\u0001A,/\u0005\u001e\u000bK\u0012\u001a\u0014L\u0005\b\u000e\u000f\u0001\u0018.@#6\u00141#\b\u0001\u0018<\f\u0007$\u0001\u0018\u0019\u000f\u0016;\u000b\u0003\u0011\"M\u0001\u0018\u0016;\u0005\u0011#\b\u0007\u000f*N\u001f\u001a\u0019\n\tO\u000e\u000f\u001f\u001a\u0003G\u001dM\u0001A\u0001\u0018\u0019P\u0003\u0011\u000e\u000f\u0012 QR\u0019O\u0005\b\u0012S\u000e\u000f\u001f 5J\u0001T3\u001a\u0012\u001b\u0012\f\tU\"M\u0001;#\u0011\u0014)\u0012\u001a#\b*!\u001f\u0015\u0019\u000f\u0016\u0018\u0001\u0012\u001a\u0019V\u001fH\u0019$\u0012%\u0019$\u001cW\u0005\u0011#\b./5\f.-\u001f\u001a,L\t\u000f\u001f\u0015\u0005&\u001f\u0015\u001d\n\u001f\u0015\u0003\u0011\u0001%7!XY\u0019Z\u0005\b\u000e\u000f./\u0003D\"\n\u001f\u001a\"M\u0001;#6Q'\u0001C./\u0019\f5J\u0001\u0018\u0003\u0006\u0005\b./3J\u001fF\u0005\b\u0001C\u0001A[\f\u001c\u0005\b\u0001\u0018\u0019$\u0003\u0011./\u0012%\u0019\u000f\u0003\r\u0005\b\u0012Z\u0003\u0011./*C\"\u000f,@\u000b\\\u0016&\u000e\u000f\u0012\u001b\u0012%\u0003\u0011./\u0019$3T\u0005\b\u000e\u000f\u0001G]N\u0016\u0018,/\u0012\u001a\u0003\u0011\u0001\u0018\u0003\u0006\u0005^\u0003\u0011\u0012%\u0019\u000f3\u001a\u0003\r\u0005\b\u0012\u0002\u001fZ\u0003\u0011\u0001\u0018\u0001 \t\u001f\u0015\u0003^\u0005\b\u000e$\u0001_\"\u000f,-\u001f\u0018\u000b?,-./\u0003\u0006\u0005 72`?\"M\u0001A\u0016\u0018.@a\n\u0016 \u001f\u0015,-,@\u000b\u001bb4Qc\u0001d\u0003\u0006\u0005\b\u0007\n\t?\u000b\\\"$,-\u001f \u000b?,/./\u0003\u0006\u0005\b\u0003^\u00141\u0012\u001a#\b*C\u0001 \tS\u001d\u001b\u000b\u0005\u0011#&\u001f e\u0006\u0001\u0018\u0016;\u0005\b\u0012\u001a#\b./\u0001\u0018\u0003\u0010\u0005\b\u000e?#\b\u0012%\u0007$3%\u000e!\u0005\b\u000e\u000f\u00016\t?.-\u0003\u0006\u0005&\u001f\u0015\u0019\u000f\u0016\u0018\u0001\u0004\u0003\u0011\"\n\u001f\u001a\u0016A\u0001D\u001f\u001a\u0019\n\t^\"\u000f,-\u001f\u0018\u000b?,-./\u0003\u0006\u0005\b\u0003'\u00141\u0012\u001a#\b*C\u0001 \t\u0007$\u0003\u0011.-\u0019$3^\u001f\u001a\u0007$\u0005\b\u0012\u001a*!\u001f\u0015\u0005\b./\u00166#\b\u0001\u0018,/\u0001\u00185\u001a\u001f\u001a\u0019\u000f\u0016A\u0001D\u00141\u0001\u0018\u0001 \t?\u001d\n\u001f\u001a\u0016&fM7R\u0000g\u0001\u0013#\b\u0001\u0018\"M\u0012\u0015#\u0011\u0005\u0004#\b\u0001\u0018\u0003\u0011\u0007$,/\u0005\b\u0003\u0010\u0012%\u0019K\u001f\t$\u001f\u0015\u0005&\u001f\u001a\u001d\u000f\u001f\u001a\u0003\u0011\u0001\u0010\u0012\u001a\u0014>\u0012\u00155%\u0001A#4h\u001ai%i%iD\u0003\u0011\u0012%\u0019$3%\u0003\u00187j\u0000g\u00014a\u000f\u0019\n\tI\u0005\b\u000e\n\u001fF\u0005cQR\u000e$\u0001\u0018\u0019!.-\u0019?\u0014)\u0012\u0015#\b*!\u001f\u0015\u0005\b./\u0012%\u0019\u001f\u0015\u001dM\u0012%\u0007$\u00059\u0005\b\u000e$\u0001c\u0003\u0011\u0012\u001a\u0019\u000f3%\u0003\u0018k\u00153\u001a\u0001\u0018\u0019$#\b\u0001'./\u0003j\u001f\u001a\t\u000f\t$\u0001\u0018\t:b\u0015./*C\"$#\b\u0012F5J\u0001\u0018*C\u0001\u0018\u0019\u001b\u0005\b\u0003:\u0012F5J\u0001A#\u0017\u0005\b\u000e$\u0001c\u001d\n\u001f\u0015\u0003\u0011./\u0016\t?.-\u0003\u0006\u0005&\u001f\u0015\u0019\u000f\u0016\u0018\u0001I*C\u0001 \u001f\u001a\u0003\u0011\u0007?#\b\u0001\r\u001f\u0015#\b\u0001B\u0012%\u001d?\u0005&\u001f\u001a./\u0019\u000f\u0001 \tlb:\u0003\u0011\u0007\u000f3%3\u001a\u0001\u0018\u0003\u0006\u0005\b./\u0019\u000f3_\u001dM\u0012\u0015\u0005\b\u000e\u0002\u001f\u0015\"\u000f\"?#\b\u0012J\u001f\u001a\u0016&\u000e\u000f\u0001A\u0003\u001fF#\b\u0001m\u0003\u0011\u0007$.@\u0005&\u001f\u001a\u001d\u000f,/\u00014\u0014)\u0012\u0015#'./\u0019\u000f\u0016A\u0012\u001a#\b\"M\u0012\u001a#&\u001fF\u0005\b./\u0019\u000f3D\u0007\u000f\u0003\u0011\u0001A#=./\u0019\u000f\"$\u0007$\u0005=\u0012\u001a#L,-\u001f\u001a\u001dM\u0001\u0018,/./\u0019\u000f36./\u0019?\u0014)\u0012\u001a#\b*!\u001fF\u001c\u0005\b./\u0012%\u0019G./\u0014=\u001f 5\u001a\u001f\u001a./,-\u001f\u001a\u001d$,-\u0001\u001a7\n1.INTRODUCTION\nThepopularity oftheMP3 compression format haschanged theway\npeople store, access andacquire music. Itisnowpossible tocarry\nhundreds ofhours ofmusic onasmall device. Through theWeb\npotentially millions ofhours areubiquitously available. This change\ninscale ofaccessible music from thetraditional albumtomillions\nofsongs raises manyunanswered questions ofhowtoefﬁciently\naccess anddisco verthisdata andbestpresent music totheuser.\nIdeally ,weimagine asystem which canautomatically sense auser’ s\nmood ordesires andplay suitable music from amassi verepository\nofavailable songs. The system would also respond toandlearn\nfrom user feedback andbeable tosuggest suitable newsongs from\nother repositories.\nAlthough wearefarfrom having such asystem, researchers have\nmade much progress towardthisgoal. Inourlab,wearefocussed\noncontent-based analysis ofmusic. Wehavepreviously developed\natechnique toquantify thesimilarity between songs based solely on\ntheir audio content [2],[3].Ourmeasure captures information about\nthenoveltyoftheaudio spectrum andtherefore relates tothetype of\ninstruments playing. Thus weareconcerned with ‘genre’ similarity\nrather than saymelodic closeness. Wepreviously found thatthis\nmeasure isuseful forplaylist generation, content-based copyright\ndetection andmusic visualization.\nInthispaper ,wefocus solely onautomatic playlist construction\ninwhich wedesire toprovide auser with aselection ofmusic\nwith acertain ‘mood’. Previously ,wechose aplaylist asthe n\nclosest songs toaseed song according toourdistance measure.\nInthispaper ,weconsider more complicated schemes inwhich we\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.\n\u0016\no\n2002IRCAM-CentrePompidoupost-process thisinitial listofsongs. Speciﬁcally ,wedescribe two\nextensions: tracing atrajectory through thedistance space andusing\nrelevance feedback. Due tospace limitations, weshall notdescribe\nourdistance measure ormanydetails ofourexperimental setup. The\nreader isreferred to[2]forfurther information.\n2.GENERA TINGPLAYLISTS\nInthissection, wedescribe techniques which post-process initial\nplaylists chosen asthe\n]closest songs totheseed song.\n2.1SongTrajectories\nWeimagine agraph ofallsongs inourdatabase. Each song isa\nnode andlinks between songs describe howclosely thesongs are\nrelated. Thesimplest graph uses ourdistance measure forthelink\nstrength. Aplaylist canbeformed using thisgraph bychoosing the\nshortest path oflength\n]emanating from theseed song.\nIfwhen tracing thispath, asong isrepeated implying aloop, we\nuseoneoftwosimple heuristics. Intheﬁrst technique, ifwhen\nchoosing theclosest song top qweencounter aloop, wechose the\nnextclosest song to p quntil weﬁndasong thatisunseen forthis\nplaylist. Thus wecontinue toexpand thepath from therAs\ft&t\bu;v\u000fw\nsong. Analternati veistorestart thepath from thenextclosest\nunseen song tothe x\u001at&y-z\u001ay)v\n{%|l}\u0018u\bu\u0011~ .\nGraph-based playlists havebeen examined previously [1].However,\nthisprior workassumes thatrelevantattrib utes havealready been\ndetermined foreach song rather than extracting them from theaudio\nasinourcase.\n2.2RelevanceFeedback\nRelevance feedback isanestablished technique intheIRcommunity\n(e.g. [4]). Itaims toimpro vethequality ofreturned documents in\nresponse toauser’ squery byincorporating feedback from theuser.\nAwell-kno wnrelated technique which does notrequire user input\nisautomatic relevance feedback. Here, itisassumed thatthetop\ndocuments returned arerelevant.Features from these documents are\nthen extracted andused tore-rank thedocuments inthecollection,\nhopefully resulting inimpro vedperformance.\nWehaveimplemented asimple version ofautomatic relevance feed-\nback forourmusic database asfollows.Ourscheme combines the\nsimple playlists fortheclosestsongs totheseed song. Speciﬁ-\ncally,foragivenseed song, wesumthedistance scores ofthesimple\nplaylists forthetopsongs andusetheresulting scores toform a\nﬁnal playlist. Table 2.2illustrates thisscheme.\n3.EXPERIMENTS\nWeconduct experiments onanin-house database ofover8000 songs\ndrawnfrom awide range ofstyles. Each song inthedatabase is\nlabeled with thegenre, song name, albumname andartist name.\nThegenres areassigned according tothe\u0010|)|$Ks\f}\byWrI's\fyW~\u001bu (AMG)\ndatabase (www .allmusic.com).Content-Based Playlist Gener ation: Explor atory Experiments\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0004\u0011\u0013\u0012\u0004\u0014\u0016\u0015\u0017\u0011\u0019\u0018\u001b\u001a\u001d\u001c\u0013\u001e\u0017\u001f \u001e\"!\"\u0015$#\f\u001a%\u001e'&(\u001e)\u001e\"*,+\f\u0015-\u001a\u0019./\u000f\f01\u001e2*3\u00114\u00125&(\u0012$\u001c%\u00146\n\u001f \u001527,\u001f\u001b\u0018\u001b08\u00119&:\u001c\u0013\u0012\u0004\u0014;01\u001e)\u001e2*=<\u0002>\u001d\u0018?01\u00114\u0015$#\f\u001a%\u001e209\u0015-\u001c\u0013\u001e@0\u0013AB\u0012\u0017CD#E\u0018\b#6\n\u0015-\u001c\u0013\u001e2#-\u0011\u0019AB\u001e204\u0018\u001b0%<]H\u001cGFc,-\u0012\u001a\u0003\u0011\u0001\u0018\u0003\u0006\u0005DHj,-\u001f\u0018\u000b$,/./\u0003\u0006\u0005\b\u0003 I\u0017./\u0019\u000f\u001f\u001a,`\f\u0001\u0018\u0001 \t `\f\u0012%\u0019\u000f3$J `?\u0012%\u0019$3LK Hj,-\u001f \u000b?,/./\u0003\u0006\u0005`?\u0012%\u0019$3\u0004J M(NLO `?\u0012\u001a\u0019\u000f3\u0017K M:P\u0017O `\f\u0012%\u0019$32N M(N\u0017O `?\u0012\u001a\u0019\u000f3LK M:Q\u0017O`?\u0012%\u0019$3LK M:R\u0017O `?\u0012\u001a\u0019\u000f3\"N M:R\u0017O `\f\u0001\u0018\u0001 \t M:R2O `?\u0012\u001a\u0019\u000f32N MTS2O`?\u0012%\u0019$3\u0017R MUJVO `?\u0012\u001a\u0019\u000f32R MTK2O `\f\u0012%\u0019$3\u0004J MUJ%O `?\u0012\u001a\u0019\u000f3\u0004J MTW2O7-7/7 7/7/7 7/7/7 7/7-7\nOurexperiments examine playlists forallsongs inthedatabase and\nreport theaverage number ofrelevantsongs forplaylists oflength\n5,10and20. Weuseobjecti vedeﬁnitions ofrelevance -songs\nofthesame style, bythesame artist andonthesame album-in\norder toconduct automatic tests overthewhole database since user\ntests arebeyond thescope ofthispaper .Wepreviously found good\ncorrelation between subjecti veandautomatic tests [2].\n3.1BaselineSystem\nTable 2showstheaverage quality ofplaylists inthebaseline case\nwhen theplaylist issimply the\n]closest songs totheseed song.\nWeseethatthatonaverage, themajority ofsongs chosen areofthe\nsame genre asthequery song andthatplaylists ofsize5contain one\nsong bythesame artist oronthesame album.Note thatthese results\ngiveonly anindication ofperformance. Forexample, severalofour\ngenre categories overlap (e.g. X {%Y1Y and Z\u0018| s\u000fu;})andsongs from both\ncategories might stillbepercei vedasrelevantbyahuman user.\u0000\u0002\u0001B\u0003\u0006\u0005\u001b\u0007\\[]\u000b_^D\u001e204\u000f,\u001f \u0011\u00130`&(\u0012$\u001cD+\f\u0015-01\u001e\u0017\u001f\u001b\u0018\b#B\u001e6\n\u001f \u001527,\u001f\u001b\u0018\u001b08\u0011\u00190%<a5J\u0001A#&\u001f\u00153%\u00016\u0019$# 7j\u0012\u0015\u00149#\b\u0001\u0018,/\u0001\u00185\u001a\u001f\u001a\u0019\u001b\u0005b\u0010\u0001\u0018,/\u0001\u00185\u001a\u001f\u001a\u0019$\u0016\u0018\u0001 \u0003\u0011\u0012%\u0019\u000f3\u001a\u0003\u0010./\u0019_\"$,-\u001f \u000b?,/./\u0003\u0006\u0005`\f.?cA\u0001\u0010W `?. c\u0018\u0001dJ\u0018i `?. c\u0018\u0001\u0010K\u0015i`$\u001f\u001a*C\u0001fe\u0004\u0001\u0018\u0019$#\b\u0001 R\f7 N2P P\f7 P\u001ai J%K\f7 P`$\u001f\u001a*C\u0001\na#\u0011\u0005\b./\u0003\u0006\u0005 J\u001a7 R)N K\u001b7 i2S R\f7 i-J`$\u001f\u001a*C\u0001\na,/\u001d\u000f\u0007$* J\u001a7?J\"J J\u001a7 P\"R K\u001b7 KLJ\n3.2SongTrajectory Playlists\nThe toppart ofTable 3showsresults forplaylists formed from\nsong trajectories. Weshowresults forboth variations discussed in\nSection 2.1. The results showthatthetechnique oftracing paths\nthough thesong space givesworse results than thebaseline. The\nsecond variation issome what better than theﬁrsthowever.\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\\g]\u000bD^_\u001e\"04\u000f,\u001f \u0011\u00130`&(\u0012$\u001c6\n\u001c\u0013\u00126\n\u0012$01\u001e2*h\u001e\"i$\u0011\u0013\u001e2#\f04\u0018?\u0012\u0004#\f0a5J\u0001;#&\u001f\u001a3%\u00016\u0019?# 7j\u0012\u001a\u0014\u0017#\b\u0001A,-\u0001A5%\u001f\u0015\u0019\f\u0005bm\u0001\u0018,/\u0001\u00185\u001a\u001f\u001a\u0019\u000f\u0016A\u0001 `?\u0016&\u000e\u000f\u0001A*C\u0001 \u0003\u0011\u0012%\u0019$3%\u0003m./\u0019G\"\u000f,-\u001f\u0018\u000b?,/.-\u0003\u0006\u0005`?. c\u0018\u0001\u0010W `\f. c\u0018\u0001dJ i `?. c\u0018\u0001\u0010K\u001ai`?\u001f\u001a*C\u0001_e\u0004\u0001\u0018\u0019?#\b\u0001 E>#&\u001fFeY\u0001\u0018\u0016A\u0005\b\u0012\u0015#\u0011\u000b\u001bb?J R?7 K\"P P?7 J%R J i?7 S\"W`?\u001f\u001a*C\u0001\na#\u0011\u0005\b.-\u0003\u0006\u0005 J%7 i%h J%7 N\u0017R J%7 P%h`?\u001f\u001a*C\u0001\na,-\u001d$\u0007\u000f* i?7 h2Q J%7 J2J J%7 K2K`?\u001f\u001a*C\u0001_e\u0004\u0001\u0018\u0019?#\b\u0001 E>#&\u001fFeY\u0001\u0018\u0016A\u0005\b\u0012\u0015#\u0011\u000b\u001bb K R?7 R2R P?7 R\u0017S JVK\f7 i\u001ah`?\u001f\u001a*C\u0001\na#\u0011\u0005\b.-\u0003\u0006\u0005 J%7 K\"R J%7 h2Q K\f7 S\"R`?\u001f\u001a*C\u0001\na,-\u001d$\u0007\u000f* J%7 i$J J%7 N\u0017Q K\f7 i%i`?\u001f\u001a*C\u0001_e\u0004\u0001\u0018\u0019?#\b\u0001 I\u000f\u0001\u0018\u0001\u0018\t$\u001d\n\u001f\u0015\u0016;f R?7 NJi P?7 W)N JVK\f7 N2P`?\u001f\u001a*C\u0001\na#\u0011\u0005\b.-\u0003\u0006\u0005 J%7 K2S J%7 Q2P K\f7 h2R`?\u001f\u001a*C\u0001\na,-\u001d$\u0007\u000f* J%7 i\u0017W J%7 W)N K\f7 i\u0017S\n3.3AutomaticRelevanceFeedback\nThe second part ofTable 3showsresults forautomatic relevance\nfeedback asdescribed inSection 2.2. Weshowresults forthebest, kjDj\nJ.Comparing these results with thebaseline results in\nTable 2,weseethatusing automatic relevance feedback results in\nslightly worse performance than thebaseline.3.4IncorporatingMoreInformation\nOur results forthetrajectory andrelevance feedback extensions\ntoourbasic distance measure aredisappointing. Investigation of\ntheplaylists formed revealed thatamajor problem was‘tangential’\nsongs. Because ourdistance measure isnotperfect, expanding a\npath orcombining scores ofplaylists from badsongs cancorrupt\nthenewplaylist with irrele vantsongs.\nWetherefore investigate whether adding information from labels or\nuser input isbeneﬁcial. Forthetrajectory scheme, weexperiment\nwith thesecond variant. When wedetect aloop andrestart the\nplaylist from thenextclosest unseen song totheseed song, we\nconstrain thissong tobefrom thesame genre astheseed. We\nusegenre tosimulate auser highlighting songs ofthesame style.\nResults using thisscheme areshownintheﬁrst half ofTable 4.\nSimilarly ,weinvestigate aversion ofautomatic relevance feedback\nwhere when choosingsongs toexpand, wechoose songs from the\nsame genre astheseed song. These results areshowninthelower\nhalf ofTable 4.Again weonly showresults forthebest scheme\nwhich was ljDj\nW.\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u00075m=\u000bD^_\u001e\"04\u000f,\u001f \u0011\u00130`C_AB\u001e2#hn-\u001e2#\f\u001c\u0013\u001e\u000e\u0018\b#B&(\u0012$\u001c%\u0014'\u0015\u0017\u0011%\u0018 \u0012B#'\u0015-*,*\f\u001e2*=<a5J\u0001A#&\u001f\u00153%\u00016\u0019$# 7j\u0012\u0015\u00149#\b\u0001\u0018,/\u0001\u00185\u001a\u001f\u001a\u0019\u001b\u0005bm\u0001\u0018,/\u0001\u00185%\u001f\u0015\u0019\u000f\u0016\u0018\u0001 `\f\u0016;\u000e$\u0001\u0018*C\u0001 \u0003\u0011\u0012\u001a\u0019\u000f3\u001a\u0003m./\u0019_\"$,-\u001f \u000b?,/./\u0003\u0006\u0005`\f.?cA\u0001\u0010W `?. c\u0018\u0001dJ\u0018i `?. c\u0018\u0001\u0010K\u0015i`$\u001f\u0015*C\u0001\na#\u0011\u0005\b./\u0003\u0006\u0005 E>#&\u001fFeY\u0001\u0018\u0016A\u0005\b\u0012\u0015#\u0011\u000b J\u001a7 N%i K\u001b7 R\"R R\f7 W\u0015h`$\u001f\u0015*C\u0001\na,/\u001d$\u0007\u000f* J\u001a7?J\u0019P J\u001a7 h2K K\u001b7 P\u001ai`$\u001f\u0015*C\u0001\na#\u0011\u0005\b./\u0003\u0006\u0005 I\u000f\u0001A\u0001 \t$\u001d\u000f\u001f\u001a\u0016&f J\u001a7 P\u001ai K\u001b7 R-J R\f7 i\"Q`$\u001f\u0015*C\u0001\na,/\u001d$\u0007\u000f* J\u001a7 R\u001ai J\u001a7 S)Q K\u001b7 KVN\nTheresults inthistable areencouraging since there isimpro vement\nintheSame Artist andSame Albummetrics overthebaseline in\nTable2.This suggests thatournewapproaches provide aframe work\ninwhich labeling information canbeincorporated intotheoriginal\ndistance measure. Also, webelie vethiscould allowuser input tobe\nincorporated intoplaylist construction.\n4.CONCLUSIONS ANDFUTURE WORK\nWehaveinvestigated theuseofcontent-based techniques toform\nplaylists from agivenseed song. Weexplored twoextensions toour\npreviously published technique which simply chose the\n]closest\nsongs toaseed [2].Theﬁrstextension forms playlists astrajectories\nthrough thedistance space. The second uses automatic relevance\nfeedback.\nWeevaluated ourtechniques onadatabase ofover8000 songs of\nvaried styles. Surprisingly ,theproposed extensions didnotperform\naswell assimply choosing the\n]closest songs totheseed song as\ntheplaylist. Weattrib utethistotheimperfect nature ofourdistance\nmeasure. However,when information about thesongs’ genre is\nadded, impro vements arenoted, suggesting both approaches provide\naframe workforincorporating user input orlabeling information.\n5.REFERENCES\n[1]M.Alghoniemy andA.H.Tewﬁk. Anetw orkﬂowmodel for\nplaylist generation. Ino)p0rqts$u\u0017u2v ,Tokyo,Japan, 2001.\n[2]B.Logan andA.Salomon. Acontent-based music similarity\nfunction. Technical report, Compaq Cambridge Research Lab-\noratory ,June 2001.\n[3]B.Logan andA.Salomon. Amusic similarity function based\nonsignal analysis. In o)p0rqws-u\u0017u\u0017v ,Tokyo,Japan, 2001.\n[4]K.Sparck-Jones andP.Willet, editors. xRu\u0011{J~\u001ay)v\u001bz\u001a}\u0004y)v\u000eo&v%y\u0018x\u001at{zC{\"|w y x\u001av}xmu;w t&y u4~\u0015{%|.MorganKaufmann, 1997."
    },
    {
        "title": "Learning to cope with Diversity in Music Retrieval.",
        "author": [
            "Thomas Mandl 0001",
            "Christa Womser-Hacker"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416560",
        "url": "https://doi.org/10.5281/zenodo.1416560",
        "ee": "https://zenodo.org/records/1416560/files/MandlW02.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416560,
        "dblp_key": "conf/ismir/MandlW02",
        "content": "Learning to Cope with Diversity in Music Retrieval  \nLearning to Cope with Diversity in Music Retrieval \nThomas Mandl \nInformation Science, University of Hildesheim \nMarienburger Platz 22 \nD-31141 Hildesheim – Germany \nTel.: ++49 - 5121 - 883 - 837 \nmandl@uni-hildesheim.de  Christa Womser-Hacker  \nInformation Science, University of Hildesheim \nMarienburger Platz 22 \nD-31141 Hildesheim – Germany \nTel.: ++49 - 5121 - 883 - 833 \nwomser@uni-hildesheim.de \n \n1. DIVERSITY IN MUSIC RETRIEVAL \nDesigners  of m usic retrieval s ystem are als o confronted with a \nlarge number of choices. A look at some of the implementations \nproves this point. The forms for the user criteria include a \nsignificant number of different criteria.  \nThe s earch for appropriate algor ithms and representation atoms \nwithin the universe of possibilities seem s to be a curse since each \nsolution neglects several aspects. Ho wever, it can turn out to be a \nblessing when the diversity  is integrated into a self adapting \nfusion sy stem which considers many  heterogeneous solutions. \nThis paper presents a mode l including a m achine learning \napproach to balance the influen ce of several sy stem parameters \naccording to us ers preferences .  \nThe com plexity  of m usic as a form al system as well as  a cultural \nphenom enon leads to difficulties for the com putational \nrepresentation. A single note by  itself has no meaning and there is \nno correspondent to the sy ntax governing natural language which \ncan be exploited in text retrieval. In addition, there exist many \ndifferent technical formats for storing musical data.  \nMusic as a cultural phenomenon has led to an abundance of \nmusical sty les. They  all use diffe rent methods to express their \nintentions or let the listener en joy a pleasing experience. Sounds, \nnotes , pauses, tempi, instruments, and voices  are com bined in \nmany ways. In this multimodal structure of m usic lies one of the \nroots of the problem of music re trieval. Depending on the sty le, \ndifferent atom s are as sembled for a com position. W hich elem ents \ncan be cons idered as  prom inent feature like a m elody or a them e \ndepends largely  upon the sty le and the user need. Therefore, a \nform al model for the representation allows an optim ization only  \nfor one sty le of m usic. W ith the possibility  to store music in \ndigital representations it has becom e increas ingly  attractive to \nsearch large music collections . As a matter of fact, the richnes s of \nmusic cannot be translated into all its dimensions (melody , \nharmony , rhythm etc.).  \nIn c ontra st to te xts, music al “documents” lack of separators \nnecessary  to identify  semantic units like “ words” or “ phras es”. \nLike textual words, the same melodic pattern may  occur in more \nthan one piece of m usic, perhaps  composed by different \ncomposers. Algorithm s for s temming are neces sary to detect \nvariants and conflate them  to th e same stem. [1]  have pointed out \nthe issues of content-based inde xing of musical data. The same \nentity  can be represented in two different m ain form s: the notated \nand the acous tic form . Music com munication is performed at two \nlevels : the com poser creates  a musical structure while the \ninterpreter (m usician or singer) translates the written score into \nsounds. The resulting perform ances may differ a lot from  each \nother. The inform ation within a m usical work can be identified at \ndifferent levels: Melody , harmony , rhy thm, and structure are dimensions, carried out by  written score, whereas in the case of \nmusical performance other dimensi ons like tim bre, articulation, \nand timing could be of interest.  \nThe m ost widely  spread m ode for m usic retrieval is to search via \nsimilarity , whereas sim ilarity  in music retrieval presents several \ndifficulties: what part of a song is likely  to be perceived as the \nthem e of the m usic?  \nMost sy stem s seek for sim ilarity  on a level of pitch. Usually  these \nsystems like SEMEX [2] only process monophonic melodies, \nhowever, for some musical sty les polyphonic matching would be \ndesirable. A global representati on might e.g. only  consider a \nhistogram  analy sis of pitch values . Approaches  from  speech \nrecognition have also been applied to music retrieval [3] . \nThe parameters for music retrieva l discussed above need to be \nconsidered when im plem enting a s ystem. Each param eter \nrepres ents one dim ension in the s olution s pace for a specific \nretrieval system. The s pace of potential s olutions  is highly  \ndimensional. The search for a solution within the high \ndimensional space has the goal of achieving a good retrieval \nquality . Therefore, the search is gui ded either by  heuristics or by  \nempirical results. Finding an optimal solution requires a large \ntestbed of tasks and evaluation of th e results by  users or experts.  \nHowever, when the conditions ch ange a different solution might \nbe optim al. Thes e changes  may be the consequence of different \nqueries, new user interests or cha nges of the music content. Now, \nanother solution might produce the optimal result.  \n2. THE MIMOR MODEL \nFusion of various approaches is wi dely used in com puter s cience. \nThe goal of apply ing several algorithms is to improve the overall \nperformance. Fusion methods delega te a task to several sy stems \nand integrate their results into one final result presented to the \nuser. Ideally , the weaknesses of one method do not have a large \nnegative influence on the final res ult becaus e they  are \nsuperimposed by  another method. A typical example are \ncommittee m achines  in m achine learning [4] . The fusion may be \nimplem ented as  a voting s cheme or as  a weighted linear  \nMIMOR (Multiple Indexing and Method-Object Relations ) is a \nfusion approach taking advantage of heterogeneity  [5, 6] . The \nMIMOR model samples users’ relevance feedback to predict \noptimal method-object relations  where methods are indexing \nalgorithm s or retrieval m odels . Thes e are as signed to the \ncharacteris tics of us ers and docum ents with the goal of improving \nthe overall retrieval quality . From  a computational viewpoint, \nMIMOR is designed as a linear combination of the results of \ndifferent retrieval s ystems. The contribution of each s ystem or \nalgorithm to the fusion result is governed by  a weight for that \nsystem.  \nPermission to m ake digital or  hard copies of all or  part of this \nwork for per sonal or  classr oom use is gr anted without fee \nprovided that copies ar e not made or distr ibuted for  profit or  \ncommercial advantage and that copies bear this notice and the full \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou Ndoc RSV\ndoc RSVN\nsystemi system system\ni MIMIR∑\n==1)) ( (\n) (ω\n Learning to Cope with Diversity in Music Retrieval  \nA central aspect in MIMOR is learning. The w eight of the linear \ncombination of each inform ation retrieval s ystem is adapted \naccording to the s uccess of the s ystem measured by the relevance \nfeedback of the us ers. A system which gave a high retrieval s tatus  \nvalue (RSV) and consequently  a high rank to a docum ent which \nthen received positive relevance feedback should be able to \ncontribute with a higher w eight to the final res ult. The follow ing \nform ula enables  such a learning proces s:  \n \nrate learningdoc RSV doc RFi system i user system\nεεω ) ( ) ( = \nLearning in M IMOR leads  to a fus ion w hich com bines the \nindividual systems in an optim al way. As a res ult, M IMOR takes  \nadvantage of two of the m ost prom ising s trategies  for im proving \ninform ation retrieval s ystems. Thes e are relevance feedback and \nfusion. However, the optim al com bination m ay depend on the \ncontext and especially  on the us ers’ individual pers pectives  and \nthe characteris tics of the docum ents. Therefore, M IMOR needs  to \nconsider context.  \nThe perform ance of inform ation retrieval s ystems differs  from  \ndomain to dom ain. Characteris tics of the docum ents relevant for \nthe indexing procedure m ay be responsible for that. MIMOR \nbuilds upon the idea that form al properties can be exploited to \nimprove fusion. Some retrieval m ethods work better e.g. for short \ndocum ents. The weight of these system s should be high for short \ndocum ents only .  \nThe properties  are m odeled as  clus ters. All docum ents which have \na property  in com mon belong to the same cluster. Each cluster can \ndevelop its  own adequate M IMOR m odel w ith w eights  for all \nparticipating s ystems.  \nThe term  clustering is usually  used for non-supervised learning \nmethods which find structures in data without hy potheses. \nHowever, the as signment of m usic pieces  to clus ters for the \nimprovem ent of inform ation retrieval proces ses may also be \ncarried out with supervised lear ning m ethods. Therefore, the term  \ncluster in this abstract does  not res trict this  proces s to algorithm s \nbased on unsupervised learning. Bo th supervised learning m ethods \nfor pre-defined clas ses and even human assignment are \ncompatible w ith M IMOR. \n3. M-MIMOR: SELF ADAPTATION FOR \nMUSIC RETRIEVAL SYSTEMS \nThe M IMOR approach is very  well suited for m usic retrieval. \nMusic retrieval incorporates high diversity  along several \ndimensions of system param eters . The choice of param eter values  \nis alm ost arbitrary . On the other hand, MIMOR offers  a fusion \nmethod which learns from  the pref erences of the user. The M-\nMIMOR approach makes productive us e of the m ulti-\ndimensionality  of m usic retrieval. It integrates  heterogeneous  \npoly-repres entation into a s elf adapting s ystem. The different \nperspectives  of users can be expres sed by relevance feedback and \nserve as  direction for a learning proces s which ultim ately  leads  to \nan optim al solution for a us er within a certain context. Ins tead of \nfocus ing on one value for each s ystem param eter, each user \nreceives  the m ost adequate m ixture of the options  available.  \nGenre detection systems have been developed for m usic [7] . \nTherefore, genre can be us ed as  one feature in M -MIMOR. The \ncalculation of the s imilarity  betw een query  and m usical objects  needs to consider not only the system s involved. In addition, the \nclusters to which an object belongs and the m embership function \nM enter the form al model. \n \nNKdoc RSV\ndoc RSVN\nsysi sys clur clussysK\nclus\ni MIMIR⋅=∑∑\n==1,\n1)) ( (\n) (ηω\n \n...), , , () (3 2 1 cluster cluster cluster idocM ηηη=  \n4. CONCLUSION \nThis article introduces a m odel for m usic retrieval which \nautom atically  learns to ad apt itself to the cogn itive preferences of \nthe user and supports the m ultimodal nature of m usic. Since the \nevaluation of m usical objects  is highly  subjective, a retrieval \nsystem needs  to dy namically  identify  the m ost adequate \ncombination of system param eters  for the us er. M -MIMOR \nmanages  this integration in a linear combination of many possible \nvariables . Cons equently , M-MIMOR takes  the pers onalization and \nadaptivity  one s tep further.  \nAs a res ult, no view point expres sed in a certain algorithm  or \nrepresentation method needs to be neglected but m ay contribute \nwith the proper w eight to the final result. The fusion of a divers ity \nof pers pectives  will ultim ately  lead to better retrieval \nperform ance.  \nMIMOR ha s been implemented in JAVA a nd has been tested in \ntext retrieval. Im provem ents agains t individual systems could be \nmeasured. The integration on m usic retrieval tools  bas ed on \nJAVA could be easily  realized.  \n5. REFERENCES \n[1] Melucci, M . and O rio, N . Musical Inform ation Retrieval \nusing M elodic Surface. Proc Conf. Digital Libraries 1999, \nBerkeley , CA . \n[2] Lemström , K. and P erttu, S . SEMEX – A n Efficient M usic \nRetrieval P rototy pe. Intl S ymp for M usic Inform ation \nRetrieval. ISMIR 2000.  \n<URL:http://ciir.cs.um ass.edu/m usic2000> \n[3] Logan, B. M el Frequency  Ceps tral Coeffficients  for M usic \nModeling. Intl S ymp for M usic Inform ation Retrieval. \nISMIR 2000. <URL:http://ciir.c s.um ass.edu/m usic2000> \n[4] Haykin, S. Neural Networks: A Com prehensive Foundation. \n1999. \n[5] Womser-Hacker, C. D as MIMOR-M odell. M ehrfachindexie-\nrung zur dy namischen M ethoden-Objekt-Relationierung im  \nInform ation Retrieval. H abilitations schrift. Univers ität \nRegensburg, Inform ationswissenschaft. 1997. \n[6] Mandl, T. and W omser-Hacker, C. Probability  Based \nClustering for Document and U ser Properties . In: O jala, T. \n(ed.). Infotech Oulo International W orkshop on Inform ation \nRetrieval (IR 2001). Oulu, Finland. 2001. pp. 100-107. \n[7] Tzanetakis, G. Essl, G. and Cook, P. Autom atic musical \ngenre clas sification of audio s ignals . Intl Symp for Music \nInform ation Retrieval. ISMIR 2001.  \n<URL:http://ism ir2001.indiana.edu>"
    },
    {
        "title": "On detecting repeated notes in piano music.",
        "author": [
            "Matija Marolt",
            "Sasa Divjak"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416078",
        "url": "https://doi.org/10.5281/zenodo.1416078",
        "ee": "https://zenodo.org/records/1416078/files/MaroltD02.pdf",
        "abstract": "One of the problems encountered in music transcription is to produce an algorithm that detects whether a note should be repeated, when a new onset is found during its duration, or not; with other words whether two or more shorter notes should be produced instead of a single longer note. The paper describes our approach to solving this problem, implemented within our system for transcription of piano music [4]. The approach is based on a multilayer perceptron neural network, trained to recognize repeated notes. We compare this method to a more naive method that tracks the amplitude of the first partial of each note and also present performance statistics of our system on transcriptions of several real piano recordings.",
        "zenodo_id": 1416078,
        "dblp_key": "conf/ismir/MaroltD02",
        "keywords": [
            "music transcription",
            "algorithm",
            "detects",
            "repeated notes",
            "multilayer perceptron neural network",
            "trained to recognize",
            "transcription of piano music",
            "system for transcription",
            "performance statistics",
            "real piano recordings"
        ],
        "content": "On Detecting Repeated Notes in Piano Music \n \nOn Detecting Repeated Notes in Piano Music \nMatija Marolt \nUniversity of Ljubljana \nFaculty of Computer and Information Science \nTrzaska 25 \n1000 Ljubljana \n+386 1 4768483 \nmatija.marolt@fri.uni-lj.si Sasa Divjak \nUniversity of Ljubljana \nFaculty of Computer and Information Science \nTrzaska 25 \n1000 Ljubljana \n+386 1 4768483 \nsasa.divjak@fri.uni-lj.si \n \nABSTRACT  \nOne of the problems encountered in music transcription is to \nproduce an algorithm that detects whether a note should be repeated, when a new onset is found during its duration, or not; with other words whether two or  more shorter notes should be \nproduced instead of a single longe r note. The paper describes our \napproach to solving this problem , implemented within our system \nfor transcription of piano music [4]. The approach is based on a multilayer perceptron neural network, trained to recognize repeated notes. We compare this method to a more naive method that tracks the amplitude of the first partial of each note and also present performance statistics of our  system on transcriptions of \nseveral real piano recordings. \n1. INTRODUCTION \nTranscription of polyphonic music (polyphonic pitch recognition) \ncan be defined as a process of converting an acoustical waveform \ninto a parametric representation, where notes, their pitches, starting times and durations are extracted from the waveform. Transcription is a difficult problem and most research efforts in building transcription systems are directed into partial tracking and note recognition algorithms, which are the central part of all current transcription systems.  \nEven with a perfect note recognition score, one of the problems \nthat each transcription system should handle in one way or another is detection of repeated notes. This can be a difficult problem, even if the played instrument has pronounced onsets (i.e. piano). An illustration of the problem is given in Figure 1.  \n \nFigure 1. Different interpretations of note recognition output \nThe upper part of the figure shows hypothetical outputs of onset \ndetection and note recognition algorithms on an unknown piece of music. Four onsets and four not es were found; note C4 lasts \nthrough the entire duration of the piece, while other notes appear for shorter periods of time. Four transcription examples show four \npossible interpretations of these outputs. Interpretations differ in \nthe way note C4 is handled; it could be transcribed as one whole note, four quarter notes... Altogether eight combinations are possible, and all of them are c onsistent with outputs of onset \ndetection and note recognition algorithms.  \nAlthough several transcription systems have recently been \ndeveloped, few have tackled th is problem in any way. Some \nauthors have simply ignored errors  related to repeated notes [1], \nsome put constraints on the tr anscribed signal [2] (i.e. by \nrestricting the minimal offset-onset distance), while some dealt with the problem implicitly by including instrument models into the transcription process. The pa per presents our experiences in \nhandling repeated notes within our system for transcription of piano music [4]. \n2. DETECTING REPEATED NOTES IN \nPIANO MUSIC \nProblems related to repeated notes are quite common in piano \nmusic, especially in pedaled part s, where long sustained notes can \nbecome a source of many errors. To make matters worse, strong onsets can temporarily mask sustained notes, resulting in fragmented output of the note recognition module and consequently in many spurious repeated notes. An illustration of the problem is given in Figure 2: it shows hypothetical outputs of the onset detector and note recognition module on a piano piece shown in example A. Strong onsets of B3D4 and B3E4 chords temporarily mask the sustained C4 note, resulting in fragmented \noutput of the note recognition module and consequently in three spurious notes.  \n \nFigure 2. Masking of a sustained note \nTo solve the problem, we first devised a simple algorithm, which \ntracks amplitude envelopes of partials of found notes. At each onset, all notes found to be present in the signal up to 50 ms before the onset and 100 ms after the onset (depending on the pitch) are taken as candidate notes for repetition. The algorithm then compares amplitudes of the fi rst four partials of candidate \nnotes before and after the onset. All notes with an increase in amplitude that exceeds a certain threshold dependant on the pitch are repeated. The approach works reasonably well, but we wanted to improve it further by including additional features into the detection algorithm. \nIn our current approach, we use a multilayer perceptron (MLP) \nneural network to determine which notes should be repeated in the \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncommercial advantage and that copies bear this notice and the full \ncitation on the first page.  \n© 2002 IRCAM – Centre Pompidou  On Detecting Repeated Notes in Piano Music \n \ntranscribed score. We chose the MLP network, because it is a well \nestablished method for solving cl assification problems, and we \nhad good experiences in using thes e networks for transcription \ntasks [4]. The MLP is activated at each new onset for all potential repeated notes (sustained notes that are active before and after the onset) and decides which notes s hould be repeated. After many \ntests and experiments,  we chose the following parameters to be \nincluded in the network's input vector: \n\u001f amplitude differences of the firs t four partials of the potential \nrepeated note after and before the new (repeated) onset and after and before its original onset; \n\u001f weighted averages of the above amplitude differences. \nWeights represent amplitude ratios of the first four partials of a note, calculated from average spectral templates of piano notes; \n\u001f weighted averages as above, but only of partials with \namplitude differences that fall w ithin the standard deviation of \nall amplitude differences. These averages were included to eliminate outliers, which might occur because of new notes that share partials with the potential repeated note; \n\u001f amplitude differences and both weighted averages of a \npotential repeated note after average spectral templates of other notes starting at the new (repeated) onset have been subtracted from the frequency spectrum; \n\u001f time difference between the original and the new onset; \n\u001f differences of partial group strengths before and after the \nnew onset. Partial group strengths are a product of the partial tracking module, described in [4]. \nAltogether, input vectors of the MLP consist of 26 parameters. \nMLP contains 5 neurons in th e hidden layer and one output \nneuron, which indicates whether a note should be repeated. The network was trained on pairs of input/output patterns taken from \nour piano music database consis ting of over 120 synthesized piano \npieces of various styles, including classical from several periods, jazz, blues and pop.  \n3. RESULTS \nWe tested the performance the MLP by transcribing a set of 40 \nsynthesized and real piano recordings (some examples can be found on http://lgm.fri.uni-lj.si/SONIC ). We compared its \nperformance to the ideal algorithm, where repeated notes were estimated from the transcription of the recording, to a system that never produced repeated notes a nd to the “amplitude envelopes” \napproach described in the previous  section. A summary of results \nis presented in Table 1. The sec ond column of the table shows the \npercentages of correctly found notes , while the third column lists \npercentages of spurious notes (not es not present in the input, but \nfound by the system) in all pieces.  \nTable 1. A comparison of different approaches \n correct spurious \ntranscription 90.4% 6.9% \nno repeated notes 75.6% 6.9% \namplitude envelopes 89.1% 12.3% \nMLP neural network 88.7% 9.0% \n \nAs expected, the ideal algorithm produces the best results. When \nno repeated notes are produced by the system, the percentage of correctly found notes diminishes substantially. The method that tracks amplitude envelopes of the first four partials of a note and the MLP network produce somewhat si milar results; the amp. env.  \nmethod produces more spurious notes and some more correct \nnotes, while the MLP produces a sort of compromise between the number of spurious and correct not es. The improvement gained by \nusing the MLP, however, is not as  substantial as we had initially \nhoped. \nAnalysis of transcriptions shows that repeated notes still represent \nthe second major cause of errors, octave errors being the most \ncommon one. Error analysis of transcriptions of three real recordings of piano music is presented in table 2. The pieces are: (1) J.S. Bach, English suite no. 5, BWV810, 1\nst mvm., performer \nMurray Perahia, Sony Classical  SK 60277; (2) R. Schumman, \nTräumerei, performer Cyprien Katsaris, TELDEC 75863; (3) S. Joplin, The Entertainer, performer unknown, MCA 11836. \nTable 2. Performance statistics on real recordings \n notes  total octave repeated \n1 1351 missed \nspurious 11.2% \n12.9% 39.1% \n72.5% 17.2% \n30.8% \n2 458 missed \nspurious 20.3% \n11.5% 55.9% \n78.6% 3.2% \n24.5% \n3 1564 missed \nspurious 11.3% \n14.2% 71.3% \n81.4% 12.9% \n11.2% \n \nThe second column of the table s hows the total number of notes in \neach piece. Percentages of missed and spurious notes are given in column 4, while columns 5 and 6 show percentages of octave and \nrepeated note errors with regard to all missed/spurious notes (both \nare frequently combined, so the sum can exceed 100%). Repeated note errors represent around 10% of all missed notes and approx. 21% of spurious notes. They ar e often combined with octave \nerrors (a note is mistakenly repeated or a repetition missed because of a note an octave apart appears in the score), which is one of the reasons that they are so difficult to detect. The percentage of repeated note erro rs is especially high in quiet \npedaled passages (Träumerei is a good example), where it sometimes exceeds 80%. We are working on a different set of features to include in MLP training that will hopefully improve current results. \n4. CONCLUSION \nThe presented method of detecting repeated notes in piano music \nprovides an improvement over th e naive amplitude envelope \ntracking method, but still lacks accuracy that would be satisfying. We contribute most errors to the inadequacy of the feature set used for training MLP networks and are further exploring new \nfeatures to improve the results. We  first plan to experiment with \nusing different features for notes in different registers (such as \nlow, middle, high), as these ha ve quite different amplitude \nenvelopes, and should be treated  differently by the detection \nalgorithm. \n5. REFERENCES \n[1] L. Rossi, Identification de Sons Polyphoniques de Piano. \nPh.D. Thesis, L'Universite  de Corse, France, 1998. \n[2] A.D. Sterian, Model-based Se gmentation of Time-Frequency \nImages for Musical Transcription. Ph.D. Thesis, Univesity of Michigan, 1999. \n[3] K. Kashino, K. Nakadai, T. Kinoshita, H. Tanaka, \n“Application of Bayesian Probability Network to Music Scene Analysis,” in \"Computational Auditory Scene Analysis\", Lawrence Erlbaum Associates, 1998. \n[4] M. Marolt, \"SONIC : transcription of polyphonic piano \nmusic with neural networks,\" in Proceedings of Workshop on Current Research Directions in Computer Music, Barcelona, Pompeu Fabra University, 2001, pp. 217-224."
    },
    {
        "title": "Introducing Feedback into an Optical Music Recogniition System.",
        "author": [
            "John R. McPherson"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417725",
        "url": "https://doi.org/10.5281/zenodo.1417725",
        "ee": "https://zenodo.org/records/1417725/files/McPherson02.pdf",
        "abstract": "Optical Music Recognition is the process of converting a graphical representation of music (such as sheet music) into a symbolic for- mat (for example, a format that is understood by music software). Music notation is rich in structural information, and the relative po- sitions of objects can often help to identify them. When objects are unidentified or mis-identified, many current systems “coerce” the set of objects into some semantic representation, for example by modifying the detected durations. This could cause correctly identified symbols to be modified. The knowledge that the current set of identified symbols cannot be semantically parsed could in- stead be used to re-examine some of the symbols before deciding whether or not the classification is correct. This paper describes work in progress involving the use of feedback between the various phases of the optical music recognition process to automatically cor- rect mistakes, such as symbolic classification errors or mis-detected staff systems.",
        "zenodo_id": 1417725,
        "dblp_key": "conf/ismir/McPherson02",
        "keywords": [
            "Optical Music Recognition",
            "Graphical representation of music",
            "Symbolic format",
            "Structural information",
            "Relative positions of objects",
            "Semantic representation",
            "Music software",
            "Objects identification",
            "Mis-identification",
            "Feedback between phases"
        ],
        "content": "IntroducingFeedbackintoanOpticalMusicRecognitionSystem\nIntroducingFeedbackintoanOpticalMusicRecognition\nSystem\nJohnR.McPherson\nDepartmentofComputer Science\nUniversityofWaikato\nHamilton\nNewZealand\njrm21@cs.waikato.ac.nz\nABSTRA CT\nOpticalMusicRecognition istheprocessofconvertingagraphical\nrepresentation ofmusic(suchassheetmusic)intoasymbolic for-\nmat(forexample,aformatthatisunderstood bymusicsoftware).\nMusicnotationisrichinstructural information, andtherelativepo-\nsitionsofobjectscanoftenhelptoidentifythem.Whenobjects\nareunidentiﬁed ormis-identiﬁed, manycurrentsystems“coerce”\nthesetofobjectsintosomesemantic representation, forexample\nbymodifying thedetecteddurations. Thiscouldcausecorrectly\nidentiﬁed symbolstobemodiﬁed. Theknowledgethatthecurrent\nsetofidentiﬁed symbolscannotbesemantically parsedcouldin-\nsteadbeusedtore-examinesomeofthesymbolsbeforedeciding\nwhetherornottheclassiﬁcation iscorrect.Thispaperdescribes\nworkinprogressinvolvingtheuseoffeedback betweenthevarious\nphasesoftheopticalmusicrecognition processtoautomatically cor-\nrectmistakes,suchassymbolic classiﬁcation errorsormis-detected\nstaffsystems.\n1.INTRODUCTION\nOpticalMusicRecognition (OMR)isofgreatimportance tomusic\ninformation retrieval—thereisanenormous amountofmusicthat\niscurrently onlyavailableinprintedform.Theprocessistypically\nsplitintodistinctphases,suchasstafflocationand/orpre-processing ,\nobjectlocationandidentiﬁcation, andmusicalsemantics analysis.\nTheworkdescribed inthispaperisacontinuation oftheworkby\nBainbridge onextensible opticalmusicrecognition [1].Theresult\nofthatwork,theCANTORsystem,ishighlycustomisable, using\nconﬁgurable components suchaspatterndescriptions, grammars,\nandsetsofrulestorecognise staff-basedmusicnotation, including\ncommon musicnotationandplainsong notation.\nUntilrecently,mostoftheworkinOMRinvolvedmakingeach\nstageofthefeed-forw ardsystemasrobustaspossible, allowingfor\nmistakesbypreviousstages.FujinagaandDroettboom [2]describe\nasystemwithtwodistinctsubsystems: aglyphrecognition stageand\nasemantic interpretation stage.SealesandRajasekar [3]describea\nrule-based systemusingfeedback todofurtherprocessing onnote-\nheadsandstemsthatcannotbesemantically joined.Baumann [4]\nusesanattributedgraphgrammar toperformprimitiveassembly and\nsemantics analysis. Hementions problems duetothequalityofthe\nprimitiverecognition, andsuggeststhepossiblebeneﬁtoffeedback\nfromthegrammar tothesymbolrecognition phase.St¨uckelburgand\nDoermann [5]describetheuseofstochastic methodsforﬁndingthe\nsemantics thatbestdescribes theinputimage.\nByusingfeedback, additional contextcanbeusedtodetermine\nhowtoautomatically correcterrors,ratherthantryingtoarbitrarily\nmodifyaninvalidsemantic structure thatwascreatedbyasingle\npass.Forexample,acommon tacticfordealingwithbarswithan\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.c\n\u0000\n2002IRCAM-CentrePompidouCo-ordinatorIMAGEMUSIC REPRESENTATION\nMusical\nSemantics/Analysis\nStaff Processing Primitive Location Primitive IdentificationPrimitive AssemblyPage Layout\nFigure1:Frameworkshowingsample`specialist' modules\nincorrect duration(relativetothetimesignature) istoinsertrests.\nInthesystemunderdevelopment, thecoordinating processhasno\nknowledgeatallaboutmusic(eithersemantics orprimitives).Its\ntaskistodetermine theorderofexecutionofthespecialist modules,\nandhandlerequestsgenerated bythem.Figure1showshowthe\nﬂowofexecutionisnolongerastatic,linearpath.Nopartofthe\nsystemassumesthatarequestwillbeaccepted. Ifthecoordinator is\nsettorejectallrequests, thesystemrunsinatypical“feed-forw ard”\nmanner.Mostofthefeedback isexpectedtobefromtheassembly\nandsemantics stagestothepatternrecognition stage,although the\nsemantics stagewillalsosendfeedback totheassembly stage.For\nexample,thepatternrecognition stagecouldbetoldthataparticular\nobjectisincorrectly classiﬁed, andbegivenalistofpossibletypes\nthatwouldmakesensesemantically basedonthenearbyidentiﬁed\nobjects.\n2.PATTERNRECOGNITION\nThepatternrecognition moduleisthemainrecipient offeedback\nwhenthecurrentsetofidentiﬁed objectscannotbeperfectly ex-\nplainedbythesemantic rulesets.\nPossiblemethodsforthismoduletotakeaccountofanynewinfor-\nmationinclude:\u0001Storingmorethanonepossibleclassiﬁcation (andamatching\ncertainty rating)foreachgraphical object.Thismeanslater\nstagescouldtrydifferentarrangements ofobjectsbasedon\ntheseclassiﬁcations. The“best”arrangement couldbeused\ntoincreasethecertainty foraparticular classiﬁcation ofthis\nobject.\u0001Loosening restrictions oncertainpatterns. Forexample,ifwe\ncandetermine thatanobjectcouldbeanaccidental (forexam-\nple,asharporﬂat)basedontheobjectsaroundit,thesystem\nmightlowerthetolerances onthepatternsforaccidentals.\u0001Tryingalternate patterns. Weshouldbeabletohavease-\nriesofpatternsthatdescribeatrebleclef(basedondifferent\npublishers’ fonts,forexample), whichareeithermanually or\nautomatically (orrandomly) givenanordertotestin.This\ncouldbedonebykeepingahistoryofwhichisthemostsuc-\ncessfulpattern,oropticallyrecognising thepublisher’ sname.\u0001Usingadifferentmethodology forrecognition, suchasaneu-\nralnetworkorFujinaga’ sadaptivesystem.IntroducingFeedbackintoanOpticalMusicRecognitionSystem\n3.FLEXIBILITY OFDESIGN\nThemodulardesignmakesiteasiertoaddfunctionality tothesys-\ntem.Anexampleisthatamoduledesigned tocorrectsegmentation\nerrorswaseasilycreatedinisolation, withoutmajormodiﬁcations\ntotheothermodules. Theonlymodiﬁcations requiredwere:\u0001thecoordinator hadtoincludethenewmodule,andexecuteit\naftertheprimitiveidentiﬁcation modules; and\u0001theprimitiveidentiﬁcation moduleandthenewmodulehad\ntoagreeonthenameoftherequestusedtocalltheformer\nagain,toprocessanychangesmadebythenewmodule.\nFigure2:BrokenObjectsasaResultofStaffProcessing\nFigure2showsthestartoftheﬁrststafffrom“Promenade” by\nMussorgsky,aftertheStaffProcessing modulehasremovedthestaff\nlines.Thisprocesscanfragment objects—bassclefsandﬂatsare\nparticularly pronetothisastheycontainthinsegmentsthatcoincide\nwithstafflines.Inthisﬁgure,thethreefragments ofthebassclef\nandthetwofragments ofthetop-most ﬂatarenotidentiﬁed withthe\ndefaultsetofpatterndescriptions. ThenewSegmentation specialist\njoinsthenearbyobjectstogetherintoconglomerate objectsandsends\narequesttothePrimitiveIdentiﬁcation specialist toprocessany\nnewlycreatedobjects.Thisresultsinbothobjectsbeingcorrectly\nidentiﬁed.\nAnotheradvantageofthedesignisthatthecurrentprimitiverecogni-\ntionstagecouldbeeasilyreplacedoraugmented byanotherpattern\nrecognition schemeasmentioned previously,suchasFujinaga’ s\nadaptiverecognition.\n4.CURRENT PROTOTYPE\nSymbolic classiﬁcation isachievedbycreatingphysicaldescriptions\nofprimitives.Thisallowsarbitrary notations toberecognised —\nasaproof-of-concept, description ﬁleswerewrittentorecognise\nchesspiecesfromacomputer-generated chessdiagram, although\nnosemantic ruleswerewrittentodescribethelayoutofthechess\nboard.Theprototype (atthetimeofwriting)madelimiteduseof\nfeedback. Forexample,feedback isusedbythestaffprocessing\nmoduletodoublechecksystemsifskewisdetected, ortousemore\nexpensivealgorithms ifnostaffsystemswerefound.Atthetimeof\nwriting,theprototype onlyhadbasicsemantics supportforcommon\nmusicnotation. Feedback fromthesemantics modulewasnotsuc-\ncessfully usedbytheprimitiveidentiﬁcation stagetomodifyobject\nclassiﬁcations. Theonlymusicalﬁleformatcurrently supported for\noutputistheGUIDOﬁleformat,althoughﬁlesinPostScript format\ncanbeproduced toshowtheinternalstateofthelattice.\n5.DISCUSSION ANDFUTURE WORK\nAspreviouslymentioned, thecoordinating moduledoesnotcontain\nanyinformation aboutmusicstructure orgraphical objects. This\ndesignensuresthatthesystemwillnotbelimitedtoanyparticular\nmusicnotation. Insightmayalsobegainedintotheapplicability ofthesetechniques tonon-musical recognition domains. Oneproblem\nfacingresearchers inrecognition domainsishowtorepresentseman-\nticinformation. Norulesetisabletocompletely describesomething\nasill-deﬁned ascommon musicnotation, ascomposers oftenbreak\nwithconventionwhereconvenient[6].Animportant partofOMRis\ntextrecognition, althoughexperimentation withfreeopticalcharac-\nterrecognition systemshasshownmixedresults.However,because\nsomuchinformation onamusicalscoreistext—forexample,\nperformance notes,document metadata, andinstrument names—a\ntextrecognition modulemustbeconsidered.\nAgoodoverviewofdiagramanalysisingeneralisgivenbyBlostein\netal.[7].Ofparticular relevanceisdiscussion ofblackboardsystems\nandcontextualfeedback. Differentcoordinating strategieswillbe\ndevelopedandevaluated—factorssuchasoverallaccuracy,levelof\nuserinterventionrequired, andprocessor andmemoryresources are\nofinterest.However,asnotedbyBainbridge, comparisons between\ndifferentrecognition systemsareproblematic duetobothdiffering\nrepresentations orformatsusedbythesystemsandthedifﬁcultyin\ndetermining relative‘correctness’ ofrecognised scores.\nFutureversionsofthesystemmighthavetwoormorepatternrecog-\nnitionmodules, andusecoordinating strategiesthattakeadvantage\nofthis.Forexample,avotingmechanism couldbeusedtoclassify\nobjects,orthecoordinator couldrandomly choosewhichmodule\ntouseﬁrst,orpreference couldbegiventothemodulethathas\nhistorically madetheleastdetectedclassiﬁcation errors.Otherco-\nordinating strategiesincludeblackboard systemsandframe-based\nsystems.\n6.REFERENCES\n[1]DavidBainbridge. Extensible OpticalMusicRecognition.PhD\nthesis,UniversityofCanterbury,Christchurch, NewZealand,\n1997.\n[2]Michael Droettboom. Selected research incomputer music.\nMaster’sthesis,ThePeabody InstituteoftheJohnHopkins\nUniversity,Baltimore, Maryland, USA,April2002.\n[3]W.B.SealesandA.Rajasekar .Interpreting musicmanuscripts:\nAlogic-based, object-oriented approach. InR.T.Chin,H.H.Ip,\nA.C.Naiman,andT.-C.Pong,editors,ImageAnalysisAppli-\ncationsandComputer Graphics,ThirdInternational Computer\nScienceConference,pages181–188, HongKong,11–13De-\ncember1995.Springer.\n[4]S.Baumann. Asimpliﬁed attributedgraphgrammar forhigh-\nlevelmusicrecognition. InProceedings oftheThirdInterna-\ntionalConferenceonDocument AnalysisandRecognition,IC-\nDAR’95,pages1080–1083, Montr´eal,Canada,August1995.\nIEEE.\n[5]MarcVuilleumier St¨uckelbergandDavidDoermann. Onmusi-\ncalscorerecognition usingprobabilistic reasoning. InProceed-\ningsoftheFifthInternational ConferenceonDocument Analy-\nsisandRecognition,ICDAR’99,Bangalore, India,1999.IEEE.\n[6]DonaldByrd.Musicnotationsoftwareandintelligence. Com-\nputerMusicJournal,18(1):17–20, 1994.\n[7]D.Blostein, E.Lank,andR.Zanibbi.Treatment ofdiagrams\nindocument imageanalysis. InM.Anderson, P.Cheng,and\nV.Haarslev,editors,TheoryandApplications ofDiagrams,vol-\nume1889ofLectureNotesinComputer Science,pages330–\n344.SpringerVerlag,2000."
    },
    {
        "title": "Johnny Can&apos;t Sing: A Comprehensive Error Model for Sung Music Queries.",
        "author": [
            "Colin Meek",
            "William P. Birmingham"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418065",
        "url": "https://doi.org/10.5281/zenodo.1418065",
        "ee": "https://zenodo.org/records/1418065/files/MeekB02.pdf",
        "abstract": "We propose a model for errors in sung queries, a variant of the Hidden Markov Model (HMM). This is related to the problem of identifying the degree of similarity between a query and a potential target in a database of musical works, in the music retrieval framework. The model comprehensively expresses the types of error or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. Results of experiments demonstrating the robustness of the model are presented.",
        "zenodo_id": 1418065,
        "dblp_key": "conf/ismir/MeekB02",
        "keywords": [
            "sung queries",
            "Hidden Markov Model (HMM)",
            "musical works",
            "music retrieval framework",
            "cumulative and non-cumulative local errors",
            "transposition",
            "tempo and tempo changes",
            "insertions",
            "deletions",
            "modulation"
        ],
        "content": "Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nJohnny Can’t Sing: A Comprehensive  \nError Model for Sung Music Queries\nColin Meek and William Birmingham \nUniversity of Michigan \nAdvanced Technologies Laboratory \n1101 Beal Avenue \n1-734-763-1561  \nmeek@umich.edu\nABSTRACT  \nWe propose a model for errors in  sung queries, a variant of the \nHidden Markov Model (HMM). This is related to the problem of \nidentifying the degree of similarity between a query and a \npotential target in a database of musical works, in the music \nretrieval framework. The model comprehensively expresses the \ntypes of error or variation between target and query: cumulative \nand non-cumulative local errors, transposition, tempo and tempo \nchanges, insertions, deletions  and modulation. Results of \nexperiments demonstrating the robustness of the model are \npresented.  \n1. INTRODUCTION \nVarious approaches have been pr oposed for the identification of \nviable targets for a query in a music database. We are interested \nhere in queries posed in the mo st natural format for untrained \nusers, the voice. Our goal is to  demonstrate a unifying model, \nexpressive enough to account for the complete range of \nmodifications  observed in the performance and transcription of \nsung musical queries. Our model is capable of expressing the \nfollowing transformations, relative  to a stored musical piece, \nwhich we call a target : \n• Transposition: the query may be sung in a different key \nor register than the target, or both. \n• Modulation: over the course of a query, the singer may \nchange transposition. \n• Tempo: the query may be slower or faster than the \ntarget. \n• Tempo change: over the course of a query, the singer \nmay speed up or slow down. \n• Non-cumulative local error: the singer might sing a note \noff-pitch or with poor rhythm. \n• Cumulative local error: local errors altering or effecting \nsubsequent events. \n• Insertions and deletions: adding or removing notes from \nthe target, respectively. \nWhile various representations and models can effectively \nrepresent some of these elements , to our knowledge no existing \nmodel explicitly accounts for all of these elements. \nAn important contribution of this work is an in depth exploration \nof the nature of note insertions  and deletions. We  assert that \ntraditional string-edit operations [9][10] must be extended in the \nmusical context and to this end introduce the corollary operations: elaborations and joins. Experiment s have demonstrated that naive \nstring edit approaches do not provide sufficient precision for \nmusic in the retrieval context [16]. \nExisting work using HMMs for query-by-humming consider a \nsubset of the error-classes identifi ed above. Shifrin et al,[15] use a \nstate representation that is flexible in terms of transposition and \ntempo, but otherwise is capable of expressing only cumulative  \nlocal error. Durey [6] does not allow for either transposition or \ntempo scaling. \nThis expressiveness has a computational cost. Conceptually our \nmodel considers states for each pe rmutation of pitch relationship, \nrhythm relationship and string -edit operation. We make the \nassumption of conditional inde pendence among th ese elements. \nFor instance, the probability that a singer will skip a note is \nassumed to be independent of how out of tune they sang the \nprevious note in our model. Thes e assumptions help control the \ncomputational cost, but also re duce the parameterization of the \nmodel, essential for training [14]. Shifrin et al, [15] make similar \nassumptions about the independence of local pitch and rhythm \nerrors, thus considerably reduci ng the amount of data needed to \ntrain the model. We recomme nd various approaches to parameter \ntying  [1] throughout this paper (effectively reusing various \nparameters in different models a nd/or parts of models) but stress \nthat this is an open area of research in this field. \n2. PROBLEM FORMULATION AND \nNOTATION \nAn assumption of our work is that pitch and inter-onset interval \n(IOI) adequately represent both the target and the query. This \nlimits our approach to monophoni c lines, or sequences of note \nevents . An event consists of a IOI Pitch ,  pair. The IOI is the \ntime difference between the onsets of successive notes, and the \npitch is the MIDI note number1. \nA crucial observation is that we are dealing with a note-level \nabstraction of music. Other systems act on a lower-level \nrepresentation of the query [6][11], a frame-based frequency \nrepresentation. Various methods for the translation of frequency \nand amplitude data into note abstraction exist [13][15]. Our group \ncurrently uses a transcriber based on the Praat f0-extractor [5], \ndesigned to analyze voice pitch c ontour. A sample Praat analysis \nis shown in Figure 8. Note that these processes are not perfect, and \nit is likely that error will be intr oduced in the transcription of the \nquery.  \n                                                                 \n1 Musical Instrument Digital Interface (MIDI) has become a \nstandard electronic transmission and storage protocol/format for \nmusic. MIDI note numbers essentially correspond to the keys of \npiano, where ‘middle C’ corresponds to the integer value 60. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the full \ncitation on the first page.  \n© 2002 IRCAM – Centre Pompidou  Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nRestricting ourselves to this event description of target and query \nignores several elements of mu sical style, including dynamics, \narticulation and timbre, am ong others. Objectively and \nconsistently characterizing these feat ures is quite difficult, and as \nsuch we have little confidence th ey can be usefully exploited for \nmusic retrieval at this point . We acknowledge, however, the \nimportance of such elements in music query/retrieval systems in \ngeneral. They will likely prove essential in refining and/or filtering \nthe search space [4][7]. \nWe further simplify the representa tion using IOI quantization, and \nby representing pitch in terms of pitch class . IOI is quantized to a \nlogarithmic scale, using q=29 quantization levels, within the range \n30 msec. to 3840 msec., chosen such that there are precisely four \ngradations between an eighth note  and sixteenth note (or quarter \nnote and sixteenth note, and so fo rth.) This representation mirrors \nconventional notation in Western music, in which the alphabet of \nrhythmic symbols (eighth, quart er, half, etc.) corresponds to a \nlogarithmic scale on duration (see Figure 1.)  \n0 5 10 15 20 25 3005001000150020002500300035004000IOI (msec.)\nIOIsym\nAt 125 beats per minute, the following relationships hold:\n \nFigure 1: IOI quantization \nWe treat pitch in terms of pitch class , where all notes are folded \ninto a single octave, and are considered in the context of the 12-\ntone, well-tempered scale. For in stance, the frequency 453 Hz is \n“binned” into MIDI note number 70. The corresponding pitch-\nclass is 70mod 12 = 10. This addresses two issues: octave errors are \nquite common in some transcriber systems, and pitch class is an \neffective, if imperfect, musical [12] and perceptual [3] abstraction. \nIn addition, this has the advant age of substantially reducing the \nmodel’s “alphabet” size. \nWe choose discrete sets of symbol s to represent pitch and duration \nsince, as will be seen, a continuous representation would \nnecessitate an unbounded number of states in our model. This \nsecond event representation is denoted: \n{} }281 ,...,1,0{ ,11,...,1,0 , , =− ∈ ∈ q IOI PC IOIPCsym sym, \nwhere PC is pitch-class and IOI sym is the quantization symbol. For \nclarity, we will alternate between the representations describe \nabove in this paper. The values are calculated as follows given a \nIOI Pitch ,  pair (where 30 and 3840 are the IOI values \nassociated with the centers of the shortest and longest bins): \n\n\n\n−⋅−−= = )1(30log 3840log30log log, mod12 qIOIround IOI Pitch PCsym \nThe goal of this paper is to presen t a model for query errors within \nthe scope of this simple event representation. We will first outline \nthe relevant error classes, and then present an extended Hidden \nMarkov Model accounting for these errors. Taking advantage of certain assumptions about the da ta, we can then efficiently \ncalculate the probability of a target model generating a query. \n3. ERROR CLASSES \n3.1 Insertions and Deletions \nInsertions and deletions in music tend to influence surrounding \nevents. For instance, when an insertion is made, the inserted event \nand its neighbor tend to occupy the temporal space of the original \nnote: if an insertion is made and the duration of the neighbors is \nnot modified, the underlying rhythmic structure (the beat) is \nchanged. We denote this type of insertion a “warping” insertion. \nFor instance, notice the alignment of notes after the warping \ninsertion in Figure 2, indicated by the dotted arrows. The inserted \nnotes are circled. For the non-warping insertion, the length of the \nsecond note is shortened to accommodate the new note. \nWith respect to pitch, insertio ns and deletions do not generally \ninfluence the surrounding events. However, previous work \nassumes this kind of effect: noting that intervallic contour tends to \nbe the strongest component in  our memory of pitch, one \nresearcher has proposed that insertions and deletions could have a \n“modulating” effect [10], where th e edit introduces a pitch offset, \nso that pitch intervals rather than the pitches themselves are \nmaintained. We argue that relative pitch, with respect to the query \nas a whole, should be preserved.  Consider the examples in Figure \n3. The first row of numbers below the staff indicates MIDI note \nnumbers, the second row indicates the intervals in semitones (‘u’ \n= up, ‘d’ = down.) Notice that the intervallic representation is \npreserved in the modulating inser tion, while the overall “profile” \n(and key) of the line is maintained in the non-modulating \ninsertion. \nThe effects of these various kinds of insertions and deletions are \nnow formalized, with respect to a target \n{ }b b a a IOI Pitch IOI Pitch , , ,  and a query \n{ }d d insert insert c c IOI Pitch IOI Pitch IOI Pitch , , , , , , where \ninsert insertIOI Pitch ,  is the inserted event. Note that deletion is \nsimply the symmetric operation, so we will show examples of \ninsertions only (Figure 2 and Figure 3): \n• Effects of a warping insertion on IOI: \nb d a c IOI IOI IOI IOI = = ,  \n• Effects of a non-warping insertion on IOI: \nb d insert a c IOI IOI IOI IOI IOI = −= ,  \n• Effects of a modulating insertion on pitch: \n4434421\ncontour pitch ,a b insert d a c Pitch Pitch Pitch Pitch Pitch Pitch −+ = = \n• Effects of a non-modulating insertion on pitch: \nb d a c Pitch Pitch Pitch Pitch = = ,  \nTarget:\nQuery, warping\ninsertion:\nQuery, non-warping\ninsertion:\n \nFigure 2: Warping and non-warping insertions Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nIn our current model, non-modula ting and non-warping insertions \nand deletions are handled explicitl y. The other types of edit are \nrepresented in combination with other error classes. For instance, a \nmodulating insertion is simply an insertion combined with a \nmodulation. Consider for instan ce insertions or deletions \nintroduced by an imperfect transcri ber. In this case, we clearly \nwould not expect the onset times or pitches of surrounding events \nto be influenced. The relationships amongst successive events \nmust be modified to avoid wa rping and modulation! Reflecting \nthis bias, we use the terms “join”  and “elaboration” to refer to \ndeletions and insertions respec tively. Mongeau and Sankoff [20] \nuse a similar notion of inserti on and deletion, described as \n“fragmentation” and “conso lidation” respectively. \nTarget:\nQuery, modulating\ninsertion:\nQuery, \nnon-modulating\ninsertion:\n \nFigure 3: Modulating and non-modulating insertions \n3.2 Transpositions and Tempo \nWe account for the phenomenon of  persons reproducing the same \n“tune” at different speeds and in different registers or keys. Few \npeople have the ability to rememb er and reproduce exact pitches \n[17], an ability known as “absolute” or “perfect” pitch. As such, \ntranspositional invariance is a desirable feature of any \nquery/retrieval model. The effect of transposition is simply to add \na certain value to all pitches. Consider for example the \ntransposition illustrated in Figure 4(a) of Trans = + 4. \ntimepitchTempo= 1.5\nTrans=+4query\ntarg et\noverlap\ntimepitch\nChange= 1.5Modu=+ 2\n2\ntimepitchRError= 1.5\nPError =-1a)\nb) c)\n \nFigure 4: error class examples, opening notes of Brahms’ \n“Cello Sonata in e-minor” \nTempo in this context is simply the translation of rhythm, which \ndescribes duration rela tionships, into actual time durations. Again, \nit is difficult to remember and reproduce an exact tempo. \nMoreover, it is very unlikely th at two persons would choose the \nsame metronome marking, much less unconstrained beat timing, for any piece of music. The effect of a tempo scaling is simply to \nmultiply all IOI values by some amount. Thus, if the query is 50% \nfaster than the target, we have a scaling value of Tempo= 1.5, as \nshown in Figure 4(a).  \nIn practice, we use quantized te mpo scaling and duration values. \nNote that addition in the logarithmic scale is equivalent to \nmultiplication, yielding a substantial computational advantage: \nthis replaces floating point multiplication with integer addition. \nFor instance, given our quantiz ation bins, a doubling of tempo \nalways corresponds to an addition of 4: \n4 2 += ↔=sym Tempo Tempo . \n0 1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.91\nTime (sec.)Ratio of Current Local Tempo to Maximum\n \nFigure 5: Tempo increase \n3.3 Modulations and Tempo Changes \nThroughout a query, the degree of transposition or tempo scaling \ncan change, referred to as modulations and tempo changes \nrespectively. Consider for a moment a query beginning with the \nidentity transposition Trans= 0 and identity tempo scaling \nTempo= 1, as in Figure 4(b). When a modulation or tempo change \nis introduced, it is always w ith respect to the previous \ntransposition and tempo. For inst ance, on the third note of the \nexample, a modulation of Modu= +2 occurs. For the remainder of \nthe query, the transposition is then equal to 0+2=+2, since 0 is the \nstarting reference transposition. Similarly, the tempo change of \nChange= 1.5 on the second note means that all subsequent events \noccur at a tempo scaling of 1*1.5=1.5.  \nConsider Figure 5, which plots the apparent tempo scaling in a \nrendition of “Row, Row, Row your Boat” on a note-by-note basis. \nWhile our model considers severa l interpretations of such a \nrendition, one approach would be to consider a constantly \nincreasing tempo, represented by  the least-square deviation \nregression line, with local rhythmic errors (see Section 3.4), \nrepresented by the note-wise deviations from that line. \n3.4 Local Pitch and IOI Errors \nIn addition to the “gross” errors we  have discussed thus far, there \nare frequently local errors in pitch and rhythm. These errors are \nrelative to the modifications desc ribed above. A local pitch error \nof PError  simply adds some value to the “ideal” pitch. A local IOI \nerror of RError has a scalar effect (or again, additive in the \nquantized domain.) Figure 4(c) show s examples of each. Note that \nthese errors do not propagate to subsequent events, and as such as \ntermed non-cumulative errors. We  model cumulative errors as \ntranspositions and tempo changes. \nIn some cases, there are multiple in terpretations for the source of \nerror in a query. Consider for instance Figure 9, which shows a Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nspecific interpretation of three disagreements between a target and \nquery. The second note in the query is treated as a local pitch error \nof -1. The final two notes, whic h are a semi-tone sharp, are \nhandled as a modulation. The e rror model, described below, \nconsiders all possible interpretati ons simultaneously, for instance \nconsidering the possibility that the error in the second note is \naccounted for by two modulations (before and after), and the final \ntwo errors by a pair of local errors. Depending on our expectation  \nthat such errors will occur, one or the other interpretation might \nappear more likely. \n4. ERROR MODEL \nHidden Markov models are the basis for our approach. For an \nexcellent tutorial on these structures, please see Rabiner [14]. \nWe account for edit errors in the query (insertions and deletions) \nin the “hidden” portion of the m odel. Using the notion of state \n“clusters,” we account for transposition, modulation, tempo and \ntempo changes. Fine pitch and rh ythm errors are accounted for in \nthe observation distribution function. \n4.1 Join and Elaborations \nFor the sake of notational clarity, we do not enumerate the states \nin the hidden model, but define them in terms of symbols that \nindirectly refer to events in the target sequence. There are three \ntypes of symbol: \n• i Same : refers to the correspondence between the ith note \nin the target and an event in the query. \n• l\niJoin : refers to a “join” of l notes, starting from the ith\n \nnote in the target sequence. In  other words, a single note \nin the query replaces l notes in the target. \n• m\njiElab,: refers to the jth\n  query note elaborating the ith \ntarget note. In other words, a single note in the target is \nreplaced by m notes in the query. \nNotice that 1\n1,1\ni i i Elab Join Same == , each referring to a one-to-one \ncorrespondence between target and query. In our implementation, \n1\niJoin  plays all three roles. We generate a set of states S for a \ngiven target consisting of, for each target event: \n• A Same state. \n• Join states for 2 ≤l≤L, where L is some arbitrary limit on \nthe number of events that can be joined. \n• Elaboration states for 2 ≤m≤M and 1≤j≤m, where M is \nsome arbitrary limit on the length of elaborations. \nWhy do we have so many states to describe each event in the \ntarget? We wish to establish a one-to-one correspondence between \nhidden states and query events, to simplify the implementation, \nwhich is why we introduce multiple states for each elaboration. \nWe choose not to implement join s by “skips” through a reduced \nset of states, since as discusse d, joins influence not only which \ntarget events we consider, but how we interpret them.  \n4.2 Transition Matrix \nWe now describe the transition matrix A, which maps from \nℜ→×SS . Where qt is the state at time t (as defined by position \nin the query note sequence), axy represents the probability \nP(qt+1=y| qt=x), or in other words, the chances we will proceed \nfrom state x to state y. \nMost of the transitions have zero probability, as suggested by the \nstate descriptions. For instance, Same i states can only precede \nstates pointing to index i+1 in the target. Elaboration states are \neven more restrictive, as they form deterministic chains of the \nform: m\nmim\nim\ni Elab Elab Elab, 2, 1, →→→ K . This last state can then proceed, like Same i, to the i+1 states. Similarly, l\niJoin  states can \nonly proceed to i+l states. A sample model topology is shown in \nFigure 6, for M=L= 2. Note that this is a left-right  model, in which \ntransitions impose a part ial ordering on states. \nBased on properties of the target, we can generate these transition \nprobabilities. We define PJoin(i,l) as the probability that the ith note \nin the target will be modified by an order l join. PElab(i,m) is the \nprobability that the ith note in the target will be modified by an \norder m elaboration. PSame(i) has the expected meaning. Since \nevery state has non-zero transitions to  all states with a particular \nstate, we must insure that: \n1),( ),( )( ,\n2 2= + + ∀ ∑∑\n= =L\nlJoinM\nmElab Same li P mi P i Pi \nThis also means that along non-zer o transitions, the probability is \nentirely determined by the s econd state. For example, the \nprobability of the transition 2\n1,52\n3 Elab Join→  is the same as for \n2\n1,5 4 Elab Same→ . \n4.2.1 Defining P Same, PElab and P Join \nWe intentionally leave PSame, PElab and PJoin undefined. With \nreference to broadly observed trends in queries and their \ntranscription, we sugge st these alternatives: \n1. The simplest and easiest  solution is simply to build up \ntables indicating the chances that, in general , a note will \nbe elaborated or joined. Thus, the probabilities are \nindependent of the particular event in the target. For \ninstance, our current test implementation uses this \napproach with M=2 and L=2, with ,95.0)( ,= ∀ i PiSame \n,03.0)2,(=i PJoin and 02.0)2,(=i PElab. \n2. Transcribers are more likely to “miss” shorter notes, as \nare singers (consider for instance Figure 8, in which the \nsecond and third note are joined.) As such, we believe it \nwill be possible to take advantage of contextual information (durations of surrounding events) to \ndetermine the likelihood of joins and elaborations at \neach point in the target sequence. \n…States referencing the 1sttarget event.\nelaboration = junction (shorthand): =\nSame1 Same2 Same3 Same4\nJoin12Join22 Join32 Join42\nElab1, 12Elab2, 12Elab3, 12Elab4, 12\nElab1, 22\n \nFigure 6: Hidden model topology \n \n4.2.2 Distribution of initial states πs. \nWe associate the initial state distribution in the hidden model with \na single target event. As such, a separate model for each possible \nsequence starting point must be built. Note however that we can \nactually reference a single larger model, and generate different \ninitial state distributions for each  separate starting-point model, Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \naddressing any concerns about the memory and time costs of \nbuilding the models. Essentially, these various “derived” models \ncorrespond to various alignments of the query with the target \nsequence. \nOur initial state distribution, for an alignment starting with the ith \nevent in the target, is therefore over the states m\ni iElab Same1, ,  and \nl\niJoin , with probabilities determined by PSame, PElab and PJoin \nrespectively. For example, )( ) ( i P SameSame i s= π  where \n) ( )(1xqPxs ==π , the probability of beginning in state x. \nSame4Stat e:Target: \nQuery:Elab3,12Join12Elab3,22\n \nFigure 7: Relati onship between states and events \n4.2.3 Translating from State to Event \nThe hidden-layer states represent note events (in the case of joins) \nor sequences of notes (in the case of elaborations.) As mentioned, \nwe treat only non-modulating a nd non-warping insertions and \ndeletions. As such, when comparing target and query events with \nrespect to a join, we generate a longer target note, with the sum \nduration of the relevant target events, and the pitch of the first. \nSimilarly, for an elaboration, we consider a longer query event. \nFigure 7 shows a portion of the hidde n state graph relating a target \nand query through a sequence of  hidden states, where the dotted \nnotes are examples of each modification. \nWhere ][ ][,i query i query IOI Pitch  is the ith query note, and \n][ ][,i target i target IOI Pitch  the ith target note, we have if the following \nrelationships between target  and query, indicated by →: \n\n\n\n= →= →= →\n−++−+\n=−+\n=\n∑∑\nm\nm i mt ttmt\ntjj query t query i target i targetl\ni t t query t queryli\nijj target i targeti t t query t query i target i target\nElab q IOI Pitch IOI PitchJoin q IOI Pitch IOI PitchSameq IOI Pitch IOI Pitch\n} ,...,2,1{, }1 ,...,1,{1\n][ ][ ][ ][][ ][1\n][ ][][ ][ ][ ][\n if, , , if , , , if , , , \n4.3 Transposition and Tempo \nIn order to account for the various ways in which target and query \ncould be related through transposition and tempo, we must refine \nour state definition. We use the notation sym Tempo Transs s , ,=′  \nto refer to a state with “type”  s, and “cluster” \nsym Tempo Trans , .  \nThe intuition here is that the type  determines the gross relationship \nof the query with the target (see Figure 7, for instance) and the \ncluster determines how particular events are related between the \ntarget and the query. \nAgain, we establish limits on how far off target a query can be. \nSince we use a pitch-class representation, we can represent all \npossible transpositions in the range -5 ≤Trans≤+6. We currently \nallow for tempi as slow as half speed, or as fast as double speed: -\n4≤Tempo sym≤+4, using the same quantization degree as for IOI sym: \nTempo Temposym 2log4= . The initial distribu tions are defined as \nfollows: • ()?,?, )(1 x qPxTrans ==π : the probability of beginning \na query with Trans =  x. \n• ()x qPxTempo ?,?, )(1==π : the probability of beginning \na query with transposition Tempo sym = x. \nNote that we treat Trans and Tempo as conditionally independent \nin this model. Relaxing this assumption entails some additional \ncomputational complexity, and a substantial increase in the \nparameterization of the model, but is not conceptually a difficult \nmodification. \nWe propose two approaches to the shaping of the initial \ndistribution πTrans: \n1. Since the overwhelming majority of people do not have \nabsolute pitch, we recommend a uniform initial \ndistribution: \n121)(=xTransπ. \n2. The distribution could be tailored to individual users’ \nabilities, thus the distributi ons might be quite different \nfor a musician with absolute pitch and a typical user. \nWe propose a single tack for πTempo. We are able to remember \nroughly how fast a song “goes”. As  such, we currently apply a \nnormal distribution2 over initial tempo, with 5.1 ,0==σµ , again \nin the quantized tempo representation. \nOriginal (transposed) Queryjoin + local error\n0 0.2 0.4 0.6 0.8 1 1.2 1.4424446485052\nTime (sec.)Pitch (60 = Middle C)\n0 0.2 0.4 0.6 0.8 1 1.2 1.40.50.60.70.80.91\nPitch tracker confidence score\n0 0.2 0.4 0.6 0.8 1 1.2 1.4-0.1-0.0500.050.1\nTime (sec.)Amplitude\n \nFigure 8: Portion of query on \"Hey Jude\", The Beatles3 \n                                                                 \n2 In our experiments, we frequent ly apply normal distributions \nover some probability function, using the normal density \nfunction: \nπσσµ\n22\n2) (x\ney−−\n=, and then normalize to sum 1 over the \nfunction range. \n3 The Praat analysis shows tw o signals, one representing \nfrequency, the other representing the confidence  score in the \nanalysis. Note segmentation was done manually for all examples \nin this paper and all experimental queries. The pitches indicated \nin the score are the weighted average (by confidence) of the \nfrequencies observed over the course of each note. The notated \nrhythms are approximations, but timing was left unaltered for \nthe purpose of the experiments. Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nThe initial distribution over these refined states is thus: \n( ) ) ( ) ( )( , ,sym Tempo Trans s sym Tempo Trans s Tempo Transs π ππ π ⋅ ⋅= , \nthe probability of beginning a query with state type s and cluster \nsym Tempo Trans , . \n4.4 Modulation and Tempo Changes \nModulation and tempo changes are modelled as transitions \nbetween clusters . We denote the probability of modulating by x \nsemitones on the ith target event as PModu(i, x) (again defined over \nthe range -5 ≤x≤+6). The probability of a tempo change of x \nquantization units is denoted PChange (i, x), allowing for a halving to \ndoubling of tempo with -4 ≤x≤+4. \nHere are possible methods for the calculation of PModu: \n1. In our current implementation, we simply a apply a \nnormal distribution over PModu centred at x=0, assuming \nthat it is most likely a singer will not modulate on every \nnote. The distribution is fixed across all events. \n2. We may wish to take advantage of some additional \nmusical context. For instance, we have noted that \nsingers are more likely to modulate for a large pitch \ninterval. \nWe have observed no clear tren d in tempo changes. Again, we \nsimply define a normal distribution centred at x=0. \nGiven these definitions, we can now  describe the transition table \nA’ for the refined hidden states, incorporating cluster transition \nprobabilities: \nℜ→+−×+−××+−×+−× ]4,4[]6,5[ ]4,4[]6,5[ :' S SA  \n\n\n\n\n−⋅\n\n\n\n\n− ⋅=\n444434444214434421\nchange Temposition in transpo change  theis n\" \"Modulatio, , , ,\n,,'\nx yysym y xsym x\nsym sym Changex y Modu xyTempo Transy Tempo Transx\nTempo Tempoi PTrans Trans i Paa \nwhere i is the index in the target referenced by state y. Consider \nfor instance the transition 2,5, 1,3,32\n1 ++ →++ Same Join. This \nrepresents a whole-tone modulation of +2 and a tempo increase of \n+1, along with a transition to the Same 3 state type. The probability \nof the transition is therefore: )1,3( )2,3( )3( +⋅+⋅Change Modu Same P P P . \n4.5 Local Pitch and Duration Errors \nThese errors are modelled in the probabilistic mapping from \nhidden state to observation (or query note.) We define PPError (i, x) \nas the probability of a pitch error x on the ith target event, and \nPRError (i, x) as the similar concept for IOI error, in quantization \nunits. \nHere are possible methods for determining PPError : \n1. In our current implementation, we simply apply a \nnormal distribution, centred at x=0. \n2. Downey and Nelson [19] notes certain tendencies in \npitch error depending on pitch interval. We can thus take \nadvantage of intervallic context in building pitch error \ndistributions. \nAgain, we have not as yet determined a clear trend in rhythmic \nerror. In our current implementation we apply a normal \ndistribution over PRError  centred at 0, though we believe an \nexamination of the relationship between duration and error \ndeviation might be fruitful. 4.6 Generation of Queries Using Model \nFor experimentation, we generate synthetic queries, while varying \nmodel parameters. The process is straightforward: \n1. Choose an initial state type, tempo scaling and \ntransposition from the distributions πs, πTrans and πTempo \nrespectively. \n2. Generate a sequence of hidden states using the Markov \ntransition matrix model A’, of some predetermined \nlength. For each hidden stat e, generate a query event \nusing the local error distributions, PPError  and PRError , and \ngiven the states’ tempo and transposition values. \nOriginal (transposed)\nQuerymodulation\nlocal pitch error\n \nFigure 9: Portion of query from \"American National \nAnthem”, examples of modulation and local pitch error \n4.7 Observation Probability \nCombining all of these factors, we now describe the probability of \nan observation (query note) given a state \nsym t Tempo Transs q , ,=. \nUsing the procedure described in Section 4.2.3, we generate a pair \nof events target target IOIsym PC ,  and query queryIOIsym PC , . Given \nan “ideal” mapping from state to query event, we would expect: \nused scales log  theof because \" compatible\" are pairs These,\n,44443444421\n↑+ +\n=sym target target\nideal idealTempo IOIsym Trans PC\nIOIsym PC \nGiven these ideal values, we calculate the local  error values: \nideal query ideal query IOIsym IOIsym RError PC PC PError − = −= ,  \nThe probability that qt would generate the observation \nquery queryIOIsym PC ,  is then: ) ,( ) ,( RErrori P PErrori PRError PError ⋅ . \nWe use the shorthand b(qt, ot) to refer to the observation \nprobability of the relevant query events ot with respect to state qt. \nIn the case of elaborations, a deterministic sequence of hidden \nstates can refer to a sequence of query notes. Conceptually, we \nconsider the observation probabilities of these sequences in their \nentirety. In practice, this means calculating the observation \nprobability over a some segment. \n5. PROBABILITY OF A QUERY \nIn music retrieval, we are primarily concerned with calculating the \nlikelihood that a certain target w ould generate a query given the \nmodel. Using these likelihood values , we can then rank a series of \npotential database targets in terms of their relevance to the query. \nConceptually, the idea is to cons ider every possible path through \nthe hidden model. Each path is represented by a sequence of states \nQ={q1, q2,…, qT} , which has a probability equal to the product of \nthe transition probabilities of each successive pair of states. In \naddition, there is a certain probab ility that each path will generate \nthe observation sequence O={o1, o2,…, oT} (or query.) The \nprobability of a query given the model (denoted λ) is: \n()()( )\n()∑∑\n\n\n\n\n⋅⋅⋅ ⋅\n==\n− } ,...,,{ all 3 32 2 1 1 1 all\n21 1 3221\n),( ' ),( '),( '),(| ,| |\nT T T q qq T T qq qqqqQ\noqb a oqb aoqb aoqbqQP QOP OP\nKπλλ λ\n \nFortunately, there is considerable redundancy in the naïve \ncomputation of this value. Usi ng the standard forward-variable Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nalgorithm [14] provides a significant reduction in complexity. We \ndefine a forward variable: \n() λ α ,' |,,, )'(2 1 sqo ooP st t t = = K  \nWe initialize the forward variable using the initial state \nprobabilities: \n() ()( )1 1 1 1 ,' ' ,' | )'( osbs sqoP s ⋅== = πλ α  \nBy induction, we can then ca lculate successive values: \n()∑\n∈+ +=\nSxt sx t t osbax s\n'1 '' 1 ,' ')'( )'(α α \nFinally, the total probability of the model generating the query is \nthe sum of the probabilities of ending  in each state: \n()∑=\n' all)'( |\nsTs OP αλ  \n5.1 Time and Space Complexity \nBased on the topology of the hidden model, and the above \noptimization, we can calculate the complexity of the forward-\nvariable algorithm for this implementation. Since each state type \nhas non-zero transition probabilities for at most L+M- 1 other \ntypes, this defines a branching factor ( b) for the forward \nalgorithm. In addition, any model can have at most bn states, \nwhere n is the length of the target. \nUpdating the transposition and tempo probabilities between two \nstate types  (including all cluster permutations) requires \nk=2)129(⋅ multiplications given the current tempo quantization, \nand the limits on tempo change. Notice that increasing either the \nallowable range for tempo fluctuation, or the resolution of the \nquantization, results in a super-linear increase in time \nrequirements! \nSo, at each induction step (for t=1,2,…), we require at most knb2 \nmultiplications. As such, given a target of length n and a query of \nlength T, the cost is O(knb2T). Clearly, controlling the branching \nfactor (by limiting the degree of join and elaboration) is critical. k \nis a non-trivial scaling factor, so we recommend minimizing the \nnumber of quantization levels as far as possible without overly \nsacrificing retrieval performance. \n6. SIMULATION RESULTS \nThis error model is intended to serve in the context of a music-\ninformation retrieval model. It is  comprehensive in the sense that \nit expresses the full range of transformations observed in the pitch \nand IOI domains for queries. Its us efulness, however, lies in the \nability to discriminate among va rious hypotheses about the source \nof a query. It has been shown that even a small number of errors \ncan lead to (fatally) low discrimination between targets [16]. We \ncontend that our subtler, probabi listic model of query errors can \nlead to greater precision in music searches, even when significant \nerror is introduced. We used sy nthetically generated queries to \ndemonstrate this claim, over a database of 100 classical/romantic \nthemes taken from a musical thematic catalogue [2]. We set the \nmodel parameters as follows: \n• We allow joins and elaborations up to order three, with \nfixed probabilities as described in Section  4.2.1. \n• We apply normal distributions over each of the \nremaining parameters discussed in the paper, examining \nthe effect of error variance for modulation, tempo \nchange, local pitch error,  and local IOI error. \nEach parameter setting corresponds, roughly, to a level of singer \nability. As we increase the λ-values, the distribution flattens, so \nthat our synthetic singers become increasingly likely to introduce \nincreasingly dramatic error to the query. For each of these \n“singers,” we generated 30 queries according to the current model settings (and the procedure desc ribed in Section 4.6), based on \nrandomly chosen database targets. The queries are limited to a \nlength of 12 notes, to prevent “default” matches for longer \nqueries: such queries might be feasible only against models with \nlonger underlying targets. We then  posed these queries to the \nmodel database, evaluating performance based on the likelihood \nrank of the correct model. \nEach synthetic singer is associated with a particular cumulative-\nerror profile, and a particular non-cumulative-error profile. The \nerror distributions associated with these profiles are shown in \nFigure 10 and Figure 11, respectively. The probability of changing \ntempo is with respect to a “tempo change factor” where, for \ninstance, 2.0 is a doubling of te mpo between two note events. \nSimilarly, rhythmic error is shown as a factor of the original IOI \nvalue. \n0.5 1 1.5 210-1010-810-610-410-2100\nTempo Change FactorProbability (log scale)\n-6 -4 -2 0 2 4 610-2510-2010-1510-1010-5100\nModulation AmountProbability (log scale)Profile 2 Profile 2\n \nFigure 10: Cumulative Error Profiles (Profile 1 has no \ncumulative error) \n-6 -4 -2 0 2 4 610-2510-2010-1510-1010-5100\nError (pitch-class)Probability (log scale)\n0.5 1 1.5 210-1010-810-610-410-2100\nError (IOI factor)Probability (log scale)Profile 2\nProfile 1Profile 3Profile 2Profile 1Profile 3\n \nFigure 11: Non-Cumula tive Error Profiles \nNotice that the first cumulative-error profile allows for no error of \nthis type, thus singers with this profile are assumed not to \nmodulate or change tempo. For each singer profile, we indicate the \nnumber of queries for which the co rrect target is ranked first, \nranked at least fifth, and ranked at least tenth. In addition, we \nindicate the Mean Reciprocal Rank (MRR), a standard measure \nused in the Text REtrieval Conference (TREC) benchmarks [21]. \nThe “rank” in question is that of the highest rated relevant result. \nIn these experiments, only one ta rget is considered relevant to \neach query--the target generating the synthetic query--and, as a \nresult, we simply take the reciprocal of that target’s rank. \nCumulative \nError \nProfile Non-\nCumu. \nProfile #  \nranked \nfirst # \nranked \n<= 5th  # \nranked \n<= 10th  MRR \n1 1 30 30 30 1.0 \n1 2 30 30 30 1.0 \n1 3 29 30 30 0.983 \n2 1 29 30 30 0.983 \n2 2 29 30 30 0.975 \n2 3 27 28 30 0.920 \nEven when substantial error is introduced, discrimination remains \nrobust. For instance, our final synt hetic singer is more likely than \nnot to introduce some local and cumulative error on every event of \nthe query, but nonetheless the error model favours the correct \ntarget in 27 of 30 queries. Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \nWe define a database entry as “problematic” if it is either a false \npositive (ranked higher than the correct target) or a false negative \n(ranked lower than an incorrect ta rget). In our experiments, we \nidentified 20 such entries. In order to study the interactions among \nthese entries in greater depth, we ran another experiment using \nonly these problematic cases in our database. Using the most \nerror-prone singer model (cumulative profile 2 and non-\ncumulative profile 3), we generated ten queries for each of these \nentries, and calculated the likelihood that each of the 20 problem \nmodels generated the query. Th e mean likelihoods of these \nquery/target comparisons are shown in a confusion matrix (Figure \n12). Probabilities are shown on a logarithmic scale, since there are orders of magnitude difference be tween values (an ‘X’ indicates \nthat a particular comparison had the highest mean probability.) In \naddition, the probabilities for each query are normalized such that \nthe highest ranked target has a probability of one. \n-35-30-25-20-15-10-50\n24 681012141618202468101214161820\nQuery NumberTarget NumberMean Log Likelihood\n \nFigure 12: Confusion Matrix  for Problematic Entries \nThe confusion matrix shows that, on average, the correct model is \nthe most likely candidate for all queries, shown by the strong \ndiagonal. This suggests  that we need not expect uniformly poor \ndiscrimination for any particular  target. More si gnificantly, the \nmodel tends to favour correct targets over spurious matches by \norders of magnitude (a factor of over 104 between probabilities on \naverage). The MRR value for this set of queries was 0.938, though \nwe must emphasize that this is across a smaller database. \n7. EXPERIMENTAL RESULTS \nTo get anecdotal evidence on the ability of our model to \ncharacterize real singing tendencies, we ran some highly \npreliminary experiments on a small set of queries by five subjects, \none a professional musician (subject A) and all others without any \nspecial musical training (subjects B-E). Each subject was asked to \nsing passages from four well-known songs, shown in Figure 13. \nEach passage was repeated twice from memory, and twice after \nhearing a piano rendition of the passage, for a total of 16 queries \nper subject. We augmented our thematic database with the four \nrelevant targets for the purpose of these experiments. The sung \nqueries were transcribed according to the process outlined in \nFigure 8, using manual note se gmentation and automated pitch \nextraction. \nIn the absence of sufficient training data for the model, we simply \nused a liberal parameterization, using cumulative error profile 2 \nand non-cumulative error profile 3.  We plan to extensively train \nour error model based on broad cla ssifications of singer, to further \nimprove performance. However, even without the benefit of \ntraining, we achieved solid performance on the test database for \nall subjects (MRR = 0.949), returni ng the correct target first for 75 \nof 80 queries. \"Hey Jude\" by John Lennon and Paul McCartney\n\"Lullaby\", trad.\n\n7\n\"Do-Re-Mi\", by Rogers and Hammerstein\n\n12\n\"The Sound of Music\", by Rogers and Hammerstein\n\nFigure 13: Queries \nTable 1: Experimental Results \n Rank of Correct Target  \n Hey Jude Lullaby Do-Re-Mi Sound of… MRR \nA 1 1 1 1 1 1 1 1 \n 1 1 1 1 1 1 1 1 1.0 \nB 1 1 1 1 1 1 1 1 \n 1 1 1 1 1 1 1 1 1.0 \nC 1 1 2 18 1 1 1 1 \n 1 1 1 1 1 1 1 1 0.909 \nD 1 1 1 1 1 1 1 1 \n 1 1 1 1 1 1 4 1 0.953 \nE 1 1 17 18 1 1 1 1 \n 1 1 1 1 1 1 1 1 0.882 \n Overall 0.949 \n  \n1 1 Å Before hearing passage \n1 1 Å After hearing passage \n \nThe “Lullaby” was problematic fo r Subjects C and E. Subject C \nwas initially unable to remember the rhythm, though he had a \nrough recollection of the melodic contour. Subject E was unable to \nremember the melodic contour, though he convincingly \nreproduced the rhythm. After hearing the passage, both singers \nsang queries resulting in correct matches. Subject D’s “Sound of \nMusic” was occasion for the only other error in these experiments. \nHer third rendition was considerably slower than the version in \nour database. \nWith the real queries, we observe much lower probabilities on \nspurious matches, as well. In cases where we did have a correct \nmatch, the second most likely targ et was assigned a probability on \naverage 1/1017 times the probability of the correct target. For this \nreason, we are optimistic about this model’s ability to scale up to \nmuch larger databases. \n8. FUTURE WORK \nEven with the generalizations described in this model, a large \nnumber of parameters remain. We  are currently gathering query \ndata to train the model, as more in-depth evaluations of \nperformance on non-synthetic querie s will be essential. Various \nimportant questions remain to be answered, such as the following:  \n• What is the effect of query representation, for instance \nusing a conventional note representation rather than \npitch-class? \n• How can we best tie parameters for training? For \nefficient training, how many equivalence classes can (or \nshould) be established? \n• HMMs are amenable to “frame-based” representations, \nwhich would allow us to bypass the note-segmentation \nstage of query transcription. Instead of modeling the \nquery as a sequence of di screte note events, it is \nrepresented as a sequence of fixed-width time-frame \nanalyses. Each state in the target model then has an associated distribution over duration – the probability of \nremaining in the state for some number of time-frames. \nWe would like to explore the effectiveness of this Johnny Can’t Sing: A Comprehensive Error Model for Sung Music Queries \n \napproach, particularly with regards to the tradeoffs \nbetween time and retrieval performance. \n• We will shortly be integrating an automatic note-\nsegmenter, currently being de veloped at the University \nof Michigan, which uses a simple neural-network to \nclassify query analysis frames. \nFinally, tests on much larger databases will be necessary. While \nwe believe that meta-data in the query process (genre, era, \ninstrumentation) will allow us to re strict searches to a subset of a \ndatabase or library, it is reasonable to assume that a large number \nof targets will be relevant to many searches.   \n9. ACKNOWLEDGMENTS \nWe gratefully acknowledge the support of the National Science \nFoundation under grant IIS-0085945, and The University of \nMichigan College of Engineering seed grant to the MusEn project. \nThe opinions in this paper are solely those of the authors and do \nnot necessarily reflect the opin ions of the funding agencies. \nWe would also like to thank members of the MusEn research \ngroup for comments and advice. This group includes Greg \nWakefield, Bryan Pardo, Norman Adams, and Mark Bartsch. \n10. REFERENCES \n[1] L. R. Bahl, F. Jelinek, and R. L. Mercer. A maximum \nlikelihood approach to continuous speech recognition. IEEE \nJournal of Pattern Analysis and Machine Intelligence , 1983. \n[2] H. Barlow and S. Morgenstern. A Dictionary of Musical \nThemes . Crown Publishers, 1948. \n[3] M. Bartsch and G. Wakefield. To catch a chorus: Using \nchroma-based representations for audio thumbnailing. In \nWASPAA01 . \n[4] W. Birmingham, B. Pardo, C. Meek, and J. Shifrin. The \nmusart music-retrieval system. D-Lib Magazine , 2002. \n[5] P. Boersma. Accurate short-term analysis of the fundamental \nfrequency and the harmonics-to-noise ratio of a sampled \nsound. In Proceedings of the Institut e of Phonetic Sciences.  \n[6] A. Durey. Melody spotting us ing hidden Markov models. In \nProc. of International Symposium on MIR , 2001. [7] W. Birmingham et al. Musart: Music retrieval via aural \nqueries. In Proc. of International Symposium on MIR , 2001. \n[8] B. J. Feng. The Structured Composite Source Representation. \nPhD thesis, University of Michigan, November 2001. \n[9] Stefan Kurtz. Foundations of sequence analysis. \nciteseer.nj.nec.com/ kurtz01foundations.html , 2001. \n[10] K. Lemstrom. String matching techniques for music retrieval. \nTechnical report, Univer sity of Helsinki, 2000. \n[11] D. Mazzoni. Melody matching directly from audio. In Proc. \nof International Symposium on MIR , 2001. \n[12] B. Pardo and W. Birmingham. Automated partitioning of \ntonal music. In Proc. of FLAIRS 2000 . \n[13] E. Pollastri. An audio front end for query-by-humming \nsystems. In Proc. of International Symposium on MIR , 2001. \n[14] L. R. Rabiner. A tutorial on hidden markov models and \nselected applications in speech recognition. In Proc. of the \nIEEE , 1992. \n[15] J. Shifrin, B. Pardo, C. Meek and W. Birmingham. HMM-\nbased musical query retrieval. In Proc. of Joint Digital \nLibraries Conference , 2002. \n[16] T. Sorsa. Melodic resolution in music retrieval. In Proc. of \nSymposium on MIR , 2001. \n[17] E. Terhardt and W. D. Ward. Recognition of musical key: \nExploratory study. J. Acoust. Soc. Am. , 1982. \n[18] E. M. Voorhees and D. Harman . Overview of the fifth text \nretrieval conference. In The Fifth Text REtrieval Conference , \n1996. \n[19] S. Downie and M. Nelson. Evaluation of a Simple and \nEffective Music Information Retrieval Method. Proc. ACM-\nSigir Conference, Athens, Greece.  \n[20] Mongeau, M.,Sankoff,D., Comparison of Musical Sequences , \nComputers and the Humanities 24,Kluwer Academic \nPublishers 1990, 161-175. \n[21] Text REtrieval Conference Web Site. http://trec.nist.gov ."
    },
    {
        "title": "A Comparison of Manual and Automatic Melody Segmentation.",
        "author": [
            "Massimo Melucci",
            "Nicola Orio"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416914",
        "url": "https://doi.org/10.5281/zenodo.1416914",
        "ee": "https://zenodo.org/records/1416914/files/MelucciO02.pdf",
        "abstract": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007 \u0007\r\f\u000f\u000e\u0010\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\u0005\b\t\u000b\u0017\u0018\u0003\u0019\u0017\u001b\u001a\u001c\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001f\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017 \u0013\u001c\u0017!\u0015\u0011\u0001\u0004\f\"\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0005\u0010\u0013\u000b&'\f\u000f(*) \u0007\u0004+\u0006\u0013\u0014\u0003,\u0015\u0011\u0003\u0006\u0017\u0004\u001e.-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%1&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u00056&2\u0013\u000b\u000e1\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%\u0002-/\f\u000f+\u0006\u0013*0*78\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017 \u0003\u0006-/\f\u00120;\t\u000b\u0015.%\u000f\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)=-?5@\u0005\u0016\u0003\u0019%.\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+BADC\"\fE\t\u000b\u000e\u0011\u001e\u00145@\fF\u0015\u0011\u0001 4\u0015G\u0005\u0016\f\u0012\u001e\u000b) -/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017H\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0006%8\u0007\u001b\u000e\u0011\u00139%\u0012\f30\u001b5\u0004\u000e\u0011\f8\u0003\u0019-/\u0007@+\u0019\f\u0012-/\f\u0012\u00179\u0015\u0016) \u0003\u0019\u0017@\u001e\u0010\t\u000b\u0017^\t\u0014+\u0019\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-_&2\u0013\u000b\u000e?-/\f\u0012+\u0019\u0013*0*7W\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O`\t\u0014\u0005FQ\u0002\f\u000f+\u0006+a\t\u0014\u0005E<97\" \u000e:\t\u000b\u0017 0\u0004\u0013\u0014-b\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000eE\t\u0014\u0017 0\b<97W\tYLM)=\u001e\u0014\u000e:\t\u000b-X)c<@\t\u0014\u0005\u0016\f30\u0010\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f\u000f\u000e3A'dM\fJ) \u0005\u00165\u0004+\u0019\u0015\u0011\u0005e\u0005\u0016\u0001\u0004\u00133Q\u0002\f30/\u0015\u0011\u0001 4\u0015e\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0006%M\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017]< \u0014\u0005\u0016\f\u00120]\u0013\u0014\u0017Y-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006% &I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005'\u0003\u0006\u0005/%\u000f+\u0006\u0013\u0014\u0005\u0016\f\u000f\u000e'\u0015\u0011\u0013H-'\t\u000b\u0017*5 \u0014+e\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017f\u0015\u0011\u0001@\t\u0014\u0017V\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005 \u0015\u0011\u0001@\t\u000b\u0015M0\u0004\u0013[\u0017@\u0013\u0014\u0015D5\u0004\u0005\u0016\f8\u0005\u00165@%:\u0001Z\u0003\u0019\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PA",
        "zenodo_id": 1416914,
        "dblp_key": "conf/ismir/MelucciO02",
        "keywords": [
            "abstract",
            "key aspects",
            "article",
            "captures",
            "Qwen",
            "helpful assistant",
            "captures",
            "Qwen",
            "Qwen",
            "Qwen"
        ],
        "content": "AComparison ofManualandAutomaticMelodySegmentation\nAComparison ofMan ualand Automatic Melod y\nSegmentation\nMassimo Melucci\nUniv ersity ofPadua\nDepar tment ofInformation Engineer ing\nViaGradenigo ,6/A\n35131 Padova,Italy\nmelo@dei.unipd.itNicola Orio\nUniv ersity ofPadua\nDepar tment ofInformation Engineer ing\nViaGradenigo ,6/A\n35131 Padova,Italy\norio@dei.unipd.it\nABSTRACT\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\r\f\u000f\u000e\u0010\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\u0005\b\t\u000b\u0017\u0018\u0003\u0019\u0017\u001b\u001a\u001c\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001f\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017 \u0013\u001c\u0017!\u0015\u0011\u0001\u0004\f\"\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0005\u0010\u0013\u000b&'\f\u000f(*)\u0007\u0004+\u0006\u0013\u0014\u0003,\u0015\u0011\u0003\u0006\u0017\u0004\u001e.-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%1&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u00056&2\u0013\u000b\u000e1\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%\u0002-/\f\u000f+\u0006\u0013*0*78\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\t\u000b\u0003\u0006-/\f\u00120;\t\u000b\u0015.%\u000f\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)=<\n\t\u0014\u0005\u0016\f\u00120>-?5@\u0005\u0016\u0003\u0019%.\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+BADC\"\fE\t\u000b\u000e\u0011\u001e\u00145@\fF\u0015\u0011\u0001\n\t4\u0015G\u0005\u0016\f\u0012\u001e\u000b)-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017H<@\t\u0014\u0005\u0016\f30\b\u0013\u001c\u0017\b-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%?&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u00058\u0003\u0019\u00058-/\u0013\u0014\u000e\u0011\f?\fJ#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f?\u0015\u0011\u0001@\t\u0014\u0017\u000e:\t\u000b\u0017\n0\u0004\u0013\u0014-K\u0013\u000b\u000e/LM)=\u001e\u0014\u000e:\t\u000b-/\u0005\u001d)=<\n\t\u0014\u0005\u0016\f\u00120N\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PO1QR\u0001@\u0003\u0019%J\u0001N\u0003\u0019\u001e\u001c\u0017\u0004\u0013\u0014\u000e\u0011\f;\t\u000b\u001797%\u0012\u0013\u0014\u00179\u0015\u0011\f\u000f(*\u00153A.C\"\fE\u0001\n\t3\u001a\u001f\fE%3\t\u000b\u000e\u0016\u000e\u0011\u0003\u0019\f30\b\u0013\u001c5\u001b\u00158\t\u0014\u0017\b\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015M\f\u0012-/\u0007@+\u0019\u001337\u001b\u0003\u0006\u0017\u0004\u001e'\f\u000f(*)\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017\u0004%\u0012\f30S\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005\u0012AU\u0000\u0002\u0001\u0004\f\"-'\t\u0014\u0017*5@\t\u0014+8\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017!\u000e\u0011\f\u000f\u0005\u00165@+,\u0015\u0010\u0001\n\t\u000b\u0005<\r\f\u0012\f\u000f\u0017V\u0007\u001b\u000e\u0011\u00139%\u0012\f\u0012\u0005\u0016\u0005\u0016\f30N\u0015\u0011\u0013W0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015X\u0015\u0011\u0001\u0004\fY-/\u0013\u0014\u0005\u001d\u0015/\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@+\u0019\fZ<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005X\u0003\u0019\u0017\u0015\u0011\u0001\u0004\f[-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%[\u0005\u00165\u0004\u000e\u0016&2\t\u0014%\u0012\f\u001cO\\5\u0004\u0005\u0016\u0003\u0006\u0017\u0004\u001eZ\t'\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0006\u0005\u001d\u0015\u0011\u0003\u0019%?0\u0004\f\u0012%\u0012\u0003\u0019\u0005\u0016\u0003\u0019\u0013\u001c\u0017\b&25\u0004\u0017@%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\A\u0000\u0002\u0001\u0004\f80\u0004\f\u000f\u0015\u0011\f\u000f%\u000f\u0015\u0011\f30/<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005\u0002\u0001\n\t3\u001a\u001f\fD\u0015\u0011\u0001@\f\u0012\u0017'<\r\f\u0012\f\u000f\u0017Y%\u0012\u0013\u0014-/\u0007\n\t4\u000e\u0011\f30XQR\u0003\u0019\u0015\u0011\u0001]\u0015\u0011\u0001@\f<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005D0\u0004\fJ\u0015\u0011\f\u0012%\u000f\u0015\u0011\f30]<*7Y\t\u000b\u0017>\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0006%8\u0007\u001b\u000e\u0011\u00139%\u0012\f30\u001b5\u0004\u000e\u0011\f8\u0003\u0019-/\u0007@+\u0019\f\u0012-/\f\u0012\u00179\u0015\u0016)\u0003\u0019\u0017@\u001e\u0010\t\u000b\u0017^\t\u0014+\u0019\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-_&2\u0013\u000b\u000e?-/\f\u0012+\u0019\u0013*0*7W\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O`\t\u0014\u0005FQ\u0002\f\u000f+\u0006+a\t\u0014\u0005E<97\"\t\u000e:\t\u000b\u0017\n0\u0004\u0013\u0014-b\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000eE\t\u0014\u0017\n0\b<97W\tYLM)=\u001e\u0014\u000e:\t\u000b-X)c<@\t\u0014\u0005\u0016\f30\u0010\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f\u000f\u000e3A'dM\fJ)\u0005\u00165\u0004+\u0019\u0015\u0011\u0005e\u0005\u0016\u0001\u0004\u00133Q\u0002\f30/\u0015\u0011\u0001\n\t4\u0015e\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0006%M\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017]<\n\t\u0014\u0005\u0016\f\u00120]\u0013\u0014\u0017Y-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005'\u0003\u0006\u0005/%\u000f+\u0006\u0013\u0014\u0005\u0016\f\u000f\u000e'\u0015\u0011\u0013H-'\t\u000b\u0017*5\n\t\u0014+e\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017f\u0015\u0011\u0001@\t\u0014\u0017V\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005\u0015\u0011\u0001@\t\u000b\u0015M0\u0004\u0013[\u0017@\u0013\u0014\u0015D5\u0004\u0005\u0016\f8\u0005\u00165@%:\u0001Z\u0003\u0019\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PA\n1.INTRODUCTION\u0000\u0002\u0001\u0004\fg-'\t\u000b\u0003\u0006\u00178%\u000f\u0013\u001c\u00179\u0015\u0016\u000e\u0011\u0003\u0019<@5\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017E\u0013\u000b&*\u0015\u0011\u0001@\u0003\u0019\u0005h\u0007@\t\u0014\u0007\r\f\u000f\u000eP\u0003\u0019\u00056\t\u0014\u00178\u0003\u0019\u0017*\u001a\u001f\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001f\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\u0013\u001c\u00178\u0015\u0011\u0001@\f\f\u000f#$\f\u000f%\u000f\u0015\u0011\u0005F\u0013\u0014&R\f\u000f(\u001b\u0007@+\u0019\u0013\u0014\u0003\u0019\u0015\u0011\u0003\u0019\u0017\u0004\u001e>-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%X&2\f\u0012\t\u000b\u0015\u00115\u0004\u000e\u0011\f\u000f\u0005E&I\u0013\u0014\u000e[\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%/-/\f\u0012+\u0019\u0013*0*7\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017F\t\u000b\u0003\u0019-/\f308\t\u000b\u0015P%\u0012\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)=<\n\t\u000b\u0005\u0016\f30.-?5\u0004\u0005\u0016\u0003\u0019%i\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u001c\t\u000b+BA`jZ\f\u0012+\u0019\u0013*0\u001b7\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017F\u0001\u0004\f\u0012+\u0019\u0007@\u0005P0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0011\u0003\u0006\u0017\u0004\u001eEk:l\u000bm\u001bn\no\u001fp\u0014q:r=sJt1<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u00178\f\u0012+\u0019\f\u0012-/\f\u000f\u0017*\u0015\u0011\u0005\\\u0013\u0014&-/\f\u0012+\u0019\u0013*0*7Z\u0015\u0011\u0001\n\t4\u0015.\u0001\u0004\u0003\u0006\u001e\u0014\u0001@+\u0019\u0003\u0019\u001e\u001c\u00019\u00158-[5@\u0005\u0016\u0003\u0019%3\t\u0014+h\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012O$\u0013\u0014\u000eG-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%E\u0005\u00165\u0004\u000e\u0016&B\t\u000b%\u0012\f\u0012\u0005\u0012OQR\u0001\u0004\u0003\u0006%:\u0001S%3\t\u0014\u0017S<\r\fW5\u0004\u0005\u0016\f30v\t\u000b\u0005;0\u0004\f\u000f\u0005\u0016%\u000f\u000e\u0011\u0003\u0019\u0007\u0004\u0015\u0011\u0013\u0014\u000e\u0011\u0005;\u0013\u000b&?\u0015\u0011\u0001\u0004\fW-?5\u0004\u0005\u0016\u0003\u0019%W0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015%\u0012\u0013\u0014\u00179\u0015\u0011\f\u0012\u00179\u00153AVw=&8\t3\u001a\u0014\t\u0014\u0003\u0019+\u0006\t\u0014<@+\u0019\f\u001cOa\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005\u0012Oa<@5\u0004\u0003\u0006+,\u0015/5\u0004\u0005\u0016\u0003\u0019\u0017@\u001eW\f\u000f\u0003\u0019\u0015\u0011\u0001\u0004\f\u000f\u000e/-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005>\u0013\u0014\u000eZ\u0017@\u0013\u000b\u00153OG%3\t\u000b\u0017x<\r\fH\u0013\u000b\u000e\u0011\u001e\u001f\t\u0014\u0017\u0004\u0003\u0019y\u0012\f30x\u0003\u0006\u0017x\u0003\u0019\u0017\n0\u001b\f\u000f(\u001b\f\u0012\u0005'\u0015\u0011\u0013f\u0005\u0016\u0007\r\f\u000f\f30z5@\u0007\u0005\u0016\f3\t4\u000e\u0011%J\u0001\u0004\f\u0012\u0005\u0012O1QR\u0003\u0019\u0015\u0011\u0001\u0004\u0013\u001c5\u001b\u0015X\u0003\u0019-/\u0007@+\u0019\f\u0012-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e>\t\u0014\u001797\"\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019\u0017@\u001e\u0010-'\t4\u0015\u0011%J\u0001\u0004\u0003\u0006\u0017\u0004\u001e\u0014)=<\n\t\u000b\u0005\u0016\f30\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@->ANCW\fZ\t4\u000e\u0011\u001e\u001c5@\fY\u0015\u0011\u0001\n\t\u000b\u0015?\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017f<\n\t\u0014\u0005\u0016\f\u00120f\u0013\u0014\u0017V-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005\u0010\u0003\u0006\u0005\u0010-/\u0013\u000b\u000e\u0011\fW\f\u000f#$\f\u0012%J\u0015\u0011\u0003\u0006\u001a\u001c\fW\u0015\u0011\u0001\n\t\u000b\u0017v\u000e:\t\u0014\u0017@0\u0004\u0013\u0014-{\u0013\u0014\u000e\u0010LD)c\u001e\u000b\u000e:\t\u0014-/\u0005\u001d)=<\n\t\u000b\u0005\u0016\f30\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O`QR\u0001@\u0003\u0019%J\u0001^\u0003\u0019\u001e\u0014\u0017@\u0013\u0014\u000e\u0011\fY\t\u0014\u001797W-?5@\u0005\u0016\u0003\u0019%3\t\u000b+a%\u0012\u0013\u0014\u00179\u0015\u0011\f\u000f(*\u00153A;C\"\f'\u0001\n\t3\u001a\u001f\f%3\t4\u000e\u0016\u000e\u0011\u0003\u0019\f30\"\u0013\u001c5\u001b\u0015E\t\u0014\u0017\"\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015F\f\u0012-/\u0007\u0004+\u0019\u001347\u001b\u0003\u0019\u0017@\u001eY\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017@%\u000f\f30H\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012|%\u0012\u0013\u0014-/\u0007\r\u0013\u001c\u0005\u0016\f\u000f\u000e\u0011\u0005\u0012OR-[5@\u0005\u0016\u0003\u0019%\u0012\u0003\u0006\t\u0014\u0017\u0004\u0005\u0012OR\t\u0014\u0017@0}-?5\u0004\u0005\u0016\u0003\u0019%Z\u0005\u001d\u0015\u00115\n0\u001b\f\u0012\u00179\u0015\u0011\u0005\u0012Av~\u001b5\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005/Q\u0002\f\u000f\u000e\u0011\f\t\u000b\u0005\u0016\u001f\f30f\u0015\u0011\u0013H\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015X-'\t\u0014\u0017*5\n\t\u000b+\u0006+,7f\t\b\u0005\u0016\f\u000f\u0015'\u0013\u0014&E\u000bH-[5@\u0005\u0016\u0003\u0019%Z\u0005\u0016%\u0012\u0013\u000b\u000e\u0011\f\u0012\u0005\u0012OR\f3\t\u000b%J\u0001\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015Y\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e\"\t\u0014+\u0019+M\u0015\u0011\u0001\u0004\f;-?5@\u0005\u0016\u0003\u0019%;\u0005\u0016%\u0012\u0013\u000b\u000e\u0011\f\u0012\u0005\u0012A \u0000\u0002\u0001\u0004\f\b\t\u0014\u0017\n\t\u000b+,7\u0004\u0005\u0016\u0003\u0019\u0005]\u0013\u001c\u0017\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u00123T\u001d5@0\u0004\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u00051\u0005\u0016\u0001\u0004\u00133QR\u0005g\u0015\u0011\u0001\n\t4\u0015g\u0015\u0011\u0001\u0004\fe\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005a\u0001\n\t3\u001a\u001f\fR\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f30\u0003\u0019\u0017z\t\b%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u0003\u0006\u0005\u001d\u0015\u0011\f\u000f\u0017*\u0015'Q\u0002\t37f\u0013\u0014\u0017@\f>\u0015\u0011\u0013W\f\u0012\t\u0014%:\u0001\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e3O\u0002\u0015\u0011\u0001\n\t4\u0015'\u0003\u0019\u0005'QR\u0003,\u0015\u0011\u0001}\tH\u000e\u0011\f\u0012+,)\t4\u0015\u0011\u0003\u0006\u001a\u001c\f\u0012+,7^\u0001@\u0003\u0019\u001e\u001c\u0001N0\u001b\f\u0012\u001e\u0014\u000e\u0011\f\u000f\fY\u0013\u000b&M\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0016)=\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f\u000f\u000e?%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017\u0004%\u000f79O`<97W\u0015\u0011\u0001*5@\u0005\u0007\u001b\u000e\u0011\u0013\u000b\u001a*\u0003\u00060\u0004\u0003\u0019\u0017\u0004\u001eE5\u0004\u0005gQR\u0003,\u0015\u0011\u0001]\tF*5\u0004\u0003,\u0015\u0011\fD\u0001\u0004\u0013\u001c-/\u0013\u0014\u001e\u001c\f\u0012\u0017\u0004\f\u0012\u0013\u001c5\u0004\u0005g\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011<\r\f30X%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019\u0017@\u001eE\u0013\u0014&\u0005\u0016%\u0012\u0013\u000b\u000e\u0011\f\u0012\u0005E\t\u0014\u0017@0W\t\u0014\u0005\u0016\u0005\u0016\u00139%\u000f\u0003\t4\u0015\u0011\f30W\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005\u0012OPQR\u0001\u0004\u0003\u0019%J\u0001H%3\t\u0014\u0017H<\r\fX\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015\u0011\f30;&2\u0013\u000b\u000e\u0015\u0011\u0001\u0004\f8\u0005\u00165@<@\u0005\u0016\f\u0012*5\u0004\f\u0012\u00179\u0015e\u0005\u001d\u0015\u0011\f\u0012\u0007\u0004\u0005D\u0013\u000b&i\u0015\u0011\u0001@\f8\fJ(\u0004\u0007\r\fJ\u000e\u0011\u0003\u0006-/\f\u000f\u0017*\u0015\u0011\u0005\u0012Aj>\t\u000b\u0017\u001b5@\t\u0014+6\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017;\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015\u0011\u0005.\u0001\n\t3\u001a\u001f\fF<\r\f\u0012\f\u0012\u0017\u0010\u0007\u001b\u000e\u0011\u00139%\u0012\f\u0012\u0005\u0016\u0005\u0016\f30;\u0015\u0011\u0013]0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0015\u0011\u0001\u0004\f[-/\u0013\u0014\u0005\u001d\u0015G\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@+\u0019\f?<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005M\u0003\u0019\u0017\u0010\u0015\u0011\u0001\u0004\f[-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%[\u0005\u00165\u001b\u000e\u0016&B\t\u0014%\u000f\f\u001cO\\5@\u0005\u0016\u0003\u0019\u0017@\u001e\t;\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0006\u0005\u001d\u0015\u0011\u0003\u0019%Y0\u0004\f\u0012%\u000f\u0003\u0006\u0005\u0016\u0003\u0019\u0013\u0014\u0017N&I5@\u0017\u0004%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017PAH\u0000\u0002\u0001\u0004\fY0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\f\u00120\"<\r\u0013\u001c5@\u0017@0@\t4\u000e\u0011\u0003\u0006\f\u000f\u0005\u0001@\t4\u001a\u001c\fM<\r\f\u0012\f\u000f\u0017/%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\f30?QR\u0003,\u0015\u0011\u0001/\u0015\u0011\u0001@\fD<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005\u00020\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\f30X<97X\t\u0014\u0017]\t\u00145\u001b)\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%H\u0007\u001b\u000e\u0011\u00139%\u0012\f30\u001b5\u0004\u000e\u0011\f\b\u0003\u0006-/\u0007\u0004+\u0019\f\u0012-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e^\t\u000b\u0017x\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-&I\u0013\u0014\u000e;-/\f\u0012+\u0019\u0013*0*7\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O*\t\u0014\u00051Q\u0002\f\u0012+\u0019+@\t\u0014\u0005a<97?\tG\u000e:\t\u0014\u0017@0\u0004\u0013\u001c- \u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\f\u000f\u000ea\t\u0014\u0017@0?<97X\t.LD)\u001e\u000b\u000e:\t\u0014-X)=<\n\t\u000b\u0005\u0016\f30\"\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000e3A;\u0000\u0002\u0001@\f'\f\u000f#$\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017@\f\u000f\u0005\u0016\u0005[\u0013\u000b&D\u0015\u0011\u0001\u0004\f'\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@->O\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feeprovided\nthat copies arenotmade ordistrib uted forprofitorcommercial\nadvantage andthatcopies bear thisnotice andthefullcitation on\nthefirstpage.\n%\n\n2002 IRCAM -Centre Pompidou\n\t\u000b\u0005MQa\f\u0012+\u0019+6\t\u0014\u0005M\u0013\u0014&`\u0015\u0011\u0001@\fF\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eG\t\u00145\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%F\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000e\u0011\u0005M\u0001@\t\u0014\u0005G<\r\f\u000f\f\u0012\u0017;\f\u0012\u001a\u0014\t\u0014+,)5@\t\u000b\u0015\u0011\f30\b%\u0012\u0013\u001c-/\u0007\u00045\u0004\u0015\u0011\u0003\u0019\u0017@\u001e]\u0015\u0011\u0001@\fX\u0007\u001b\u000e\u0011\u0013\u001c<@\t\u0014<@\u0003\u0019+\u0019\u0003,\u0015u7\b\u0013\u0014&G?r2t\u0011t?\t\u0014\u0017\n0\u0010\u0015\u0011\u0001@\fX\u0007\u001b\u000e\u0011\u0013\u001c<@\t\u0014<@\u0003\u0019+,)\u0003,\u0015c7Z\u0013\u000b&\\\u0012p\u001c t\u000fs[p\u001c\u0019p\u000bqJ[O@\u0015\u0011\u0001@\f.&I\u0013\u0014\u000e\u0011-/\f\u000f\u000eD<\r\f\u0012\u0003\u0019\u0017\u0004\u001eX\u0015\u0011\u0001@\f.\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7Z\u0015\u0011\u0001@\t\u000b\u0015R\u0015\u0011\u0001@\f\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-0\u001b\u00139\f\u0012\u00058\u0017@\u0013\u000b\u0015F\u0003\u0019\u0017@\u0005\u0016\f\u000f\u000e\u0016\u0015E\t]<\r\u0013\u001c5@\u0017@0@\t4\u000e\u00167\b0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0011\f30\b<97;\u0015\u0011\u0001\u0004\f?\u0005\u00165@<\u001b)Tu\f\u0012%\u000f\u0015\u0011\u0005\u0012O\u001b\t\u000b\u0017\n0[\u0015\u0011\u0001\u0004\fe+\t4\u0015\u0016\u0015\u0011\f\u000f\u000eg<\r\f\u0012\u0003\u0019\u0017@\u001e.\u0015\u0011\u0001@\fe\u0007\u001b\u000e\u0011\u0013\u001c<@\t\u0014<@\u0003\u0019+\u0019\u0003,\u0015u7[\u0015\u0011\u0001\n\t\u000b\u0015`\u0015\u0011\u0001\u0004\fR\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-\u0003\u0019\u0017\u0004\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\u0005E\tZ<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u00167;\u0015\u0011\u0001\n\t4\u0015F\u0001\n\t\u0014\u00058\u0017\u0004\u0013\u0014\u0015F<\r\f\u0012\f\u000f\u0017\"0\u0004\fJ\u0015\u0011\f\u0012%\u000f\u0015\u0011\f30\b<*7;\u0015\u0011\u0001\u0004\fX\u0005\u00165@<\u001b)Tu\f\u0012%\u000f\u0015\u0011\u0005\u0012A1~*\u0003\u0019\u0017@%\u0012\f1\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005P\u0015u7\u001b\u0007\u0004\u0003\u0006%\u0012\t\u0014+\u0019+,7.\f\u000f(\u001b\u0007@+\u0019\u0013\u0014\u0003\u0019\u0015h-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%`&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u0005\\\u0015\u0011\u0013D\u0005\u0016\f\u000f\u001e\u0014)-/\f\u000f\u0017*\u0015.-/\f\u0012+\u0019\u0013*0\u001b79O$\u0015\u0011\u0001@\f\u0012\u0003,\u000eE\u00133QR\u0017W\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005E\t\u000b\u000e\u0011\f/\f\u000f-/\u0007@+\u0019\u0013379\f30\t\u000b\u0005`\tD<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\f1\u0015\u0011\u0013M%\u000f\u0013\u001c-/\u0007\n\t4\u000e\u0011\f\u0002\t\u000b5\u0004\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019%a\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\t\u0014+\u0019\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-/\u0005\u0012A\u0000\u0002\u0001\u0004\fY<\r\f\u000f\u0005\u001d\u0015X\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-U\u0003\u0019\u0005E\u0015\u0011\u0001\u0004\f'\u0013\u001c\u0017@\f'<\r\f\u0012\u0003\u0019\u0017\u0004\u001e;%\u0012+\u0019\u0013\u001c\u0005\u0016\f\u0012\u0005\u001d\u0015[\u0015\u0011\u0013>\u0015\u0011\u0001@\f'-'\t\u0014\u0017*5\n\t\u000b+\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PAgdM\f\u0012\u0005\u00165\u0004+,\u0015\u0011\u0005M\u0005\u0016\u0001\u0004\u00134Qa\f30Y\u0015\u0011\u0001@\t\u000b\u0015M\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%8\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017<@\t\u0014\u0005\u0016\f30;\u0013\u0014\u0017\b-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%E&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005.\u0003\u0019\u0005G%\u0012+\u0019\u0013\u0014\u0005\u0016\f\u000f\u000e.\u0015\u0011\u0013'-'\t\u0014\u0017*5\n\t\u000b+h\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0015\u0011\u0001@\t\u0014\u0017^\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-/\u0005F\u0015\u0011\u0001@\t\u000b\u0015[0\u0004\u0013Z\u0017@\u0013\u0014\u0015F5\u0004\u0005\u0016\f/\u0005\u00165@%:\u0001\"\u0003\u0019\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PO6+\u0019\u0003\u0019\u001f\fX\u0015\u0011\u0001@\f\u0013\u0014\u0017@\f\u0012\u0005g<\n\t\u000b\u0005\u0016\f30X\u0013\u001c\u0017X\u000e:\t\u000b\u0017\n0\u0004\u0013\u0014-\u0013\u0014\u000ea\u0004(\u001b\f30\u001b)=\u0005\u0016\u0003\u0019y\u0012\fe\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005MB\u0003BA \f\u001cA\u0019O*LM)=\u001e\u0014\u000e:\t\u000b-/\u0005:JA\u0000\u0002\u0001\u0004\u0003\u0019\u0005.-/\f3\t\u0014\u0017\u0004\u0005D\u0015\u0011\u0001@\t\u000b\u0015.\t\u00145\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%E-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%8&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u0005\u001d)=<\n\t\u000b\u0005\u0016\f30\u0010\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u0017\u001b)\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017>%3\t\u000b\u0017Z<\r\f80\u001b\f\u0012\u0005\u0016\u0003\u0019\u001e\u001c\u0017\u0004\f30>\t\u0014\u0017@0]\u0003\u0019-/\u0007\u0004+\u0006\f\u000f-/\f\u0012\u00179\u0015\u0011\f30X\u0015\u0011\u0013?\u0007\u0004\u000e\u0011\u00134\u001a*\u0003\u00060\u0004\f.\fJ/%\u0012\u0003\u0019\f\u0012\u00179\u0015\t\u000b\u0017\n0W\fJ#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\fX\u0005\u0016\f\u0012-'\t\u000b\u00179\u0015\u0011\u0003\u0006%?%\u0012\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)=<\n\t\u000b\u0005\u0016\f30W\t\u000b%\u0012%\u0012\f\u0012\u0005\u0016\u0005F\u0015\u0011\u0013>-?5\u0004\u0005\u0016\u0003\u0019%/0\u0004\u00139%\u00125\u001b)-/\f\u000f\u0017*\u0015R%\u000f\u0013\u001c+\u0019+\u0019\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012A\n2.BACKGROUND\n2.1MusicInformation Retrieval\u0000\u0002\u0001\u0004\f\b0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0011\u0003\u0006\u0013\u0014\u0017z\u0013\u0014&E%\u0012\u0013\u0014\u0017*\u0015\u0011\f\u000f\u0017*\u0015Y0\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019\u0007\u0004\u0015\u0011\u0013\u000b\u000e\u0011\u0005Y&2\u0013\u000b\u000eZ\u0015\u0011\f\u000f(*\u00153OeQR\u0001\u0004\u0003\u0006%:\u0001z\u0003\u0019\u0005Z\t4\u0015\u0015\u0011\u0001\u0004\f8<\n\t\u000b\u0005\u0016\u0003\u0006\u0005D\u0013\u0014&6\t\u00145\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019y3\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017;\u0013\u0014&`0\u001b\u00139%\u00125@-/\f\u000f\u0017*\u0015R\u0003\u0019\u0017@0\u0004\f\u000f(\u001b\u0003\u0019\u0017@\u001e\u001bO\u0004\u0001@\t\u0014\u0005D<\r\f\u0012\f\u0012\u0017+\u0019\f\u0012\u0005\u0016\u0005[0\u0004\u0003,/%\u000f5@+,\u0015F\u0015\u0011\u0001\n\t\u0014\u0017\b&2\u0013\u000b\u000e[\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000eE-/\f30\u001b\u0003\u0006\t\u001bOi+\u0019\u0003\u0019\u001f\f/-[5@\u0005\u0016\u0003\u0019%\u001cOP<\r\f\u0012%3\t\u00145\u0004\u0005\u0016\fX\u0015\u0011\f\u000f(*)\u0015\u00115@\t\u0014+iQa\u0013\u0014\u000e:0\u0004\u0005.\t\u000b\u000e\u0011\f?+\u0019\f\u000f(\u001b\u0003\u0019%3\t\u000b+15\u0004\u0017@\u0003,\u0015\u0011\u0005G\u0005\u0016\f\u000f\u0007\n\t\u000b\u000e:\t4\u0015\u0011\f30\u0010<*7>\u0017@\u0013\u001c\u0017\u001b)c\t\u0014+\u0019\u0007@\u0001@\t\u0014\u0017*5@-/\f\u000f\u000e\u0011\u0003\u0019%%:\u0001\n\t4\u000e:\t\u0014%\u000f\u0015\u0011\fJ\u000e\u0011\u0005\u0012O1\t\u0014\u00058\u000e\u0011\f\u0012\u001e\u001f\t4\u000e:0\u0004\u0005FC\"\f\u0012\u0005\u001d\u0015\u0011\fJ\u000e\u0011\u0017\"+\u0006\t\u0014\u0017@\u001e\u00145\n\t\u000b\u001e\u001c\f\u0012\u0005[\t\u000b\u0015E+\u0019\f3\t\u0014\u0005\u001d\u00153AZ\u0000h\f\u000f(*\u0015\u00115@\t\u0014+\u0015\u0011\u0013\u0014\u001f\f\u0012\u0017F\u000e\u0011\f\u0012%\u0012\u0013\u0014\u001e\u001c\u0017@\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017[\u0003\u0019\u0005`\t\u000b#$\f\u0012%\u000f\u0015\u0011\f\u00120E<978+\u0019\u0003,\u0015\u0016\u0015\u0011+\u0019\fe\t\u0014-[<@\u0003\u0019\u001e\u001c5\u0004\u0003\u0019\u0015c79O3\u0015\u0011\u0001@\u0013\u00145@\u001e\u001c\u0001F\u0005\u0016\u0013\u0014-/\f\u0005\u001d7\u001b-[<\r\u0013\u001c+\u0019\u0005\u0012O9\u0005\u00165@%:\u0001/\t\u000b\u0005g\u0007\r\f\u000f\u000e\u0011\u0003\u0019\u0013*0\u001b\u0005\u0012O9-'\t\u00127?<\r\fe\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0011\u0007\u0004\u000e\u0011\fJ\u0015\u0011\f30?\f\u0012\u0003,\u0015\u0011\u0001@\fJ\u000e\u0002\t\u000b\u0005a\t.\u0005\u0016\f\u0012\u0007\u001b)\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e?\u0013\u000b\u000e?\t\u000b\u0005E\tY\u0015\u0011\u0013\u001c\u001f\f\u000f\u0017W\f\u0012+\u0019\f\u0012-/\f\u0012\u00179\u00153A'wc\u0017W%\u0012\u0013\u0014\u0017*\u0015\u0016\u000e:\t\u000b\u0005\u001d\u00153O6-?5\u0004\u0005\u0016\u0003\u0019%X+\u0006\t\u0014\u0017@\u001e\u00145\n\t\u000b\u001e\u001c\f\u001cO+\u0019\u0003\u0019\u001f\fF\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eD\u0017@\u0013\u0014\u0017\u0004)\u0015\u0011\f\u000f(*\u0015\u00115\n\t\u000b+\\+\u0006\t\u000b\u0017@\u001e\u001c5@\t\u0014\u001e\u0014\f\u0012\u0005\u0012O\r+\u0006\t\u0014%:*\u0005D\u0013\u000b&6\u0015\u0011\u0001@\u0013\u0014\u0005\u0016\f8\f\u000f(\u001b\u0007@+\u0019\u0003\u0019%\u0012\u0003,\u0015G\u0005\u0016\f\u0012\u0007\u001b)\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0005G<\r\f\u0012%3\t\u000b5@\u0005\u0016\f8\u0015\u0011\u0001@\fJ\u000e\u0011\f8\u0003\u0006\u0005D\u0017@\u0013?%\u0012\u0013\u001c5\u0004\u0017*\u0015\u0011\fJ\u000e\u0011\u0007\n\t\u000b\u000e\u0016\u0015D\u0013\u000b&1\u0015\u0011\fJ(\u001b\u0015\u00115@\t\u0014+\\<@+\u0006\t\u0014\u0017\u0004*\u0005D\u0013\u000b\u000e%\u000f\u0013\u001c-/-'\t\u0014\u00058\u0003\u0019\u0017H-?5@\u0005\u0016\u0003\u0019%X\u0017\u0004\u0013\u0014\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PA/M\u00133Q\u0002\f\u0012\u001a\u001f\fJ\u000e3Oi+\u0019\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017@\f\u000f\u000e\u0011\u0005F5\u0004\u0005\u00165\n\t\u0014+\u0019+,7H\u0007\r\f\u000f\u000e\u0016)%\u000f\f\u0012\u0003\u0019\u001a\u001f\f[\u0003\u0019\u0017Z\u0015\u0011\u0001\u0004\fF-?5@\u0005\u0016\u0003\u0019%.@\u00134Qv\u0015\u0011\u0001@\f8\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017@%\u000f\fE\u0013\u000b&g0\u0004\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019\u0017@%\u000f\u0015M+\u0019\f\u000f(\u001b\u0003\u0006%\u0012\t\u0014+i5\u0004\u0017@\u0003,\u0015\u0011\u0005\u0015\u0011\u0001@\t\u000b\u00156&I\u0013\u0014\u000e\u0011-\u0018-[5@\u0005\u0016\u0003\u0019%3\t\u0014+9\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u00115\u0004\u000e\u0011\f\u000f\u0005\u0012O\u001c\u0017\u0004\u0013\u0014\u0015uQR\u0003,\u0015\u0011\u0001\u0004\u0005\u001d\u0015:\t\u0014\u0017\n0\u001b\u0003\u0019\u0017@\u001eM\u0015\u0011\u0001\u0004\f\u0002\t\u000b<@\u0005\u0016\f\u0012\u0017\u0004%\u0012\fa\u0013\u000b&\u0007\u001b\u000e\u0011\f30\u0004\fJ\n\u0017@\f\u00120Z\u0005\u0016\f\u0012\u0007\n\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0012A.\u0000\u0002\u0001*5@\u0005\u0012O\n\u0005\u0016\u0013\u001c-/\f8\u0015\u0011\u0001@\f\u0012\u0013\u000b\u000e\u00167Z\u0013\u0014\u0017\u0010-?5@\u0005\u0016\u0003\u0019%3\t\u000b+h\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f)\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005R%3\t\u0014\u0017Z<\r\fF0\u0004\f\u000f@\u0017@\f30$|gjZ5@\u0005\u0016\u0003\u0019%\u0012\u0013\u001c+\u0019\u0013\u0014\u001e\u001c\u0003\u0019\u0005\u001d\u0015\u0011\u0005.\u0007\u0004\u000e\u0011\u0013\u0014\u0007\r\u0013\u001c\u0005\u0016\f30Z0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015R\u0015\u0011\u0001@\f\u0012\u0013\u000b)\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012O\u0004\u0015\u0011\u0001@\fM-/\u0013\u001c\u0005\u001d\u0015\u0002\u000e\u0011\f\u0012+\u0019\f\u0012\u001a\u0014\t\u0014\u00179\u0015R<\r\f\u0012\u0003\u0019\u0017\u0004\u001e[\u0015\u0011\u0001@\u0013\u0014\u0005\u0016\fG\u000e\u0011\f\u0012\u0007\r\u0013\u000b\u000e\u0016\u0015\u0011\f30Y\u0003\u0019\u0017W,\u0012\u000bh\t\u0014\u0017\n0\b \u0014\u000bOQR\u0001\u0004\u0003\u0019%J\u0001'\u0003\u0019-/\u0007@+,7?\u0015\u0011\u0001\u0004\fM\u0007\r\u0013\u0014\u0005\u0016\u0005\u0016\u0003\u0019<@\u0003\u0019+\u0019\u0003\u0019\u0015c7/\u0013\u0014&P\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0003\u0006\u0017\u0004\u001eF-?5@\u0005\u0016\u0003\u0019%e\u0015\u0011\u00138&2\u0013\u000b\u000e\u0011-+\u0019\f\u000f(*)\u0003\u0019%3\t\u000b+\r5@\u0017\u0004\u0003\u0019\u0015\u0011\u00051\u0015\u0011\u0001\n\t4\u0015\u0002%3\t\u000b\u0017]<\r\fR5\u0004\u0005\u0016\f30'\t\u0014\u0005\u00020\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019\u0007\u0004\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0002\u0013\u0014&P-?5\u0004\u0005\u0016\u0003\u0019%M0\u001b\u00139%\u00125@-/\f\u000f\u0017*\u0015%\u000f\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u00153A\n2.2DigitalLibraries\u0000\u0002\u0001\u0004\f;\t\u0014%\u000f%\u0012\f\u0012\u0005\u0016\u0005X\u0015\u0011\u0013H0\u0004\u0003\u0019\u001e\u001c\u0003,\u0015:\t\u0014+D+\u0019\u0003\u0019<\u0004\u000e:\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005/\u0003\u0019\u0005XQR\u0003\u00060\u0004\f\u000f+\u00197N\u0005\u0016\u0007\u001b\u000e\u0011\f3\t\u001c0N\u0015\u0011\u0013H5\u0004\u0005\u0016\f\u000f\u000e\u0011\u0005X\u0013\u000b&\t\u000b\u001797/\u0015u7\u001b\u0007\r\f\u001cO\u001fQR\u0001\u0004\u0013?-'\t\u00127X\u0017\u0004\u0013\u0014\u0015e\u0001@\t4\u001a\u001c\fG\t[0\u0004\f\u0012\f\u0012\u0007'*\u0017@\u00133QR+\u0019\f30\u001b\u001e\u001c\f.\u0013\u000b&h-[5@\u0005\u0016\u0003\u0019%M+\u0006\t\u000b\u0017\u0004)\u001e\u00145\n\t\u000b\u001e\u001c\f\u001cA1D-/\u0013\u001c\u0017@\u001eD\u0015\u0011\u0001@\fe0\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015i&2\f\u0012\t\u000b\u0015\u00115\u0004\u000e\u0011\f\u000f\u00051\u0015\u0011\u0001@\t\u000b\u0015`%:\u0001\n\t\u000b\u000e:\t\u000b%\u000f\u0015\u0011\f\u000f\u000e\u0011\u0003\u0019y\u0012\fR-?5\u0004\u0005\u0016\u0003\u0019%\u001cO-/\f\u000f+\u0006\u0013*0*7.\u0005\u0016\f\u0012\f\u000f-/\u0005\\\u0015\u0011\u0013R<\r\f`\u0015\u0011\u0001@\f`-/\u0013\u001c\u0005\u001d\u0015P\u0005\u00165@\u0003,\u0015:\t\u0014<\u0004+\u0019\f1&I\u0013\u0014\u000ei\u0003\u0019\u0017\u0004\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017@%\u0012\f\u00120F5\u0004\u0005\u0016\f\u000f\u000e\u0011\u0005\u0012Awc\u0017?&B\t\u000b%\u000f\u00153O\u001b\t\u0014+\u0019-/\u0013\u0014\u0005\u001d\u0015a\f\u0012\u001a\u001c\f\u000f\u000e\u00167\u001b<\r\u0013*0\u001b7E%3\t\u0014\u0017X\u000e\u0011\f\u000f%\u0012\u0013\u001c\u001e\u0014\u0017@\u0003\u0019y\u0012\fD\u0005\u0016\u0003\u0006-/\u0007\u0004+\u0019\fD-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019\f\u0012\u0005a\t\u000b\u0017\n0\u0007\r\fJ\u000e\u0016&2\u0013\u0014\u000e\u0011-\u0015\u0011\u0001\u0004\f\u0012-\t4\u00158+\u0019\f3\t\u0014\u0005\u001d\u0015E<97;\u0005\u0016\u0003\u0019\u0017@\u001e\u0014\u0003\u0019\u0017@\u001e>\u0013\u000b\u000eF\u0001\u001b5\u0004-/-/\u0003\u0019\u0017@\u001e\u0004AFM\u0015\u0011\u0001@\f\u000f\u000e.\u0015\u0011\u0001@\t\u0014\u0017<\r\f\u000f\u0003\u0006\u0017\u0004\u001e]\u0015\u0011\u0001\u0004\fF\f3\t\u0014\u0005\u0016\u0003\u0019\f\u0012\u0005\u001d\u00158\u0007\r\fJ\u000e\u0011%\u0012\f\u0012\u0007\u0004\u0015\u0011\u0003\u0019\u001a\u001f\fF&2\f\u0012\t\u000b\u0015\u00115\u0004\u000e\u0011\f\u0014O$-/\f\u0012+\u0019\u0013*0\u001b7>\u0007\u0004+\u0006\t37\u001b\u0005G\t'%\u000f\f\u0012\u00179\u0015\u0016\u000e:\t\u0014+\u000e\u0011\u0013\u0014+\u0019\f/\u0003\u0006\u0017W\t]QR\u0003\u00060\u0004\f[\u000e:\t\u0014\u0017@\u001e\u0014\f/\u0013\u0014&gQ\u0002\u0013\u0014\u000e\u0011*\u0005E\t\u0014\u0017@0\b\u0003,\u0015F\u0003\u0019\u0005.\u0015\u0011\u0001@\f?-/\u0013\u001c\u0005\u001d\u00158\u0003\u0019-/\u0007\r\u0013\u0014\u000e\u0016\u0015:\t\u000b\u00179\u0015&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f'\u0003\u0006\u0017^\u0005\u0016\u0013\u0014-/\f/\u0005\u0016\u0007\r\f\u0012%\u0012\u0003,\n%'\u001e\u0014\f\u0012\u0017\u0004\u000e\u0011\f\u0012\u0005\u0012Oi\f\u001cA \u001e\u0004A>&I\u0013\u001c+\u0019W-[5@\u0005\u0016\u0003\u0019%\u001cAZ\u0000\u0002\u0001\u001b5\u0004\u0005\u0012Oi\u0015\u0011\u0001@\f\u0003\u0019\u0017*\u001a\u001f\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001c\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&.\u0015\u0011\u0001@\fY\u000e\u0011\u0013\u001c+\u0019\f;\u0013\u0014&8-/\f\u000f+\u0006\u0013*0*7N\u0003\u0019\u0005Y\t\b\u0017@\f\u0012%\u000f\f\u0012\u0005\u0016\u0005\u0011\t\u000b\u000e\u001679O\u00027\u001f\f\u000f\u0015/\u0017@\u0013\u000b\u0015\u0005\u00165\u001b/%\u0012\u0003\u0019\f\u0012\u00179\u00153Oe\u0005\u001d\u0015\u0011\f\u0012\u0007N\u0015\u0011\u0013\"0\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f>\u0015\u0011\u0001@\fY%\u0012\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015/\u0013\u000b&8-?5\u0004\u0005\u0016\u0003\u0019%YQ\u0002\u0013\u000b\u000e\u0011*\u0005\u0012Awc&-/\f\u000f+\u0006\u0013*0*7>\u0003\u0019\u0005.\f\u0012-/\u0007\u0004+\u0019\u001347\u001f\f30Y\u0015\u0011\u0013]0\u0004\f\u0012\u0005\u0016%J\u000e\u0011\u0003\u0006<\r\fE-?5\u0004\u0005\u0016\u0003\u0019%E%\u000f\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u00153O$+\u0019\f\u000f(\u001b\u0003\u0006%\u0012\t\u0014+i5\u0004\u0017@\u0003,\u0015\u0011\u0005\t4\u000e\u0011\fZ's\u000f\u0019l3o\u000brXt\u0012s\u0014/sJn@Bt\u0011Oh\u0015\u0011\u0001@\t\u000b\u00158\u0003\u0019\u0005\u0012O\\\u0005\u0016\u0001@\u0013\u000b\u000e\u0016\u0015F\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005G\u0013\u0014&1\u0015\u0011\u0001@\f?-/\f\u000f+\u0006\u0013*0*7AComparison ofManualandAutomaticMelodySegmentationQR\u0001\u0004\u0003\u0006%:\u0001'\t\u000b\u000e\u0011\fD\u0007\r\f\u000f\u000e\u0011%\u0012\f\u000f\u0003\u0006\u001a\u001c\f30/\t\u0014\u0005g\t8\u0005\u0016\u0003\u0006\u0017\u0004\u001e\u001c+\u0019\fD-?5@\u0005\u0016\u0003\u0019%3\t\u000b+@\u001e\u0014\f\u0012\u0005\u001d\u0015\u00115\u0004\u000e\u0011\f\u001cA1\u0000\u0002\u0001\u0004\fe\u000e\u0011\u0013\u001c+\u0019\fM<\r\fJ)\u0003\u0019\u0017@\u001e8\u0007\u0004+\u0006\t37\u001f\f30?<97E-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%e\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005`-'\t\u00127E<\r\fe\u0005\u0016\u0003\u0019-/\u0003\u0006+\u0006\t4\u000eg\u0015\u0011\u0013.\u0015\u0011\u0001@\t\u000b\u00151<\r\f\u0012\u0003\u0019\u0017@\u001e\u0007\u0004+\t\u001279\f\u00120\"<*7W\u001f\f\u000f7*Q\u0002\u0013\u000b\u000e:0\u0004\u0005E\u0003\u0019\u0017N\u0015\u0011\fJ(\u001b\u0015E\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+B|]M\u0005F\u0015\u0011\f\u000f(*\u0015\u00115\n\t\u000b+a\u001f\fJ7\u001bQa\u0013\u0014\u000e:0\u001b\u0005\t4\u000e\u0011\f[-/\f3\t\u000b\u0017@\u0003\u0019\u0017@\u001e\u000b&25\u0004+i0\u0004\f\u000f\u0005\u0016%\u000f\u000e\u0011\u0003\u0019\u0007\u0004\u0015\u0011\u0013\u0014\u000e\u0011\u0005M\u0013\u0014&`\u0015\u0011\u0001@\fF\u0005\u0016\f\u0012-'\t\u000b\u00179\u0015\u0011\u0003\u0006%8%\u000f\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015G\u0013\u000b&g0\u0004\u00139%\u00125\u001b)-/\f\u0012\u00179\u0015\u0011\u0005\u0012O\u001b-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%8\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005R-'\t37]0\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f8\u0015\u0011\u0001@\f/\u0011l\u0014n\u0004csJn@R\u0013\u000b&`-?5@\u0005\u0016\u0003\u0019%0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015\u0011\u0005\u0012Ag\u0000\u0002\u0001@\fF\f\u000f(\u001b\u0007\u0004+\u0006\u0013\u0014\u0003,\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017;\u0013\u000b&`-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%8\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005e&I\u0013\u0014\u000eM-?5@\u0005\u0016\u0003\u0019%wcdv\u0003\u0019-/\u0007@+\u0019\u0003\u0019\f\u0012\u0005R\u0005\u0016\u0013\u001c-/\fG\u0005\u0016\u0013\u0014\u000e\u0016\u0015M\u0013\u0014&6-/\f\u0012+\u0019\u0013*0*7Y\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017Z\u0007\u0004\u000e\u0011\u00139%\u000f\f\u0012\u0005\u0016\u0005\u0016\f\u0012\u0005\u0012A\n2.3MelodySegmentationjZ\f\u0012+\u0019\u0013*0*7}\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017}\u0001\n\t\u0014\u0005]\tW\u0007\r\fJ\u000e\u0011%\u0012\f\u0012\u0007\u0004\u0015\u00115@\t\u0014+D\t\u0014\u0017@0}\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f;\u0017\n\t4)\u0015\u00115\u001b\u000e\u0011\f\u001cA\u0001\u0000.\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015D\u0007\r\f\u000f\u000e\u0011\u0005\u0016\u0013\u001c\u0017\u0004\u0005G-'\t\u00127]\u0007\u0004\u000e\u0011\u0013*0\u001b5@%\u0012\fE0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015D\u000e\u0011\f\u0012\u0005\u00165\u0004+\u0019\u0015\u0011\u0005M\t\u000b\u0015M\u0015\u0011\u0001@\f\u0005\u0011\t\u000b-/\f[\u0015\u0011\u0003\u0019-/\f\u001cOP\t\u0014\u0017@0H\t'\u0007\r\f\u000f\u000e\u0011\u0005\u0016\u0013\u001c\u0017H-'\t\u00127\u00100\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015\u0011+,7;\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015.\u0015\u0011\u0001@\fE\u0005\u0011\t\u0014-/\f\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015[\t\u000b\u0015[0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u00158\u0015\u0011\u0003\u0019-/\f\u0012\u0005\u0012AZ~*5@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a*\u0003,\u0015u7\b%3\t\u000b\u0017^\t\u0014+\u0019\u0005\u0016\u0013;<\r\f/%3\t\u000b5@\u0005\u0016\f30<97S\u0015\u0011\u0001@\fN\t\u00145\u001b\u0015\u0011\u0001@\u0013\u000b\u000e3O[QR\u0001\u0004\u0013z-'\t37S\u0001@\t4\u001a\u001c\f^\u0003\u0006\u0017\u0004\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\f30 \t\u000b-?<@\u0003\u0019\u001e\u00145@\u0013\u001c5\u0004\u0005\u0010-?5@\u0005\u0016\u0003\u0019%\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u00115\u0004\u000e\u0011\f\u0012\u0005D\u0015\u0011\u0001\n\t4\u00158\t\u000b\u000e\u0011\fE\u0015\u0011\u0001@\f\u0012\u0017;\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0011\u0007\u001b\u000e\u0011\f\u000f\u0015\u0011\f30;0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015\u0011+,7;<97Z+\u0006\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017\u0004\f\u000f\u000e\u0011\u0005\u0012A\u0000\u0002\u0001\u0004\f]\u0007\r\fJ\u000e\u0011%\u0012\f\u0012\u0007\u0004\u0015\u00115@\t\u0014+\u0002\u0017@\t\u000b\u0015\u00115\u001b\u000e\u0011\f/\u0013\u0014&D-/\f\u0012+\u0019\u0013*0\u001b7H\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017\"Q\u0002\u0013\u00145@+\u00060\"\u0005\u00165@\u001e\u000b)\u001e\u0014\f\u0012\u0005\u001d\u0015[\u0015\u0011\u0001@\t\u000b\u0015F\u0015\u0011\u0001@\f\u0003\u0002c<\r\f\u0012\u0005\u001d\u0015\u0005\u0004;\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\"\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015EQa\u0013\u001c5\u0004+0\"<\r\fX\u000e\u0011\f\u0012\t\u0014%:\u0001@\f30\u0015\u0011\u0001\u001b\u000e\u0011\u0013\u001c5@\u001e\u0014\u0001^-'\t\u0014\u0017*5\n\t\u000b+1Qa\u0013\u0014\u000e\u0011\rA\b\u0000\u0002\u0001@\f'\u0005\u00165\u0004\u0007\r\f\u000f\u000e\u0011\u0003\u0019\u0013\u0014\u000e\u0011\u0003,\u0015u7W\u0013\u0014&D-'\t\u0014\u0017*5\n\t\u000b+g\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u0017\u001b)\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017H%\u0012\t\u0014\u0017\b<\r\f?0\u001b5@\fF\u0015\u0011\u0013]\u0015\u0011\u0001\u0004\f?0\u001b\u0003\u0019#$\fJ\u000e\u0011\f\u0012\u00179\u001580\u0004\u0003\u0019-/\f\u0012\u0017\u0004\u0005\u0016\u0003\u0019\u0013\u001c\u0017@\u0005M\u0013\u0014&g-?5\u0004\u0005\u0016\u0003\u0006%\u0014O$+\u0006\u0003\u0019\u001f\f\u000e\u0011\u000197*\u0015\u0011\u0001@- \t\u0014\u0017@0X\u0001\n\t4\u000e\u0011-/\u0013\u001c\u0017979O\u001c\u0015\u0011\u0001@\t\u000b\u0015g\u0003\u0006\u00179\u0015\u0011\f\u000f\u000e:\t\u000b%\u000f\u0015aQR\u0003,\u0015\u0011\u0001'-/\f\u0012+\u0019\u0013*0\u001b7[\u0015\u0011\u00138&2\u0013\u0014\u000e\u0011- \u0015\u0011\u0001@\f%\u0012\u0013\u0014\u00179\u0015\u0011\f\u0012\u00179\u0015/\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017z\u0013\u0014&8\tW-?5\u0004\u0005\u0016\u0003\u0006%>0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015'\t\u0014\u0017@0V\u0015\u0011\u0001\n\t4\u0015'%3\t\u0014\u0017<\r\f\u00100\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\f30f<97N\u0001\u001b5\u0004-'\t\u0014\u0017\u0004\u0005\u0012Awu\u0017V\u0013\u0014\u000e:0\u001b\f\u000f\u000e/\u0015\u0011\u0013W\u000e\u0011\f3\t\u000b%J\u0001V+\u0006\f\u000f\u001a\u001f\f\u0012+\u0019\u0005'\u0013\u0014&8\f\u000f#$\f\u000f%\u000f)\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017\u0004\f\u0012\u0005\u0016\u0005'<\r\f\u0012\u0003\u0019\u0017@\u001e\"%\u000f\u0013\u001c-/\u0007\n\t4\u000e:\t\u0014<\u0004+\u0006\f>QR\u0003,\u0015\u0011\u0001-'\t\u000b\u0017*5\n\t\u0014+R\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\Oe\u0015\u0011\u0001@\f\t\u000b5\u0004\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019y3\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017N\u0013\u000b&G\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\"Qa\u0013\u001c5\u0004+0^\u0003\u0019-/\u0007@+,7\b\u0015\u0011\u0001@\f]0\u0004\fJ\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&h\u0015\u0011\u0001@\fM0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015\u00020\u0004\u0003\u0019-/\f\u0012\u0017\u0004\u0005\u0016\u0003\u0006\u0013\u0014\u0017@\u0005\u0002\u0013\u000b&i-[5@\u0005\u0016\u0003\u0019%\u001cO\u001fQR\u0001\u0004\u0003\u0006%:\u0001]\u0003\u0006\u0005\u0002\tE\u001a\u001f\fJ\u000e\u00167'0\u0004\u0003,/%\u00125\u0004+,\u0015\u0015:\t\u000b\u0005\u0016$A \u0000\u0002\u0001@\u0013\u00145@\u001e\u001c\u0001x+\u0019\u0003\u0019-/\u0003,\u0015\u0011\f30\u0015\u0011\u0013f-/\f\u000f+\u0006\u0013*0*7z\u0013\u001c\u0017\u0004+,79OM-'\t\u000b\u0017*5\n\t\u0014+G\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4)\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b<\r\f\u0012%\u0012\u0013\u0014-/\f\u0012\u0005.\t\u000b\u0017\u0010\u0003\u0006\u0017\u001b&2\f3\t\u000b\u0005\u0016\u0003\u0019<@+\u0019\f[\u0015:\t\u000b\u0005\u0016>&2\u0013\u0014\u000eG%\u00125\u0004\u000e\u0016\u000e\u0011\f\u000f\u0017*\u0015G+\u0006\t\u000b\u000e\u0011\u001e\u001c\f[%\u0012\u0013\u001c+\u0019+\u0019\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005\u0013\u000b&G-[5@\u0005\u0016\u0003\u0019%Z0\u001b\u00139%\u00125@-/\f\u000f\u0017*\u0015\u0011\u0005\u0012O6QR\u0001\u0004\u0003\u0019%J\u0001V\t\u000b\u000e\u0011\f]%\u00125\u0004\u000e\u0016\u000e\u0011\f\u000f\u0017*\u0015\u0011+,7^-'\t\u000b\u0017\n\t\u000b\u001e\u001c\f30^<97^0\u001b\u0003\u0006\u001e\u000b)\u0003,\u0015:\t\u0014+D+\u0019\u0003\u0019<\u0004\u000e:\t\u000b\u000e\u00167f\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-/\u0005\u0012A}\u0000\u0002\u0001@\f>+\u0006\t4\u0015\u0016\u0015\u0011\f\u000f\u000e'\f\u0012\u001a*\u0003\u00060\u0004\f\u0012\u0017\u0004%\u0012\f>\u0005\u00165@\u001e\u0014\u001e\u001c\f\u0012\u0005\u001d\u0015\u0011\u0005/\u0015\u0011\u0001\u0004\fZ5@\u0005\u0016\f\u0013\u000b&Xp\u0014m*=l\u0014Xp\u0014BrZ/s\u000f\u0019l3o\u0007\u0006Ht\u000fs\u0014'sJn\u0004=p\u0014Brl\u0014nz\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005\u0012O`QR\u0001@\u0003\u0019%J\u0001f\t\u000b+\u0006+\u0019\u00133Q&I\u0013\u0014\u000ee\u0015\u0011\u0001@\fG\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%M\f\u000f(*\u0015\u0016\u000e:\t\u0014%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017Y\u0013\u0014&h-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%G\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005a&I\u000e\u0011\u0013\u0014- +\u0006\t\u000b\u000e\u0011\u001e\u0014\f-[5@\u0005\u0016\u0003\u0019%?0\u001b\u00139%\u00125@-/\f\u000f\u0017*\u0015D%\u0012\u0013\u0014+\u0019+\u0006\f\u000f%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005\u0012Oh\f\u0012\u001a\u001f\f\u0012\u00179\u0015\u00115@\t\u0014+\u0019+,7>\u0005\u0016\u0003\u0019-/\u0007@+\u0019\u0003,&I7\u001b\u0003\u0019\u0017@\u001e'\u0015\u0011\u0001@\fE\u0007\u0004\u000e\u0011\u0013\u000b)%\u0012\f\u000f\u0005\u0016\u0005Z<97f\u0005\u0011\t\u0014%\u000f\u000e\u0011\u0003,\n%\u000f\u0003\u0006\u0017\u0004\u001e^\u0015\u0011\u0001@\f\b0\u0004\f\u000f\u0015\u0011\f\u000f%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&[\u0005\u0016\u0013\u0014-/\f;%\u0012\u0013\u001c-/\u0007\u0004+\u0019\f\u000f(}\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e:\t\u000b%\u000f)\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005F<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u0017\b-/\f\u0012+\u0019\u0013*0\u001b7\b\t\u0014\u0017@0H\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e8-[5@\u0005\u0016\u0003\u0019%X0\u001b\u0003\u0019-/\f\u0012\u0017@\u0005\u0016\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012A'G\u0017\u0004\f?\u0013\u0014&\u0015\u0011\u0001\u0004\f\u0012\u0005\u0016\f'\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@-/\u0005F\u0003\u0019\u0005E<@\t\u0014\u0005\u0016\f30\b\u0013\u001c\u0017\b\u0015\u0011\u0001@\f\b\u0002\n\tP\u00139%3\t\u000b+\f\u000ba\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\r\u0000.\f\u000f\u0015\u0011\f\u0012%J)\u0015\u0011\u0003\u0019\u0013\u001c\u0017SjZ\u0013*0\u001b\f\u0012+\u000e\u0004\u0004OD\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\f\u00120}\u0003\u0019\u0017\u0018 \u000bORQR\u0001@\u0003\u0019%J\u0001z\u0003\u0019\u0005Y\t\u000b\u0015'\u0015\u0011\u0001@\f;<@\t\u0014\u0005\u0016\u0003\u0019\u0005]\u0013\u0014&F\u0015\u0011\u0001@\f\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@- \u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\f\u00120Z\u0003\u0019\u0017Z\u0015\u0011\u0001@\u0003\u0019\u0005e\u0007\n\t\u000b\u0007\r\f\u000f\u000e3A\n2.4TopicDetection\u0000\u0002\u0001\u0004\fW\u0017@\f\u000f\f30z\t\u0014\u0017@0x5\u0004\u0005\u0016\f\u000f&25\u0004+\u0019\u0017@\f\u0012\u0005\u0016\u0005>\u0013\u000b&E\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001ef0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015\u0011\u0005'\u0015\u0011\u0013N\f\u000f(*)\u0015\u0016\u000e:\t\u000b%\u000f\u0015;-/\f3\t\u000b\u0017@\u0003\u0019\u0017@\u001e\u000b&25@+G\u0007\n\t\u000b\u000e\u0016\u0015\u0011\u0005]\u0015\u0011\u0013V<\r\f\b\u0007\u0004\u000e\u0011\u00139%\u000f\f\u0012\u0005\u0016\u0005\u0016\f30x\u0003\u0006\u0017@0\u0004\u0003\u0019\u001a*\u0003\u00060\u00045\n\t\u000b+\u0019+\u00197z0@\t\u000b\u0015\u0011\f\u000f\u0005<@\t\u0014%:'\u0015\u0011\u0013E\u0015\u0011\u0001@\fG\u000e\u0011\f\u000f\u0005\u0016\f3\t\u000b\u000e\u0011%:\u0001>\u0003\u0019\u0017>0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015\u0002\u000e\u0011\fJ\u0015\u0016\u000e\u0011\u0003\u0006\f\u000f\u001a\u001c\t\u000b+O\u0004\u0005\u001d\u0015\u0016\u000e\u00115@%J\u0015\u00115\u0004\u000e\u0011\u0003\u0019\u0017@\u001e\u001bO@\t\u000b\u0017\n0\u0015\u0011\u0013\u0014\u0007@\u0003\u0019%>0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\AWwc\u0017\"\u0015\u0011\u0001@\f]%3\t\u0014\u0005\u0016\fY\u0013\u0014&M\f\u000f(\u001b\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0013\u0014\u000e\u00167W\u0015\u0011\f\u000f(*\u0015\u0011\u0005\u0012O1\u0015\u0011\u0001\u0004\f]\t\u00145\u0004\u0015\u0011\u0013\u000b)-'\t4\u0015\u0011\u0003\u0006%?\u0007\u0004\u000e\u0011\u00139%\u0012\f\u0012\u0005\u0016\u0005E\u0013\u0014&e0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0011\u0003\u0006\u0017\u0004\u001eZ\u0015\u0011\u0001@\f[\u0015\u0011\u0013\u001c\u0007@\u0003\u0019%\u0012\u00058\u0015\u0011\u0001@\t\u000b\u0015E\t\u000b\u000e\u0011\f'\t\u00140@0*\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30H\u0003\u0019\u0017\t]+\t4\u000e\u0011\u001e\u001c\f?\u0015\u0011\fJ(\u001b\u0015\u00115@\t\u0014+`0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015M\u0003\u0019\u0005F\t'&I\f3\t\u0014\u0005\u0016\u0003\u0019<@+\u0019\fX\u0015:\t\u000b\u0005\u0016^ 9Og\u0014\u001cOi\u001c3=A?\u0000\u0002\u0001@\f\f\u000f#$\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017@\f\u000f\u0005\u0016\u0005[\u0013\u000b&\u0002\u0015\u0011\u0001@\f\u0012\u0005\u0016\f/\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015\u0011\u0005[\t4\u000e\u0011\f'\u0005\u0016\u0013\u001c-/\f\u000f\u0015\u0011\u0003\u0019-/\f\u0012\u0005F0\u001b5@\fX\u0015\u0011\u0013Z\u0015\u0011\u0001\u0004\f?&B\t\u000b%\u000f\u0015\u0015\u0011\u0001@\t\u000b\u0015E+\u0019\u0013\u001c\u001e\u001c\u0003\u0019%3\t\u000b+\u0002\u0005\u001d\u0015\u0016\u000e\u00115@%J\u0015\u00115\u0004\u000e\u0011\fX\u000e\u0011\f\u000f@\f\u0012%\u000f\u0015\u0011\u0005F\u0005\u0016\f\u0012-'\t\u0014\u00179\u0015\u0011\u0003\u0019%/\u0005\u001d\u0015\u0016\u000e\u00115@%J\u0015\u00115\u0004\u000e\u0011\f\u001cOPQR\u0001@\u0003\u0019%J\u0001\"\t\u0014+,)+\u0019\u00133QR\u0005Z\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-/\u0005X&I\u0013\u0014\u000eY\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015\u0011\u0003\u0019\u0017@\u001e\"\u0015\u0011\u0001\u0004\fZ&2\u0013\u000b\u000e\u0011-/\f\u000f\u000eY\u0015\u0011\u0013W\u0003\u0006\u0017\u001b&2\f\u000f\u000e]\u0015\u0011\u0001@\fZ+\t4\u0015\u0016\u0015\u0011\f\u000f\u000e3O\t\u000b\u0017\n0f\u0015\u0011\u0013H0\u0004\fJ\u0015\u0011\f\u0012%\u000f\u0015/\u0015\u0011\u0013\u001c\u0007\u0004\u0003\u0019%\u0012\u0005\u0012A\u000f\t\\\f\u0012\u0005\u0016\u0005/\u0003\u0019-/\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\u0003\u0019\u001a\u001f\f]\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005'\t\u000b\u000e\u0011\fZ+\u0006\u0003\u0019\u001f\f\u000f+\u00197N\u0015\u0011\u0013<\r\fY\u0013\u001c<@\u0005\u0016\fJ\u000e\u0011\u001a\u001f\f30N\u0003\u0019&M\u0015\u0011\f\u000f(*\u0015\u00115\n\t\u000b+e0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015\u0011\u0005X\t\u00140@0\u001b\u000e\u0011\f\u000f\u0005\u0016\u0005?\u0015\u0011\u0013\u001c\u0007\u0004\u0003\u0006%\u000f\u0005/\u0003\u0006\u0017V\t\u0010+\u0006\u0013\u0014\u001e\u001c\u0003,)%3\t\u000b+$\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u00115\u001b\u000e\u0011\fG<\r\f\u0012\u0003\u0019\u0017@\u001e[0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015g&I\u000e\u0011\u0013\u0014-\u0015\u0011\u0001@\fM\u0005\u0016\f\u0012-'\t\u0014\u00179\u0015\u0011\u0003\u0019%M\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u00115\u0004\u000e\u0011\f\u0014A`wu\u0017%\u0012\u0013\u0014\u00179\u0015\u0011\f\u0012\u00179\u0015/<\n\t\u000b\u0005\u0016\f30\u001b)=-?5\u0004\u0005\u0016\u0003\u0019%]\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+BOg\u0015\u0011\u0001@\f]-'\t\u0014\u0003\u0019\u0017V\u0007\u001b\u000e\u0011\u0013\u001c<\u0004+\u0006\f\u000f-K\u0003\u0019\u0005'0\u00045\u0004\f]\u0015\u0011\u0013\u0015\u0011\u0001\u0004\f'\t\u0014<@\u0005\u0016\f\u000f\u0017@%\u0012\fX\u0013\u000b&D\tZ%\u0012+\u0019\f3\t4\u000e?\t\u000b\u0017\n0\b5@\u0017\n95\u0004\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u0013\u001c\u0017\n\t\u000b<@+\u0019\f/+\u0019\u0013\u001c\u001e\u0014\u0003\u0006%\u0012\t\u0014+a\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u00115\u0004\u000e\u0011\f\u0015\u0011\u0001@\t\u000b\u00156Q\u0002\u0013\u00145@+\u00060[\u0001\u0004\f\u0012+\u0019\u0007?\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\Aiwc\u0017\n0\u0004\f\u000f\f30\\O\u000b\u0015\u0011\u0001@\fg+\t\u000b%JF\u0013\u000b&$\t\u0014\u0017[\f\u000f(\u001b\u0007@+\u0019\u0003\u0019%\u0012\u0003,\u0015\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u00115\u0004\u000e\u0011\fM\u0013\u0014&h\u0015\u0011\u0001\u0004\fG-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%G\u0007\u0004\u000e\u0011\u0013\u0014@+\u0019\fG-'\t\u00127/<\r\f8\tF%3\t\u00145\u0004\u0005\u0016\f.\u0013\u0014&i\t[\u0007\r\u0013\u0014\u0005\u0016\u0005\u0016\u0003\u0006<\u0004+\u0019\f\u0007@\t\u000b\u000e\u0016\u0015\u0011\u0003\u0006\t\u0014+h\t\u0014\u001e\u0014\u000e\u0011\f\u000f\f\u0012-/\f\u0012\u00179\u0015R<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u0017>\u0001*5\u0004-'\t\u0014\u0017]\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\f\u000f\u000e\u0011\u0005\u0012A\n2.5RelatedWorkj>\t\u000b\u0017*7/\t\u000b\u000e\u0011\fM\u0015\u0011\u0001\u0004\fD\u000e\u0011\f\u000f\u0005\u0016\f3\t\u000b\u000e\u0011%:\u0001]Qa\u0013\u0014\u000e\u0011*\u0005\u0002<\r\f\u0012\u0003\u0019\u0017\u0004\u001eE\u0007\u001b\u000e\u0011\u0013\u001c\u0007\r\u0013\u0014\u0005\u0016\f30]\u0013\u0014\u0017'-?5@\u0005\u0016\u0003\u0019%D\u0003\u0019\u0017\u0004&2\u0013\u000b\u000e\u0016)-'\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017H\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+`<@\t\u0014\u0005\u0016\f30\b\u0013\u001c\u0017W-/\f\u000f+\u0006\u0013*0*79AX\u0000\u0002\u0001\u0004\fX\u000e\u0011\f3\t\u001c0\u001b\f\u000f\u000eF\u0003\u0019\u00058\u0005\u00165@\u001e\u0014\u001e\u001c\f\u0012\u0005\u001d\u0015\u0011\f30\u0015\u0011\u0013F\u000e\u0011\f\u000f&2\fJ\u000ea\u0015\u0011\u0013]\u0019\u0014O\u0011\u0010\u0004O\u0013\u00129O\u0015\u0014*O\r\u0016\u0010\u001bO\r\u0018\u0017\u001bO@\u0007\u00194\r\u0015\u0011\u0001\n\t4\u0015e\t\u00140@0\u001b\u000e\u0011\f\u000f\u0005\u0016\u0005a-?5\u0004\u0005\u0016\u0003\u0019%D\u0003\u0019\u0017@0\u0004\f\u000f(\u001b\u0003\u0019\u0017@\u001e\t\u000b\u0017\n0\u0010-/\f\u0012+\u0019\u0013*0\u001b7;\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O$\u0015\u0011\u0013H \u0017\u001bOi\u001446\u0015\u0011\u0001\n\t4\u0015F\t\u001c0\u00040\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005.-[5@\u0005\u0016\u0003\u0019%F\u000e\u0011\f\u000f)\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+\u001b\f\u0012\u001a\u0014\t\u0014+\u00195\n\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\u001c\u0013\u0014\u000e`\u0015\u0011\u0013X \u0019*O\u001b\u001a\u001bO@\u0012\u001bO\n\u0018\u0019*O\u0004\u001c\u0012*O@\u0018\u001a\u001bO9*J@\u0015\u0011\u0001\n\t4\u001510\u0004\f\u0012\u0005\u0016%J\u000e\u0011\u0003\u0006<\r\fQa\u0013\u0014\u000e\u0011*\u0003\u0019\u0017@\u001e/\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-/\u0005\u0012A\n3.EXPERIMENTSCW\f?\u0003\u0019\u0017*\u001a\u001f\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001f\t4\u0015\u0011\f/\u0013\u001c\u0017\u0010\u0015\u0011\u0001@\f?\fJ#$\f\u0012%\u000f\u0015\u0011\u00058\u0013\u0014&a\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015\u0011\u0003\u0019\u0017@\u001eZ-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%[&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005&I\u0013\u0014\u000e`\t\u000b5\u0004\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019%g-/\f\u0012+\u0019\u0013*0*7.\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O\u000b\u0015\u0011\u0001@\t\u000b\u0015h\u0003\u0006\u0005\\QR\u0001@\f\u000f\u0015\u0011\u0001\u0004\f\u000f\u000ei\f\u000f\u001a\u001b\u0003\u00060\u001b\f\u0012\u0017@%\u0012\f\n<\r\f\u000f\u0003\u0006\u0017\u0004\u001ez\u0007\u0004\u000e\u0011\u00134\u001a*\u0003\u00060\u0004\f30!<97v-/\f\u0012+\u0019\u0013*0\u001b7v\u0001@\f\u0012+\u0019\u0007\u0004\u0005\b\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001ez-[5@\u0005\u0016\u0003\u0019%N0\u0004\u00139%\u00125\u001b)-/\f\u000f\u0017*\u00151-/\u0013\u000b\u000e\u0011\fD\fJ#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012+,7X\u0015\u0011\u0001@\t\u0014\u0017/\u0007\u0004\u000e\u0011\u00139%\u000f\f30\u00045\u001b\u000e\u0011\f\u0012\u00051\u0015\u0011\u0001@\t\u000b\u0015a\t\u000b\u000e\u0011\fD\u0017@\u0013\u000b\u0015\u0002<\n\t\u000b\u0005\u0016\f30X\u0013\u001c\u0017-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%E&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005\u0012AGLM\u0013\u0014\u0015\u0011\fF\u0015\u0011\u0001\n\t4\u0015G\u0013\u00145\u0004\u000eG\t\u0014\u0003\u0019- Q\u0002\t\u0014\u0005G\u0017\u0004\u0013\u0014\u0015M\u0015\u0011\u0013X\u0015\u0011\f\u0012\u0005\u001d\u0015G\u0015\u0011\u0001\u0004\fE\fJ&I)&I\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017\u0004\f\u0012\u0005\u0016\u0005R\u0013\u0014&`\t[\u0005\u0016\u0007\r\f\u0012%\u0012\u0003,\n%.-/\f\u000f+\u0006\u0013*0*7]\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017>\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-\u001e\u001d\u001b\u0015\u0011\u0001@\f\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-_Qa\fX5@\u0005\u0016\f\u00120H\u0003\u0019\u0005E\t\u0014\u0017W\u0003\u0019\u0017\u0004\u0005\u001d\u0015:\t\u0014\u0017@%\u000f\f/\u0013\u0014&a\u0015\u0011\u0001\u0004\fX%\u0012+\u0006\t\u0014\u0005\u0016\u0005E\u0013\u0014&e\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005\u0015\u0011\u0001@\t\u000b\u0015G%3\t\u0014\u0017\u0010<\r\fE5@\u0005\u0016\f30>\u0015\u0011\u0013'\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015M-?5\u0004\u0005\u0016\u0003\u0019%\u001cA8G5\u001b\u000e.\t\u000b\u0003\u0019- Qe\t\u000b\u0005G\u000e:\t\u000b\u0015\u0011\u0001\u0004\f\u000f\u000eG\u0015\u0011\u0013\u0003\u0019\u0017*\u001a\u001f\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001c\t\u000b\u0015\u0011\f/\u0003,&\u0002\u0015\u0011\u0001@\fJ\u000e\u0011\f?\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005E\t\u0014\u0017\"\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-\u0015\u0011\u0001\n\t\u000b\u0015F\u0007\r\fJ\u000e\u0016&2\u0013\u0014\u000e\u0011-/\u0005F95\u0004\u0003\u0019\u0015\u0011\fQa\f\u0012+\u0019+M5\u0004\u0005\u0016\u0003\u0019\u0017@\u001e\"-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%Y&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u0005\u0012OR\t\u000b\u0017\n0N\u0015\u0011\u0001@\f\u0012\u0017N\u0015\u0011\u0001\n\t4\u0015X\u0015\u0011\u0001\n\t4\u0015]\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-%\u0012\t\u0014\u0017]<\r\fG\u0003\u0019-/\u0007\u0004\u000e\u0011\u00134\u001a\u001f\f30?\u0015\u0011\u0013E\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-\f\u000f\u001a\u001f\f\u0012\u0017'<\r\f\u000f\u0015\u0016\u0015\u0011\f\u000f\u000e\u0002\u0015\u0011\u0001@\t\u0014\u0017/\u0015\u0011\u0001@\fD\u001a\u001f\f\u000f\u000e\u0011\u0005\u0016\u0003\u0019\u0013\u0014\u0017]Qa\f5\u0004\u0005\u0016\f30\\A;\u0000\u0002\u0001@\f/\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u0005F%3\t\u000b\u000e\u0016\u000e\u0011\u0003\u0019\f30\"\u0013\u00145\u0004\u0015E\u0015\u0011\u0013Z\u0015\u0011\f\u0012\u0005\u001d\u0015E\u0015\u0011\u0001\n\t4\u0015[\u000197\u001b\u0007\r\u0013\u0014\u0015\u0011\u0001\u0004\f\u0012\u0005\u0016\u0003\u0019\u0005\t\u000b\u0003\u0019-/\f30Y\u0015\u0011\u0013\u0004|\u0014A \u001f\u0002q\u0011s\"!\u001bp\u0014q\u001dp\u0014Brl\u0014n!l\u001d;pNk\u0011p\u0014t\u000fs\u000f r2n$s\u001c|\u0010G\u0003\u0019\u001e\u0014\u0001@+\u0019\u0003\u0019\u001e\u001c\u00019\u0015Y\tH\u000e\u0011\f\u000f&2\fJ\u000e\u0011\f\u0012\u0017@%\u0012\f;\u0005\u0016\f\u000f\u001e\u0014)-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017W<\n\t\u000b\u0005\u0016\f30H\u0013\u0014\u0017\"\u0001*5@-'\t\u000b\u0017'Tu5\n0\u001b\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005\u0012O\\\u0015\u0011\u0013ZQR\u0001@\u0003\u0019%:\u0001\"%3\t\u0014\u0017\u001b)0\u0004\u0003\u00060\u0004\t\u000b\u0015\u0011\f\u0010\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0006%Z\u0007\u0004\u000e\u0011\u00139%\u0012\f30\u001b5\u0004\u000e\u0011\f\u0012\u0005'%3\t\u000b\u0017<\r\f>%\u0012\u0013\u0014-/\u0007\n\t4\u000e\u0011\f30\\AS\u0000\u0002\u0001\u0004\f\u0007\u0004\u000e\u0011\f\u000f\u0007\n\t\u000b\u000e:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\u0013\u0014&@\u0015\u0011\u0001\u0004\fg<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\f1\u000e\u0011\f395@\u0003,\u000e\u0011\f30.\u0015\u0011\u0001@\f1&2\u0013\u0014+\u0006+\u0019\u00133QR\u0003\u0019\u0017@\u001e.\u0005\u001d\u0015\u0011\f\u000f\u0007@\u0005\u0012|\t\u001c$#$sJt\u0011Ms&%\u001f:sJq\"!@Bt\u0014|M\u0005\u0016\f\u0012+\u0019\f\u0012%\u000f\u0015.\t/\u000e\u0011\f\u000f\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u001a\u001c\f[\u0005\u0011\t\u000b-/\u0007@+\u0019\f8\u0015\u0011\f\u0012\u0005\u001d\u0015\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005/\u0015\u0011\u0013^<\r\f\b\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\f30}<97V\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017@%\u000f\f30-?5\u0004\u0005\u0016\u0003\u0006%\u0005\u0016%:\u0001@\u0013\u001c+\u0006\t4\u000e\u0011\u0005\u0016\u001dB<\r\u001e'@m\u0004k)(4s\u0016\u000fBt\u000b|f\u0005\u0016\f\u0012+\u0019\f\u0012%\u000f\u0015;\t^\u0005\u0016\f\u000f\u0015Z\u0013\u0014&[\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005\u0012OMQR\u0003,\u0015\u0011\u0001S\t^<\n\t\u0014%:9)\u001e\u0014\u000e\u0011\u0013\u00145@\u0017@0[\u0003\u0019\u0017?-[5@\u0005\u0016\u0003\u0019%\u001cO\u000bQR\u0001@\u0013GQa\f\u000f\u000e\u0011\fR\t\u0014\u0005\u0016\u001f\f\u00120E\u0015\u0011\u0013.\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015`-'\t\u0014\u0017\u001b)5\n\t\u000b+\u0019+\u00197\b\u0015\u0011\u0001@\fX\u0015\u0011\f\u000f\u0005\u001d\u0015?\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0016\u001d`%\u0012\u0013\u0014-/-/\f\u0012\u00179\u0015\u0011\u0005.Q\u0002\f\u000f\u000e\u0011\f/\u000e\u0011\f\u0012%\u0012\u0013\u000b\u000e:0\u0004\f30<97/\u0015\u0011\u0001*5@\u0005e\u0007\r\f\u000f\u000e\u0011-/\u0003,\u0015\u0016\u0015\u0011\u0003\u0019\u0017@\u001e[5@\u0005e\u0015\u0011\u0013?\u000e\u0011\f\u0012\u0007\r\u0013\u000b\u000e\u0016\u0015D\u0015\u0011\u0001\u0004\f\u0012\u0003,\u000eR<\r\f\u0012\u0001\n\t3\u001a*\u0003\u0019\u0013\u0014\u000e\u0018\u001dB%3+*` m*t\u0011csJq-,Dn\rp\u001c \u0006\u000bt\u0011r2tN\t\u000b\u0017\n0/.;m\u0004 BrBo\u0014r2'sJn\u0004t\u0011rl\u0014n\rp\u00140'\n\u0011p\u001c r2n*\n|-/\f3\t\u000b\u0005\u00165\u0004\u000e\u0011\fW\u0015\u0011\u0001\u0004\fN0\u0004\f\u0012\u001e\u000b\u000e\u0011\f\u0012\f^\u0015\u0011\u0013}QR\u0001@\u0003\u0019%:\u0001!\u0015\u0011\u0001@\f^\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\b\u0007\r\f\u000f\u000e\u0016)&2\u0013\u000b\u000e\u0011-/\f30G\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017F%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u00179\u0015\u0011+,7.\u0005\u0016\u0013e\u0015\u0011\u0001\n\t4\u0015P\u0015\u0011\u0001\u0004\f6<\n\t\u000b\u0005\u0016\f\u000f)+\u0019\u0003\u0019\u0017@\f?%\u0012\t\u0014\u0017\u0010<\r\f?5\u0004\u0005\u0016\f30\u0010\t\u0014\u0005G\u0015\u0011\u0001\u0004\fE\u000e\u0011\f\u000f&I\f\u000f\u000e\u0011\f\u0012\u0017\u0004%\u0012\f[&I\u0013\u0014\u000eF%\u0012\u0013\u0014-/\u0007\n\t\u000b\u000e\u0011\u0003\u0019\u0005\u0016\u0013\u0014\u00171\u001d\u0015\u0011\u0001@\u0003\u0019\u0005P-/\f3\t\u0014\u0005\u00165\u001b\u000e\u0011\f\u0012-/\f\u0012\u00179\u0015h\u0001@\t\u0014\u0005i<\r\f\u000f\f\u0012\u0017[%\u0012\t\u000b\u000e\u0016\u000e\u0011\u0003\u0019\f30F\u0013\u001c5\u0004\u0015i5@\u0005\u0016\u0003\u0019\u0017@\u001e32a+\u00195@\u0005\u001d)\u0015\u0011\f\u000f\u000eDD\u0017\n\t\u0014+,7\u001b\u0005\u0016\u0003\u0019\u0005D\t\u0014\u0017\n0>jZ5@+,\u0015\u0011\u0003\u00060\u0004\u0003\u0019-/\f\u0012\u0017@\u0005\u0016\u0003\u0019\u0013\u0014\u0017\n\t\u0014+h~\u001b%\u0012\t\u0014+\u0019\u0003\u0019\u0017@\u001e\u0013\u001d0\n54Ml\u0014m*n\no\u001fp\u0014q6\u0006Fo9sJcs\u0016\u000fBrl\u0014ni|60\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015i-/\u0013\u001c\u0005\u001d\u0015h\u0007@+\u0006\t\u00145\u0004\u0005\u0016\u0003\u0019<@+\u0019\f\u0002<\r\u0013\u001c5\u0004\u0017\n0*)\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005R&2\u0013\u000b\u000eG\f\u0012\t\u0014%:\u0001>\u0015\u0011\f\u0012\u0005\u001d\u0015D\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u0004\u0015\u0018\u001d\u0004\u0015\u0011\u0001@\f8<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\f8\u0015\u0011\u0001@\f\u000f\u0017>%\u0012\u0013\u001c\u0017\u001b)\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f30^\u0003\u0019\u0017\"\u0015\u0011\u0001@\f'\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015[\u0015\u0011\u0013\u0014\u001e\u001c\f\u000f\u0015\u0011\u0001\u0004\f\u000f\u000e?QR\u0003,\u0015\u0011\u0001N-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0011\u0005[\u0005\u0016\f\u0012\u0007\u001b)\t\u000b\u000e:\t4\u0015\u0011\u0003\u0019\u0017@\u001e\b\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005?\u0015\u0011\u0001@\t\u000b\u0015?-/\u0013\u001c\u0005\u001d\u0015X+\u0019\u0003\u0019\u001f\f\u0012+,7N%\u0012\u0013\u000b\u000e\u0016\u000e\u0011\f\u0012\u0005\u0016\u0007\r\u0013\u001c\u0017@0N\u0015\u0011\u0013<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012A9A7,Dn\rp\u0014 \u0006\u0014t\u0011r2tRlueq\u0016sJt\u0011m\u0004 Bt\u000b|82a\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\f\u0002%\u0012\t\u0014\u0017\n0\u001b\u0003\u00060@\t\u000b\u0015\u0011\f\u0002\t\u00145\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%e\u0007\u0004\u000e\u0011\u0013\u000b)%\u0012\f30\u001b5\u0004\u000e\u0011\f\u0012\u0005M\t\u0014\u0017@0;0\u0004\f\u0012%\u000f\u00030\u001b\f8QR\u0001\n\t\u000b\u0015G\u0007\u0004\u000e\u0011\u00139%\u0012\f\u00120\u00045\u0004\u000e\u0011\fF\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011- <\r\f\u0012\u0005\u001d\u0015DQR\u0003\u0019\u0015\u0011\u0001\u000e\u0011\f\u0012\u0005\u0016\u0007\r\f\u0012%J\u0015D\u0015\u0011\u0013[\u0015\u0011\u0001@\f.<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\f\u001cAwc\u0017Y\u0015\u0011\u0001\u0004\f.&2\u0013\u001c+\u0019+\u0019\u00133QR\u0003\u0019\u0017@\u001e\u0004O@Q\u0002\fF0\u0004\f\u000f\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\fF\u0015\u0011\u0001\u0004\f.\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u0005\u0002\u0003\u0019\u0017;0\u001b\f\u000f\u0015:\t\u0014\u0003\u0019+BA\n3.1TestExcerptsG\u0017\u0004\fM\u0013\u000b&P\u0015\u0011\u0001\u0004\fM<@\t\u0014\u0005\u0016\u0003\u0019%G\f\u000f+\u0006\f\u000f-/\f\u0012\u00179\u0015\u0011\u0005g\u0013\u000b&P\u0015\u0011\u0001\u0004\fM\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u00051\u0003\u0019\u0005\u0002%\u000f\u000e\u0011\f3\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017Y\u0013\u000b&i\t\u0015\u0011\f\u000f\u0005\u001d\u0015\u0011<\r\f30\\AiCW\fR\t\u0014\u0005\u0016\u001f\f\u00120X\t\u0014\u0017/\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0016\u0015`-[5@\u0005\u0016\u0003\u0019%\u0012\u0013\u0014+\u0006\u0013\u0014\u001e\u001c\u0003\u0019\u0005\u001d\u0015a\u0015\u0011\u0013G\u0005\u0016\f\u0012+\u0019\f\u0012%\u000f\u0015e\tM\u0017\u001b5\u0004-?<\r\f\u000f\u000e\u0013\u000b&G-[5@\u0005\u0016\u0003\u0019%'\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005[\u0013\u000b&D\u0015\u0011\u0013\u0014\u0017\n\t\u000b+\u0002C\"\f\u0012\u0005\u001d\u0015\u0011\f\u000f\u000e\u0011\u0017^-[5@\u0005\u0016\u0003\u0019%\u001cO6QR\u0001\u0004\u0003\u0006%:\u0001\"\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015\t'\u001e\u001c\u00139\u0013*0\b\u0005\u0011\t\u0014-/\u0007\u0004+\u0006\u0003\u0019\u0017\u0004\u001eY\u0013\u000b&a\u001a\u0014\t\u000b\u000e\u0011\u0003\u0019\u0013\u001c5\u0004\u0005G\u0015u7\u001b\u0007\r\u0013\u0014+\u0006\u0013\u0014\u001e\u001c\u0003\u0019\f\u0012\u00058\u0013\u000b&\u0002-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%E\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u00115\u001b\u000e\u0011\f\u001cA\u0000\u0002\u0001\u0004\f/-?5\u0004\u0005\u0016\u0003\u0006%\u000f\u0013\u001c+\u0019\u0013\u001c\u001e\u0014\u0003\u0006\u0005\u001d\u0015[\u0007\u0004\u000e\u0011\u0013\u0014\u0007\r\u0013\u001c\u0005\u0016\f30\"\u0014Z\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u00058\u0013\u0014&D0\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015F+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001@\u0005\u0012O\u000e:\t\u000b\u0017@\u001e\u0014\u0003\u0006\u0017\u0004\u001eE&\u000e\u0011\u0013\u001c-9\u0012G\u0015\u0011\u0013?:\u0017E<@\t\u000b\u000e\u0011\u0005a\t\u0014\u0017\n0?&\u000e\u0011\u0013\u001c-;\u0017.\u0015\u0011\u0013/\u0018\u001a\u001f8\u0017@\u0013\u000b\u0015\u0011\f\u0012\u0005\u0012O\u001b0\u0004\f\u000f\u0007\r\f\u0012\u0017\n0*)\u0003\u0019\u0017\u0004\u001e]\u0013\u0014\u0017>\u0015\u0011\u0001@\fF-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%E\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u00115\u001b\u000e\u0011\fE\t\u0014\u0017\n0>\u0013\u0014\u0017>\u0015\u0011\u0001@\fF+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001\u0010\u0013\u0014&`\u0015\u0011\u0001@\fF-'\t\u0014\u0003\u0019\u0017\u0015\u0011\u0001\u0004\f\u0012-/\f\u001cA\"\u0000\u0002\u0001@\f]%\u0012\u0013\u001c-/\u0007\u0004+\u0006\fJ\u0015\u0011\fY+\u0019\u0003\u0019\u0005\u001d\u0015/\u0013\u0014&D\u0015\u0011\u0001@\f]-?5\u0004\u0005\u0016\u0003\u0019%]Qa\u0013\u0014\u000e\u0011*\u0005\u0012Og&\u000e\u0011\u0013\u001c-UQR\u0001@\u0003\u0019%J\u0001\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005/Q\u0002\fJ\u000e\u0011\f\u0010\u0015:\t\u000b\u001f\f\u0012\u0017\\Oe\u0003\u0019\u0005]\u000e\u0011\f\u0012\u0007\r\u0013\u000b\u000e\u0016\u0015\u0011\f30}\u0003\u0019\u0017z\u0000i\t\u000b<@+\u0019\f\u0010\u001bA\u0019\u0014A \t\u0014+\u0019+M\u0013\u0014&8\u0015\u0011\u0001@\f\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005D\t\u000b\u000e\u0011\f.\u0015\u0011\u0001\u0004\f.\u0003\u0019\u0017@%\u0012\u0003\u0019\u0007\u0004\u0003\u0019\u0015D\u0013\u000b&6\u0015\u0011\u0001@\f.-[5@\u0005\u0016\u0003\u0019%GQ\u0002\u0013\u000b\u000e\u0011*\u0005:JA\n3.2SubjectsG5\u001b\u000eX\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u0005E\t\u000b\u000e\u0011\fY<\n\t\u000b\u0005\u0016\f30^\u0013\u001c\u0017^\u0015\u0011\u0001@\f'\u0003\u0019\u00179\u0015\u0011\f\u0012+\u0019+\u0006\f\u000f%\u000f\u0015\u00115\n\t\u000b+e\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0016\u0015\u0011\u0003\u0019\u0005\u0016\fY\u0013\u000b&-[5@\u0005\u0016\u0003\u0019%?\u0005\u0016%:\u0001@\u0013\u0014+\u0006\t\u000b\u000e\u0011\u0005\u0012AXCW\f?<\r\f\u0012+\u0019\u0003\u0019\f\u0012\u001a\u001f\fE\u0015\u0011\u0001\n\t\u000b\u0015F\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005F\t4\u000e\u0011\f?\u0015\u0011\u0001@\f[%3\t\u0014\u0017@0\u0004\u0003\u00060@\t4\u0015\u0011\f\u0002c\u0007\u0004\u000e\u0011\u0013*0\u001b5@%\u0012\f\u000f\u000e\u0011\u0005&\u0004/\u0013\u000b&6\u001e\u001c\u00139\u0013*0>-[5@\u0005\u0016\u0003\u0019%.\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017Z<\r\f\u0012%3\t\u00145\u0004\u0005\u0016\f8\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4)\u0015\u0011\u0003\u0019\u0013\u0014\u0017\u0001\n\t\u000b\u0005Y\tH\u0007\r\f\u000f\u000e\u0011%\u0012\f\u0012\u0007\u001b\u0015\u00115\n\t\u0014+D\t\u000b\u0017\n0V\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f;\u0017@\t\u000b\u0015\u00115\u0004\u000e\u0011\f\u0014AzC\"\f;\t\u000b\u0005\u0016\u001f\f30}\t\u001e\u000b\u000e\u0011\u0013\u001c5\u0004\u0007\"\u0013\u0014&8\u0018\u0012Z\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u00058\u0015\u0011\u0013;\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015F-'\t\u0014\u0017*5\n\t\u000b+\u0019+\u00197\u0010\u0015\u0011\u0001\u0004\f'\u0014>\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0012AD+\u0019+i\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005DQa\f\u000f\u000e\u0011\f[\fJ(\u0004\u0007\r\fJ\u000e\u0016\u0015G-[5@\u0005\u0016\u0003\u0019%\u0012\u0003\u0006\t\u0014\u0017\u0004\u0005\u0012AeLG\u0013\u000b\u0015\u0011\fF\u0015\u0011\u0001@\t\u000b\u0015DQ\u0002\fE\t\u0014\u0005\u0016\u001c\f30>\f\u000f(*)\u0007\r\fJ\u000e\u0011\u0003\u0006\f\u000f\u0017@%\u0012\f30[\u0005\u0016%J\u0001\u0004\u0013\u001c+\u0006\t\u000b\u000e\u0011\u0005`\u0015\u0011\u0013.\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-\u0018\u0015\u0011\u0001@\fa\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017?\u0015:\t\u0014\u0005\u0016\rO\u001c\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0011\f3\t\u001c0\u0013\u000b&\\\u0003\u0006\u0017\u0004\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017@%\u000f\f30X\f\u0012\u0017\n0X5\u0004\u0005\u0016\f\u000f\u000e\u0011\u0005\u0012A`wu\u0017[&B\t\u0014%J\u00153O9\u0015\u0011\u0001@\fe\u0015:\t\u0014\u0005\u0016[\u0013\u0014&P0\u0004\f\u000f\u0015\u0011\f\u000f%\u000f\u0015\u0011\u0003\u0019\u0017@\u001eF\u0005\u0016\u0003\u0019\u001e\u0014)\u0017\u0004\u0003,\n%3\t\u000b\u0017*\u0015.-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%?\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005M&\u000e\u0011\u0013\u001c-tJ\u0011l\u0014q\u0016sJt?\u000e\u0011\f395@\u0003,\u000e\u0011\f\u0012\u0005.\t]\u0017\u0004\u0013\u0014\u0015\u0011\u0003\u0019%\u0012\f3\t\u000b<@+\u0019\f\u0003\u0019\u00179\u0015\u0011\f\u0012+\u0019+\u0019\f\u0012%\u000f\u0015\u00115@\t\u0014+RQ\u0002\u0013\u0014\u000e\u0011\"\u0015\u0011\u0001@\t\u000b\u0015/\u0017\u0004\f\u0012\f30\u001b\u0005?\u0015\u0011\u0001@\fY\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017}\u0013\u0014&G*\u0017@\u00133QR+\u0019\f30\u0004\u001e\u0014\f\u0013\u0014\u0017Y-[5@\u0005\u0016\u0003\u0019%R\u0015\u0011\u0001@\f\u0012\u0013\u000b\u000e\u001679O\u001b\t\u0014\u0017\n0'\u0003,\u0015e%3\t\u000b\u0017Y<\r\fD\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u000f+\u00197'\u0007\r\fJ\u000e\u0016&2\u0013\u0014\u000e\u0011-/\f\u00120]\u0013\u0014\u0017@+,7/<97\u0015\u0011\u0001\u0004\f\u0012->Az\u0000P\f\u0012\u0005\u001d\u0015\u0011\u0005'%\u0012\u0013\u001c5\u0004+0f<\r\f>%3\t4\u000e\u0016\u000e\u0011\u0003\u0006\f\u00120}\u0013\u001c5\u001b\u0015]\t\u0014+\u0019\u0005\u0016\u0013WQR\u0003,\u0015\u0011\u0001\u0003\u0019\u0017@\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017\u0004%\u0012\f30AComparison ofManualandAutomaticMelodySegmentation\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0006\u000f\u0011\u0010\u0013\u0012\u0002\u0014\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0010\u001c\u000f\u0011\u001d\u001f\u001e \u0014\"!$#\f\u0010%\u0019\u001b\u0010'&)(\u001a\u0015*\u0014\"!%\u0012$+\f&,\u0010'&.-\u0004\u0017\u001a&0/\u00041\u0012\u001c23\u0012$\u000f4\u0014\u0004/,\u0012\u001c&)\u00105\u0012.63\u001e7\u000f8\u00129+7\u00129+\f&;:4&0/\f-<\u00129+=\u000f\b/=>\u001b2\u0016!$\u0010?\u0014\u0016\u0015\u001b&.2\u0016\u001d9+@&.A\u001b\u001d9&)!$B\f\u0012.CD\n\u0014EC F;\u000fG\u0012$:4& H;2\u0016!$\u0010I0JEK\fJ\fLNMPO5QR S.T8U0V\bW9U0TGX;Y'X$U.Z[X\\Z[X7U0W3]NR'^9_0`<aNb<X$c9T8W de f%ghYji3k5lhZhgjX$m\u001bS)n3T ZhkoU3W3]\u0018p0`<a%ghTGX _p f%ghYji3k5lhZhgjX$m\u001bS)n3T ZhkoU3W3]\u0018e0`\u0016q?W9n3g[ghk\u0016r k R'ps ti3W9g[X$m8k e9_u v\u0018g[k5m8n3b3T8n0wxU\f]\u0018y0`3q{z}|~^9u\ns^{JN\"M9LNj\u001c\u0011Q09$'_ SP0w,3i0W9U.U\f]\u0018u0`\nsZhiw,W'Pk'w,k5UPZ e9ed SP0w,3i0W9U.U\f]{d)`\fR5lZ%w,W'Pk5w,k'UPZ e0R^ S.W9U<X\\ZjXU\u0004]R\ns`<p$g[b=w,W'Pk5w,k'UPZ R'ey S.W9U<X\\ZjXU\u0004]?d)`<T8U)n3k\u0013ZZ[W R\u001cdR' S.W9U<X\\ZjXU\u0004]{^0`<W9U3b\u001b W R'^\u0006J\u001bQ0\u0013\"4R9R q\u0006X$m8mGX$b0k\u001fU3W3]R9`<W9\f]\u0018e9p R9RR'e *w,0g[W9w,0Zhn7W9\f]?_9_0`3e$U3bw,W'Pk'w,k5UPZ R'_R'p W9n0Pk5m8m8k\u001f\u0018Zhn3b3koU3W3]\u0018p e0RR\nsz\u001aX$m ZhoU3W3]{d R'_R'u z\u001aX$m ZhoU3W3]\u0018y R\u001cdJ\u001b J\u001b\\ 'MP¡\u0013R'_\ntW9U3Y5k5ghZhW,U3W3]R9`\u0016¢\u001fp0R'p R'R\u001cd £j¤NW9U¥%T8W\u001c$X$U3U3T8¦3`<aNg[TGX R'^R'^ £[§\fkoNW955k b0T\f¨©T8cPX$g[W9¦3`<aNg[TGX R'R'y S.W9U<X\\ZjXU0W3]NR9R9`3¢\u001fp9p0R R'^e9 S.W9U<X\\ZjXU0W3]?y0`3¢\u001fp0R' e9e+\u0019\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017@\fJ\u000e\u0011\u0005e\u0003\u0019&\\\u000e\u0011\f\u0012%\u0012\u0013\u000b\u000e:0\u0004\u0003\u0019\u0017@\u001e\u0014\u0005\u0002\u0013\u0014&P\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-'\t\u0014\u0017\u0004%\u0012\f\u0012\u00051Q\u0002\u0013\u001c5\u0004+\u00060]\u0001@\t4\u001a\u001c\fM<\r\f\u0012\f\u000f\u0017/5@\u0005\u0016\f30\u0003\u0019\u0017@\u0005\u001d\u0015\u0011\f\u0012\t\u001c0\"\u0013\u0014&R\u0005\u0016%\u000f\u0013\u0014\u000e\u0011\f\u0012\u0005\u0012O`<\r\f\u0012%\u0012\t\u00145@\u0005\u0016\f/\u0003\u0019\u0017W\u0015\u0011\u0001\u0004\u0003\u0019\u0005E%\u0012\t\u0014\u0005\u0016\f'+\u0019\u0003\u0006\u0005\u001d\u0015\u0011\f\u000f\u0017@\f\u000f\u000e\u0011\u0005FQ\u0002\u0013\u00145@+\u00060\"\u0001\n\t3\u001a\u001f\f<\r\f\u0012\f\u000f\u0017\"\t\u0014<\u0004+\u0019\fX\u0015\u0011\u0013Z\u0007\r\f\u000f\u000e\u0011%\u0012\f\u000f\u0003\u0006\u001a\u001c\f/<\r\u0013\u001c5@\u0017@0@\t4\u000e\u0011\u0003\u0006\f\u000f\u00058\u0003\u0006\u0017H\u0015\u0011\f\u000f\u000e\u0011-/\u0005.\u0013\u0014&\u0002+\u0006\u00139%\u0012\t\u0014+a\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u0017\u001b)\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017>\u0007\r\u0013\u001c\u0003\u0019\u00179\u0015\u0011\u0005e\u0003\u0019\u0017Y\u0015\u0011\u0001\u0004\f.-?5\u0004\u0005\u0016\u0003\u0006%D\n\u00133Q8Ag\u0000\u0002\u0001@\f.%:\u0001@\u0013\u0014\u0003\u0019%\u0012\f8\u0013\u0014&65\u0004\u0005\u0016\u0003\u0019\u0017@\u001eX0\u001b\u0003,\u000e\u0011\f\u0012%\u000f\u0015\u0011+,7\u0005\u0016%\u0012\u0013\u000b\u000e\u0011\f\u0012\u0005F\u0003\u0019\u0005F0\u00045@\fE\u0015\u0011\u0013]\u0015\u0011\u0001\u0004\f[&2\t\u0014%\u000f\u00158\u0015\u0011\u0001@\t\u000b\u00158\f3\t\u000b%J\u0001H\u0007\r\f\u000f\u000e\u0016&2\u0013\u000b\u000e\u0011-'\t\u0014\u0017\u0004%\u0012\f?\u0003\u0019\u0005F\t\u0014\u0017H\u0003\u0019\u0017*\u0015\u0011\fJ\u000e\u0016)\u0007\u001b\u000e\u0011\f\u000f\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017N\u0013\u000b&D\u0015\u0011\u0001\u0004\f'\u0005\u0016%\u0012\u0013\u0014\u000e\u0011\f\u001cOa\t\u0014\u0017@0W\u0015\u0011\u0001@\f'\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-/\f\u000f\u000e[-'\t\u00127W\u0005\u00165\u0004\u001e\u001c\u001e\u001c\f\u000f\u0005\u001d\u0015X\u0015\u0011\u0001@\f\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017@%\u000f\f8\u0013\u0014&i\u0005\u0016\u0013\u0014-/\f.-?5\u0004\u0005\u0016\u0003\u0019%3\t\u0014+$\u0007@\u0001\u001b\u000e:\t\u0014\u0005\u0016\f\u0012\u0005R0\u001b\f\u0012\u0007\r\f\u0012\u0017@0\u0004\u0003\u0019\u0017@\u001e[\u0013\u001c\u0017Z\u0001\u0004\u0003\u0019\u0005e\u0007\r\f\u000f\u000e\u0011\u0005\u0016\u0013\u001c\u0017@\t\u0014+%:\u0001@\u0013\u0014\u0003\u0006%\u000f\f\u0012\u0005\u0012AaG\u0017?\u0015\u0011\u0001@\fR\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eg\u0001\n\t\u0014\u0017@0\\O9%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017\u0004%\u000f7X<\r\fJ\u0015uQ\u0002\f\u000f\f\u0012\u0017/\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000e\u0011\u0005\u0003\u0019\u0005X+\u0019\u0003\u0019\u001f\f\u0012+,7\"\u0015\u0011\u0013\u0010<\r\f]\u0001@\u0003\u0019\u001e\u001c\u0001\u0004\f\u000f\u000eX\u0003,&D\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017N\u0003\u0019\u0005X%3\t\u000b\u000e\u0016\u000e\u0011\u0003\u0019\f30^\u0013\u00145\u0004\u0015X<97W\f\u000f(*)\u0007\r\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0017\u0004%\u0012\f30\u0010\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005\u0012Oi\t\u000b\u0005.\u0003,\u00158%\u0012\u0013\u001c5\u0004+\u00060\b<\r\f[\u0013\u001c<@\u0005\u0016\fJ\u000e\u0011\u001a\u001f\f30>&I\u000e\u0011\u0013\u0014-\u0005\u001d\u0015\u00115\n0\u001b\u0003\u0006\f\u000f\u0005.\u0013\u001c\u0017\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0016)=\u0003\u0019\u0017\n0\u0004\fJ(\u0004\fJ\u000eR%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u0003\u0006\u0005\u001d\u0015\u0011\f\u000f\u0017@%\u000f7H,44Aª1\t\u0014%:\u0001Z\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0002Q\u0002\t\u0014\u0005e\u001e\u001c\u0003\u0019\u001a\u001f\f\u000f\u0017Z\tE\u0007\n\t\u0014%:\u0014\t\u0014\u001e\u001c\fM%\u0012\u0013\u001c\u00179\u0015:\t\u000b\u0003\u0006\u0017\u0004\u0003\u0019\u0017@\u001e[\u0015\u0011\u0001\u0004\f.\u000b[-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005?\u0015\u0016\u000e:\t\u000b\u0017@\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f30f\u0013\u001c\u0017}-[5@\u0005\u0016\u0003\u0019%]\u0005\u0016\u0001@\f\u0012\f\u000f\u00153A}\u0000\u0002\u0001@\fJ\u000e\u0011\fZQe\t\u000b\u0005/\u0017@\u0013\b-'\t\u000b(\u001b\u0003,)-[5@- \u0015\u0011\u0003\u0019-/\fE&I\u0013\u0014\u000e.\u000e\u0011\f\u000f\u0015\u00115\u001b\u000e\u0011\u0017@\u0003\u0019\u0017@\u001e'\u0015\u0011\u0001@\fE%\u0012\u0013\u0014-/\u0007@\u0003\u0019+\u0019\f30>\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\u0005\u0012AE~\u001b5\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005.%\u000f\u0013\u001c5@+\u00060\u0001\u0004\f\u0012+\u0019\u0007Y\u0015\u0011\u0001@\f\u000f-/\u0005\u0016\f\u0012+\u0019\u001a\u001f\f\u0012\u0005\u0002<97/\u0007\u0004+\t\u00127\u001b\u0003\u0019\u0017@\u001eE\u0015\u0011\u0001@\fG\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005e\u0013\u0014\u0017]\u0015\u0011\u0001\u0004\f\u0012\u0003,\u000eR\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0016\u000e\u00115@-/\f\u0012\u00179\u0015\t\u000b\u0017\n0}%\u0012\u0013\u000b\u000e\u0016\u000e\u0011\f\u0012%\u000f\u0015Y\u0007\u0004\u000e\u0011\f\u0012\u001a*\u0003\u0019\u0013\u001c5\u0004\u0005Y%:\u0001@\u0013\u0014\u0003\u0019%\u0012\f\u0012\u0005\u0012A/\u000b170\u001b\u0003\u0019\u000e\u0011\f\u000f%\u000f\u0015\u0011+,7\u0007@+\u0006\t\u00127\u001b\u0003\u0019\u0017@\u001eW\u0015\u0011\u0001\u0004\f;\f\u000f(*)%\u0012\fJ\u000e\u0011\u0007\u0004\u00153Oi\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u00058Q\u0002\fJ\u000e\u0011\f/\u0017@\u0013\u0014\u0015F<\u0004\u0003\u0006\t\u0014\u0005\u0016\f30W<97\b\t\u000b\u0017*7\b\f\u000f(*\u0015\u0011\f\u000f\u000e\u0011\u0017@\t\u0014+1\u0003\u0019\u0017*\u0015\u0011\fJ\u000e\u0011\u0007\u0004\u000e\u0011\f\u000f\u0015:\t4)\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\u0015\u0011\u0001\n\t4\u0015`-'\t37.\u0005\u00165@\u001e\u001c\u001e\u0014\f\u0012\u0005\u001d\u0015\u0011\u00051\tM\u0007@\t\u000b\u000e\u0016\u0015\u0011\u0003\u0019%\u00125@+\u0006\t4\u000eg\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PAª1\t\u0014%:\u0001?\u0007@\t\u0014\u001e\u001c\f\u0001@\t\u001c0F\u0005\u0016\u0013\u001c-/\fa\f\u000f-/\u0007\u0004\u0015u7G+\u0019\u0003\u0006\u0017\u0004\f\u0012\u0005iQR\u0001@\fJ\u000e\u0011\fa\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005iQ\u0002\f\u000f\u000e\u0011\fa\f\u0012\u0017@%\u0012\u0013\u00145\u0004\u000e:\t\u000b\u001e\u001c\f308\u0015\u0011\u0013G\t\u00140@0%\u0012\u0013\u0014-/-/\f\u0012\u00179\u0015\u0011\u0005R\t\u0014\u0017@0Y\f\u000f(\u001b\u0007@+\u0006\t\u000b\u0017\n\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005D\u0013\u0014&6\u0015\u0011\u0001@\f\u000f\u0003\u0019\u000eD%:\u0001@\u0013\u0014\u0003\u0006%\u000f\f\u0012\u0005\u0012A \u000ba\f\u000f%3\t\u00145\u0004\u0005\u0016\fE\u0013\u000b&6\u0015\u0011\u0001@\f\t\u000b<@\u0005\u0016\f\u0012\u0017\u0004%\u0012\fR\u0013\u0014&$\u0015\u0011\u0003\u0019-/\fe%\u0012\u0013\u001c\u0017\u0004\u0005\u001d\u0015\u0016\u000e:\t\u0014\u0003\u0019\u00179\u0015\u0011\u0005a\t\u000b\u0017\n0?\u0015\u0011\u0001@\fe\u0017\u0004\f\u0012\f30?\u0013\u000b&P0\u0004\f\u000f\u0015:\t\u000b\u0003\u0006+\u0019\f30X\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u0017\u001b)\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017Z\u000e\u0011\f\u0012\u0005\u00165\u0004+\u0019\u0015\u0011\u0005\u0012O@%\u0012\u0013\u001c+\u0019+\u0019\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0017\u0004\u001e'\t\u0014+\u0019+\\\u0015\u0011\u0001\u0004\f.\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f30Y\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0002&I\u000e\u0011\u0013\u0014- \t\u0014+\u0019+\u0015\u0011\u0001\u0004\f8\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005e\u000e\u0011\f395\u0004\u0003\u0019\u000e\u0011\f\u00120Z\t?%\u0012\u0013\u00145@\u0007\u0004+\u0006\f8\u0013\u000b&`-/\u0013\u001c\u00179\u0015\u0011\u0001@\u0005\u0012A\u0000\u0002\u0001\u0004\f8\u0007\n\t\u0014%:\u0014\t\u0014\u001e\u001c\f\u000f\u0005\u0002Q\u0002\f\u000f\u000e\u0011\fF\t\u00140@0\u0004\f\u00120]QR\u0003,\u0015\u0011\u0001Z\u0003\u0019\u0017@\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005M\t\u000b\u0017\n0'-/\u0013\u0014\u0015\u0011\u0003\u0019\u001a\u0014\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0013\u000b&@\u0015\u0011\u0001@\f`\u0015\u0011\f\u0012\u0005\u001d\u00153Ai\u0000\u0002\u0001@\fa-'\t3T\u001d\u0013\u000b\u000e6\u0003\u0019\u0017\n0\u001b\u0003\u0006%\u0012\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017FQe\t\u000b\u00056\t\u0014<\r\u0013\u00145\u0004\u0015`\t\u000b\u0017E\u0013\u0014\u0007\r\f\u000f\u000e:\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\fa0\u0004\f\u000f&)\u0003\u0019\u0017@\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017Y\u0013\u0014&P\u0015\u0011\u0001@\fG-[5@\u0005\u0016\u0003\u0019%3\t\u000b+\\\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005a\u0015\u0011\u0001\u0004\f\u000f7/\u0001\n\t\u00140'\u0015\u0011\u0013[\u0001@\u0003\u0019\u001e\u001c\u0001\u0004+\u0019\u0003\u0006\u001e\u0014\u00019\u00153O\u001bQR\u0001\u0004\u0003\u0019%J\u0001Qa\f\u000f\u000e\u0011\fW\f\u000f(\u001b\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30x\t\u0014\u0005 \u0002c\u0015\u0011\u0001@\f\b+\u0019\f\u000f(\u001b\u0003\u0019%3\t\u0014+85\u0004\u0017@\u0003,\u0015\u0011\u0005Z\u0013\u000b&?-/\f\u0012+\u0019\u0013*0*79ODQR\u0001\u0004\u0003\u0019%J\u0001zQ\u0002\f-'\t\u00127Z0\u0004\f\u000f@\u0017@\fF\t\u0014+\u0019\u0005\u0016\u0013]\t\u0014\u0005F?m*t\u0011r\u0011p\u001c$9sJt\u0011Bm*q\u0011sJt\u0011O$\u0015\u0011\u0001\n\t4\u0015D\u0007@+\u0006\t\u00127;\tX\u0005\u0016\u0003\u0019-/\u0003\u0019+\t4\u000eM\u000e\u0011\u0013\u0014+\u0019\f\u0013\u000b&iQ\u0002\u0013\u000b\u000e:0\u0004\u0005R\u0003\u0019\u0017Y\u0015\u0011\u0001@\fM\u0005\u0016\u0007\r\u0013\u001c\u001f\f\u000f\u0017Z+\u0006\t\u0014\u0017\u0004\u001e\u001c5\n\t\u000b\u001e\u001c\f\u001cA \u0004\"wc\u0017@\u0005\u001d\u0015\u0016\u000e\u00115\u0004%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005e\u0005\u00165\u0004\u001e\u001c\u001e\u0014\f\u0012\u0005\u001d\u0015\u0011\f30Y\u0015\u0011\u00135\u0004\u0005\u0016\f[\u0015uQa\u0013Z0\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015.\u001e\u000b\u000e:\t\u0014\u0007@\u0001\u0004\u0003\u0019%X\u0005\u0016\u0003\u0019\u001e\u001c\u0017@\u0005M\u0015\u0011\u0013Y<\r\f?0\u001b\u000e:\t\u0012QR\u0017W\t4\u0015.\u0015\u0011\u0001@\fE\f\u0012\u0017@0\u0010\u0013\u000b&e\t-[5@\u0005\u0016\u0003\u0019%3\t\u0014+6\u0007\u0004\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012OP\t]\u0005\u0016\u0003\u0019-/\u0007@+\u0019\f[\t\u0014\u0017@0H\t]0\u0004\u0013\u00145@<@+\u0019\fE<\n\t4\u000e.\u000e\u0011\f\u0012\u0005\u0016\u0007\r\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u000f+\u00197\u0010\u0003\u0019\u0017\u001b)0\u001b\u0003\u0006%\u0012\t\u000b\u0015\u0011\u0003\u0019\u0017@\u001e\u0010\u0015\u0011\u0001@\f'\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u0017@%\u0012\f'\u0013\u000b&.\t;\u0017@\u0013\u000b\u000e\u0011-'\t\u0014+g\u0013\u0014\u000eX\u0013\u000b&M\t\u0010\u0005\u001d\u0015\u0016\u000e\u0011\u0013\u0014\u0017@\u001e\u0010\u0005\u0016\f\u0012\u0007@\t\u000b\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017>-?5\u0004\u0005\u0016\u0003\u0006%\u0012\t\u0014+\\\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012A\n3.3SubjectBehavior\u0000\u0002\u0001\u0004\fF\u0004\u000e\u0011\u0005\u001d\u00153O\r95@\u0003,\u0015\u0011\f8\u0005\u00165\u001b\u000e\u0011\u0007\u0004\u000e\u0011\u0003\u0019\u0005\u0016\u0003\u0019\u0017@\u001e\u0004O@\u000e\u0011\f\u0012\u0005\u00165@+,\u0015DQe\t\u0014\u0005D\u0015\u0011\u0001\n\t4\u0015M-/\u0013\u000b\u000e\u0011\f.\u0015\u0011\u0001\n\t\u000b\u0017;\tX\u0001\n\t\u000b+\u0019&\u0013\u000b&P\u0015\u0011\u0001\u0004\fD\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u00051&2\u0013\u001c+\u0019+\u0019\u00133Q\u0002\f30/\u0015\u0011\u0001@\fD\u001e\u001c\u0003\u0019\u001a\u001f\f\u000f\u0017/\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005a\u0013\u0014\u0017@+,7X\u0007@\t\u000b\u000e\u0016\u0015\u0011\u0003\u0006\t\u0014+\u0019+,79O\u0015\u0011\u0001\u0004\u0013\u001c5@\u001e\u0014\u0001\"\u0015\u0011\u0001@\f\u000f7\b\u0007\u001b\u000e\u0011\u0013\u000b\u001a*\u0003\u00060\u0004\f\u00120W5@\u0005E\u0003\u0019\u0017\n0\u0004\u0003,\u000e\u0011\f\u0012%J\u0015\u0011+\u00197HQR\u0003,\u0015\u0011\u0001N\t>5@\u0005\u0016\f\u000f&I5@+g&I\f\u0012\f30\u001b<\n\t\u0014%:\rA\u0000\u0002\u0001\u0004\f/\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005E\u0001\n\t\u001c0\b\u0015\u0011\u0001@\fX\u0003\u0019-/\u0007@+\u0019\u0003\u0019%\u0012\u0003,\u0015[\t\u0014\u0005\u0016\u0005\u00165\u0004-/\u0007\u0004\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b\u0015\u0011\u0001\n\t\u000b\u0015E-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006%\n+\u0019\f\u000f(\u001b\u0003\u0019%3\t\u000b+M5\u0004\u0017@\u0003,\u0015\u0011\u0005/0\u0004\u0013\b\u0017@\u0013\u000b\u0015/\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u0014\u0007\\A}~\u001b\u0013\u001c-/\f]\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005\u0012O\u0002\u0003BA \f\u001cA \u0014\b\u0013\u00145\u0004\u0015/\u0013\u000b&\u0018\u0012.\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005\u0012O90\u0004\u0003\u0019\u0005\u001d\u000e\u0011\f\u0012\u001e\u001f\t4\u000e:0\u0004\f30?\u0015\u0011\u0001\u0004\u0003\u0019\u0005a\t\u000b\u0005\u0016\u0005\u00165@-/\u0007\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017/\t\u0014\u0017@0?\u0003\u0019\u0017\u001b\u001a\u001c\f\u0012\u00179\u0015\u0011\f30?\t.\u0017\u0004\f\u000fQ\u0005\u0016\u0003\u0019\u001e\u0014\u0017«?0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015a\t\u0014-/\u0013\u0014\u0017@\u001e8\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012O*<@5\u001b\u0015gQR\u0003,\u0015\u0011\u0001/\u0015\u0011\u0001@\fR\u0005\u0011\t\u000b-/\fD-/\f\u0012\t\u0014\u0017@\u0003\u0019\u0017\u0004\u001e\u0004O\t\u000b\u0005G%\u0012\u0013\u00145@+\u00060Z<\r\fF5@\u0017@0\u0004\f\u000f\u000e\u0011\u0005\u001d\u0015\u0011\u00139\u0013*0Y<*7]\u0015\u0011\u0001@\f\u000f\u0003\u0019\u000eD\f\u000f(\u001b\u0007\u0004+\t\u000b\u0017\n\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005D\u0013\u001c\u0017>\u0015\u0011\u0001\u0004\f.\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\u0005o«\u0015\u0011\u0001@\t\u000b\u0015`%\u0012+\u0019\f3\t4\u000e\u0011+,7X\u0003\u0019\u0017\n0\u001b\u0003\u0006%\u0012\t\u000b\u0015\u0011\f30F\u0015\u0011\u0001\n\t\u000b\u0015`\u0005\u0016\u0013\u0014-/\f\u0002\u0017@\u0013\u000b\u0015\u0011\f\u0012\u00056Q\u0002\f\u000f\u000e\u0011\fe<\r\u0013\u000b\u0015\u0011\u0001[\u0015\u0011\u0001\u0004\f\u0002+\u0006\t\u0014\u0005\u001d\u00151\u0013\u0014&$\t-[5@\u0005\u0016\u0003\u0019%3\t\u000b+\n\u0007@\u0001\u001b\u000e:\t\u0014\u0005\u0016\fM\t\u0014\u0017\n0?\u0015\u0011\u0001\u0004\fD\u0004\u000e\u0011\u0005\u001d\u0015a\u0013\u000b&P\u0015\u0011\u0001\u0004\fR\u0017@\f\u000f(*\u0015g\u0013\u001c\u0017@\f\u0014A¬@\u0013\u0014\u000e\u0002\t\u0014+\u0019+\n\u0015\u0011\u0001@\fR\u0005\u00165@<\u001b)Tu\f\u0012%\u000f\u0015\u0011\u0005\u0012O*\u0015\u0011\u0001@\fD\f\u0012\u001a\u001f\f\u000f\u0017*\u0015\u00115@\t\u0014+\u0004\u0013\u000b\u001a\u001c\f\u000f\u000e\u0011+\u0006\t\u0014\u0007/Qe\t\u000b\u0005\u0002\u0013\u0014&P\u0013\u001c\u0017\u0004+\u00197X\u0013\u001c\u0017@\fD\u0017@\u0013\u000b\u0015\u0011\fM+\u0019\f\u0012\u0017\u0004\u001e\u0014\u0015\u0011\u0001\\A1\u0000\u0002\u0001@\u0003\u0019\u0005\u000e\u0011\f\u000f\u0005\u00165@+,\u00151\u0003\u0006-/\u0007\u0004+\u0019\u0003\u0006\f\u000f\u0005i\u0015\u0011\u0001\n\t\u000b\u00153O\u0014&2\u0013\u000b\u000e1\u0015\u0011\u0001\u0004\f\u0012\u0005\u0016\f\u0002\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012O\u001c\u0015\u0011\u0001\u0004\f\u0002%\u0012\u0013\u0014\u0017@%\u0012\f\u0012\u0007\u001b\u0015`\u0013\u0014&\r-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%%\u000f\u0013\u001c\u00179\u0015\u0011\u0013\u001c5\u001b\u000e8%3\t\u0014\u0017\u0004\u0017@\u0013\u000b\u00158<\r\fX\t\u000b\u0007@\u0007@+\u0019\u0003\u0019\f30$OP5\u0004\u0017@+\u0019\f\u0012\u0005\u0016\u0005GQa\f[\u0015:\t\u000b\u001f\f?\u0003\u0019\u00179\u0015\u0011\u0013Z\t\u000b%\u0012%\u0012\u0013\u00145@\u00179\u0015.\u0015\u0011\u0001@\f&2\t\u0014%\u000f\u0015G\u0015\u0011\u0001\n\t\u000b\u0015G%\u0012\u0013\u001c\u00179\u0015\u0011\u0013\u00145\u0004\u000e\u0011\u0005G-'\t\u00127Y\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u0014\u0007\u0010\u0013\u0014&cOP\t\u000b\u0015.+\u0019\f3\t\u000b\u0005\u001d\u00153O$\u0013\u001c\u0017\u0004\f[\u0017\u0004\u0013\u0014\u0015\u0011\f\u001cA.~\u001b\u0003\u0019\u0017@%\u000f\f\u0015\u0011\u0001\u0004\u0003\u0019\u00051\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015a%\u0012\u0013\u00145@+\u00060X\u0017@\u0013\u000b\u0015a<\r\fR\u0003\u0019\u001e\u001c\u0017\u0004\u0013\u0014\u000e\u0011\f30$O\u001fQ\u0002\fD0\u001b\f\u0012%\u0012\u0003\u00060\u0004\f30?\u0015\u0011\u0013F0\u001b\f3\t\u0014+\u001bQR\u0003,\u0015\u0011\u0001X\u0015\u0011\u0001@\u0003\u0019\u0005\u0017\u0004\f\u000fQS*\u0003\u0006\u0017@0Y\u0013\u000b&`-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e3O\u0004\t\u000b\u0005M0\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f30Z\u0003\u0019\u0017>~\u001b\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005D\u001bA \u0019X\t\u0014\u0017@0Z*A \u00129AD\u0017@\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000eM\u000e\u0011\f\u0012\u0005\u00165@+,\u00158\u0003\u0019\u0005G\u0015\u0011\u0001@\t\u000b\u0015.\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005.\u001a\u001c\f\u000f\u000e\u00167>\u0005\u0016\f\u0012+\u00060\u001b\u0013\u001c- \u0001\u0004\u0003\u0019\u001e\u001c\u0001@+\u0019\u0003\u0019\u001e\u001c\u00019\u0015\u0011\f30;\u0015\u0011\u0001@\f\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\f8\u0013\u0014&1\t?\u0005\u001d\u0015\u0016\u000e\u0011\u0013\u0014\u0017@\u001eX<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u00167]<97Y0\u001b\u000e:\t\u0012QR\u0003\u0019\u0017@\u001e/\t?0\u0004\u0013\u001c5\u0004<@+\u0019\f8-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e3A\u0000\u0002\u0001\u0004\f\u0010\u0017\u001b5\u0004-?<\r\f\u000f\u000e'\u0013\u000b&E0\u0004\u0013\u00145@<@+\u0019\f\u0010-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e\u0011\u0005/\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u000f\u0017*\u0015'\u0015\u0011\u0001\u0004\f \u0010\u0004­ \u00190® \u0013\u0014&F\u0015\u0011\u0001@\f\u00134\u001a\u001f\fJ\u000e:\t\u0014+\u0019+.\u0017*5@-[<\r\f\u000f\u000e'\u0013\u0014&E-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e\u0011\u0005«f\u0003\u0006\u0017\u0004%\u0012+\u00195\n0\u001b\u0003\u0006\u0017\u0004\u001eN\t\u0014+\u0019\u0005\u0016\u0013^\u0015\u0011\u0001@\f;\u0013\u0014\u0017@\f\u0012\u0005Y5@\u0005\u0016\f30&I\u0013\u0014\u000eG\u0013\u000b\u001a\u001c\f\u000f\u000e\u0011+\u0006\t\u0014\u0007@\u0007\u0004\u0003\u0019\u0017@\u001e/\u0007@\u0001\u001b\u000e:\t\u0014\u0005\u0016\f\u0012\u0005\u0002«'\u0015\u0011\u0001*5@\u0005R\u0007\u001b\u000e\u0011\f\u0012\u001a\u001f\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e?&I\u0013\u0014\u000eG\t/95\n\t\u0014\u00179\u0015\u0011\u0003,\u0015:\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\f\t\u000b\u0017\n\t\u000b+\u00197\u001b\u0005\u0016\u0003\u0019\u0005G\u0013\u000b&6\u0005\u001d\u0015\u0016\u000e\u0011\u0013\u001c\u0017\u0004\u001e/\u0005\u0016\f\u0012\u0007\n\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0005M<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017>-?5\u0004\u0005\u0016\u0003\u0006%\u0012\t\u0014+\\\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012Aa\u0000\u0002\u0001@\u0003\u0019\u0005\u000e\u0011\f\u000f\u0005\u00165@+,\u0015X%3\t\u000b\u0017^<\r\f'\u0007\n\t\u000b\u000e\u0016\u0015\u0011\u0003\u0006\t\u000b+\u0006+,7^\fJ(\u0004\u0007\u0004+\u0006\t\u0014\u0003\u0019\u0017@\f30\"%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u00060\u001b\f\u000f\u000e\u0011\u0003\u0019\u0017@\u001e\u0010\u0015\u0011\u0001\n\t\u000b\u00153O`\u0003\u0019\u0017^-/\u0013\u0014\u0005\u001d\u0015%\u0012\t\u0014\u0005\u0016\f\u0012\u0005\u0012O\r-[5@\u0005\u0016\u0003\u0019%3\t\u000b+P\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0002Q\u0002\fJ\u000e\u0011\fF\u0015\u0011\u00139\u0013?\u0005\u0016\u0001@\u0013\u0014\u000e\u0016\u0015R\u0015\u0011\u0013/\t\u000b+\u0019+\u0006\u00133Qv\u0015\u0011\u0001@\f.\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017@%\u000f\f\u0013\u000b&R\u0005\u001d\u0015\u0016\u000e\u0011\u0013\u001c\u0017\u0004\u001e>\u0005\u0016\f\u0012\u0007\n\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0012AZw=\u0015F\u0003\u0006\u0005E+\u0019\u0003\u0019\u001f\f\u0012+,7\u0010\u0015\u0011\u0001\n\t\u000b\u0015F\u0015\u0011\u0001\u0004\fX-?5\u0004\u0005\u0016\u0003\u0006%\u000f\u0013\u001c+\u0019\u0013\u001c\u001e\u0014\u0003\u0006\u0005\u001d\u0015EQR\u0001@\u0013\u0005\u00165\u0004\u001e\u001c\u001e\u0014\f\u0012\u0005\u001d\u0015\u0011\f30}QR\u0001@\u0003\u0019%:\u0001}-[5@\u0005\u0016\u0003\u0019%ZQ\u0002\u0013\u000b\u000e\u0011*\u0005]\u0005\u0016\u0001\u0004\u0013\u001c5\u0004+0V<\r\f>5@\u0005\u0016\f30N&2\u0013\u000b\u000eY\u0015\u0011\u0001\u0004\fZ\u0015\u0011\f\u0012\u0005\u001d\u00153O0\u001b\f\u0012%\u0012\u0003\u00060\u0004\f\u00120]\u0015\u0011\u0013[\u0015\u0016\u000e\u00115\u0004\u0017@%3\t4\u0015\u0011\fM\u0015\u0011\u0001\u0004\fG\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015e\u0003\u0019\u0017Z%\u0012\u0013\u0014\u0003\u0019\u0017@%\u0012\u0003\u00060\u0004\f\u000f\u0017@%\u0012\fGQR\u0003,\u0015\u0011\u0001]\u0015\u0011\u0001@\fM\u0004\u000e\u0011\u0005\u001d\u0015\u0005\u001d\u0015\u0016\u000e\u0011\u0013\u0014\u0017@\u001e\b\u0005\u0016\f\u0012\u0007\n\t4\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e3A\"C\"\fY0\u0004\f\u0012%\u000f\u00030\u001b\f30\"\u0015\u0011\u0013\u0010\u0017@\u0013\u0014\u0015X0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015\u0011\u0003\u0006\t\u000b\u0015\u0011\f]<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u0012\u00170\u001b\u0013\u001c5\u0004<@+\u0019\f8\u0013\u0014\u000eD\u0005\u0016\u0003\u0019\u0017@\u001e\u0014+\u0006\fF-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0012A\n3.4Algorithm\u0000\u0002\u0001\u0004\fG-'\t\u000b\u0003\u0006\u0017Y\t\u0014\u0003\u0019- \u0013\u0014&i\u0013\u00145\u0004\u000eg\u000e\u0011\f\u0012\u0005\u0016\f3\t\u000b\u000e\u0011%:\u0001YQa\u0013\u0014\u000e\u0011XQ\u0002\t\u0014\u0005\u0002\u0015\u0011\u00138\u0015\u0011\f\u0012\u0005\u001d\u0015\u0002\u0015\u0011\u0001\u0004\fG\u000197\u001b\u0007\r\u0013\u0014\u0015\u0011\u0001@\fJ)\u0005\u0016\u0003\u0019\u0005R\u0015\u0011\u0001\n\t4\u0015D\t?-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%.&2\f\u0012\t\u000b\u0015\u00115\u0004\u000e\u0011\fJ)c<@\t\u0014\u0005\u0016\f30Z\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004- \u0007\r\f\u000f\u000e\u0016&2\u0013\u000b\u000e\u0011-/\u0005D-/\f\u000f+\u0006\u0013*0*7\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017f-/\u0013\u000b\u000e\u0011\f/\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u000f+\u00197H\u0015\u0011\u0001\n\t\u000b\u0017N\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005[\u0015\u0011\u0001@\t\u000b\u0015?0\u0004\u0013;\u0017@\u0013\u000b\u00155\u0004\u0005\u0016\f;-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%Z&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005\u0012AS\u0000h\u0013^0\u001b\u0013H\u0015\u0011\u0001\n\t4\u00153O\u0002Q\u0002\f;\u0017\u0004\f\u0012\f30N\u0013\u001c\u0017\u0004\f\u0010\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-\t\u000b\u0005G\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\fF\u0013\u0014&`\u0015\u0011\u0001\u0004\fE%\u000f+\t\u000b\u0005\u0016\u0005.\u0013\u0014&g-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%8&2\f\u0012\t\u000b\u0015\u00115\u0004\u000e\u0011\fJ)c<@\t\u0014\u0005\u0016\f30\u0010\t\u000b+\u0006\u001e\u0014\u0013\u0014)\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-/\u0005\u0012A\u0010\u0000\u0002\u0001@\fY\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@-_Q\u0002\f'\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\f30\"\u0003\u0019\u0005[<@\t\u0014\u0005\u0016\f30^\u0013\u0014\u0017N\t>-/\u0013*0\u0004\f\u0012+a0\u00045@\f\u0015\u0011\u0013 2\u0002\t\u0014-[<\r\u0013\u001c5\u0004\u000e\u0011\u0013\u0014\u0007\r\u0013\u001c5\u0004+\u0006\u0013\u0014\u00058 \u000bO\u0004QR\u0001\u0004\u0013[\u0007\u001b\u000e\u0011\u0013\u001c\u0007\r\u0013\u0014\u0005\u0016\f30]\u0015\u0011\u0001\u0004\f \tP\u00139%\u0012\t\u0014+ \u000ba\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0000G\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017HjZ\u0013*0\u001b\f\u0012+a \t1\u000b \u0000Fj;JA$\u0000\u0002\u0001\u0004\fE<@\t\u0014\u0005\u0016\u0003\u0019%E\u00030\u001b\f3\tX\u0013\u0014&8\t1\u000b \u0000Fjb\u0003\u0006\u0005D\u0015\u0011\u0001\n\t4\u0015\t/+\u0019\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017@\fJ\u000e.\u0007\r\f\u000f\u000e\u0011%\u0012\f\u000f\u0003\u0006\u001a\u001c\f\u0012\u0005D\u0015\u0011\u0001\u0004\f8\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u000f\u0017@%\u0012\fE\u0013\u0014&1\t/<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u00167Y\u0003\u0006\u0017\u0010\tX-/\f\u000f+\u0006\u0013*0*7QR\u0001\u0004\f\u0012\u0017@\f\u000f\u001a\u001f\f\u000f\u000e8\u0015\u0011\u0001@\fJ\u000e\u0011\f/\t\u000b\u000e\u0011\fX%:\u0001\n\t\u000b\u0017@\u001e\u0014\f\u0012\u00058\u000e\u0011\f\u0012\u001e\u001f\t4\u000e:0\u0004\u0003\u0019\u0017@\u001eZ\u0015\u0011\u0001\u0004\f?-[5@\u0005\u0016\u0003\u0019%3\t\u0014+`\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0011\u001a\u0014\t\u0014+\u0019\u0005\t\u000b\u0017\n0W\u0015\u0011\u0001\u0004\f]\u0017\u0004\u0013\u0014\u0015\u0011\f]0\u00045\u001b\u000e:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012AWjZ\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%Y<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005?\t\u000b\u000e\u0011\f'5@\u0017\u0004%\u0012\f\u000f\u000e\u0016\u0015:\t\u000b\u0003\u0006\u0017\f\u000f\u001a\u001f\f\u0012\u00179\u0015\u0011\u00056<\r\f\u0012%3\t\u000b5@\u0005\u0016\f\u0002\u0005\u0016\u0013\u0014-/\f\u0002+\u0019\u0003\u0006\u0005\u001d\u0015\u0011\f\u000f\u0017@\f\u000f\u000e\u0011\u0005`\u0007\r\f\u000f\u000e\u0011%\u0012\f\u000f\u0003\u0006\u001a\u001c\fg\u0015\u0011\u0001@\f\u000f->O\u000bQR\u0001\u0004\u0003\u0019+\u0006\fe\u0005\u0016\u0013\u0014-/\f\u0002\u0013\u0014\u0015\u0011\u0001\u001b)\fJ\u000e\u0011\u0005;0\u0004\u0013N\u0017@\u0013\u000b\u0015\u001a«f\u0015\u0011\u0001@\u0003\u0019\u0005Y\u0003\u0006\u0005Z\u0003\u0006\u0017x%\u0012\u0013\u0014\u00179\u0015\u0016\u000e:\t\u0014\u0005\u001d\u0015ZQR\u0003,\u0015\u0011\u0001x\u0015\u0011\f\u000f(*\u0015\u00115\n\t\u000b+.0@\t4\u0015:\t\u001bOMQR\u0001@\u0003\u0019%J\u0001\u0003\u0019\u0017\u0004%\u0012+\u00195\n0\u0004\fE\u0005\u0016\f\u0012\u0007@\t\u000b\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u00058<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u0017>\u0015\u0011\u0013\u0014\u001f\f\u0012\u0017@\u0005D\u0015\u0011\u0001\n\t4\u0015.%3\t\u000b\u0017\b<\r\fE0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\f30\u00100\u001b\f\u000f)\u0015\u0011\fJ\u000e\u0011-/\u0003\u0006\u0017\u0004\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0006%\u0012\t\u0014+\u0019+,7V2Qa\fX\t4\u000e\u0011\fX\t\u0012Qe\t4\u000e\u0011\f?\u0015\u0011\u0001\n\t4\u00158\u0005\u0016\u0013\u001c-/\f\u000f\u0015\u0011\u0003\u0019-/\f\u0012\u0005G\t\u001c0\u00040\u0004\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\n\t\u000b+`\u0003\u0006\u0017\u001b)\u0015\u0011\f\u000f+\u0006+\u0019\u0003\u0019\u001e\u001c\f\u000f\u0017@%\u0012\fF\u0003\u0019\u0005R\u0017@\f\u0012\f30\u001b\f30]\u0015\u0011\u0013X0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015eQ\u0002\u0013\u0014\u000e:0\u001b\u0005R\u0003\u0006\u0017\u0004%\u0012+\u00195\n0\u001b\u0003\u0006\u0017\u0004\u001e?\u0005\u001d7\u001b-?<\r\u0013\u0014+\u0019\u0005\u0012O@\u0005\u00165\u0004%J\u0001\t\u000b\u0005Z0\u001b\u0013\u0014\u0015\u0011\u0005:JA \u000ba\f\u0012%3\t\u000b5@\u0005\u0016\f\u0010\u0013\u0014&8\u0015\u0011\u0001\u0004\u0003\u0006\u0005]5@\u0017\u0004%\u0012\f\u000f\u000e\u0016\u0015:\t\u000b\u0003\u0006\u00179\u0015c79O\u0002\u0015\u0011\u0001@\f \t1\u000b \u0000Fj 0\u0004\f\u000f\u0015\u0011\f\u000f%\u000f\u0015\u0011\u0005<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005h<*7.\u001e\u001c\u0003\u0019\u001a*\u0003\u0006\u0017\u0004\u001eG\teQ\u0002\f\u000f\u0003\u0006\u001e\u0014\u00019\u00156\u0015\u0011\u0013M\t\u000b+\u0019+*\u0015\u0011\u0001\u0004\fg\u0007\r\u0013\u0014\u0005\u0016\u0005\u0016\u0003\u0019<@+\u0019\f\u0002\u0007@+\u0006\t\u000b%\u0012\f\u0012\u0005iQR\u0001@\fJ\u000e\u0011\f\tD<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u001678-'\t\u00127.\u00139%\u0012%\u000f5\u0004\u000e3A`\"Q\u0002\f\u000f\u0003\u0006\u001e\u0014\u00019\u00156\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u000f\u0017*\u0015\u0011\u0005P\u0015\u0011\u0001@\fg0\u0004\f\u0012\u001e\u000b\u000e\u0011\f\u0012\f\u0002\u0013\u0014&\u00045@\u0017\u001b)%\u000f\f\u000f\u000e\u0016\u0015:\t\u0014\u0003\u0019\u00179\u0015u78\u0013\u000b&\u0004\u0015\u0011\u0001\u0004\f`\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\f1\u0013\u0014&\u001b\u0015\u0011\u0001@\f1%\u0012\u0013\u0014\u000e\u0016\u000e\u0011\f\u0012\u0005\u0016\u0007\r\u0013\u0014\u0017\n0\u001b\u0003\u0006\u0017\u0004\u001eR<\r\u0013\u001c5@\u0017@0@\t4\u000e\u001679Ai\u0000\u0002\u0001\u0004\f<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005D%3\t\u000b\u0017;<\r\f[0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\f\u00120Z<97Z\t\u000b\u0017\n\t\u000b+\u00197\u001by\u0012\u0003\u0019\u0017\u0004\u001e'\u0015\u0011\u0001\u0004\f.Q\u0002\f\u0012\u0003\u0019\u001e\u0014\u0001*\u0015\u0011\u0005D\u0015\u0016\u000e\u0011\f\u0012\u0017\n0$|2\u0002\t\u000b-?<\r\u0013\u00145\u0004\u000e\u0011\u0013\u001c\u0007\r\u0013\u00145@+\u0019\u0013\u001c\u0005/\u0007\u001b\u000e\u0011\u0013\u001c\u0007\r\u0013\u0014\u0005\u0016\f30N\u0015\u0011\u0001@\t\u000b\u0015[\u0015\u0011\u0001@\f\u000f7N\t4\u000e\u0011\f>\t\u000b\u0005\u0016\u0005\u0016\u00139%\u0012\u0003\u0006\t\u000b\u0015\u0011\f30V\u0015\u0011\u0013\u0010\u0015\u0011\u0001@\f\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\f'\u0013\u0014&R+\u0019\u00139%3\t\u0014+g-'\t\u000b(\u001b\u0003\u0019-'\tY\u0003\u0019\u0017W\u0015\u0011\u0001\u0004\fXQ\u0002\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015F&25\u0004\u0017@%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017PA]C\"\f/\u0001\n\t3\u001a\u001f\f\u0015\u0011\u0001\u0004\f\u0012\u0017]0\u0004\f\u0012\u001a\u001c\f\u0012+\u0019\u0013\u001c\u0007\r\f30'\t\u0014\u0017]\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@- <97X\u0003\u0019-/\u0007@+\u0019\f\u0012-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e.\u0015\u0011\u0001\u0004\f3\t1\u000b \u0000Fj \u0003\u0006\u0017\u0015\u0011\fJ\u000e\u0011-/\u0005e\u0013\u0014&i-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%.&2\f3\t4\u0015\u00115\u0004\u000e\u0011\f\u0012\u0005R\t\u000b\u0017\n0]\u000e\u00115\u0004+\u0019\f\u0012\u0005e\u0015\u0011\u0013?%\u0012\u0013\u0014-/\u0007@5\u0004\u0015\u0011\fR\u0015\u0011\u0001@\fMQ\u0002\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0005\u0012ACW\f[\u0001@\t3\u001a\u001f\f[\fJ(\u001b\u0015\u0011\f\u000f\u0017\n0\u0004\f\u00120Z\u0015\u0011\u0001@\fE\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@-b\t\u00140@0\u0004\u0003\u0019\u0017\u0004\u001e'&I\u0013\u001c5\u001b\u000e.\u0017@\u0013\u000b\u000e\u0011-'\t\u0014+\u0019\u0003\u0019y3\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017+\u0019\f\u0012\u001a\u001c\f\u0012+\u0019\u0005D\u0015\u0011\u0013X\u0015\u00115\u0004\u0017@\fG\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+P\u0007\u0004\u000e\u0011\f\u0012%\u000f\u0003\u0006\u0005\u0016\u0003\u0019\u0013\u0014\u0017\u0010\t\u0014\u0017\n0]\u000e\u0011\f\u0012%3\t\u0014+\u0019+BAR~*\u0007\r\f\u0012%\u0012\u0003,\n%\u0012\t\u0014+\u0019+,79O@\u0015\u0011\u0001@\f\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-U%\u0012\t\u0014\u0017W%\u000f\u0013\u001c-?<\u0004\u0003\u0019\u0017\n\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0003\u0006\t\u0014+\u0019+,7\b\u0015\u0016\u000e:\t\u000b\u0017@\u0005\u0016\u0007\r\u0013\u0014\u0005\u0016\f/\u0007@\u0003,\u0015\u0011%J\u0001\u0004\f\u0012\u0005\u0012OP\u0017@\u0013\u0014\u000e\u0011-'\t\u000b+\u0019\u0003\u0006y\u000f\f0\u001b5\u0004\u000e:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005\u0012OD\u0017@\u0013\u000b\u000e\u0011-'\t\u0014+\u0019\u0003\u0019y\u0012\f;\u0007@\u0003,\u0015\u0011%:\u0001@\f\u0012\u0005\u0012OD\t\u000b\u0017\n0V\u000e\u0011\f\u000f-/\u0013\u000b\u001a\u001c\f;0\u00045\u001b\u000e:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012A°¯6\u000e\u0011\f\u000f)\u001a*\u0003\u0019\u0013\u001c5\u0004\u0005Y\fJ(\u0004\u0007\r\fJ\u000e\u0011\u0003\u0006-/\f\u000f\u0017*\u0015\u0011\u0005/\u0005\u0016\u0001\u0004\u00133Q \u0015\u0011\u0001@\t\u000b\u0015'\u0015\u0011\u0001@\f;\u0017\u0004\u0013\u0014\u000e\u0011-'\t\u000b+\u0006\u0003\u0019y3\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017z-/\f\u000f\u0015\u0011\u0001\u0004\u0013*0V\u0003\u0019\u0005%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u00179\u0015EQR\u0003,\u0015\u0011\u0001\"QR\u0001\n\t\u000b\u0015E\u0013\u0014\u0017@\fXQ\u0002\u0013\u00145@+\u00060\"\f\u000f(\u001b\u0007\r\f\u0012%\u000f\u0015E\u0005\u0016\u0003\u0019\u0017@%\u000f\f/\u0001@\u0003\u0019\u001e\u001c\u0001^+\u0019\f\u0012\u001a\u001f\f\u000f+\u0006\u0005F\u0013\u000b&\u0017\u0004\u0013\u0014\u000e\u0011-'\t\u000b+\u0006\u0003\u0019y3\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017?\u0007\u001b\u000e\u0011\u0013*0\u00045\u0004%\u0012\fa\u0001\u0004\u0003\u0006\u001e\u0014\u0001E\u000e\u0011\f\u000f%3\t\u0014+\u0019+BO9\t\u000b\u0005i\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30F\u0003\u0019\u0017[-'\t\u000b\u0017*7G\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e\u0003\u0019\u0017\u001b&2\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017H\u000e\u0011\fJ\u0015\u0016\u000e\u0011\u0003\u0006\f\u000f\u001a\u001c\t\u000b+1\fJ(\u0004\u0007\r\fJ\u000e\u0011\u0003\u0006-/\f\u000f\u0017*\u0015\u0011\u0005\u0012A \u0000.\f\u000f\u0015:\t\u0014\u0003\u0019+\u0019\u00058\u0013\u0014&g\u0015\u0011\u0001@\f?\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-\t4\u000e\u0011\f8\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\f30]\u0003\u0019\u0017\",\u0018\u0017\u000bA\n3.5Modeling Boundariesw=\u0015Z\u0003\u0019\u0005]Q\u0002\u0013\u0014\u000e\u0016\u0015\u0011\u0001S0\u001b\u0003\u0006\u0005\u001d\u0015\u0011\u0003\u0019\u0017\u0004\u001e\u001c5@\u0003\u0019\u0005\u0016\u0001\u0004\u0003\u0006\u0017\u0004\u001eN<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u0017z\u0015\u0011\u0001@\f\u0010\u0017@\u0013\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005Y\u0013\u0014&E-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e3O<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u001679OP\t\u000b\u0017\n0H\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017PAXCW\f/0\u0004\fJ\n\u0017@\f?\t\u0014\u0005//p\u000bq5±\u001fsJq'\u0015\u0011\u0001@\f[\u0007\r\f\u000f\u000e\u0011\u0005\u0016\u0013\u001c\u0017@\t\u0014+%:\u0001@\u0013\u0014\u0003\u0019%\u0012\f^\u0013\u0014&'\tf\u001e\u001c\u0003\u0019\u001a\u001f\f\u0012\u0017v\u0005\u00165@<*T\u001d\f\u0012%J\u0015H\u0013\u000b&X\u0001\u0004\u0003\u0006\u001e\u0014\u0001@+\u0019\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0003\u0019\u0017@\u001ez\tf<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u001679Abk\u0011l\u0014m*n\no\u001fp\u0014q6\u0006Z\u0003\u0019\u0005e\u0015\u0011\u0001@\f \u0002c\u0007\n\t\u000b\u000e:\t\u000b-/\f\u000f\u0015\u0011\f\u000f\u000e\u0005\u0004X\u0005\u0016\u0003\u0019\u001e\u0014\u0017\n\t\u0014+\u0019\u0003\u0019\u0017@\u001e/\u0015\u0011\u0001\u0004\f.&B\t\u0014%J\u0015M\u0015\u0011\u0001@\t\u000b\u0015D\tX-?5\u0004\u0005\u0016\u0003,)AComparison ofManualandAutomaticMelodySegmentation%3\t\u000b+\r\u0007@\u0001\u001b\u000e:\t\u0014\u0005\u0016\fD\u0003\u0006\u0005a\f\u0012\u0017\n0\u001b\u0003\u0006\u0017\u0004\u001eE\t\u0014\u0017\n0?\u0015\u0011\u0001\u0004\fM\u0005\u00165\u0004<@\u0005\u0016\f395\u0004\f\u0012\u00179\u0015a\u0007\u0004\u0001\u0004\u000e:\t\u0014\u0005\u0016\fD\u0003\u0019\u0005a<\r\f\u0012\u001e\u0014\u0003\u0019\u0017@\u0017@\u0003\u0019\u0017\u0004\u001e\u0013\u001d\u0017\u0004\u0013\u0014\u0015\u0011\f?\u0015\u0011\u0001@\t\u000b\u00158\u0015\u0011\u0001@\fJ\u000e\u0011\fX-/\u0003\u0019\u001e\u001c\u00019\u00158<\r\f?-/\u0013\u000b\u000e\u0011\f?\u0015\u0011\u0001\n\t\u000b\u0017H\u0013\u0014\u0017@\fX\u0017\u0004\u0013\u0014\u0015\u0011\f/\t4\u0015FQR\u0001@\u0003\u0019%J\u0001\b\u0015\u0011\u0001@\f<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u00167W-/\u0003\u0006\u001e\u0014\u00019\u0015X\u00139%\u0012%\u00125\u001b\u000e3A^ <\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u00167^\u0017\u0004\f\u0012\f30W\u0015\u0011\u0013\b<\r\f'\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019-'\t\u000b\u0015\u0011\f30\u0005\u0016\u0003\u0019\u0017@%\u000f\fY\u0003,\u0015E\u0003\u0019\u0005?5@\u0017\u0004*\u0017@\u00133QR\u0017\"\t>\u0007\u001b\u000e\u0011\u0003\u0006\u0013\u000b\u000e\u0011\u0003BOa\t\u000b\u0017\n0\"\t>-'\t4\u000e\u0011\u001f\f\u000f\u000eE\u0003\u0019\u0005E\u0015\u0011\u0001\u0004\f/\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30\u0005\u001d7\u001b-?<\r\u0013\u0014+\u001f<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u0012\u0017F\u0015uQa\u0013M\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005\u0012O4QR\u0001@\u0003\u0019%:\u0001[%3\t\u000b\u0017F<\r\fa5@\u0005\u0016\f\u001208\u0015\u0011\u0013D\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019-'\t\u000b\u0015\u0011\f<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012Ag\u000f!\u001bl\u0014t\u0011r2Brl\u0014n\u0010\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0]\t[\u0017@\u0013\u000b\u0015\u0011\f.%3\t\u000b\u0017Y\u00139%\u0012%\u000f5\u0004\u000e6T\u001d5\u0004\u0005\u001d\u0015R<\r\f\u000f&2\u0013\u000b\u000e\u0011\f\u001cOTu5@\u0005\u001d\u00158\t4&I\u0015\u0011\f\u000f\u000e3O$\u0013\u000b\u000eG\fJ(@\t\u000b%\u000f\u0015\u0011+,7>\u00134\u001a\u001f\f\u000f\u000eD\u0015\u0011\u0001@\fF\u0017\u0004\u0013\u0014\u0015\u0011\f;\u001d\n\u0015\u0011\u0001*5\u0004\u0005\u0012O$\tX\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0010\u0003\u0019\u0005.\t\u000b\u0005\u0016\u0005\u0016\u0013\u0014)%\u0012\u0003\u0006\t4\u0015\u0011\f30HQR\u0003,\u0015\u0011\u0001W\u0015uQa\u0013Z\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0012Oh\u0005\u0016\u0003\u0006\u0017\u0004%\u0012\fX\u0015\u0011\u0001@\f?\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017^<\r\f\u0012\u0003\u0019\u0017@\u001eET\u001d5\u0004\u0005\u001d\u0015?\t4&I\u0015\u0011\f\u000f\u000e[\t\u0017\u0004\u0013\u0014\u0015\u0011\f.%\u0012\u0013\u0014\u0003\u0019\u0017@%\u0012\u0003\u00060\u0004\f\u000f\u0005\u0002QR\u0003\u0019\u0015\u0011\u0001]\u0015\u0011\u0001@\fM\u0013\u001c\u0017@\fgT\u001d5@\u0005\u001d\u0015R<\r\fJ&2\u0013\u0014\u000e\u0011\fM\u0015\u0011\u0001@\fG\u0005\u00165\u0004<@\u0005\u0016\f395\u0004\f\u0012\u00179\u0015e\u0017@\u0013\u000b\u0015\u0011\f\u001cA \u0007\u0004\u000e\u0011\f\u0012+\u0019\u0003\u0019-/\u0003\u0019\u0017\n\t\u000b\u000e\u00167H95\n\t\u000b+\u0019\u0003\u0019\u0015:\t4\u0015\u0011\u0003\u0019\u001a\u001f\f'\t\u0014\u0017@\t\u0014+,7\u001b\u0005\u0016\u0003\u0006\u0005E\u0013\u0014&a\u0015\u0011\u0001@\f?\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\u00058Qa\f/%\u0012\u0013\u001c+\u0019+\u0019\f\u0012%\u000f\u0015\u0011\f30&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001@\f.\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005D\u0005\u0016\u0001@\u00133Q\u0002\f\u00120]\u0015\u0011\u0001\n\t4\u00153|\u0014AD~\u001b\u0013\u0014-/\f6-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%g\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u00051l\u0001\u0000\u0014sJqJ\u0019p !\nO\u000b\u0015\u0011\u0001\n\t\u000b\u0015\\\u0003\u0019\u0005\u0012O\u000b\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005h%\u0012+\u0019\f3\t4\u000e\u0011+\u00197\t\u000b\u0015\u0016\u0015\u0016\u000e\u0011\u0003\u0019<@5\u001b\u0015\u0011\f30'\u0005\u0016\u0013\u001c-/\fD\u0017@\u0013\u0014\u0015\u0011\f\u000f\u0005\u0002<\r\u0013\u0014\u0015\u0011\u0001'\u0015\u0011\u0013?\tF-?5\u0004\u0005\u0016\u0003\u0019%3\t\u0014+\r\u0007\u0004\u0001\u0004\u000e:\t\u0014\u0005\u0016\fG\t\u0014\u0017@0'\u0015\u0011\u0013\u0015\u0011\u0001@\f.\u0005\u00165\u0004<@\u0005\u0016\f395\u0004\f\u0012\u00179\u0015R\u0013\u001c\u0017@\f\u0014O\n\t\u000b\u0005D\fJ(\u0004\u0007\u0004+\u0006\t\u0014\u0003\u0019\u0017@\f30Y\u0003\u0006\u0017>~\u001b\f\u0012%J\u0015\u0011\u0003\u0006\u0013\u0014\u0017;\u001bA \u0011\u001d9AR\u0000\u0002\u0001@\f\u000f\u000e\u0011\f'Qe\t\u000b\u0005/\t\u0010\u001e\u001c\u00139\u0013*0f\t\u0014\u001e\u0014\u000e\u0011\f\u000f\f\u0012-/\f\u0012\u00179\u0015X\t\u000b-/\u0013\u001c\u0017\u0004\u001e\u0010\u0015\u0011\u0001\u0004\f\u0012-_\u0003\u0019\u0017f\u0007\u0004+\u0006\t\u0014%\u0012\u0003\u0019\u0017@\u001e-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0011\u00058p\u0014q\u0016l\u000bm\u001bn\no]\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0012O\u001b\u0015\u0011\u0001\n\t4\u0015e\u0003\u0019\u0005\u0002\u0015\u0011\u0001@\f\u000f\u000e\u0011\fDQ\u0002\f\u000f\u000e\u0011\f8\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0002\t\u000b\u000e\u0011\u0013\u001c5\u0004\u0017\n0QR\u0001@\u0003\u0019%:\u00018-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\\\u0007@+\u0006\t\u000b%\u0012\f30.<97.0\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015h\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005P%\u0012\u0013\u001c\u0017\u0004%\u0012\f\u0012\u00179\u0015\u0016\u000e:\t\u000b\u0015\u0011\f30$O\u000e:\t\u000b\u0015\u0011\u0001\u0004\f\u000f\u000eR\u0015\u0011\u0001\n\t\u0014\u0017Z\t\u000b\u0015D\f\u000f(\u0004\t\u0014%J\u0015M\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005\u0012A\u0000\u0002\u0001\u0004\f/\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\fX\u0013\u000b&e\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u0014\u0007\u0004\u0007@\u0003\u0019\u0017@\u001e>\u0007\u0004\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u00058%\u0012\u0013\u001c\u0017\u001b@\u000e\u0011-/\u0005.\u0015\u0011\u0001\u0004\fX\u000197\u0004\u0007\r\u0013\u000b\u0015\u0011\u0001@\f\u000f)\u0005\u0016\u0003\u0019\u0005.\u0015\u0011\u0001\n\t4\u00158-/\f\u0012+\u0019\u0013*0\u001b7>\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017H\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015\u0011\u00058\t'%J\u0001@\t\u000b\u000e:\t\u000b%\u000f\u0015\u0011\f\u000f\u000e\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019%?\u0015\u0011\u0001\n\t4\u0015\u0003\u0019\u0005?\u0017\u0004\u0013\u0014\u000e\u0011-'\t\u000b+\u0006+,7H\u0017@\u0013\u000b\u0015E\u0015:\t\u000b\u001f\f\u0012\u0017^\u0003\u0019\u00179\u0015\u0011\u0013;\t\u0014%\u000f%\u0012\u0013\u001c5\u0004\u0017*\u0015EQR\u0001@\f\u000f\u0017^\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001eZ\u0013\u0014\u0015\u0011\u0001@\fJ\u000e-/\f30\u001b\u0003\u0006\t\u001bOi+\u0019\u0003\u0019\u001f\fX\u001a*\u0003\u00060\u0004\f\u000f\u0013>\u0013\u0014\u000eF\u0015\u0016\u000e:\t\u0014\u0017\u0004\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f30H\u0005\u0016\u0007\r\f\u000f\f\u0012%J\u0001\\A'wc\u0015F\u0003\u0019\u0005F\u0003\u0019-/\u0007\r\u0013\u0014\u000e\u0016\u0015:\t\u0014\u00179\u00158\u0015\u0011\u0013\u0017\u0004\u0013\u0014\u0015\u0011\fe\u0015\u0011\u0001\n\t4\u0015g\u0015\u0011\u0001\u0004\fD\u0007\r\u0013\u0014\u0005\u0016\u0005\u0016\u0003\u0019<@\u0003\u0019+\u0019\u0003\u0019\u0015c7/\u0013\u0014&\\\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u000b\u0007@\u0007@\u0003\u0019\u0017\u0004\u001e8\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005a\u0003\u0019\u0005g%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u00030\u001b\f\u000f\u000e\u0011\f30<97V-?5@\u0005\u0016\u0003\u0019%Z\u0015\u0011\u0001\u0004\f\u0012\u0013\u0014\u000e\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0005H\u0019\u0012\u000bF\t\u000b\u0017\n0V\u0015\u0011\u0001\u0004\f;-/\u0013\u0014\u0015\u0011\u0003\u0019\u001a\u0014\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&F\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u0014\u0007\u0004\u0007@\u0003\u0019\u0017@\u001e\u0007\u0004\u0001\u0004\u000e:\t\u0014\u0005\u0016\f\u000f\u0005e\u0001\n\t\u0014\u0005a<\r\f\u0012\f\u0012\u0017'\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\f30'<97/-/\u0013\u0014\u000e\u0011\fD\u0015\u0011\u0001\n\t\u000b\u0017Y\u0001@\t\u0014+,&i\u0013\u000b&h\u0015\u0011\u0001@\fD\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005\u0003\u0019\u0017N\u0015\u0011\u0001@\f\u0012\u0003,\u000e'%\u0012\u0013\u0014-/-/\f\u0012\u00179\u0015\u0011\u0005\u0012AN\u0000\u0002\u0001@\f>\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\fZ\u0013\u000b&.\u00134\u001a\u001f\f\u000f\u000e\u0011+\u0006\t\u0014\u0007\u0004\u0007@\u0003\u0019\u0017@\u001eH\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005-'\t\u00127\u0010<\r\f?\u000e\u0011\f\u0012+\u0006\t4\u0015\u0011\f30H\u0015\u0011\u0013Z\u0015\u0011\u0001\u0004\f?&B\t\u000b%\u000f\u0015F\u0015\u0011\u0001\n\t\u000b\u00158\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\f?Q\u0002\t\u0014\u0005E\t\u0014\u0017H\u0001@\u0003\u0019\u001e\u001c\u0001\u0004\f\u000f\u000eE\t\u0014\u001e\u000b\u000e\u0011\f\u0012\f\u000f)-/\f\u0012\u00179\u0015M\t\u0014-/\u0013\u0014\u0017@\u001e/\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005G\u0003\u0019\u0017;\u0007\u0004+\t\u000b%\u0012\u0003\u0019\u0017@\u001e'-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e\u0011\u0005G\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0>\u0017@\u0013\u000b\u0015\u0011\f\u0012\u0005\u0012ARwc&g\t\u0017\u0004\u0013\u0014\u0015\u0011\fa%\u0012\t\u0014\u0017?<\r\f\u000f+\u0006\u0013\u0014\u0017@\u001eD\u0015\u0011\u0013D\u0015cQ\u0002\u0013D\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012O\u001f\u0003,\u00156\u0003\u0019\u00056+\u0019\u0003\u0019\u001f\f\u0012+,78\u0015\u0011\u0001\n\t4\u00156\u0005\u0016\u0013\u001c-/\f1\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005QR\u0003\u0019+\u0019+`\t\u0014\u0005\u0016\u0005\u0016\u0003\u0019\u001e\u001c\u0017\b\u0003,\u0015M\u0015\u0011\u0013'<\r\u0013\u000b\u0015\u0011\u0001;\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u0012\u0005\u0012O\rQR\u0001@\u0003\u0019+\u0019\fE\u0005\u0016\u0013\u001c-/\fF\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\u0005MQR\u0003\u0019+\u0019+`\t\u0014\u0005\u0016\u0005\u0016\u0003\u0019\u001e\u001c\u0017\u0003,\u0015D\u0013\u001c\u0017\u0004+\u00197]\u0015\u0011\u0013?\u0015\u0011\u0001\u0004\fG@\u000e\u0011\u0005\u001d\u0015R\u0013\u000b\u000eD\u0015\u0011\u0013[\u0015\u0011\u0001@\f.\u0005\u0016\f\u0012%\u000f\u0013\u001c\u0017\n0Y\u0007@\u0001\u0004\u000e:\t\u000b\u0005\u0016\f\u001cAG<*\u001a*\u0003\u0019\u0013\u001c5@\u0005\u0016+,79OP\u0003\u0019\u0017W\u001e\u001c\f\u000f\u0017@\f\u000f\u000e:\t\u000b+1\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\fX\u0003\u0019\u0005E\tY+\u0019\u00133Q\u0002\f\u000f\u000e[\t\u0014\u001e\u000b\u000e\u0011\f\u0012\f\u0012-/\f\u0012\u00179\u0015F\u0003\u0019\u0017W\u0007\u0004+\u0006\t\u0014%\u0012\u0003\u0019\u0017@\u001e-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u00056\t4\u0015`\f\u000f(\u0004\t\u0014%\u000f\u0015`\u0007\r\u0013\u0014\u0005\u0016\u0003\u0019\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u00056\u0015\u0011\u0001\n\t\u000b\u0017X\t\u000b\u000e\u0011\u0013\u00145@\u0017@0E\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0012O\u001c\u0005\u0016\u0003\u0006\u0017\u0004%\u0012\fe\tD-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0007\u0004+\t\u000b%\u0012\f30\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0}\tH\u0017@\u0013\u0014\u0015\u0011\fZ%3\t\u0014\u0017V<\r\f>\u0003\u0019\u0017@\u0005\u001d\u0015:\t\u0014\u00179\u0015\u0011\u0003\u0006\t\u000b\u0015\u0011\f\u00120\t\u0014\u0005/\u0015uQa\u0013^0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u000f\u0017*\u0015\u0007\r\u0013\u0014\u0005\u0016\u0003\u0019\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005\u0012A+\u0000.\f\u0012\u0005\u0016\u0007\u0004\u0003,\u0015\u0011\fY\u0015\u0011\u0001@\f'\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u0014\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017f\u0013\u0014&D\u0015\u0011\u0001@\u0003\u0019\u0005/0\u001b\u0003\u0019\u0005\u0011\t\u0014\u001e\u0014\u000e\u0011\f\u000f\f\u0012-/\f\u0012\u00179\u0015/\t4\u0015\f\u000f(\u0004\t\u000b%\u000f\u0015/\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012O\u0002Q\u0002\fY<\r\f\u0012+\u0019\u0003\u0019\f\u0012\u001a\u001f\fY\u0015\u0011\u0001@\t\u000b\u0015X\u0015\u0011\u0001\u0004\fY\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005X\t\u000b\u000e\u0011\u0013\u001c5\u0004\u0017\n0^QR\u0001@\u0003\u0019%:\u0001}\t-'\t3T\u001d\u0013\u000b\u000e>\t\u000b\u001e\u0014\u000e\u0011\f\u0012\f\u0012-/\f\u000f\u0017*\u0015'Q\u0002\t\u0014\u0005Z\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u001c\f30\t\u000b\u000e\u0011\f\u0010+\u0019\u0003\u0019\u001f\f\u0012+,7f\u0015\u0011\u0013^<\r\f>\u000e\u0011\f\u0012+\u0006\t\u000b\u0015\u0011\f30}\u0015\u0011\u0013\tW<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u001679O\u0002\u0003\u0006\u0017@0\u0004\f\u0012\u0007\r\f\u000f\u0017\n0\u0004\f\u000f\u0017*\u0015\u0011+,7N\u0013\u000b&.\u0015\u0011\u0001@\f;\f\u000f(\u0004\t\u000b%\u000f\u0015'\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017x\t4\u0015]QR\u0001\u0004\u0003\u0019%J\u0001-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u00058Q\u0002\f\u000f\u000e\u0011\f]\u0007@+\u0006\t\u000b%\u0012\f30\\A>wu\u0017\n0\u001b\f\u0012\f30$O6-'\t\u0014\u001797\b-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0011\u0005EQa\f\u000f\u000e\u0011\f'\u0003\u0006\u0017\u0004\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\f30Tu5@\u0005\u001d\u0015'k:s2\u0012l\u0014q\u0016sJO*T\u001d5@\u0005\u001d\u0015Xp\u001dJcsJq:O\\\u0013\u0014\u000e8\f\u000f(\u0004\t\u000b%\u000f\u0015\u0011+,7Nl\u0001\u0000\u0014sJq;B\u0013\u000b\u000e8\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005.\u0003\u0019\u0017\u001b\u001a\u001c\f\u0012\u00179\u0015\u0011\f30\t;0\u001b\u0003\u0019#$\fJ\u000e\u0011\f\u0012\u00179\u0015[\u0005\u0016\u0003\u0019\u001e\u0014\u0017f\fJ(\u0004\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\u0003\u0019\u0017@\u001e>\u0015\u0011\u0001@\t\u000b\u0015F\u0015\u0011\u0001@\f/\u0017\u0004\u0013\u0014\u0015\u0011\f'\u0005\u0016\u0001@\u0013\u00145@+\u00060\"<\r\f/\u0005\u0016\u0007@+\u0019\u0003,\u0015?\u0003\u0019\u0017\u0015cQ\u0002\u00139.\u0015\u0011\u0001@\f?\u0017@\u0013\u0014\u0015\u0011\f'\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0HQR\u0001\u0004\u0003\u0006%:\u0001^\tZ<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u00167\u0010Q\u0002\t\u0014\u0005E\u001a\u001f\f\u000f\u000e\u00167\b+\u0019\u0003\u0006\u001c\f\u0012+,7H\u0015\u0011\u0013\u00139%\u0012%\u000f5\u0004\u000e3AE\u0000\u0002\u0001@\u0003\u0019\u0005.-/\f\u0012\t\u0014\u0017@\u0005D\u0015\u0011\u0001\n\t4\u00153OP\u0003,&1\u0015\u0011\u0001@\f\u000f\u000e\u0011\f[\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u00058\t/\u0017@\u0013\u0014\u0015\u0011\f?\t\u000b\u0015.QR\u0001\u0004\u0003\u0019%J\u0001\u0010\u0015\u0011\u0001@\f\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005Y\t\u0014\u001e\u0014\u000e\u0011\f\u000f\fW\t\u000b<\r\u0013\u001c5\u0004\u0015'\u0015\u0011\u0001\u0004\f;\f\u0012\u001a\u001f\f\u0012\u00179\u0015+\u0002\u001d\t\"<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u00167V\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005&\u0004\u0004OR\u0015\u0011\u0001@\fJ7\u0003\u0019\u0017\n0\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015\u0011+,7\"\u0007@+\u0006\t\u000b%\u0012\f30^-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0011\u0005[<\r\f\u000f&I\u0013\u0014\u000e\u0011\f\u001cOg\t4&I\u0015\u0011\f\u000f\u000e3Og\u0013\u0014\u000e?\u0013\u000b\u001a\u001c\f\u000f\u000e[\u0015\u0011\u0001\u0004\f'\u0017@\u0013\u0014\u0015\u0011\f\u0015\u0011\u0013'-/\f3\t\u000b\u0017\u0010\u0015\u0011\u0001\u0004\fF\u0005\u0011\t\u0014-/\fE\f\u0012\u001a\u001f\f\u0012\u00179\u00153A;¯`\u000e\u0011\f\u000f+\u0006\u0003\u0019-/\u0003\u0019\u0017\n\t4\u000e\u00167\u0010\t\u0014\u0017@\t\u0014+,7\u001b\u0005\u0016\f\u0012\u0005.\u0005\u0016\u0001\u0004\u00134Qa\f30>\u0015\u0011\u0001\n\t4\u0015\u0001\u0004\u0003\u0006\u001e\u0014\u0001@+,7/%\u0012\u0013\u0014\u0017@%\u0012\f\u000f\u0017*\u0015\u0016\u000e:\t4\u0015\u0011\f30'-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005`Q\u0002\fJ\u000e\u0011\fMQ\u0002\f\u000f+\u0006+$0\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0006\u0017\u0004%\u000f\u0015\u0002&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001\u0004\fD\u000e\u0011\f\u000f\u0005\u001d\u0015\u0013\u000b&6\u0015\u0011\u0001@\f.\u0005\u0016%\u0012\u0013\u000b\u000e\u0011\f\u0012\u0005RQR\u0001@\f\u000f\u000e\u0011\f.&I\f\u000fQ8O\n\u0013\u000b\u000eM\u0017\u0004\u0013X-'\t\u000b\u000e\u0011\u001c\f\u000f\u000e\u0011\u0005e\u00139%\u0012%\u00125\u001b\u000e\u0016\u000e\u0011\f30\\ACz\u0003,\u0015\u0011\u0001/\u0015\u0011\u0001@\fM\t\u0014\u0003\u0019- \u0013\u0014&\\\u0015:\t\u0014*\u0003\u0019\u0017@\u001eE\u0003\u0006\u00179\u0015\u0011\u0013E\t\u0014%\u000f%\u0012\u0013\u001c5\u0004\u0017*\u0015\u0002\t\u0014+\u0019\u0005\u0016\u0013[\u0015\u0011\u0001\u0004\fM%\u0012\t\u0014\u0005\u0016\fMQR\u0001\u0004\f\u0012\u0017'\u0005\u00165@<\u001b)Tu\f\u0012%\u000f\u0015\u0011\u0005?\u0007\r\fJ\u000e\u0011%\u0012\f\u0012\u0003\u0019\u001a\u001f\f30W\u0015\u0011\u0001\u0004\f]\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017\u0004%\u0012\fY\u0013\u000b&D\t;<\r\u0013\u001c5@\u0017@0@\t4\u000e\u001679O6<@5\u001b\u0015[\u0017\u0004\u0013\u0014\u0015[\u0017@\f\u0012%\u000f\f\u0012\u0005\u001d)\u0005\u0011\t4\u000e\u0011\u0003\u0006+,7S\t\u000b\u001e\u0014\u000e\u0011\f\u0012\f30z\u0003\u0019\u0017x\t\u0014\u0005\u0016\u0005\u0016\u0003\u0019\u001e\u0014\u0017@\u0003\u0019\u0017@\u001eV\t^\u0017@\u0013\u0014\u0015\u0011\f\u0010<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017x\u0015cQ\u0002\u0013^\u0005\u00165\u0004<@\u0005\u0016\f395@\f\u000f\u0017*\u0015\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005\u0012O\u0004Qa\f8\u0003\u0019\u0017*\u0015\u0016\u000e\u0011\u0013*0\u001b5@%\u0012\f.\t[\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017>\u0013\u000b&`\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005\u0012@%:\u0001@\u0013\u0014\u0003\u0006%\u000f\f\u0012\u0005\u0003\u0019\u0017\u0010\u0007@+\u0006\t\u0014%\u000f\u0003\u0006\u0017\u0004\u001e'-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005D\u0013\u001c\u0017>\u0015\u0011\u0001\u0004\fF\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0012A;¬@\u0013\u000b\u000eG\f\u0012\t\u0014%:\u0001\b\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015\u0003\u0002\u001fO\n\u0015\u0011\u0001@\f%:\u0001@\u0013\u0014\u0003\u0006%\u000f\f\u0012\u0005[\u0013\u000b&e\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0005\u0004'\t4\u000e\u0011\fX\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u00179\u0015\u0011\f30H<97\b\t\u0014\u0017\"\t\u000b\u000e\u0016\u000e:\t\u00127\b\u0013\u0014&\u0002Qa\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0005\u0006\b\u0007\n\t\n\u0013\u0014&M\u0005\u0016\u0003\u0019y\u0012\fZ<\r\f\u000f\u0003\u0006\u0017\u0004\u001e\u0010\f395\n\t\u0014+g\u0015\u0011\u0013\b\u0015\u0011\u0001\u0004\fY\u0017*5@-[<\r\f\u000f\u000eE\u0013\u0014&M\u0007\r\u0013\u001c\u0005\u0016\u0005\u0016\u0003\u0019<@+\u0019\fY-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0007\r\u0013\u0014\u0005\u0016\u0003\u0019\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005\b\u000b\u0016O\u001b\u0015\u0011\u0001@\t\u000b\u0015R\u0003\u0019\u0005D\u0001\f\t\u000e\r\nGQR\u0001@\fJ\u000e\u0011\f\u000f\f\t\n\u0003\u0019\u0005\u0002\u0015\u0011\u0001@\fM\u0017\u001b5\u0004-?<\r\f\u000f\u000ea\u0013\u0014&h\u0017@\u0013\u0014\u0015\u0011\f\u000f\u0005\u0013\u000b&8\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0010\u0002;\t\u0014\u0017@0};\u000e\u0011\f\u000f&2\f\u000f\u000e\u0011\u0005X\u0015\u0011\u0013\b\u0015\u0011\u0001@\f]&B\t\u0014%J\u0015]\u0015\u0011\u0001@\t\u000b\u00153Oa\t\u0014\u0007@\t\u000b\u000e\u0016\u0015/&I\u000e\u0011\u0013\u0014- \u0015\u0011\u0001@\f+\u0006\t\u0014\u0005\u001d\u0015.\u0017@\u0013\u0014\u0015\u0011\f\u0014O\n\u0015\u0011\u0001@\f\u000f\u000e\u0011\fE\t4\u000e\u0011\fE\u0015cQ\u0002\u0013'\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005.\t\u000b\u0015MQR\u0001@\u0003\u0019%:\u0001H\tX-'\t4\u000e\u0011\u001f\f\u000f\u000eM%3\t\u0014\u0017;<\r\f\u0003\u0019\u0017@\u0005\u0016\fJ\u000e\u0016\u0015\u0011\f30H&2\u0013\u000b\u000e?\f\u0012\t\u0014%:\u0001\"\u0017@\u0013\u000b\u0015\u0011\f=«\b\u0003A \f\u001cA\u0019O`\u00134\u001a\u001f\f\u000f\u000eE\t\u000b\u0017\n0\"\t4&I\u0015\u0011\f\u000f\u000e3AZ\u0000\u0002\u0001@\f/&I\u0013\u001c+\u0019+\u0019\u00134QR\u0003\u0019\u0017\u0004\u001eQa\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0003\u0019\u0017@\u001e?\u000e\u00115\u0004+\u0006\f\u000f\u0005\u0012O\u001bQR\u0001\u0004\u0003\u0019%J\u0001]\u0015:\t\u0014\u001f\fM\u0003\u0006\u00179\u0015\u0011\u0013?\t\u0014%\u0012%\u0012\u0013\u00145@\u00179\u0015\u0002\u0015\u0011\u0001\u0004\f.+\u0019\u00139%3\t\u0014+\\\f\u000f#$\f\u0012%\u000f\u0015\u0002\u0013\u0014&`\t\n-'\t4\u000e\u0011\u001f\f\u000f\u000e3O\u0004\t\u000b\u000e\u0011\fE\t\u000b\u0007@\u0007@+\u0019\u0003\u0019\f30$|\u0006\u0007\n\t\n\u0011\u000bc\u0013\u0012 \u0014\n \u0003\u0019&6\t?-'\t\u000b\u000e\u0011\u001f\fJ\u000eD\t\u000b\u0015\u0015\u000b\r\n\u001cO\u0016\u000b\u0016O@\u0013\u000b\u000e\u0017\u000b\u0019\u0018z \u0013\u0014\u0015\u0011\u0001@\fJ\u000e\u0016QR\u0003\u0006\u0005\u0016\f\u0000\u0002\u0001\u0004\fgQa\f\u0012\u0003\u0019\u001e\u001c\u0001@\u0003\u0019\u0017\u0004\u001eG\u0005\u0016%:\u0001@\f\u0012-/\f1\u0003\u0019\u0005695@\u0003,\u0015\u0011\fg\u0005\u0016\u0003\u0019-/\u0007\u0004+\u0006\fg\t\u0014\u0017@08<\n\t\u0014\u0005\u0016\u0003\u0019%3\t\u000b+\u0019+\u00197E\u0005\u001d\u0015:\t4\u0015\u0011\f\u0012\u0005i\u0015\u0011\u0001\n\t4\u0015\tE-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0002\u0003\u0019-/\u0007@+\u0019\u0003\u0019\f\u0012\u0005a\u0015\u0011\u0001\u0004\fG-'\t\u000b(\u001b\u0003\u0019-?5\u0004- Q\u0002\f\u0012\u0003\u0019\u001e\u001c\u00019\u00153O*QR\u0001@\fJ\u000e\u0011\f3\t\u0014\u0005\u0002\u0015\u0011\u0001\u0004\f.\t\u0014<\u0004\u0005\u0016\f\u0012\u0017@%\u000f\f\u0003\u0019-/\u0007\u0004+\u0006\u0003\u0019\f\u0012\u0005/\u0015\u0011\u0001\u0004\f>\u0017*5@+\u0019+eQ\u0002\f\u0012\u0003\u0019\u001e\u001c\u00019\u00153AS\u0000\u0002\u0001\u0004\f;%J\u0001\u0004\u0013\u001c\u0003\u0019%\u0012\f;\u0013\u0014&8\tW<@\u0003\u0019\u0017\n\t4\u000e\u00167NQ\u0002\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015]\u0003\u0019\u0005<@\t\u0014\u0005\u0016\u0003\u0019%3\t\u000b+\u0006+,7W%3\t\u00145\u0004\u0005\u0016\f30W<97\u0010\u0015\u0011\u0001@\f/\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015:\t\u0014+`\f\u0012\u001a*\u0003\u00060\u0004\f\u000f\u0017@%\u0012\f?Qa\f/%\u0012\u0013\u001c+\u0019+\u0019\f\u0012%\u000f\u0015\u0011\f\u00120\u0015\u0011\u0001@\t\u000b\u0015a\u0005\u00165@\u001e\u001c\u001e\u0014\f\u0012\u0005\u001d\u0015\u0011\f30X\u0015\u0011\u0001\n\t4\u0015g\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\feQe\t\u000b\u0005\u0002\u0017@\u00138\u0007\u001b\u000e\u0011\f\u000f&2\f\u000f\u000e\u0011\f\u000f\u0017@%\u0012\fM\t\u0014-/\u0013\u0014\u0017@\u001eF\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\u0004\u0015\u0011\u0001\n\t\u000b\u0015D\u0003\u0019\u0005D\t\u0014+\u0019+h\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005M\t\u000b\u000e\u0011\f8\f395@\t\u0014+\u0019+,7Y\u0003\u0019-/\u0007\r\u0013\u000b\u000e\u0016\u0015:\t\u0014\u00179\u00153A\n3.6Statistical Analyses\u0000P\u0013W\u0001\n\t3\u001a\u001f\fY\t\u0010<\n\t\u0014\u0005\u0016\f\u000f+\u0006\u0003\u0019\u0017\u0004\fZ\u0015\u0011\u0013\bQR\u0001\u0004\u0003\u0019%J\u0001f%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\fZ\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005\u0012OaQa\fZ\u0001\n\t3\u001a\u001f\f\t\u000b\u0005\u0016\u0005\u0016\f\u0012\u0005\u0016\u0005\u0016\f30\u0010\u0015\u0011\u0001@\fE%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u0003\u0006\u0005\u001d\u0015\u0011\f\u000f\u0017@%\u000f7\b\t\u000b-/\u0013\u001c\u0017@\u001e'\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012O\\5@\u0017@0\u0004\f\u000f\u000eM\u0015\u0011\u0001@\fF\u000e\u0011\f3\t\u0014\u0005\u0016\u0013\u0014\u0017\u0004)\t\u000b<@+\u0019\f>\t\u000b\u0005\u0016\u0005\u00165@-/\u0007\u001b\u0015\u0011\u0003\u0006\u0013\u0014\u0017^\u0015\u0011\u0001\n\t\u000b\u0015E\u0015\u0011\u0001@\f]-/\u0013\u0014\u000e\u0011\f'\u0015\u0011\u0001@\f'\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005/\t4\u000e\u0011\fZ%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u00179\u0015\u0013\u0014\u0017@\f.\u0015\u0011\u0013[\f3\t\u0014%:\u0001>\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eR\u0003\u0019\u0017>\u0007@+\u0006\t\u000b%\u0012\u0003\u0019\u0017@\u001eX-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0012O*\u0015\u0011\u0001@\f.-/\u0013\u000b\u000e\u0011\fG\u0015\u0011\u0001@\fG\f\u0012\u001a\u001f\f\u0012\u00179\u0015\u00115\n\t\u000b+<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\fY\u0003\u0019\u0005X\t;\u001e\u001c\u00139\u0013*0\"\u000e\u0011\f\u000f&I\f\u000f\u000e\u0011\f\u0012\u0017@%\u000f\f]&I\u0013\u0014\u000e/%\u0012\u0013\u0014-/\u0007\n\t4\u000e\u0011\u0003\u0006\u0005\u0016\u0013\u0014\u0017PA\u0010C\"\f'\u0001@\t4\u001a\u001c\fY\t\u000b+\u0019\u0005\u0016\u0013%\u000f\u0013\u001c-/\u0007\n\t4\u000e\u0011\f30E\u0015\u0011\u0001\u0004\f\u0002%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017@%J7[<\r\fJ\u0015uQ\u0002\f\u000f\f\u0012\u0017?\u0015\u0011\u0001@\f \t \u000b \u00008j \t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004- \t\u000b\u0017\n0\u0015\u0011\u0001\u0004\fF\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005M\u0015\u0011\u0013/\u0001\n\t3\u001a\u001f\fE\t/\u0007\u0004\u000e\u0011\f\u0012+\u0019\u0003\u0019-/\u0003\u0019\u0017\n\t\u000b\u000e\u00167Z\u0003\u00060\u0004\f3\t/\u0013\u001c\u0017>\u0015\u0011\u0001\u0004\fF\u0007\r\f\u000f\u000e\u0016&2\u0013\u000b\u000e\u0011-'\t\u0014\u0017@%\u000f\f\u0013\u000b&8\u0015\u0011\u0001@\f;\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-\u0003,\u0015\u0011\u0005\u0016\f\u0012+,&uA!\u0000h\u0013\"\u0001@\t3\u001a\u001f\f;\t\"0\u001b\f\u000f\u0015:\t\u0014\u0003\u0019+\u0019\f30}\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&6\u0015\u0011\u0001@\f8%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017\u0004%\u000f7><\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017>\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005G\t\u000b\u0017\n0>\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@->O\u0004Q\u0002\fF\u0001\n\t3\u001a\u001f\f\f\u000f-/\u0007@+\u0019\u0013379\f30f0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015/\u001e\u000b\u000e:\t\u0014\u0007@\u0001\u0004\u0003\u0019%3\t\u0014+M\t\u0014\u0017@0f\u0017*5@-/\fJ\u000e\u0011\u0003\u0006%\u0012\t\u0014+e\u0015\u0011\u00139\u0013\u001c+\u0019\u0005/\u0015\u0011\u0001\n\t4\u0015/%3\t\u0014\u0017\u0007\u001b\u000e\u0011\u00134\u001a\u001b\u0003\u00060\u001b\f8\t?-/\f3\t\u000b\u0005\u00165\u0004\u000e\u0011\fG\u0013\u000b&i\u0015\u0011\u0001@\f.0\u0004\f\u0012\u001e\u000b\u000e\u0011\f\u0012\f.\u0015\u0011\u0013[QR\u0001\u0004\u0003\u0006%:\u0001Y\u0015cQ\u0002\u0013X\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005FB\u0013\u0014\u000eD\t\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015.\t\u0014\u0017\n0Z\u0015\u0011\u0001\u0004\f?\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-]D\t4\u000e\u0011\f-\u0002c%\u0012+\u0019\u0013\u001c\u0005\u0016\f\u0016\u0004Y\u0013\u001c\u0017@\f8\u0015\u0011\u0013'\f3\t\u000b%J\u0001\u0010\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eM\u0003\u0006\u0017\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e?\u0015\u0011\u0001\u0004\f8\u0005\u0016\f\u000f\u0015D\u0013\u0014&6\u0015\u0011\f\u0012\u0005\u001d\u0015D\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0012Aa\u0000\u0002\u0001@\f8\u0017*5@-/\f\u000f\u000e\u0011\u0003\u0019%3\t\u000b+P-/\f\u0012\t\u0014\u0005\u00165\u0004\u000e\u0011\f%\u0012\t\u0014\u0017Y\u0015\u0011\u0001\u0004\f\u0012\u0017'<\r\fG\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\f30/\u0015\u0011\u0013[\t\u0014\u0005\u0016\u0005\u0016\f\u0012\u0005\u0016\u0005R\u0003,\u0015\u0011\u0005e\u0005\u001d\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019%3\t\u0014+\\\u0005\u0016\u0003\u0019\u001e\u001c\u0017\u0004\u0003\u0019@%3\t\u0014\u0017\u0004%\u0012\f\u001cO\u001bQR\u0001@\u0003\u0019+\u0019\f\u0015\u0011\u0001\u0004\f8%\u0012\u0013\u0014\u000e\u0016\u000e\u0011\f\u000f\u0005\u0016\u0007\r\u0013\u001c\u0017\n0\u001b\u0003\u0019\u0017@\u001eX\u001e\u000b\u000e:\t\u0014\u0007@\u0001\u0004\u0003\u0019%3\t\u0014+\\\u0015\u0011\u00139\u0013\u001c+h\u0007\u001b\u000e\u0011\u00134\u001a\u001b\u0003\u00060\u001b\f\u0012\u0005e\u001a*\u0003\u0006\u0005\u00165@\t\u0014+$\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u00179\u0015:\t\u000b)\u0015\u0011\u0003\u0019\u0013\u0014\u0017PA?~*\u0015:\t4\u000e\u0016\u0015\u0011\u0003\u0006\u0017\u0004\u001e]&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001\u0004\f?\u001a\u001f\f\u000f%\u000f\u0015\u0011\u0013\u0014\u000e\u0011\u0003\u0006\t\u0014+6\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017W\u0013\u0014&g\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005\t\u000b\u0017\n0^&I\u000e\u0011\u0013\u0014-\u0015\u0011\u0001@\f]\u0005\u0016%:\u0001@\f\u0012-/\f]5@\u0005\u0016\f30\"\u0015\u0011\u0013\u0010Q\u0002\f\u0012\u0003\u0019\u001e\u0014\u0001V-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0012Og\t\b\u0005\u001d7\u001b-/-/\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019%-'\t4\u0015\u0016\u000e\u0011\u0003,('\u0013\u0014&`0\u001b\u0003\u0006\u0005\u001d\u0015:\t\u000b\u0017@%\u0012\f\u000f\u0005\b\u001a\t\n<\r\fJ\u0015uQ\u0002\f\u000f\f\u0012\u0017Y\u0007@\t\u0014\u0003,\u000e\u0011\u0005R\u0013\u0014&h\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005\u0015\u00048\t\u000b\u0017\n0\u001c\u001bg&I\u0013\u0014\u000e\f\u0012\t\u0014%:\u0001>\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0015\u00029O\u001bQe\t\u000b\u0005M%\u0012\t\u0014+\u0019%\u00125@+\u0006\t4\u0015\u0011\f30>\t\u0014%\u000f%\u0012\u0013\u0014\u000e:0\u001b\u0003\u0006\u0017\u0004\u001eX\u0015\u0011\u0013?\u0015\u0011\u0001@\fM&2\u0013\u0014\u000e\u0011-[5@+\u0006\t\u001b|\u001d\t\n\u001e\u0004 \u001f!\u001b\u0011\"\u0012!\r\n\u0006\u0017#\u0007\n\t%$\n\u0006\b&\t'\u0006\b\u0007\n\t\n'('\u0006\n&\t\n'\nQR\u0003,\u0015\u0011\u0001\n'\u0006\b)*\t\n'\u0012 +,,\n- .0/\n1 24365\u000687)*\t\n\u0011\u000bcM\f\u0012\u0017@%\u000f\f\u001cO\u0004\t\u0014\u0005\u0002\u0003\u0019\u0017]5@\u0005\u00165\n\t\u000b+\\0\u0004\u0003\u0019\u0005\u001d\u0015:\t\u0014\u0017\u0004%\u0012\f\u0012\u0005\u0002<@\t\u0014\u0005\u0016\f30]\u0013\u001c\u0017'\u0015\u0011\u0001@\fD%\u0012\u0013\u001c\u0005\u0016\u0003\u0019\u0017@\f\u000f\u0005D\u0013\u000b&P\u0015\u0011\u0001\u0004\fG\u001a\u001f\f\u000f%\u000f)\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0012O9\u001d\t\n\u001e\u0004 \u001f\n\u001b\u0016:\u0012 >-/\f\u0012\t\u0014\u0017@\u00058\u0015\u0011\u0001@\t\u000b\u0015RT\u001d5@0\u0004\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005F\u0013\u000b&R\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005\u0005\u0004Y\t\u000b\u0017\n0\u001bE\t4\u000e\u0011\fY\u0007\r\fJ\u000e\u0016&2\f\u0012%\u000f\u0015\u0011+,7^\f\u0012*5@\t\u0014+a\t\u0014\u0017\n0;\u001d\t\n\u001e\u0004 \u001f\n\u001b\u0011<\u0012Y-/\f\u0012\t\u0014\u0017@\u0005F\u0015\u0011\u0001\n\t4\u0015[\u0015\u0011\u0001\u0004\f/\u0015uQ\u0002\u0013\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005[0\u0004\u0003\u00060W\u0017\u0004\u0013\u0014\u0015[0\u001b\u000e:\t\u0012Q \t\u0014\u001797\b%\u0012\u0013\u001c-/-/\u0013\u0014\u0017W-'\t4\u000e\u0011\u001f\f\u000f\u000e3A/C\"\f'\t\u000b\u0007@\u0007\u0004+\u00197\b\u0015\u0011\u0001@\f\u0005\u0011\t\u000b-/\f1Qa\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0003\u0019\u0017@\u001eM\u0005\u0016%J\u0001\u0004\f\u0012-/\f6\u0015\u0011\u0013R\u0015\u0011\u0001\u0004\fg%:\u0001@\u0013\u0014\u0003\u0019%\u0012\f\u0012\u0005i-'\t\u00140\u0004\f1<*7M\u0015\u0011\u0001@\fg\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@->OQR\u0001\u0004\u0003\u0019%J\u0001^\u0003\u0019\u0005F%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u0003\u00060\u0004\f\u000f\u000e\u0011\f30^\t\u000b\u0005F\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015(\u0004\u0010\u0012 \u0018\u0014*A;G\u0017\u0004\f/\u0013\u0014&\u0002\u0015\u0011\u0001\u0004\fX\u000e\u0011\f3\t\u0014\u0005\u0016\u0013\u0014\u0017@\u0005QR\u000197HQ\u0002\f]\f\u0012-/\u0007@+\u0019\u0013379\f30H\u0015\u0011\u0001\u0004\f'%\u0012\u0013\u001c\u0005\u0016\u0003\u0019\u0017@\f'Qe\t\u000b\u0005?%3\t\u000b5@\u0005\u0016\f30^<97W0\u0004\f\u0012\u0005\u0016\u0003\u0019\u001e\u001c\u0017\\|/%\u0012\u0013\u0014\u0005\u0016\u0003\u0006\u0017\u0004\f%\u0012\t\u0014\u0017><\r\fF\t\u000b\u0007@\u0007@+\u0019\u0003\u0019\f30Z\u0003\u0019\u0017@0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015\u0011+,7]\u0013\u001c\u0017>0\u001b\u0003\u0019\u001a\u001f\f\u000f\u000e\u0011\u0005\u0016\f.Q\u0002\f\u000f\u0003\u0006\u001e\u0014\u0001@\u0003\u0019\u0017@\u001e/\u0005\u0016%:\u0001@\f\u000f-/\f\u0012\u0005\u0012O@\u0005\u0016\u0013\u0003,\u0015`%3\t\u000b\u0017?<\r\fa\u000e\u0011\f\u000f5@\u0005\u0016\f30E\u0013\u001c\u0017@%\u000f\f\u0002\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015:\t\u0014+9\f\u0012\u001a*\u0003\u00060\u0004\f\u000f\u0017@%\u0012\f1QR\u0003\u0006+\u0019+\u0004\u0005\u00165@\u001e\u001c\u001e\u0014\f\u0012\u0005\u001d\u0015`\u0015\u0011\u0001\n\t4\u0015\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005[\u0005\u0016\u0001@\u0013\u001c5\u0004+\u00060\"<\r\f'Qa\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\f30^0\u001b\u0003\u0019#$\fJ\u000e\u0011\f\u0012\u00179\u0015\u0011+\u001979Oi\t\u0014\u0017@0W\u0015\u0011\u0001@\f\u0012\u0017\u0006\b\u0007!\t\nQR\u0003\u0006+\u0019+\u0001@\t3\u001a\u001f\f8\u0017@\u0013\u0014\u0017\u0004)=<@\u0003\u0019\u0017\n\t4\u000e\u00167'\u001a\u001c\t\u000b+\u00195@\f\u0012\u0005\u0012A\u0000aQa\u0013/\u0005\u001d\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019%3\t\u0014+P\u0015\u0011\f\u0012%J\u0001\u0004\u0017@\u0003\u000695@\f\u0012\u0005\u0002Qa\f\u000f\u000e\u0011\fF\f\u0012-/\u0007@+\u0019\u0013379\f30/\u0015\u0011\u0013/\t\u000b\u0017\n\t\u0014+,7\u001by\u0012\f.\u0015\u0011\u0001\u0004\f.\u000e\u0011\f\u000f)+\u0006\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005\u0016\u0001\u0004\u0003\u0006\u0007\u0004\u0005W<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017!\u0015\u0011\u0001@\f^\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\b\t\u0014\u0017@0\u0018\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@->A 2a+\u00065\u0004\u0005\u001d\u0015\u0011\f\u000f\u000eD\u0017\n\t\u000b+\u00197\u001b\u0005\u0016\u0003\u0019\u0005] 2\u0002.GQe\t\u000b\u0005[%3\t4\u000e\u0016\u000e\u0011\u0003\u0019\f30\"\u0013\u001c5\u001b\u0015[\u0013\u0014\u0017^\t\u0014\u001797\b\u0005\u0016\u0003\u0019\u0017@\u001e\u0014+\u0006\f'\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015E5@\u0005\u0016\u0003\u0019\u0017@\u001e\u001a\t\n\t\u0014\u00058\u0003\u0019\u0017\u0004\u0007@5\u0004\u00153A 2\u0002\t\u000b+\u0006+\u0019\u00133QR\u0005[\u0015\u0011\u0013Y\u0001\n\t3\u001a\u001f\f?\tZ\u0007@\u0003\u0019%\u000f\u0015\u0011\u0013\u000b\u000e\u0011\u0003\t\u000b+1\u000e\u0011\f\u000f\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\u0003BA \f\u001cA!0\u0004\f\u0012\u0017@0\u001b\u000e\u0011\u0013\u0014\u001e\u0014\u000e:\t\u0014-/\u0005X\u0015\u0011\u0001@\t\u000b\u0015Y0\u0004\f\u000f\u0007@\u0003\u0019%\u000f\u0015'\u0001@\u00133Q \u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005Z\t\u000b\u0017\n0V\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-\t4\u000e\u0011\fG\u001e\u000b\u000e\u0011\u0013\u001c5@\u0007\r\f\u00120/QR\u0003,\u0015\u0011\u0001\u0004\u0003\u0006\u0017/\u0015\u0011\u0001\u0004\fM\u001a\u001f\f\u000f%\u000f\u0015\u0011\u0013\u0014\u000ea\u0005\u0016\u0007\n\t\u0014%\u000f\fM\u001e\u0014\u0003\u0006\u001a\u001c\f\u0012\u0017'<*7?\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005\u0012A1jZ5@+,)\u0015\u0011\u0003\u00060\u001b\u0003\u0006-/\f\u000f\u0017@\u0005\u0016\u0003\u0019\u0013\u001c\u0017@\t\u0014+g~\u001b%3\t\u000b+\u0019\u0003\u0006\u0017\u0004\u001eW=j \u0000F~\u0004D\u0001@\t\u0014\u00058<\r\f\u0012\f\u0012\u0017\b\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-/\f30\u00105@\u0005\u0016\u0003\u0019\u0017@\u001e=\u001a\t\t\u000b\u0005.\u0003\u0019\u0017@\u0007\u00045\u0004\u0015D\u0015\u0011\u0013]\u0007\u001b\u000e\u0011\u0013*0\u00045\u0004%\u0012\fE\t'\u001e\u000b\u000e:\t\u0014\u0007\u0004\u0001@\u0003\u0019%3\t\u0014+`0\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019\u0007\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017H\u0013\u000b&g%\u000f+\u0006\u0013\u0014\u0005\u0016\f\u0012\u0017@\f\u000f\u0005\u0016\u00058<\r\f\u000f)\u0015cQ\u0002\f\u0012\f\u000f\u0017^\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005[\t\u0014\u0017@0H\u0015\u0011\u0001@\f]\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@->AZdM\f\u000f\u0005\u00165@+,\u0015\u0011\u0005?\t4\u000e\u0011\f/\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\f30H\u0003\u0006\u0017~*\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017 \u0010\u0004A\n3.7Boundary DetectionD\u0005.\u0005\u001d\u0015\u0016\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30\b\t\u0014<\r\u00134\u001a\u001f\f\u0014O\\\u0015\u0011\u0001\u0004\f[\u0007\u001b\u000e\u0011\f\u0012\u0007\n\t4\u000e:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017H\u0013\u0014&1\u0015\u0011\u0001@\f[<\n\t\u0014\u0005\u0016\f\u000f+\u0006\u0003\u0019\u0017\u0004\f?\u0003\u0019\u00058\u0013\u001c\u0017\u0004\f[\u0013\u000b&\u0015\u0011\u0001\u0004\f]&I5@\u0017@0@\t\u0014-/\f\u000f\u0017*\u0015:\t\u000b+a\u0005\u001d\u0015\u0011\f\u0012\u0007\u0004\u0005?\u0013\u000b&M\u0015\u0011\u0001\u0004\fY\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u0005\u0012O`\u0005\u0016\u0003\u0019\u0017@%\u000f\f]\u0015\u0011\u0001\u0004\fY\u001e\u0014\u00139\u0013*0\u001b)\u0017\u0004\f\u0012\u0005\u0016\u0005E\u0013\u0014&\u0002\u0015\u0011\u0001\u0004\f/%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\u0003\u0019\u0005\u0016\u0013\u001c\u0017\"\u0013\u000b&e\u0015\u0011\u0001@\f/\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-/\u0005EQR\u0003,\u0015\u0011\u0001W\u0015\u0011\u0001\u0004\fX<\n\t\u000b\u0005\u0016\f\u0012+\u0019\u0003\u0006\u0017\u0004\f0\u001b\f\u0012\u0007\r\f\u0012\u0017@0\u0004\u0005R\u0013\u001c\u0017Z\u0015\u0011\u0001\u0004\f8\u001e\u001c\u00139\u0013*0\u001b\u0017@\f\u0012\u0005\u0016\u0005M\u0013\u0014&6\u0015\u0011\u0001@\fG<\n\t\u0014\u0005\u0016\f\u000f+\u0006\u0003\u0019\u0017\u0004\f[\u0003,\u0015\u0011\u0005\u0016\f\u0012+,&uAgC\"\fF\t\u000b\u0005\u0016\u0005\u00165@-/\f30\u0015\u0011\u0001@\t\u000b\u0015i\u0015\u0011\u0001\u0004\fa-/\u0013\u000b\u000e\u0011\f1\u0015\u0011\u0001\u0004\fa\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005i\u0007@+\u0006\t\u000b%\u0012\f30[-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005h\t\u000b\u000e\u0011\u0013\u001c5\u0004\u0017\n0E\tD\u0017\u0004\u0013\u0014\u0015\u0011\f\u001cO\u000b\u0015\u0011\u0001@\f\u0001\u0004\u0003\u0019\u001e\u001c\u0001@\fJ\u000e8\u0015\u0011\u0001@\f?\u0007\u001b\u000e\u0011\u0013\u001c<@\t\u0014<@\u0003\u0019+\u0019\u0003,\u0015u7;\u0015\u0011\u0001@\t\u000b\u0015F\t]<\r\u0013\u001c5@\u0017@0@\t4\u000e\u00167\u0010\f\u000f(\u001b\u0003\u0006\u0005\u001d\u0015\u0011\u0005.\t\u000b\u00158\u0015\u0011\u0001\n\t4\u00158\u0017@\u0013\u000b\u0015\u0011\f\u001cA\u0000\u0002\u0001\u0004\u0003\u0019\u0005Y-/\f3\t\u000b\u0017@\u0005X\u0015\u0011\u0001@\t\u000b\u00153OR\t4&I\u0015\u0011\f\u000f\u000e]\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005Y\u0001@\t3\u001a\u001f\f>\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f30f\u0005\u0016%\u000f\u0013\u0014\u000e\u0011\f\u0012\u0005\u0012OD\u0003,\u0015\u0003\u0019\u0005Y\u0017@\f\u0012%\u0012\f\u000f\u0005\u0016\u0005\u0011\t\u000b\u000e\u00167\u0015\u0011\u0013 \u0002u\u0017\u0004\u0013\u0014\u000e\u0011-'\t\u000b+\u0006\u0003\u0019y\u0012\f \u0004N\u0015\u0011\u0001\u0004\f\u0012- \t\u0014\u0017@0}\u0015\u0011\u0013V0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015]\u0015\u0011\u0001@\f\u0010-/\u0013\u0014\u0005\u001d\u0015AComparison ofManualandAutomaticMelodySegmentation\u0007\u0004+\t\u000b5@\u0005\u0016\u0003\u0019<@+\u0019\fF<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012Ag\u0000h\u0013?\u0015\u0011\u0001\u0004\u0003\u0006\u0005R\f\u0012\u0017@0\\O*Q\u0002\f8\fJ(\u001b\u0015\u0011\f\u000f\u0017\n0]\u0015\u0011\u0001\u0004\f.\u0007\u0004\u000e\u0011\f\u000f\u001a\u001b\u0003\u0019\u0013\u00145@\u0005\u0016+,70\u001b\f\u0012\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<\r\f30ZQ\u0002\f\u000f\u0003\u0006\u001e\u0014\u00019\u0015\u0011\u0003\u0006\u0017\u0004\u001e'\u0005\u0016%J\u0001\u0004\f\u0012-/\f8\u0015\u0011\u0013'\tX\u0005\u001d\u0015\u0011\u00139%J\u0001@\t\u0014\u0005\u001d\u0015\u0011\u0003\u0019%[-/\u0013*0\u001b\f\u0012+BAR\u0000\u0002\u0001@\u0003\u0019\u0005M\f\u000f(*)\u0015\u0011\f\u0012\u0017\u0004\u0005\u0016\u0003\u0019\u0013\u001c\u0017\u0010\u0003\u0019\u0005G0\u001b5@\f8\u0015\u0011\u0013X\u0015\u0011\u0001@\fF\u0003\u0019\u00179\u0015\u0016\u000e\u0011\u0003\u0006\u0017\u0004\u0005\u0016\u0003\u0019%[5\u0004\u0017@%\u0012\f\u000f\u000e\u0016\u0015:\t\u000b\u0003\u0019\u0017*\u0015c7Z\u0013\u000b&`\u0015\u0011\u0001@\fF\u00139%\u0012%\u00125\u001b\u000e\u0016\u000e\u0011\f\u0012\u0017@%\u0012\f\u0013\u000b&\u0002<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012AFD\u0005.\t'<\r\u0013\u001c5@\u0017@0@\t4\u000e\u00167>\u0003\u0019\u0005.5@\u0017\u0004*\u0017@\u00133QR\u0017\u0010\t\u0014\u0017@0\u0010%\u0012\t\u0014\u0017\b<\r\f[\u0013\u0014\u0017@+,7\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f\u00120Y\u0015\u0011\u0001\u0004\u000e\u0011\u0013\u00145@\u001e\u001c\u0001Z-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0012O\u001b\u0015\u0011\u0001@\fF0\u001b\f\u0012%\u0012\u0003\u0019\u0005\u0016\u0003\u0019\u0013\u001c\u0017\b\t\u0014\u0005R\u0015\u0011\u0013?QR\u0001\u0004\f\u000f\u0015\u0011\u0001@\f\u000f\u000eD\u0003,\u0015M\f\u000f(*)\u0003\u0019\u0005\u001d\u0015\u0011\u0005\u0012O\n\u001e\u0014\u0003\u0019\u001a\u001f\f\u0012\u0017>\t[\u0005\u0016\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0005R\u0013\u0014&6\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u0014\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\O\n\u0003\u0019\u0005R\t\u000b#$\f\u000f%\u000f\u0015\u0011\f30Z<97'5@\u0017\u0004%\u0012\f\u000f\u000e\u0016\u0015:\t\u000b\u0003\u0006\u00179\u0015c79A\t\\\f\u000f\u0015X5\u0004\u0005?\u0005\u001d\u0015:\t\u000b\u000e\u0016\u0015[\u0015\u0011\u0013\u0010-/\u0013*0\u0004\f\u0012+aQR\u0001@\t\u000b\u0015?\u0003\u0006\u0005[\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30$Og\u0003BA \f\u001cAW-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\u0012O1\t\u000b\u0017\n0\u0000\n2\u0012 \u0001\u0000\n2\u0002 \u0003\n\u001f\u0004\u0000\n2\u0002\n5\u001f\u0005\u0000\n2\u0002\n7\n]<\r\f\b\t\"\u000e:\t\u0014\u0017@0\u0004\u0013\u001c- \u001a\u0014\t\u000b\u000e\u0011\u0003\u0006\t\u0014<\u0004+\u0006\fH0\u0004\f\u000f\u0005\u0016%\u000f\u000e\u0011\u0003\u0019<@\u0003\u0019\u0017@\u001eN\u0015\u0011\u0001@\f\u0007\r\u0013\u0014\u0005\u0016\u0005\u0016\u0003\u0006<\u0004+\u0019\fe\u0007\u00061\u0013\u001c5\u001b\u0015\u0011%\u0012\u0013\u001c-/\f\u0012\u0005P\t\u000b&\u0015\u0011\f\u000f\u000e6\u0003\u0019\u0017@\u0005\u0016\fJ\u000e\u0016\u0015\u0011\u0003\u0006\u0017\u0004\u001eR\u0013\u001c\u0017@\f\u0014O\u000b\u0015cQ\u0002\u0013\u001bO\u0014\u0013\u000b\u000eh\u0015\u0011\u0001\u001b\u000e\u0011\f\u0012\f`-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\t4\u000e\u0011\u0013\u001c5@\u0017@0Z\u0017@\u0013\u000b\u0015\u0011\f8\u000b\u0016Aa~\u001b\u0007\r\f\u0012%\u0012\u0003,@%3\t\u0014+\u0019+,7$|\u0000\n2\u0002\n2\b\n\t\n\u0012 \u0014\n \u0003,&1\t?-'\t\u000b\u000e\u0011\u001f\fJ\u000ee\u0003\u0019\u0005G\t4\u0015R\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017 \u000b\u0019\u0018\f\u000b\r\n \u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e\u0016QR\u0003\u0019\u0005\u0016\fQR\u0001\u0004\f\u000f\u000e\u0011\f\r\u000b\u000f\u000e\u0011\u0010\u000b \u001f\u000f \u001fJ\u0013\u0012\u001fAR\u0000\u0002\u0001@\fF%:\u0001@\u0013\u0014\u0003\u0006%\u000f\f[\u0013\u000b&`5@\u0005\u0016\u0003\u0019\u0017@\u001e'\t?\u0015\u0011\u0001\u001b\u000e\u0011\f\u0012\f\u000f)=\u001a\u001c\t4\u000e\u0011\u0003\u0006\t\u000b\u0015\u0011\f8\u000e:\t\u0014\u0017\u001b)0\u001b\u0013\u001c- \u001a\u001c\t4\u000e\u0011\u0003\u0006\t\u0014<@+\u0019\f\u001cO\u0004\u000e:\t\u000b\u0015\u0011\u0001\u0004\f\u000f\u000eR\u0015\u0011\u0001\n\t\u000b\u0015M\t\u000f\u0014\r)=\u001a\u001c\t4\u000e\u0011\u0003\u0006\t\u000b\u0015\u0011\fF\u0013\u001c\u0017\u0004\f\u001cO\n\u0007\u001b\u000e\u0011\u0013\u000b\u001a*\u0003\u00060\u0004\f\u00120\u0015\u0014W\u0003\u0019\u0005R\u0015\u0011\u0001@\f\u0017*5@-[<\r\f\u000f\u000eY\u0013\u0014&X\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0012O.\u0003\u0006\u0005Z%3\t\u00145\u0004\u0005\u0016\f30S<97\u0015\u0011\u0001\u0004\fH\f\u000f-/\u0007@\u0003,\u000e\u0011\u0003\u0019%3\t\u0014+8\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u0014\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u00057\u001b\u0003\u0019\f\u0012+\u00060\u0004\f30E<978\u0015\u0011\u0001@\fg\u0015\u0011\f\u000f\u0005\u001d\u0015\u0011\u0005\u0012A6wu\u0017@0\u0004\f\u0012\f30$O\u001f\t\u000b+\u0019-/\u0013\u001c\u0005\u001d\u00151\t\u0014+\u0019+*\u0015\u0011\u0001@\fg-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005h\u0005\u0016\u0003\u0006\u001e\u0014\u0017\n\t\u000b+\u0006\u0003\u0019\u0017\u0004\u001e\t8\u0001@\u0003\u0019\u001e\u0014\u0001@+,7X\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@+\u0019\fD<\r\u0013\u001c5@\u0017@0@\t4\u000e\u00167X0\u001b\u0003\u0019#$\fJ\u000e\u0011\f30X\u0003\u0019\u0017'\u001bO\rD\u0013\u0014\u000e\u0002.\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005\u0012O\u0004\t\u000b\u0005\u0007\r\u0013\u0014\u0003\u0006\u00179\u0015\u0011\f30Y\u0013\u001c5\u0004\u0015R\u0003\u0019\u0017\u0010~*\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017Z\u001bA \u0019*O\u0004QR\u0001@\u0003\u0019+\u0019\fE\t\u0014+\u0019+P\u0015\u0011\u0001\u0004\f.\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\u0005eQ\u0002\f\u000f\u000e\u0011\f8\u0017\u0004\f\u0012\u001e\u001c+\u0019\u0003\u0019\u001e\u001c\u0003,)<\u0004+\u0006\f\u0014A~*\u0003\u0006\u0017\u0004%\u0012\feQ\u0002\fR\u0001\n\t3\u001a\u001f\fe\fJ(\u001b\u0015\u0011\f\u000f\u0017\n0\u0004\f\u00120E\u0015\u0011\u0001@\f\u0002Qa\f\u0012\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0003\u0019\u0017@\u001eF\u0005\u0016%:\u0001@\f\u0012-/\fa\u0015\u0011\u0013F\t.\u0005\u001d\u0015\u0011\u00139%:\u0001\n\t\u000b\u0005\u001d\u0015\u0011\u0003\u0019%-/\u0013*0\u001b\f\u0012+BO\u0016\u0000_\u0003\u0006\u0005]\t\u000b\u0005\u0016\u0005\u0016\u00139%\u0012\u0003\u0006\t\u000b\u0015\u0011\f30}\u0015\u0011\u0013W\tW\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u0014<\u0004\u0003\u0019+\u0006\u0003,\u0015c7}0\u001b\u0003\u0019\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0006<\u00045\u0004\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\Ax\u0000\u0002\u0001@\f0\u001b\u0003\u0006\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019<\u00045\u0004\u0015\u0011\u0003\u0019\u0013\u001c\u0017>\u0013\u000b&6\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0019\u0015c7Z\u0013\u000b&\u0017\u0000 \u0003\u0019\u0005D0\u0004\fJ\n\u0017@\f\u00120Z\t\u0014\u0005\u0012|\u0018\u001a\u0019\u0004\u0001\u0000\n2\u0003\n\u0012\u001c\u001b\n2\u0003\n\u001f\u0005\u0000\n2 5\u0012\u001d\u001b\n2\u00115\u001f\u0005\u0000\n27\n\u0012\u001c\u001b\n27\n9\u0012\n7\u001e\u001f\n3\u0003! \n\"$#&%\n2\u001f\n\u001d\r \n2\u001f\n\n\"'#(%QR\u0001\u0004\f\u000f\u000e\u0011\f)\u001b \u0012\u0001\u001b\n2\u0003\n\u001f*\u001b\n2 5\u001f\u0005\u001b\n27\nD\u0003\u0019\u0005.\t\u000b\u0017\u0010\u0013\u001c5\u0004\u0015\u0011%\u000f\u0013\u001c-/\fF\u0013\u0014&1\u0015\u0011\u0001\u0004\u000e\u0011\f\u0012\fE<@\u0003\u0019\u0017\u0004\u0013\u001c-/\u0003\u0006\t\u0014+i\u0003\u0019\u0017\u001b)0\u001b\f\u0012\u0007\r\f\u0012\u0017\n0\u001b\f\u0012\u00179\u0015\u0011+,7/0\u0004\u0003\u0019\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019<@5\u001b\u0015\u0011\f30X\u001a\u0014\t\u000b\u000e\u0011\u0003\u0006\t\u0014<\u0004+\u0006\f\u000f\u0005\u0016\u001d\u0004\u0005\u0016\u0007\r\f\u000f%\u0012\u0003,\n%3\t\u000b+\u0006+,79O+\u001b\n2\u001f\n\u0012\u0018D\u0003,&h\t\u000b\u0017\n0\u0013\u0014\u0017@+,7;\u0003,&g\t'-'\t4\u000e\u0011\u001f\f\u000f\u000eRQe\t\u000b\u0005G\u0003\u0019\u0017@\u0005\u0016\fJ\u000e\u0016\u0015\u0011\f30\u0010\t\u000b\u0015.\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017\u0015,\r\n[\t\u000b\u000e\u0011\u0013\u00145@\u0017@0>\u0017@\u0013\u0014\u0015\u0011\f\u000bG\t\u0014\u0017\n0 \n2\u001f\n\u0003\u0006\u0005E\u0015\u0011\u0001@\f/\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7W\u0015\u0011\u0001@\t\u000b\u0015-\u001b\n2\u001f\n\u0012 \u001cA>wu\u0017\n0\u001b\f\u0012\u0007\r\f\u0012\u0017@0\u0004\f\u0012\u0017\u0004%\u0012\fX\u0013\u0014&\u0015\u0011\u0001\u0004\f]\u0015\u0011\u0001\u001b\u000e\u0011\f\u0012\f/\u000e:\t\u0014\u0017\n0\u001b\u0013\u001c-U\u001a\u001c\t4\u000e\u0011\u0003\u0006\t\u0014<@+\u0019\f\u0012\u0005[\u0003\u0019\u0005X\t;\u0005\u0016\u0003\u0019-/\u0007@+\u0019\u0003,\n%3\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017f0\u00045@\f/\u0015\u0011\u0013\u0010%\u0012\u0013\u0014-X)\u0007\u00045\u0004\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017\n\t\u0014+R\t\u000b\u0017\n0\"\u0005\u001d\u0015:\t4\u0015\u0011\u0003\u0006\u0005\u001d\u0015\u0011\u0003\u0019%3\t\u000b+e\u000e\u0011\f3\t\u0014\u0005\u0016\u0013\u0014\u0017@\u0005\u0012|'\u0003,&.\t\u000b\u0005\u0016\u0005\u00165@-/\f30$O10\u0004\f\u0012\u0007\r\f\u000f\u0017\n0\u0004\f\u000f\u0017@%\u0012\fQa\u0013\u001c5@+\u00060}\u0003\u0019-/\u0007\u0004+\u0006\u0003\u0019\f30N\u0015\u0011\u0001@\f>\f\u000f\u0005\u001d\u0015\u0011\u0003\u0006-'\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017}\u0013\u000b&F\tW-[5@%:\u0001f+\u0006\t4\u000e\u0011\u001e\u001c\f\u000f\u000eY\u0017*5@-?<\r\fJ\u000eX\u0013\u0014&\u0007@\t\u000b\u000e:\t\u0014-/\fJ\u0015\u0011\f\u000f\u000e\u0011\u0005\u0012O*<97[\u0015\u0011\u0001*5@\u0005g0\u0004\fJ\u0015\u0011\f\u000f\u000e\u0011\u0003\u0019\u0013\u0014\u000e:\t\u000b\u0015\u0011\u0003\u0019\u0017\u0004\u001eF\u0015\u0011\u0001\u0004\fR\u001e\u001c\u00139\u0013*0\u0004\u0017\u0004\f\u0012\u0005\u0016\u0005\u0002\u0013\u000b&\\\u0015\u0011\u0001\u0004\fD\t\u0014%\u000f\u0015\u00115@\t\u0014+\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019-'\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017;\t\u0014\u0017@0Y\u0015\u0011\u0001@\f\u000f\u0017Z\u0013\u0014&h\u0015\u0011\u0001@\f.\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015\u0011\u0005\u0012A¬\u0004\u000e\u0011\u0013\u0014-U\u0015\u0011\u0001\u0004\f'-'\t\u0014\u0017*5\n\t\u000b+a\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\"\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015\u0011\u0005\u0012Oa\u0003,\u0015[%3\t\u0014\u0017N<\r\f'\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30\u0015\u0011\u0001@\t\u000b\u0015E\u0015\u0011\u0001@\f/\u001e\u0014\u000e\u0011\f\u0012\t\u000b\u0015[-'\t4Tu\u0013\u0014\u000e\u0011\u0003,\u0015u7H\u0013\u0014&D\u0013\u001c5\u001b\u0015\u0011%\u0012\u0013\u001c-/\f\u000f\u0005F\u000e\u0011\f\u000f+\t4\u0015\u0011\f/\u0015\u0011\u0013>\u0015\u0011\u0001@\f/\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u0017@%\u0012\f\u0013\u000b&PTu5@\u0005\u001d\u00158\u0013\u0014\u0017@\f?-'\t4\u000e\u0011\u001f\f\u000f\u000eM\u00139%\u0012%\u00125\u001b\u000e\u0016\u000e\u0011\u0003\u0006\u0017\u0004\u001eZ\t4\u00158\f\u0012\u0003,\u0015\u0011\u0001@\fJ\u000e8\t\u000b&\u0015\u0011\f\u000f\u000e3O\\<\r\f\u000f&2\u0013\u000b\u000e\u0011\f\u001cOP\u0013\u0014\u000e.\u00134\u001a\u001f\fJ\u000e\tE\u0017@\u0013\u0014\u0015\u0011\f\u0014O\u001bQR\u0001\u0004\f\u000f\u000e\u0011\f3\t\u000b\u0005\u0002\u0015uQ\u0002\u0013E\u0013\u0014\u000ee\u0015\u0011\u0001\u001b\u000e\u0011\f\u0012\fG-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u00051\u000e:\t\u000b\u000e\u0011\f\u0012+,7/Q\u0002\fJ\u000e\u0011\f.\u0007@5\u001b\u0015\u0002<97/\u0015\u0011\u0001@\f\u0005\u0011\t\u000b-/\fZ\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015'\t\u000b\u000e\u0011\u0013\u00145@\u0017@0N\u0015\u0011\u0001\u0004\fY\u0005\u0011\t\u000b-/\fZ\u0017@\u0013\u000b\u0015\u0011\f\u001cAf\u0000\u0002\u0001@\fJ\u000e\u0011\f\u000f&2\u0013\u000b\u000e\u0011\f\u001cO\u0002\u0015\u0011\u0001@\f]\f\u0012\u001a\u001f\f\u000f\u0017*\u0015\u0015\u0011\u0001@\t\u000b\u0015[\tZ<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u00167\b\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005?\t4\u0015?\tY\u0017@\u0013\u0014\u0015\u0011\f/\u0003\u0019\u0005F+\u0019\u0003\u0019\u001f\f\u0012+,7H\u0015\u0011\u0013;%\u000f\u0013\u0014\u000e\u0016\u000e\u0011\f\u0012\u0005\u0016\u0007\r\u0013\u0014\u0017\n0H\u0015\u0011\u0013\u0015\u0011\u0001\u0004\f.\f\u0012\u001a\u001f\f\u0012\u00179\u0015e\u0015\u0011\u0001@\t\u000b\u0015R\u0013\u001c\u0017\u0004\f8-'\t\u000b\u000e\u0011\u001f\fJ\u000ee\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005M\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0]\u0015\u0011\u0001\u0004\f8\u0017@\u0013\u000b\u0015\u0011\f\u001cA1\u0000\u0002\u0001*5@\u0005\u0012O\u0004+\u0006\fJ\u0015.\n2<\r\fF\t[\u000e:\t\u000b\u0017\n0\u001b\u0013\u001c- \u001a\u0014\t\u000b\u000e\u0011\u0003\u0006\t\u0014<\u0004+\u0006\f8<\r\f\u000f\u0003\u0006\u0017\u0004\u001e/0\u0004\fJ\n\u0017@\f\u00120Y\t\u000b\u0005D&I\u0013\u001c+\u0019+\u0019\u00134QR\u0005\u0012|.\n2\u0012\n/00000010000002\n\n\u0003,&$\u0015\u0011\u0001@\f\u000f\u000e\u0011\fe\u0003\u0019\u0005g\t.<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u00167X\t4\u0015a\u0017\u0004\u0013\u0014\u0015\u0011\f\b\u000bh\u0003,&\\\t\u000b\u0017\n0\u0013\u0014\u0017@+,7f\u0003\u0019&G\u0015\u0011\u0001@\f\u000f\u000e\u0011\f>\u0003\u0019\u0005]\t\u000b\u0015'+\u0019\f3\t\u000b\u0005\u001d\u0015]\u0013\u001c\u0017@\f;-'\t4\u000e\u0011\u001f\f\u000f\u000e\t4\u000e\u0011\u0013\u001c5@\u0017@0 \u000b\n\u0003,&a\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\fE\u0003\u0006\u0005.\t\u000b\u000e\u0011\f?\u0017\u0004\u0013'<\r\u0013\u001c5@\u0017@0@\t4\u000e\u0011\u0003\u0006\f\u000f\u00058\t\u000b\u00158\u0017\u0004\u0013\u0014\u0015\u0011\f\u000be\u0003,&a\t\u0014\u0017\n0;\u0013\u0014\u0017@+,7;\u0003,&g\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\f[\t\u000b\u000e\u0011\f?\u0017\u0004\u0013'-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005\t4\u000e\u0011\u0013\u001c5@\u0017@0 \u000b3\u0003\u0019\u001a\u001f\f\u0012\u0017V\t;\u0005\u0016\f\u000f\u001554\n2\u0013\u0014&.\u0013\u00145\u0004\u0015\u0011%\u0012\u0013\u0014-/\f\u0012\u0005[\u000e\u0011\f\u000f\u0005\u00165@+,\u0015\u0011\f30^&I\u000e\u0011\u0013\u001c-U\u0015\u0011\u0001@\f]\u0003\u0019\u0017@\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\u0003\u0019\u0013\u001c\u0017f\u0013\u0014&-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u00056\t4\u000e\u0011\u0013\u001c5@\u0017@0[\u0017\u0004\u0013\u0014\u0015\u0011\f \u000b$<97<\f\u0007\n\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012O\u001cQa\fR\t\u0014%\u0012%\u000f\f\u0012\u0007\u0004\u0015i\u0015\u0011\u0001\u0004\f\u0002\u000197\u001b\u0007\r\u0013\u0014\u0015\u0011\u0001\u0004)\f\u0012\u0005\u0016\u0003\u0019\u0005h\u0015\u0011\u0001@\t\u000b\u0015i\tD<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u00167G\f\u000f(\u001b\u0003\u0006\u0005\u001d\u0015\u0011\u0005h\t\u000b\u0015h\u0017@\u0013\u0014\u0015\u0011\f9\u000b\r\u0003,&\n\t\u000b\u0017\n0.\u0013\u0014\u0017@+,78\u0003,&6\u0018\u001a\u0019\u001b7.\n2\u0012!-84\n2:9;\u0018\u001a\u0019\u00047.\n2\u0012x\u000f8<4\n2JO\u001f\u0015\u0011\u0001\n\t4\u0015a%3\t\u000b\u0017'\f\u000f(\u001b\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30/<97E\u0015\u0011\u0001@\fD\u0003\u0006\u0017\u0004\f395\n\t\u000b+\u0006\u0003,\u0015c7\u0018\u001a\u0019\u001b7.\n2\u0012 =8>4\n2?9\n57\nO\rQR\u0001\u0004\f\u000f\u000e\u0011\f-4\n2\u0012\u00014\n2 5\u001f\\­4­G­ \u001f*4\n2.A@\nD\u0003\u0019\u0005G\u0015\u0011\u0001\u0004\f[\u0005\u0016\fJ\u0015\u0013\u000b&`\u0013\u001c5\u0004\u0015\u0011%\u000f\u0013\u001c-/\f\u0012\u0005M\t\u0014\u0017@0B4\n2\u001f\n\u0012 \u00014\n2\u001f\u0003\n\u001f\u00054\n2\u001f\n5\u001f\u00054\n2\u001f7\nR\u0003\u0019\u0005R\u0015\u0011\u0001@\f8\u0013\u001c5\u001b\u0015\u0011%\u0012\u0013\u001c-/\fM\u000e\u0011\f\u000f)\u0005\u00165\u0004+\u0019\u0015\u0011\f\u00120]&I\u000e\u0011\u0013\u0014-\u0015\u0011\u0001@\f\u0016,\u0014)=\u0015\u0011\u0001]\u0005\u00165@<*T\u001d\f\u000f%\u000f\u00153A6C\"\f80\u001b\f\u000f\n\u0017\u0004\fM\u0015\u0011\u0001\u0004\fG\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0019\u0015c7/\u0015\u0011\u0001\n\t4\u0015\t?<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u00167]\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005D\t\u0014\u0005\u0018\u001a\u0019\u001b7.\n2\u0012!-8<4\n2\"\u0012\n1)!C!D\n\u0018\u001a\u0019\u0004\u00014\n2\u0012\u001dE\n2\nQR\u0001\u0004\f\u000f\u000e\u0011\fGF\u0012 \u0014\n \u001f\u0011 \u001f33  \u001f\u0012\u0001\u001f:\u001f \u001d \u001f\u0016 \u001f:\u001f \u001f\u0012 \u001f33 \u001d \u001f\u0011 \u001f34 \u001d \u001f\u000f \u001f:\u001f \u001d\u0001\u001f\u0012 \u001f33\u0003\u0019\u0005a\u0015\u0011\u0001\u0004\fM\u0005\u0016\fJ\u0015e\u0013\u0014&P\u0013\u001c5\u001b\u0015\u0011%\u0012\u0013\u001c-/\f\u0012\u0005g%\u0012\u0013\u0014\u000e\u0016\u000e\u0011\f\u000f\u0005\u0016\u0007\r\u0013\u001c\u0017\n0\u001b\u0003\u0019\u0017@\u001eF\u0015\u0011\u0013H.\n2\u0012!\u001cA1\u0000\u0002\u0001\u0004\fG%\u0012\u0013\u0014-/\u0007@5\u001b)\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017W\u0013\u000b&\u0016\u0018\u001a\u0019\u001b7.\n28I4\n2D\u000e\u0011\f395\u0004\u0003\u0019\u000e\u0011\f\u000f\u0005G\u0015\u0011\u0001@\f[\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b\u0013\u0014& \n2\u0003\n\u001f \n2\u00115\u001f \n27\u0015\u0011\u0001@\t\u000b\u0015D%3\t\u000b\u0017><\r\f.\u0013\u001c<\u001b\u0015:\t\u0014\u0003\u0019\u0017@\f\u00120Y&I\u000e\u0011\u0013\u0014- \u0015\u0011\u0001@\f8+\u0019\u0003\u0019\u001f\f\u0012+\u0019\u0003\u0019\u0001@\u00139\u0013*0Z&25\u0004\u0017@%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017PO\u001b\u0015\u0011\u0001\n\t\u000b\u0015R\u0003\u0019\u0005\u0012|\u0018\u001a\u0019\u0004\u00014\n2\u0012\u001dE\n2\"\u0012.A@\n\u001e\u001f\n365\n7\u001eJ\n3\u0003\n \n)#&%GK\n2J\n\u001d\r \n2J\n\n)#&%GKQR\u0001\u0004\f\u000f\u000e\u0011\f-E\n2\u001f$L\n\u0012 F\u0003\u0019&1\t\u000b\u0017\n0Z\u0013\u0014\u0017@+,7>\u0003,&`\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015M,]\u0003\u0019\u0017@\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\f\u00120>\tX-'\t\u000b\u000e\u0011\u001f\fJ\u000eG\t4\u0015\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017(\u000b \u0018)N\r\n\u001cO\u00145@\u0017@0\u0004\f\u000f\u000eP\u0015\u0011\u0001@\f\u0002\t\u000b\u0005\u0016\u0005\u00165@-/\u0007\u001b\u0015\u0011\u0003\u0006\u0013\u0014\u0017E\u0015\u0011\u0001@\t\u000b\u00156\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u00056-'\t4\u000e\u0011\u001f\f30\u0015\u0011\u0001\u0004\f'\u0005\u0016%\u0012\u0013\u0014\u000e\u0011\f]\u0003\u0019\u0017\n0\u0004\f\u000f\u0007\r\f\u0012\u0017\n0\u001b\f\u0012\u00179\u0015\u0011+\u00197\b\u0013\u0014\u0017@\f]\u0013\u0014&R\f3\t\u000b%J\u0001N\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000e3A\u0010\u0000\u0002\u0001\u0004\fY-'\t4(\u0004\u0003\u0019-[5@-+\u0019\u0003\u0019\u001f\f\u0012+\u0019\u0003\u0019\u0001@\u00139\u0013*0>\u0007@\t\u000b\u000e:\t\u000b-/\f\u000f\u0015\u0011\f\u000f\u000eD\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019-'\t\u000b\u0015\u0011\u0013\u000b\u000e\u0011\u0005M\t4\u000e\u0011\f8\u001e\u001c\u0003\u0019\u001a\u001f\f\u0012\u0017Z<*7$|O \n2L\n\u0012QP.A@\n\u001f\n3 5E\n2\u001fRL\f\u0007CW\f>0\u001b\f\u0012%\u0012\u0003\u00060\u0004\fZ\u0015\u0011\u0001@\t\u000b\u0015'\t\u0010<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u00167N\f\u000f(\u001b\u0003\u0019\u0005\u001d\u0015\u0011\u0005/\t\u000b\u0015'\u0017\u0004\u0013\u0014\u0015\u0011\f=\u000b8\u0003\u0019&.\t\u0014\u0017@0f\u0013\u0014\u0017@+,7f\u0003\u0019&S\u0018\u001a\u0019\u001b7.\n2\u0012!\r8\u00074\n2:9\n57\nQR\u0001@\f\u000f\u000e\u0011\fS\u0018\u001a\u0019\u001b7.\n2\u0012!-8T4\n29\u0012\n1)!C!D\nS\u0018\u001a\u0019*\u00014\n2\u0012UE\n2\t\u000b\u0017\n0S\u0018\u001a\u0019\u0004\u00014\n2\u0012\u001dE\n2\"\u0012.@\n\u001e\u001f\n365\n7\u001eJ\n3\u0003\nO \n)#&%GK\n2J\n\u001d\r\nO \n2J\n\n)#&%GK\u0000\u0002\u0001\u0004\f\u000f\u000e\u0011\f\u000f&I\u0013\u0014\u000e\u0011\f\u001cO\r\u0015\u0011\u0001\u0004\f8<\n\t\u0014\u0005\u0016\f\u000f+\u0006\u0003\u0019\u0017\u0004\f[\u0013\u000b&1\t\u0014\u0017;\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015D\u0003\u0006\u0005D\u001e\u001c\u0003\u0019\u001a\u001f\f\u000f\u0017><97>\tX\u0005\u0016\f395\u0004\f\u0012\u0017@%\u0012\f\u0013\u000b&1\u0017@\u0013\u0014\u0015\u0011\f\u000f\u0005M\u0005\u00165\u0004%J\u0001>\u0015\u0011\u0001@\t\u000b\u0015D\u0015\u0011\u0001@\f8\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0019\u0015c7\nS\u0018\u001a\u0019*7.\n2\u0012 \u000f8\u00134\n2\u00169\n57\nO\n\u0003BA \f\u001cAS\u0018\u001a\u0019\u001b7.\n2\u0012\u000f8\u00134\n2e\u0003\u0019\u0005G%\u0012\u0013\u0014-/\u0007@5\u0004\u0015\u0011\f\u00120Y&2\u0013\u000b\u000e.\f3\t\u000b%J\u0001;\u0017\u0004\u0013\u0014\u0015\u0011\f:\u000be\t\u0014\u0017@0;\t/0\u0004\f\u0012%\u000f\u0003\u0006\u0005\u0016\u0003\u0019\u0013\u0014\u0017\u0003\u0019\u0005D-'\t\u001c0\u001b\f\u001cAw=\u0015'\u0003\u0019\u0005]\u0003\u0019-/\u0007\r\u0013\u000b\u000e\u0016\u0015:\t\u0014\u00179\u0015X\u0015\u0011\u0013W\u0017\u0004\u0013\u0014\u0015\u0011\fZ\u0015\u0011\u0001@\t\u000b\u0015/\u0015\u0011\u0001\u0004\f>\u0005\u001d\u0015\u0011\u00139%J\u0001@\t\u0014\u0005\u001d\u0015\u0011\u0003\u0019%;-/\u0013*0\u0004\f\u000f+D5\u0004\u0005\u0016\f30f\u0015\u0011\u00130\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015R\u0015\u0011\u0001\u0004\f8-/\u0013\u001c\u0005\u001d\u0015R\u0007@+\u0006\t\u000b5@\u0005\u0016\u0003\u0019<@+\u0019\fF<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005R\u0003\u0006\u0005R<@\t\u0014\u0005\u0016\f30>\u0013\u0014\u0017Y\u0015\u0011\u0001@\f8\u0017\u0004\u0013\u0014\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u000b&6+\u0019\u0003\u0019\u001f\f\u0012+\u0019\u0003\u0019\u0001@\u00139\u0013*0\\O@\t\u0014\u0017@0'\u0015\u0011\u0001\u0004\f\u0012\u0017]\u0013\u001c\u0017Y\u0015\u0011\u0001\u0004\f.\t\u000b\u0005\u0016\u0005\u00165@-/\u0007\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017]\u0015\u0011\u0001@\t\u000b\u0015\u0002\u0015\u0011\u0001\u0004\f.\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u00058\t\u000b\u0017\n0;\u0015\u0011\u0001@\f\u0012\u0003,\u000e.&\u000e\u0011\f395@\f\u0012\u0017\u0004%\u0012\u0003\u0019\f\u0012\u0005.\u0003\u0019\u0005.\u0015\u0011\u0001@\f[-/\u0013\u001c\u0005\u001d\u0015M\u0015\u0016\u000e\u00115@\u0005\u001d\u0015\u0011\f30\u0010\u0005\u0016\u0013\u001c5\u001b\u000e\u0011%\u0012\f?\u0013\u000b&\f\u000f\u001a\u001b\u0003\u00060\u001b\f\u0012\u0017@%\u0012\f\u0014A.~*\u0003\u0019\u0017@%\u0012\f8\u0015\u0011\u0001@\fF-/\u0013\u0014\u0005\u001d\u0015G\u0007\u0004+\t\u000b5@\u0005\u0016\u0003\u0019<@+\u0019\fE<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005.\t4\u000e\u0011\fE\u0015\u0011\u0001\u0004\u0013\u001c\u0005\u0016\fF\u000e\u0011\f\u000f)+\u0006\t4\u0015\u0011\f30XQR\u0003\u0019\u0015\u0011\u0001/\u0015\u0011\u0001\u0004\fe\u0001@\u0003\u0019\u001e\u001c\u0001\u0004\f\u0012\u0005\u001d\u0015\u0002\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f\u00120?&I\u000e\u0011\f395@\f\u000f\u0017@%\u0012\u0003\u0019\f\u0012\u0005\u0012O*\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000eg\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4)\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u00058<\r\f\u0012\u0003\u0019\u0017@\u001e]+\u0019\f\u0012\u0005\u0016\u0005G&\u000e\u0011\f395@\f\u0012\u00179\u0015\u0011+,7;\u0013\u001c<\u0004\u0005\u0016\f\u000f\u000e\u0011\u001a\u001f\f30\b\t\u000b\u000e\u0011\f?0\u0004\u0003\u0019\u0005\u0016%3\t4\u000e:0\u0004\f30$O\\7\u001f\f\u000f\u0015G\u0015\u0011\u0001\u0004\f\u000f7-/\u0003\u0019\u001e\u0014\u0001*\u00151<\r\fe\u0007\u0004+\t\u000b5@\u0005\u0016\u0003\u0019<@+\u0019\fe\u0015\u0011\u00139\u0013\u0004A`M\u00133Q\u0002\f\u0012\u001a\u001f\fJ\u000e3O\u001fQ\u0002\fR\u0017@\f\u000f\f30\u0004\f30F\u0015\u0011\u00138\u0001\n\t3\u001a\u001f\fR\t.-/\u0013*0\u001b\f\u0012+\u0015\u0011\u00138\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0011\u0007\u001b\u000e\u0011\f\u000f\u0015`\u0015\u0011\u0001@\f\u0002\u000e\u0011\f\u0012\u0005\u00165\u0004+\u0019\u0015\u0011\u0005g\t\u0014\u0017@0[\u0015\u0011\u0013F0\u0004\fJ\u0015\u0011\f\u0012%\u000f\u0015`\u0015\u0011\u0001@\fe<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u00051\u0003\u0019\u0017X\u0013\u0014\u000e:0\u001b\f\u000f\u000e\u0015\u0011\u0013M%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\fe\t\u000b\u0017\n0F\f\u0012\u001a\u0014\t\u0014+\u00195\n\t4\u0015\u0011\fa\u0015\u0011\u0001@\fe\t\u000b5\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%a\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\f\u000f\u000e3O\u0014\u0005\u0016\u0013MQ\u0002\fe\u0001@\t\u001c0\u0015\u0011\u0013\b%:\u0001@\u00139\u0013\u001c\u0005\u0016\f\u0014AfM+,\u0015\u0011\f\u000f\u000e\u0011\u0017\n\t4\u0015\u0011\u0003\u0019\u001a\u001f\fZ\u0005\u001d\u0015\u0011\u00139%:\u0001\n\t\u000b\u0005\u001d\u0015\u0011\u0003\u0006%Y-/\u0013*0\u0004\f\u0012+\u0019\u0005X-/\u0003\u0019\u001e\u0014\u0001*\u0015?\u0001\n\t3\u001a\u001f\f]<\r\f\u0012\f\u0012\u00175\u0004\u0005\u0016\f30f\u0015\u0011\u0013W0\u0004\f\u0012\u0005\u0016%J\u000e\u0011\u0003\u0006<\r\f>\u0015\u0011\u0001\u0004\f;0\u0004\f\u000f\u001e\u0014\u000e\u0011\f\u0012\f>\u0013\u000b&8\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001b\u0003,\u0015c7V5\u0004\u0017\n0\u001b\f\u000f\u000e\u0011+,7\u0004\u0003\u0019\u0017\u0004\u001eH\u0015\u0011\u0001@\f%:\u0001@\u0013\u0014\u0003\u0019%\u0012\f>\u0013\u0014&.\t\b\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PA ¬@\u0013\u000b\u000e/\u0003\u0019\u0017@\u0005\u001d\u0015:\t\u0014\u0017\u0004%\u0012\f\u001cO\u0002\t-\u000b\u0002\t\u001279\f\u000f\u0005\u0016\u0003\t\u000b\u0017}-/\u0013*0\u001b\f\u0012+%\u000f\u0013\u001c5@+\u00060}\u0003\u0019\u00179\u0015\u0011\f\u0012\u001e\u000b\u000e:\t\u000b\u0015\u0011\f\b\tW\u0007\u0004\u000e\u0011\u0003\u0019\u0013\u000b\u000eZ\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7}0\u0004\u0003\u0019\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019<@5\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017V\u0015\u0011\u0001\n\t4\u0015]%\u0012\u0013\u001c5\u0004+0-/\u0003,\u0015\u0011\u0003\u0019\u001e\u001f\t4\u0015\u0011\f]\u0015\u0011\u0001\u0004\fY\u0003\u0019\u0017\u001b\n5@\f\u000f\u0017@%\u0012\f'\u0013\u0014&R\u0015\u0011\u0001\u0004\fY+\u0019\u0003\u0019\u001f\f\u0012+\u0019\u0003\u0019\u0001@\u00139\u0013*0N<97W&2\u0013\u000b\u000eX\f\u000f(\u0004\t\u000b-/\u0007@+\u0019\f]\t\u0014\u0005\u001d)\u0005\u0016\f\u000f\u0005\u0016\u0005\u0016\u0003\u0006\u0017\u0004\u001eZ\t\u000b+\u0019+i\u0015\u0011\u0001@\fF\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0010\t\u0014\u0005M\u0007\r\u0013\u001c\u0005\u0016\u0005\u0016\u0003\u0019<@+\u0019\f\u001cA?VG\u0017\u001b&2\u0013\u000b\u000e\u0016\u0015\u00115@\u0017\n\t4\u0015\u0011\f\u0012+,79O\n\u0015\u0011\u0001@\f%:\u0001@\u0013\u0014\u0003\u0019%\u0012\fY\u0013\u000b&e\u0015\u0011\u0001@\f'\u0007\u0004\u000e\u0011\u0003\u0019\u0013\u000b\u000e?\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7W0\u0004\u0003\u0019\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019<@5\u0004\u0015\u0011\u0003\u0019\u0013\u0014\u0017\"Q\u0002\u0013\u001c5\u0004+\u00060\"\u0001\n\t3\u001a\u001f\f/\u0003\u0019-X)\u0007\u0004+\u0019\u0003\u0006\f\u00120[\te\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0002\t\u000b\u0017\n08\u0007\r\f\u000f\u000e\u0011\u0001\n\t\u000b\u0007@\u0005h\t\u000b\u000e\u0011<@\u0003,\u0015\u0016\u000e:\t4\u000e\u00167E0\u0004\f\u0012%\u0012\u0003\u0019\u0005\u0016\u0003\u0019\u0013\u001c\u0017FQR\u0001@\u0003\u0019%:\u0001F%\u0012\u0013\u001c5\u0004+0\u0017\u0004\u0013\u0014\u0015D<\r\f.\u0015:\t\u0014\u001c\f\u0012\u0017>\t4\u0015e\u0015\u0011\u0001@\u0003\u0019\u0005R\u0005\u001d\u0015:\t\u0014\u001e\u001c\fF\u0013\u000b&i\u0015\u0011\u0001@\f.Qa\u0013\u0014\u000e\u0011\rA\n4.DISCUSSION\n4.1CAandMDS2\u0002^Qe\t\u000b\u00056%3\t\u000b\u000e\u0016\u000e\u0011\u0003\u0019\f30E\u0013\u001c5\u001b\u00156<\r\u0013\u0014\u0015\u0011\u0001E\u0013\u001c\u0017[\t\u0014\u0017978\u0005\u0016\u0003\u0019\u0017@\u001e\u0014+\u0019\f\u0002\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015`\t\u0014\u0017@0F\u0013\u0014\u0017?\t\u000b\u0017[-'\t4)\u0015\u0016\u000e\u0011\u0003,(Y\u0013\u0014&`\t3\u001a\u001f\fJ\u000e:\t\u0014\u001e\u001c\fE0\u001b\u0003\u0006\u0005\u001d\u0015:\t\u000b\u0017@%\u0012\f\u000f\u0005\u0012O\n<@5\u001b\u0015D0\u0004\u0003\u00060Y\u0017\u0004\u0013\u0014\u0015R\u0001\u0004\u0003\u0006\u001e\u0014\u0001@+\u0019\u0003\u0019\u001e\u001c\u00019\u0015D\u0015\u0011\u0001@\f.\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u0017@%\u000f\f\u0013\u000b&\u0002%\u0012+\u00195\u0004\u0005\u001d\u0015\u0011\f\u000f\u000e\u0011\u0005.\u0013\u0014&a\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005\u0012AE\u0000\u0002\u0001@\u0003\u0019\u0005.\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005.-'\t\u00127Z<\r\fX0\u00045\u0004\fE\u0015\u0011\u0013'\u0015\u0011\u0001@\fF&B\t\u000b%\u000f\u0015\u0015\u0011\u0001@\t\u000b\u0015?\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005[Q\u0002\f\u000f\u000e\u0011\fZ\t\u0014+\u0019+e\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0016\u0015X-[5@\u0005\u0016\u0003\u0019%\u0012\u0003\u0006\t\u0014\u0017\u0004\u0005\u0012O`QR\u0001@\u0013H\t\u000b\u0015\u0016\u0015\u0011\f\u0012\u0017@0\u0004\f30\"\u0015\u0011\u0001@\f\u0005\u0011\t\u000b-/\f[\u0015c7\u0004\u0007\r\fF\u0013\u000b&a-[5@\u0005\u0016\u0003\u0019%[\u0005\u0016%:\u0001@\u00139\u0013\u0014+eBwc\u0015:\t\u000b+\u0006\u0003\u0006\t\u000b\u0017\b2a\u0013\u0014\u0017@\u0005\u0016\f\u000f\u000e\u0011\u001a\u0014\t\u000b\u0015\u0011\u0013\u000b\u000e\u00167\nD\f\u0012\u001a\u001f\f\u0012\u0017;\u0003,&g\u0003\u0006\u00170\u001b\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u00151%\u0012\u0003,\u0015\u0011\u0003\u0006\f\u000f\u0005g\u0013\u000b&$\u0017@\u0013\u0014\u000e\u0016\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\u0017?w=\u0015:\t\u0014+,79A6\u0000\u0002\u0001\u0004\fD\t\u0014\u0017\n\t\u000b+,7\u0004\u0005\u0016\u0003\u0019\u0005g\t\u0014+,Qe\t\u00127\u001b\u00051\u0005\u0016\u0001@\u00133Q\u0002\f30\u0013\u0014\u0017@+,7Y\t[\u0005\u0016\u0003\u0019\u0017@\u001e\u0014+\u0019\f8%\u0012+\u00195@\u0005\u001d\u0015\u0011\f\u000f\u000e3O*QR\u0001@\u0003\u0019%:\u0001Y\u000e\u0011\f\u0012\u001e\u00145@+\u0006\t\u000b\u000e\u0011+,7Y\u0003\u0019\u0017@%\u000f\u000e\u0011\f3\t\u000b\u0005\u0016\f30Z\t4\u0015R\f3\t\u0014%:\u0001Z\u0005\u001d\u0015\u0011\f\u000f\u0007PO%\u000f\f\u0012\u00179\u0015\u0011\f\u000f\u000e\u0011\f30;\t\u000b\u000e\u0011\u0013\u00145@\u0017@0Z\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005.\u001bOP3*O$\t\u0014\u0017@0W4*Ae\u0000\u0002\u0001@\fE\t\u0014\u0017@\t\u0014+,7\u001b\u0005\u0016\u0003\u0006\u0005M\u0013\u0014&`\u0015\u0011\u0001@\f\u0007\u001b\u000e\u0011\u0013\u0014@+\u0006\f;\u0013\u000b&.\u0015\u0011\u0001@\f\u000f\u0005\u0016\f>5@\u0005\u0016\f\u000f\u000e\u0011\u0005'0\u001b\u0003\u00060V\u0017@\u0013\u000b\u0015'\u0005\u0016\u0001@\u00133Q \t\u0014\u001797N\u0007\n\t4\u000e\u0016\u0015\u0011\u0003\u0019%\u00125@+\u0006\t\u000b\u000e]\u0005\u0016\u0003\u0019-/\u0003\u0019+\u0006\t\u000b\u000e\u0016)\u0003,\u0015c7Y\u0003\u0019\u0017]\u0015\u0011\u0001@\f\u0012\u0003,\u000eD<\n\t\u000b%J*\u001e\u0014\u000e\u0011\u0013\u00145@\u0017@0 \u001d*\u0015\u0011\u0001@\f\u000f7'\u0007\u0004+\u0006\t37Y0\u001b\u0003\u0019#$\fJ\u000e\u0011\f\u0012\u00179\u0015R\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0016\u000e\u00115@-/\f\u0012\u00179\u0015\u0011\u0005e\t\u000b\u0017\n0\u0015\u0011\u0001\u0004\f\u000f7\u0010\u0005\u001d\u0015\u00115\n0\u001b\u0003\u0019\f30\u0010QR\u0003,\u0015\u0011\u0001W0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015G\u0015\u0011\f\u0012\t\u0014%:\u0001@\f\u000f\u000e\u0011\u0005\u0012AX\u0000\u0002\u0001@\fMTu5\n0\u001b\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005G\u0013\u000b&\u0002\u0015\u0011\u0001@\fAComparison ofManualandAutomaticMelodySegmentation\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-SQ\u0002\f\u000f\u000e\u0011\fg\u0017@\u0013\u000b\u0015i0\u001b\u0003\u0006\u0005\u001d\u0015:\t\u000b\u00179\u0015h&I\u000e\u0011\u0013\u0014-!\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012\u001dTu5\n0\u0004\u001e\u000b)-/\f\u0012\u00179\u0015\u0011\u0005\u0012O9\f\u000f\u001a\u001f\f\u0012\u0017'\u0003,&P\u0015\u0011\u0001\u0004\fG\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-\u0017@\f\u0012\u001a\u001f\fJ\u000ea\f\u0012\u00179\u0015\u0011\f\u000f\u000e\u0011\f\u00120/\u0015\u0011\u0001\u0004\fM%\u000f+\u00065\u0004\u0005\u001d\u0015\u0011\f\u000f\u000eR\t4\u0015\u0002\u0015\u0011\u0001@\f\u0004\u000e\u0011\u0005\u001d\u0015[\u0005\u001d\u0015\u0011\f\u000f\u0007@\u0005F\u0013\u0014&\u0002\u0015\u0011\u0001\u0004\f/\t\u0014\u0017@\t\u0014+,7\u001b\u0005\u0016\u0003\u0019\u0005\u0012A;jZ\u0013\u000b\u000e\u0011\f\u0012\u00134\u001a\u001f\f\u000f\u000e3O 2\u00020\u0004\u0003\u00060H\u0017\u0004\u0013\u0014\u0015E\u0001@\u0003\u0019\u001e\u001c\u0001\u0004+\u0019\u0003\u0006\u001e\u0014\u00019\u0015\t\u000b\u0017*7f\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015/\u0015\u0011\u0001\n\t4\u0015'\u0005\u0016\u0001@\u0013\u00145@+\u00060V<\r\fY\u0003\u0006\u001e\u0014\u0017@\u0013\u000b\u000e\u0011\f30}<\r\f\u0012%3\t\u000b5@\u0005\u0016\f>\u0013\u000b&F\t\b\u0015\u0011\u00139\u0013W\u0001@\u0003\u0019\u001e\u001c\u00010\u001b\u0003\u0006\u0005\u001d\u0015:\t\u000b\u0017@%\u0012\f8&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001@\f.\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e\u0011\u0005\u001f«]0\u001b5@\f\u001cO\u001b&2\u0013\u000b\u000eG\u0003\u0019\u0017@\u0005\u001d\u0015:\t\u000b\u0017@%\u0012\f\u0014O@\u0015\u0011\u0013/\tX-/\u0003\u0019\u0005\u00165@\u0017@0\u0004\f\u000f\u000e\u0016)\u0005\u001d\u0015:\t\u000b\u0017\n0\u0004\u0003\u0019\u0017\u0004\u001eX\u0013\u0014&6\u0015\u0011\u0001\u0004\fG\u0015:\t\u0014\u0005\u0016'\u000e\u0011\f395@\u0003,\u000e\u0011\f30]<*7/\u0015\u0011\u0001\u0004\f.\u0015\u0011\f\u0012\u0005\u001d\u00153AdD\f\u0012\u0005\u00165@+,\u0015\u0011\u0005Y\u0013\u0014& 2\u0002Q\u0002\f\u000f\u000e\u0011\f\b%\u0012\u0013\u001c\u0017\u001b@\u000e\u0011-/\f30<97f\u0015\u0011\u0001@\f\b<@\u0003,)c0\u0004\u0003\u0019-/\f\u0012\u0017\u0004\u0005\u0016\u0003\u0006\u0013\u0014\u0017\n\t\u000b+G\u0007@+\u0019\u0013\u000b\u0015\u0013\u0014<\u0004\u0015:\t\u0014\u0003\u0019\u0017\u0004\f30V\u0015\u0011\u0001\u0004\u000e\u0011\u0013\u00145@\u001e\u0014\u0001xj \u0000F~$O`QR\u0001\u0004\u0003\u0006%:\u0001\u0003\u0019\u0005Y0\u0004\f\u0012\u0007\u0004\u0003\u0006%J\u0015\u0011\f30V\u0003\u0019\u0017 ¬i\u0003\u0019\u001e\u001c5\u001b\u000e\u0011\f\">&2\u0013\u000b\u000e\u0015\u0011\u0001\u0004\fZ0\u001b\u0003\u0006\u0005\u001d\u0015:\t\u000b\u0017@%\u0012\f\u000f\u0005X\t3\u001a\u001f\f\u000f\u000e:\t\u000b\u001e\u001c\f30^\u00134\u001a\u001f\f\u000f\u000eX\t\u0014+\u0019+\u0002\u0015\u0011\u0001@\fY\u0014\u0010\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005\u0012AHM\u0005?\u0003,\u0015?%3\t\u0014\u0017<\r\f.\u0005\u0016\f\u000f\f\u0012\u0017PO\u001b\u0015\u0011\u0001@\faTu5\n0\u001b\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005e\t4\u000e\u0011\f.\u0005\u0016\u0007\u0004\u000e\u0011\f3\t\u00140>\t\u0014+\u0019\u0013\u0014\u0017@\u001e?\u0015\u0011\u0001\u0004\f.\u0007@+\u0006\t\u0014\u0017\u0004\f\u001cO*QR\u0003,\u0015\u0011\u0001Z\u0015\u0011\u0001@\f\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@- T\u001d5@0\u0004\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005FB\u0017*5@-[<\r\f\u000f\u000e.\u0016\u0014\u001fD%\u000f+\u0006\u0013\u0014\u0005\u0016\f[\f\u000f\u0017@\u0013\u001c5\u0004\u001e\u001c\u0001>\u0015\u0011\u0013/\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005\u0012Tu5\n0\u0004\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0005\u0012Ai\u0000\u0002\u0001\u0004\fa\u0007\u0004+\u0006\u0013\u000b\u0015i\u0005\u0016\u0001\u0004\u00134QR\u0005P\u0015\u0011\u0001\n\t\u000b\u0015h\u0005\u00165@<*T\u001d\f\u0012%J\u0015\u0011\u0005a\u0002\t\u000b\u0017\n0Ee\t\u000b\u000e\u0011\f1\u0015\u0011\u0001@\f1\u0013\u001c\u0017@\f\u000f\u0005QR\u0003,\u0015\u0011\u0001;\u0015\u0011\u0001@\fE\u0001@\u0003\u0019\u001e\u001c\u0001\u0004\f\u0012\u0005\u001d\u001580\u001b\u0003\u0019\u0005\u001d\u0015:\t\u0014\u0017@%\u000f\fE&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001\u0004\fF\u0013\u0014\u0015\u0011\u0001@\fJ\u000e.\u0013\u001c\u0017\u0004\f\u0012\u0005\u0012A8D\u001797\u001bQ\u0002\t\u001279O@Q\u0002\f0\u001b\f\u0012%\u0012\u0003\u00060\u0004\f30;\u0015\u0011\u0013'\u0003\u0019\u0017@%\u0012+\u00195\n0\u001b\fX\t\u000b+\u0006\u0005\u0016\u0013'\u0015\u0011\u0001@\f\u000f- \u0003\u0019\u0017;\u0015\u0011\u0001@\fE%3\t\u0014+\u0019%\u00125\u0004+\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017W\u0013\u000b&\u0002\u0007\r\u0013\u000b\u0015\u0011\f\u0012\u00179\u0015\u0011\u0003\t\u000b+<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005e&I\u000e\u0011\u0013\u0014- -'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0002\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005\u0012A\n−0.03 −0.02 −0.01 0 0.01 0.02 0.03−0.03−0.02−0.0100.010.020.03\n1\n2\n34\n56\n78\n910\n1112\n13\n14\n15161718\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\u0007 \t\u001b\u000b\n\t~\u0019E:8\u00129\u000f4(©\u000f\b\u0017\u001a&0/\u001b\u0010\u001c\u000f4\u0014\u0004/\f2\":\u0002\u000b\u001b\u001dP2\":\u0011\u000f\b/\f- B©:4\u0014<\u0012\n\u001d9\u0014\f\u0017\u001aB©\u0019\u0004\u0012\\&)( \u0014\u0004/\f\n\u0019\u001b(\u001b-\u0004\u0017\u001a&0/\u0016\u0012\\\u0010;>\u000e\r \u0010\\\u0019\u001b>\f\n&)\u001d$\u0012$\u0010\u0010\u000f\u0012\u0011P1\u0013\u0011\u0015\u0014\u000e\u0016\u001f2\"/\u001b( >\u0017\r \u0012$+\f&=2\":G-\"\u0014\"!9\u000f8\u00129+E\u0017\u0018\u000f\u0019\u0011\u0015\u001a\u000e\u0016$C\n4.2Detection ofBoundaries fromMarkersdD\f\u0012\u0005\u00165@+,\u0015\u0011\u0005.&\u000e\u0011\u0013\u001c- 2\u0002 \t\u0014\u0017@0Hj \u00008~\u0010\u0005\u0016\u0001\u0004\u00134QR\u0005.\u0015\u0011\u0001@\t\u000b\u0015.\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\f?\u0003\u0019\u00058%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u0017\u0004%\u000f7\t\u000b-/\u0013\u001c\u0017@\u001e\u0010\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005/%:\u0001@\u0013\u001c\u0003\u0019%\u0012\f\u000f\u0005\u0012AN\u0000\u0002\u0001@\u0003\u0019\u0005X-/\f3\t\u000b\u0017@\u0005E\u0015\u0011\u0001\n\t\u000b\u0015?\u0003,\u0015/%3\t\u0014\u0017N<\r\f]\u0007\r\u0013\u001c\u0005\u0016\u0005\u0016\u0003,)<\u0004+\u0006\fE\u0015\u0011\u0013Y\fJ(\u001b\u0015\u0016\u000e:\t\u000b%\u000f\u0015F\t\bk\u0011p\u0014t\u000fs\u000f r2n$s[&\u000e\u0011\u0013\u001c- \u0015\u0011\u0001\u0004\f\u0012\u0003,\u000e8%J\u0001\u0004\u0013\u001c\u0003\u0019%\u0012\f\u0012\u0005.\t\u0014\u0017\n0;\u0015\u0011\u0013]\f\u0012\u001a\u001c\t\u000b+\u00195\n\t\u000b\u0015\u0011\f\u0015\u0011\u0001\u0004\f/\t\u00145\u001b\u0015\u0011\u0013\u001c-'\t\u000b\u0015\u0011\u0003\u0019%/\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017H<97W0\u001b\u0003,\u000e\u0011\f\u0012%\u000f\u0015F%\u0012\u0013\u0014-/\u0007\n\t\u000b\u000e\u0011\u0003\u0019\u0005\u0016\u0013\u0014\u0017WQR\u0003,\u0015\u0011\u0001W\u0015\u0011\u0001@\f<@\t\u0014\u0005\u0016\f\u0012+\u0019\u0003\u0019\u0017@\f\u001cA`CW\fG\t\u000b+\u0019\u000e\u0011\f\u0012\t\u001c0\u001b7X\u0003\u0006\u00179\u0015\u0016\u000e\u0011\u0013*0\u001b5@%\u0012\f30X\u0015\u0011\u0001\u0004\fM\u0005\u001d\u0015:\t4\u0015\u0011\u0003\u0006\u0005\u001d\u0015\u0011\u0003\u0019%3\t\u000b+\r\u0015\u0011\f\u0012%J\u0001\u0004\u0017@\u0003\u000695@\feQ\u0002\f5\u0004\u0005\u0016\f30\b\u0015\u0011\u0013>%\u000f\u0013\u001c-/\u0007@5\u001b\u0015\u0011\f \u0002=\u000e\u0011\f3\t\u0014+1<\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005&\u0004Z&I\u000e\u0011\u0013\u0014-_\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f?-'\t\u000b\u000e\u0011\u001f\f\u000f\u000e\u0007\r\u0013\u0014\u0005\u0016\u0003\u0019\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005\u0012A\u0000\u0002\u0001\u0004\fE\u0017*5\u0004-?<\r\f\u000f\u000eR\u0013\u000b&`<\r\u0013\u001c5@\u0017@0@\t4\u000e\u0011\u0003\u0006\f\u000f\u0005G0\u001b\f\u0012\u0007\r\f\u0012\u0017\n0\u001b\u0005R\u0013\u001c\u0017>\u0015\u0011\u0001@\f8\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015\u0011\u0005\u0012O\n\t\u0014\u0005RQ\u0002\f\u0012+\u0019+\t\u000b\u0005F\u0015\u0011\u0001\u0004\f\u0012\u0003,\u000eF\u000e\u0011\f\u0012\u001e\u001c5\u0004+\t4\u000e\u0011\u0003,\u0015u7H\u0013\u001c\u0017H\u0015\u0011\u0001\u0004\f/\u0005\u0016%\u0012\u0013\u0014\u000e\u0011\f\u0014AYwc\u0017 ¬i\u0003\u0019\u001e\u00145\u0004\u000e\u0011\f]*A  \t\u000b\u0017\n0\b\u0003\u0006\u0017 ¬i\u0003\u0006\u001e\u000b)5\u001b\u000e\u0011\fE*A \u000bx\u0015\u0011\u0001\u0004\f.&I\u000e\u0011\f395\u0004\f\u0012\u0017@%\u0012\u0003\u0019\f\u0012\u0005D\u0013\u0014&`-'\t4\u000e\u0011\u001f\f\u000f\u000e\u0011\u0005R\t\u000b\u0017\n0Y\u0015\u0011\u0001\u0004\fF0\u0004\f\u000f\u0015\u0011\f\u0012%J\u0015\u0011\f30Z<\r\u0013\u001c5\u0004\u0017\n0*)\t4\u000e\u0011\u0003\u0006\f\u000f\u00056\t\u000b\u000e\u0011\f`\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\f\u001208&I\u0013\u0014\u000eh\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u0004\u0015g1\t\u0014\u0017@0F\fJ(\u0004%\u000f\f\u000f\u000e\u0011\u0007\u0004\u0015 \u001a\u001bAi\u0000\u0002\u0001@\f\u0012\u0005\u0016\f1\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005%3\t\u000b\u0017'<\r\fD%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u00060\u001b\f\u000f\u000e\u0011\f30/\t\u0014\u00051\u000e\u0011\f\u0012\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\fR\u0013\u0014&$\u0015\u0011\u0001@\fR\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u0015\u0011\u0005a<\r\f\u000f\u0001\n\t3\u001a*\u0003\u0006\u0013\u000b\u000e3|ª`\u001a\u001f\f\u0012\u0017V\u0003\u0019&M\u0015\u0011\u0001@\f\u000f\u000e\u0011\f>\t\u000b\u000e\u0011\f;\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005/Q\u0002\f\u000f\u000e\u0011\f>\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\fZ\u0003\u0019\u0005'\u0013\u001c\u0017\u0004+\u00197f+\u0019\u0003,\u0015\u0016\u0015\u0011+\u0019\f;\t\u0014\u001e\u000b\u000e\u0011\f\u0012\f\u000f)-/\f\u0012\u00179\u0015DB+\u0019\u00133Q}\u001a\u0014\t\u0014+\u00195@\f\u000f\u0005g\u0013\u000b&\\\u0015\u0011\u0001\u0004\fe-'\t\u000b\u000e\u0011\u001f\fJ\u000e\u0011\u0005`&\u000e\u0011\f395@\f\u0012\u0017\u0004%\u000f7@JO\u001c\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\fR\f\u000f(\u001b\u0003\u0006\u0005\u001d\u0015g\u001e\u001c\u00139\u0013*0\t\u000b\u001e\u0014\u000e\u0011\f\u0012\f\u0012-/\f\u000f\u0017*\u0015[\t4\u0015[\u001e\u0014\u0003\u0019\u001a\u001f\f\u0012\u0017W\u0005\u0016%\u000f\u0013\u0014\u000e\u0011\f'\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005E\u0015\u0011\u0001@\t\u000b\u0015E-'\t\u00127\u0010<\r\f/%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u00030\u001b\f\u000f\u000e\u0011\f30\t\u000b\u0005D\u0015\u0011\u0001\u0004\f.<\n\t\u000b\u0005\u0016\f\u0012+\u0019\u0003\u0006\u0017\u0004\fF\u0013\u0014&6<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012A\n4.3Performances ofAutomaticSegmenters\u0000\u0002\u0001\u0004\fE\u0007\r\fJ\u000e\u0016&2\u0013\u0014\u000e\u0011-'\t\u000b\u0017@%\u0012\fF\u0013\u000b&1\t\u00145\u0004\u0015\u0011\u0013\u0014-'\t\u000b\u0015\u0011\u0003\u0019%F\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\fJ\u000e\u0011\u0005M%3\t\u000b\u0017><\r\fF-/\f3\t\u0014\u0005\u00165\u001b\u000e\u0011\f305\u0004\u0005\u0016\u0003\u0006\u0017\u0004\u001ef\tN\u0015\u0011\f\u0012%:\u0001@\u0017\u0004\u0003\u0006*5\u0004\f;\u0003\u0019\u0017*\u0015\u0016\u000e\u0011\u0013*0\u001b5@%\u0012\f\u00120x\u0003\u0019\u0017  48&I\u0013\u0014\u000e>\u0015\u0011\fJ(\u001b\u0015Y\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0003\u0019\u0017\u0010%\u0012\u0013\u001c\u0001\u0004\f\u000f\u000e\u0011\f\u0012\u00179\u0015M\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005M\t\u0014\u0017@0>\u0005\u00165@%\u0012%\u0012\f\u000f\u0005\u0016\u0005\u0016\u0003\u0006\u001a\u001c\f\u0012+,7;\t\u001c0\u001b\u0013\u001c\u0007\u001b\u0015\u0011\f30;\u0003\u0006\u0017>\u0015\u0011\u0001\u0004\fE\u0000h\u0013\u001c\u0007\u0004\u0003\u0006%\u0000.\fJ\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017]\t\u0014\u0017@0'\u0000P\u000e:\t\u0014%:*\u0003\u0006\u0017\u0004\u001e'\u0000\f\u00008\u0000R1&I\u000e:\t\u000b-/\f\u000fQ\u0002\u0013\u000b\u000e\u0011> :\u0010\u000b=Ag\u0000h\u00138\u0015\u0011\u0001\u0004\u0003\u0006\u0005g\f\u0012\u0017\n0$O\u0003,\u0015D\u0003\u0019\u0005M0\u001b\f\u000f\n\u0017\u0004\f30]\u0015\u0011\u0001\u0004\f.\u0007\u0004\u000e\u0011\u0013\u0014<\n\t\u000b<@\u0003\u0019+\u0019\u0003\u0019\u0015c7Z\u0013\u000b&`\t\u0014\u001e\u000b\u000e\u0011\f\u0012\f\u0012-/\f\u0012\u00179\u0015M\t\u0014\u0005\u0012|\u0018D\u0006\u001b\u001d\u001c \t\n\t\n\u0012\n1\n2\u001f\n\u001d>\u0011\u000b \u001fG,9 \u000b\u001f\u001ea\u0011\u000b \u001fG,9 \u000bD\n\u0011\u000b \u001fG,9QR\u0001\u0004\f\u000f\u000e\u0011\f/\u0015\u0011\u0001@\f\u000f\u000b\u001f a\u0011\u000b \u001f ,9:\u0012 /\u0003,&e\u0015cQ\u0002\u0013>\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005<\u000b \u001f ,\u0010<\r\f\u000f+\u0006\u0013\u0014\u0017@\u001eZ\u0015\u0011\u0013>\u0015\u0011\u0001\u0004\fX\u0005\u0011\t\u0014-/\f\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015G\u0007\u001b\u000e\u0011\u0013*0\u00045\u0004%\u0012\f30;<97\"!.O\\\t\u0014\u0017\n0;/\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e\u0016QR\u0003\u0019\u0005\u0016\f\u001cO#!\u0018<\r\f\u0012\u0003\u0019\u0017@\u001e\nF\t\u0014+\u0019\u001e\u0014\u0013\u0014)\u000e\u0011\u0003,\u0015\u0011\u0001@-]G\u0013\u000b\u000e\u000f. B\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005:JA;jZ\u0013\u0014\u000e\u0011\f\u0012\u00134\u001a\u001f\fJ\u000e3O9\u001d>\u0011\u000b \u001f7,9<\u0012 /\u0003,&e\u0015\u0011\u0001\u0004\f/\u0017@\u0013\u0014\u0015\u0011\f\u000f\u00050 20 40 60 80 100 12000.20.40.60.8\n0 10 20 30 40 50 60 70 8000.20.40.60.81A \nB \u0000\u0002\u0001\u0004\u0003$\u0005#\u0007\u0007&% \u000b('\u000f4\u00105\u0012\u001c\u0014\u0016-\u0004!\u001c2\"\u0017\n\u0010 \u0014\u0016\u0015o\u0017\u001a2\u0016!$#<&)!$\u0010,B©\u0014\u0004\u0010\u001c\u000fG\u0012$\u000f4\u0014\u0004/\u001b\u001096\u0006\u000f\u0011/ -\"!\\2\u0015\r<6\u0018\u0015*\u0014\u0004!&.A\u001b\u001d9&)!$B\f\u0012R\n\u000f*)+\u0016?2\"/\u001b(y\n\u000fhH,\u0016.-0(\u001b&.\u0012\u001c&)\u001d9\u0012\u001c&)( >©\u0014\u0004\u0019E/\u001b(\u001b2\u0016!9\u000f4&)\u0010\u00062\u0016!\\& \u000f\b/>©:42\u0016\u001d\\# C\t4\u000e\u0011\f0/?\u0017@\u0013\u000b\u0015\u0011\f\u0012\u0005a\t\u000b\u0007\n\t4\u000e\u0016\u0015a\t\u000b\u0017\n0/G\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e\u0016QR\u0003\u0019\u0005\u0016\f;\u001d\u001b\u0001\u0004\f\u0012\u0017@%\u0012\f \u0018D1\u001b\u001d\u001c \t\n\t\n\u0003\u0019\u0005a\t.-/\f\u0012\t\u0014\u0005\u00165\u0004\u000e\u0011\f\u0013\u000b&R\u0001@\u00133Q \u0013\u0014&\u0015\u0011\f\u0012\u0017^\tZ\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017W\u0003\u0006\u0005F%\u0012\u0013\u000b\u000e\u0016\u000e\u0011\f\u0012%\u000f\u0015FQR\u0003,\u0015\u0011\u0001W\u000e\u0011\f\u0012\u0005\u0016\u0007\r\f\u0012%J\u0015E\u0015\u0011\u0013Y\u0015uQ\u0002\u0013\u0007\r\u0013\u0014\u0005\u0016\u0003,\u0015\u0011\u0003\u0006\u0013\u0014\u0017@\u0005/\t4\u0015X0\u001b\u0003\u0019\u0005\u001d\u0015:\t\u0014\u0017@%\u000f\f(/\rA;w=\u0015[%\u0012\t\u0014\u0017^<\r\f'\u0005\u0016\u0001@\u00133QR\u0017\"\u0015\u0011\u0001\n\t4\u0015[\u0015\u0011\u0001\u0004\f/%\u0012\u0013\u001c-/\u0007\u0004+\u0019\f\u000f)-/\f\u000f\u0017*\u0015\u0002\u0013\u0014& \u0018D1\u001b\u001d\u001c \t\n\t\n%\u0012\t\u0014\u0017Z<\r\fG%\u0012\u0013\u001c-/\u0007\u00045\u0004\u0015\u0011\f30/\u0003\u0019\u0017]\u0015\u0011\fJ\u000e\u0011-/\u0005\u0002\u0013\u0014&i\u0015\u0011\u0001\u0004\fG\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7\u0013\u000b&\r-/\u0003\u0019\u0005\u0016\u0005\u0016\u0003\u0019\u0017@\u001eM\tM<\r\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u00167-\u001832\n2\u0007\n\u0007\n\u0013\u0014\u000e`\u0007\u0004+\t\u000b%\u0012\u0003\u0019\u0017@\u001eG\tM-'\t\u000b\u000e\u0011\u001c\f\u000f\u000eiQR\u0001@\fJ\u000e\u0011\fg\u0015\u0011\u0001@\fJ\u000e\u0011\f\u0003\u0019\u0005D\u0017@\u0013[<\r\u0013\u001c5@\u0017@0@\t4\u000e\u00167B\u00185416.7\u0007!\t\nO@\t\u0014\u0017\n0]%3\t\u0014\u0017Z<\r\f8\f\u000f(\u001b\u0007\u0004\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30Y\t\u0014\u0005\u0012|\u001858\n2\u0007\n6\u001b\n\u0012U\u00182\n2\u0007\n\u0007\n\u001839\t:\u001b\n\u0018 \u00185416.7\u0007!\t\n\u001d\r\n\u001839\t:\u001b\nQR\u0001\u0004\f\u000f\u000e\u0011\f \u001839\t:\u001b\n\u0003\u0019\u0005`\u0015\u0011\u0001\u0004\fR\tM\u0007\u001b\u000e\u0011\u0003\u0006\u0013\u000b\u000e\u0011\u0003@\u0007\u001b\u000e\u0011\u0013\u001c<\n\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7[\u0013\u0014&$\tG\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u00151\t\u0014\u0017@0[\u0003,\u00151%3\t\u0014\u0017<\r\f8%\u000f\u0013\u001c-/\u0007@5\u001b\u0015\u0011\f30/&I\u000e\u0011\u0013\u001c- \u0015\u0011\u0001\u0004\f8\t3\u001a\u001f\f\u000f\u000e:\t\u000b\u001e\u001c\fE0\u0004\u0003\u0019\u0005\u001d\u0015:\t\u0014\u0017\u0004%\u0012\f.<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u000f\u0017><\r\u0013\u001c5\u0004\u0017\n0@\t4\u000e\u0011\u0003\u0019\f\u0012\u0005\t\u000b\u0017\n0}\u0015\u0011\u0001\u0004\f;%J\u0001\u0004\u0013\u001c\u0003\u0019%\u0012\f\b\u0013\u0014&+/\rA wu\u0017\u0013\u00145\u0004\u000eY%3\t\u0014\u0005\u0016\f\u0014OG\u001e\u0014\u0003\u0006\u001a\u001c\f\u0012\u0017\u0015\u0011\u0001\n\t4\u0015]\u0015\u0011\u0001\u0004\f\u0010\t3\u001a\u001f\f\u000f\u000e:\t\u0014\u001e\u0014\f\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005/+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001}\u0003\u0019\u0005;\u001c\u0019<­ :\u0010\u0004OR&2\u0013\u0014+\u0019+\u0006\u00133QR\u0003\u0019\u0017@\u001e^\u0015\u0011\u0001\u0004\f>\u001e\u001c5\u0004\u00030\u001b\f\u0012+\u0019\u0003\u0019\u0017@\f\u0012\u0005'\u0013\u0014&8\u0000\f\u00008\u0000&\u000e:\t\u0014-/\f\u000fQa\u0013\u0014\u000e\u0011?Qa\fM\u0005\u0016\fJ\u0015;/?\u0015\u0011\u0013F\t\u0014\u0007\u0004\u0007\u0004\u000e\u0011\u00133(\u001b\u0003\u0019-'\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012+,7X\u0003,\u0015\u0011\u0005g\u0001\n\t\u0014+,&cO*\u0001@\f\u0012\u0017\u0004%\u0012\f0/\u001c\u0012\u000f\u0014\u001bO\t\u000b\u0017\n0Y\u0015\u0011\u0001\u0004\f\u0012\u0017 \u001859\t<\u001b\n\u0012 =\n5>\u001d?\u0003\u0019@\n\u0012x\u0016­ \u0019\u0014\u001f9A\u0000h\t\u0014<@+\u0019\f \u0010\u0004A ;\u000e\u0011\f\u0012\u0007\r\u0013\u0014\u000e\u0016\u0015\u0011\u0005E\u0015\u0011\u0001@\f'%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\u0003\u0019\u0005\u0016\u0013\u001c\u0017^<\r\fJ\u0015uQ\u0002\f\u000f\f\u0012\u0017^\u0015\u0011\u0001@\f/\u0015\u0011\f\u0012\u0005\u001d\u0015\u0011\f30^\t\u000b+\u0006\u001e\u0014\u0013\u0014)\u000e\u0011\u0003,\u0015\u0011\u0001\u0004- \t\u0014\u0017\n0]\u0013\u0014\u0015\u0011\u0001\u0004\f\u000f\u000eM\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004-/\u0005DQR\u0001\u0004\u0003\u0019%J\u0001;\t4\u000e\u0011\f8\u0017@\u0013\u000b\u0015R<\n\t\u0014\u0005\u0016\f\u00120Z\u0013\u001c\u0017>-/\f\u000f+\u0006\u0013*0\u001b\u0003\u0019%\u0003\u0019\u0017\u001b&2\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017PAV\u0000\u0002\u0001\u0004\fY\u0015uQa\u0013W\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-/\u0005X5@\u0005\u0016\f30^&2\u0013\u000b\u000e/\u0015\u0011\u0001@\f]%\u0012\u0013\u001c-/\u0007@\t\u000b\u000e\u0011\u0003\u0019\u0005\u0016\u0013\u001c\u0017%\u0012\t\u0014+\u0019%\u00125@+\u0006\t4\u0015\u0011\fR\u000e\u0011\f\u000f\u0005\u0016\u0007\r\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012+,7E\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u00056\u0013\u000b&\n\u000e:\t\u0014\u0017@0\u0004\u0013\u0014-\u0018+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001Z2&I\u000e\u0011\u0013\u0014- 3M\u0015\u0011\u0013\u000b\u001f`\t\u0014\u0017@0[LM)=\u001e\u000b\u000e:\t\u0014-/\u00056\u0013\u0014&\n+\u0006\f\u000f\u0017@\u001e\u0014\u0015\u0011\u0001 \u0014G\t\u0014\u0017@0Y\u001c\u0019 \u001d\u001f\u0015\u0011\u0001@\fg&2\u0013\u0014\u000e\u0011-/\fJ\u000e1\u000e:\t\u000b\u0017\n0\u001b\u0013\u001c-/+,7F\u0005\u0016\f\u000f)+\u0019\f\u0012%J\u0015\u0011\f30/\t.\u0017*5@-?<\r\fJ\u000e \u0019.<\r\f\u000f\u0015cQ\u0002\f\u0012\f\u0012\u0017Y38\t\u0014\u0017@0/\u00148\t\u000b\u0017\n0X\u0003\u0019\u0017@\u0005\u0016\fJ\u000e\u0016\u0015\u0011\f30X\t.\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\t4&I\u0015\u0011\f\u000f\u000e?\u0019E\u0017@\u0013\u0014\u0015\u0011\f\u000f\u0005\u0012O\u0004\u0015\u0011\u0001\u0004\f.+\u0006\t\u000b\u0015\u0016\u0015\u0011\f\u000f\u000eD\u0003\u0019\u0017@\u0005\u0016\f\u000f\u000e\u0016\u0015\u0011\f\u00120>\t?-'\t\u000b\u000e\u0011\u001c\f\u000f\u000ee\f\u0012\u001a\u001f\fJ\u000e\u00167 \f\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005\u0012A1w=\u0015%\u0012\t\u0014\u0017^<\r\f/\u0005\u0016\f\u0012\f\u0012\u0017\"\u0015\u0011\u0001@\t\u000b\u0015F\u0015\u0011\u0001@\f \t1\u000b \u0000Fj \u0001\n\t\u000b\u0005[<\r\fJ\u0015\u0016\u0015\u0011\f\u000f\u000eE\u0007\r\f\u000f\u000e\u0016&2\u0013\u000b\u000e\u0011-'\t\u0014\u0017@%\u000f\f\u0012\u0005E\u0015\u0011\u0001@\t\u0014\u0017\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e'\u0015\u0011\f\u0012%:\u0001@\u0017\u0004\u0003\u0006*5\u0004\f\u0012\u0005/\u0015\u0011\u0001\n\t4\u0015]0\u0004\u0013\"\u0017\u0004\u0013\u0014\u0015]\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015]-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%;\u0003\u0019\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\f\u000f\u001a\u001f\f\u0012\u0017\u0003,&F\u0015\u0011\u0001@\f;\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-'\t\u000b\u0017@%\u0012\f\u0012\u0005]\u0017@\f\u000f\f30\u0003\u0019-/\u0007\u0004\u000e\u0011\u00134\u001a\u001f\f\u0012-/\f\u0012\u00179\u00153Oa\u0003\u0006\u0017}\u0007\n\t\u000b\u000e\u0016\u0015\u0011\u0003\u0019%\u00125\u0004+\u0006\t\u000b\u000e<\r\f\u000f%3\t\u00145\u0004\u0005\u0016\fZ\u0013\u0014&D\u0015\u0011\u0001@\f]\u0001@\u0003\u0019\u001e\u0014\u0001f\u0007\u001b\u000e\u0011\u0013\u001c<@\t\u0014<@\u0003\u0019+\u0019\u0003,\u0015u7^\u0013\u000b&M&2\t\u0014+\u0019\u0005\u0016\f;\t\u000b+\t4\u000e\u0011-/\u0005\u0012A^\u0000\u0002\u0001\u0004\fZ\t\u000b+\u0006\u001e\u0014\u0013\u0014)\u0000\u0002\u0001\f\u0003?\u0005\b\u0007A% \u000bCB\u001f!\u001c\u0014\u0004>\u001b2\u0016>©\u000f\u0011:\b\u000f8\u0012\u0012\r=\u0014\u0016\u0015\u0018\u0017 \u000f\u0011\u0010'\u0010'&)\u001096\u0016\u0015*2\":\u0011\u00105& 2\":42\u0016!$\u0017\n\u0010P602\"/\u001b((©\u000f\u0011\u0010\u001312<-\"!\\&.&3\u0017\u001a&0/\u0016\u0012N\u0014\u0016\u0015\u0018\u0015*\u0014\u0004\u0019\u001b!\u001f(©\u000fED{&)!\\&0/\u0016\u0012o\u00105&)-\u0004\u0017\u001a&0/\u0016\u0012\u001c23\u0012$\u000f4\u0014\u0004/=2\":4-\u0016\u0014\"!9\u000fG\u0012$+E\u0017\n\u00109CD+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@- \u00182\n2\u0007\n\u0007\n\u00185416.7\u0007!\t\n\u001838\n2\u0007\n6\u001b\t1\u000b \u0000Fj \u001bA ;\u0019:\u0010 \u001bA \u0007\u0010\u001f *A\u0019\u0018\u0014;\u001adM\t\u0014\u0017@0\u0004\u0013\u001c- \u001bA \u0017;\u0019\u0014 \u001bA \u001c:\u0010 *A \u0010 \u0012\u0007\u0017¬i\u0003,(\u001b\f30H \f \u0012\u000f\u0014\u001f \u001bA \u0010\u001f* \u001bA \u0019;\u0019:\u0014 *A \u0010\u001b\u0014\u001b\u0019¬i\u0003,(\u001b\f30H \f \u0012!\u001c\u0019\u0014 \u001bA \u0012\u0014\u0014 \u001bA \u0007\u0014\u0007\u0017 *A \u0019*\u001c\u0012\u000e\u0011\u0003,\u0015\u0011\u0001\u0004- \u0001\n\t\u000b\u0005G\u0015\u0011\u0001@\fF\u0015\u0011\f\u0012\u0017@0\u0004\f\u0012\u0017\u0004%\u000f7;\u0013\u0014&.l\u0001\u0000\u0014sJq\u001dF=t\u000fs\u0014'sJn\u0004.\u0015\u0011\u0001@\fE\f\u000f(\u001b%\u0012\f\u000f\u000e\u0011\u0007\u001b\u0015\u0011\u0005\u0012AE\u0000\u0002\u0001@\u0003\u0019\u0005<\r\f\u000f\u0001\n\t3\u001a*\u0003\u0006\u0013\u000b\u000eE\u0003\u0006\u0005F%\u0012\u0013\u0014\u0017\u0004\u0004\u000e\u0011-/\f30\u0010\t\u0014+\u0019\u0005\u0016\u0013;<97;\u0015\u0011\u0001@\f/\t3\u001a\u001f\f\u000f\u000e:\t\u000b\u001e\u001c\f/\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015F+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001@\u0005\u0012OQR\u0001\u0004\u0003\u0019%J\u0001Y\u0003\u0006\u00058\u0018\u0019\u0016­ ?\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005e&2\u0013\u0014\u000ee\u0015\u0011\u0001\u0004\f.\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005R\t\u0014\u0017@0 \u0014\"­ \u0017E\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\u0005e&I\u0013\u0014\u000e\u0015\u0011\u0001\u0004\f\u0010\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@->AS\u0000\u0002\u0001@\u0003\u0019\u0005'\u0003\u0019\u0005/\u0015\u0011\u0001@\f>\u000e\u0011\f3\t\u000b\u0005\u0016\u0013\u001c\u0017}QR\u000197NQ\u0002\f;%:\u0001@\u0013\u0014\u0005\u0016\f;LM)=\u001e\u0014\u000e:\t\u000b-/\u0005\u0013\u000b&e+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001PO\\\u000e\u0011\f\u0012\u0005\u0016\u0007\r\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u000f+\u001979OP\u0013\u0014&G\u001c\u0019Y\t\u000b\u0017\n0-\u0014\u001bA/\u0000\u0002\u0001\u0004\f\u000f\u000e\u0011\fX\u0003\u0019\u0005F\u0017@\u0013Y\u0005\u0016\u0003\u0006\u001e\u0014\u0017@\u0003,\n%\u0012\t\u0014\u00179\u0015\u001a\u0014\t\u000b\u000e\u0011\u0003\u0006\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017>\u0003\u0019\u0017 \u001838\n2\u0007\n6\u001b\n<\r\f\u000f\u0015uQa\f\u0012\f\u0012\u0017]\u0015\u0011\u0001@\fG\u0015cQ\u0002\u0013?LD)=\u001e\u0014\u000e:\t\u0014-/\u0005\u0012O\u001b-/\f3\t\u000b\u0017@\u0003\u0019\u0017@\u001eE\u0015\u0011\u0001\n\t4\u0015\u001e\u000b\u000e:\t\u0014- +\u0019\f\u0012\u0017\u0004\u001e\u0014\u0015\u0011\u0001V0\u0004\u00139\f\u0012\u0005/\u0017\u0004\u0013\u0014\u0015]\t\u000b#$\f\u000f%\u000f\u0015/\u0015\u0011\u0001@\fY\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017\u0004\f\u0012\u0005\u0016\u0005/\u0013\u0014&G\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4)\u0015\u0011\u0003\u0019\u0013\u0014\u0017><@5\u001b\u0015e\u0003,\u0015R\u0003\u0006\u0005e\u0015\u0011\u0001\u0004\f.-/\f\u000f\u0015\u0011\u0001@\u0013*0/\u0015\u0011\u0013X0\u001b\f\u000f\u0015\u0011\f\u0012%\u000f\u0015R<\r\u0013\u00145@\u0017@0@\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005e\u0015\u0011\u0001@\t\u000b\u0015D\t\u0014%J\u0015\u00115\n\t\u0014+\u0019+,7AComparison ofManualandAutomaticMelodySegmentation\u0003\u0019-/\u0007\u0004\u000e\u0011\u00134\u001a\u001f\f\u000f\u0005\u0002\u0015\u0011\u0001@\f.\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-'\t\u000b\u0017@%\u0012\f\u0012\u0005\u0012A\u0000\u0002\u0001\u0004\fH\u0015\u0011\f\u0012\u0017@0\u0004\f\u0012\u0017\u0004%\u000f7\u0013\u000b&?\u00134\u001a\u001f\f\u000f\u000e\u0016)=\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e\u0004OD\u0015\u0011\u0013\u001c\u001e\u0014\f\u000f\u0015\u0011\u0001@\fJ\u000eZQR\u0003,\u0015\u0011\u0001S\u0015\u0011\u0001\u0004\f\b\u000e\u0011\f\u000f+\t4)\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012+,7;+\u0019\u00133Q\u0018\u001a\u0014\t\u0014+\u00195\u0004\f[\u0013\u000b&:\u001852\n2\u0007\n\u0007\nO$-'\t\u00127Z\u0003\u0019-/\u0007\u0004+\u00197]\u0015\u0011\u0001\n\t4\u0015.-'\t\u000b\u0017*7Y\u0013\u0014&`\u0015\u0011\u0001\u0004\f[\u0005\u0016\f\u0012\u001e\u000b)-/\f\u0012\u00179\u0015\u0011\u00056\u0013\u0014<\u0004\u0015:\t\u0014\u0003\u0019\u0017\u0004\f30?<97X\t\u000b\u0017\n\t\u000b+\u00197\u001b\u0005\u0016\u0003\u0019\u00051\u0013\u0014&$\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005a\t4\u000e\u0011\fe\u0005\u0016\u0007@+\u0019\u0003,\u0015a\u0003\u0019\u0017?\u0015cQ\u0002\u0013F0\u001b\u0003\u0019#$\fJ\u000e\u0016)\f\u0012\u00179\u0015h\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005\\<97G\u0015\u0011\u0001@\f1\t\u000b+\u0006\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@->Ai\u0000\u0002\u0001@\u0003\u0019\u0005h\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u0015i-'\t\u00127G<\r\f1\u0003\u0019-/\u0007\r\u0013\u0014\u000e\u0016\u0015:\t\u000b\u0017*\u0015\u0003,&\n\u0015\u0011\u0001\u0004\fa\u0005\u0011\t\u0014-/\f\u0002\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003\u0019\u0015\u0011\u0001\u0004- \u0003\u0019\u0005`\t\u0014\u0007@\u0007\u0004+\u0019\u0003\u0006\f\u00120E\u0015\u0011\u0013.95\u0004\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012O\u001f<\r\f\u0012%\u0012\t\u00145@\u0005\u0016\fg\u0015\u0011\u0001@\f\u0002+\u0006\t\u000b\u0015\u0016\u0015\u0011\fJ\u000e\t4\u000e\u0011\fM-[5@%:\u0001?\u0005\u0016\u0001@\u0013\u000b\u000e\u0016\u0015\u0011\f\u000f\u000ea\u0015\u0011\u0001@\t\u0014\u0017'0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015\u0011\u00051\t\u0014\u0017@0X\u0001@\f\u0012\u0017\u0004%\u0012\fR95@\f\u000f\u000e\u00167?\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005%3\t\u000b\u0017><\r\f8%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u00179\u0015RQR\u0003\u0019\u0015\u0011\u0001>0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015e\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\u0005\u0012A\u0000\u0002\u0001\u0004\f]\u0005\u00165\u0004\u0007\r\f\u000f\u000e\u0011\u0003\u0019\u0013\u0014\u000e\u0011\u0003,\u0015u7H\u0013\u0014&7\t1\u000b \u0000FjKQR\u0003,\u0015\u0011\u0001\"\u000e\u0011\f\u0012\u0005\u0016\u0007\r\f\u0012%\u000f\u0015E\u0015\u0011\u0013Z\u0015\u0011\u0001@\f/LD)=\u001e\u0014\u000e:\t\u0014-_\t\u000b\u0017\n0dM\t\u0014\u0017@0\u0004\u0013\u001c- \t\u0014+\u0019\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-/\u0005\u0002-/\u0003\u0019\u001e\u0014\u0001*\u0015a\u0005\u0016\f\u0012\f\u0012- \u0017@\u0013\u0014\u0015g\u0015\u0011\u00139\u0013[\u0005\u00165\u001b\u000e\u0011\u0007\u0004\u000e\u0011\u0003\u0019\u0005\u0016\u0003\u0019\u0017@\u001e,«X\u0003\u0019\u0017@0\u0004\f\u0012\f30$O\u0003,\u0015XQ\u0002\t\u0014\u0005/\t\b95@\u0003,\u0015\u0011\fY\f\u0012\t\u0014\u0005\u001d7\"&2\t\u0014%\u000f\u0015/\u0015\u0011\u0001@\t\u000b\u0015X\u0005\u0016\u0013\u0014-/\f\u000f\u0015\u0011\u0001@\u0003\u0019\u0017\u0004\u001e\b-/\u0013\u000b\u000e\u0011\f \u0002c\u0003\u0019\u0017*\u0015\u0011\f\u000f+\u0006+\u0019\u0003\u0019\u001e\u001c\f\u000f\u0017*\u0015\u0005\u0004\u0015\u0011\u0001@\t\u0014\u0017>\u0005\u0016\u0007\u0004+\u0006\u0003,\u0015\u0016\u0015\u0011\u0003\u0019\u0017@\u001e'\f\u0012\u001a\u001c\f\u000f\u000e\u00167 \fb\u0013\u000b\u000e.\t?\u000e:\t\u000b\u0017\n0\u0004\u0013\u0014-/+,7Y%\u000f\u0013\u001c-/\u0007@5\u001b\u0015\u0011\f30Y\u0017*5\u0004-?<\r\f\u000f\u000eR\u0013\u000b&uO\u0017\u0004\u0013\u0014\u0015\u0011\f\u0012\u0005.Qa\f\u000f\u000e\u0011\fX-/\u0013\u0014\u000e\u0011\fE\f\u000f#$\f\u0012%J\u0015\u0011\u0003\u0006\u001a\u001c\f\u001cAFCz\u0001@\u0003\u0019+\u0019\f[\u0015\u0011\u0001\u0004\u0003\u0006\u0005F\t\u000b+\u0019-/\u0013\u001c\u0005\u001d\u00158%\u0012\fJ\u000e\u0016\u0015:\t\u0014\u0003\u0019\u0017@+,7;\u0015\u0016\u000e\u00115@\f&I\u0013\u0014\u000eYdG\t\u000b\u0017\n0\u0004\u0013\u0014->Oe\u0003,\u0015]\u0003\u0006\u0005'Q\u0002\u0013\u000b\u000e\u0016\u0015\u0011\u0001\u0017@\u0013\u000b\u0015\u0011\u0003\u0019\u0017@\u001e\u0004OD\u0001\u0004\u00134Qa\f\u0012\u001a\u001f\f\u000f\u000e3Oe\u0015\u0011\u0001@\t\u000b\u0015]\u0005\u0016\f\u0012\u001a\u001f\fJ\u000e:\t\u0014+R\u000e\u0011\f\u000f)\u0005\u0016\f3\t4\u000e\u0011%J\u0001Qa\u0013\u0014\u000e\u0011*\u0005'\u0003\u0019\u0017-?5\u0004\u0005\u0016\u0003\u0006%Zwud \t\u001c0\u001b\u001a\u001f\u00139%3\t\u000b\u0015\u0011\f\u00120V\u0015\u0011\u0001\n\t4\u0015YLD)=\u001e\u0014\u000e:\t\u0014-/\u0005'\t4\u000e\u0011\f\u0010\t\u0014\u0017\f\u000f#$\f\u000f%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f.-/\f3\t\u000b\u0017@\u0005\u0002\u0015\u0011\u0013?\u0003\u0019\u0017@0\u0004\f\u000f('-[5@\u0005\u0016\u0003\u0019%\u001cO*\u0007\r\f\u000f\u000e\u0011\u0001@\t\u0014\u0007@\u0005e\u0003,&i\u000e\u0011\fJ\u0015\u0016\u000e\u0011\u0003\u0006\f\u000f\u001a\u001f\f30]\u0015\u0011\u0001\u001b\u000e\u0011\u0013\u001c5\u0004\u001e\u001c\u0001\t\u000b\u0007@\u0007\u0004\u000e\u0011\u00133(\u001b\u0003\u0019-'\t\u000b\u0015\u0011\f30>\u0005\u001d\u0015\u0016\u000e\u0011\u0003\u0019\u0017\u0004\u001e]-'\t4\u0015\u0011%J\u0001\u0004\u0003\u0019\u0017@\u001e'\t\u0014+\u0019\u001e\u0014\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001@-/\u0005\u0012AM\u0000\u0002\u0001@\fE5@\u0005\u0016\fE\u0013\u0014&1dM\t\u0014\u0017\u001b)0\u001b\u0013\u001c-Q\u0002\t\u0014\u0005R-'\t\u0014\u0003\u0019\u0017@+,7'-/\u0013\u000b\u0015\u0011\u0003\u0006\u001a\u0014\t\u000b\u0015\u0011\f\u00120]<97/\u0015\u0011\u0001@\fG\u0017@\f\u0012\f30'\u0013\u0014&i\u0001\n\t3\u001a*\u0003\u0006\u0017\u0004\u001eX\tE<\n\t\u0014\u0005\u0016\f\u000f+\u0006\u0003\u0019\u0017\u0004\f\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015\u0011\u0003\u0019\u0017@\u001e?\u0015\u0011\u0001\u0004\f.Q\u0002\u0013\u000b\u000e\u0011\u0005\u001d\u0015M%\u0012\t\u0014\u0005\u0016\f\u001cA\n4.4ScopeoftheWork\u0000\u0002\u0001\u0004\fH\u0005\u001d\u0015\u00115@0\u001b7V%\u0012\u0013\u001c\u0017\u0004%\u0012\f\u0012\u00179\u0015\u0016\u000e:\t\u000b\u0015\u0011\f30\u0013\u0014\u0017\u0015\u0011\u0001@\f;\f\u0012\u001a\u0014\t\u0014+\u00195@\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017x\u0013\u0014&E-/\f\u0012+\u0019\u0013*0*7}\u0005\u0016\f\u0012\u001e\u000b)-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017'<\r\f\u0012%3\t\u00145\u0004\u0005\u0016\fM\u0013\u00145\u0004\u000ea\u0007\u0004\u000e\u0011\f\u0012\u001a*\u0003\u0019\u0013\u001c5\u0004\u0005aQ\u0002\u0013\u000b\u000e\u0011/\t\u0014\u0017@0/\u0015\u0011\u0001\u0004\fDQa\u0013\u0014\u000e\u0011X<97?\u0013\u0014\u0015\u0011\u0001@\fJ\u000e\u000e\u0011\f\u0012\u0005\u0016\f\u0012\t\u000b\u000e\u0011%:\u0001@\f\u000f\u000e\u0011\u00056Q\u0002\fJ\u000e\u0011\fe%\u0012\u0013\u001c\u0017@0\u00045@%J\u0015\u0011\f30E\u0007\u001b\u000e\u0011\u0003\u0019-'\t\u000b\u000e\u0011\u0003\u0019+,7F\u0013\u001c\u0017?-/\f\u000f+\u0006\u0013*0*79AiD\u0005i\u0005\u001d\u0015\u0016\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30\t\u000b<\r\u0013\u000b\u001a\u001c\f\u001cOh\u0015\u0011\u0001\u0004\f/\t\u001c0\u00040\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30\b\u0003\u0019\u0005\u0016\u0005\u00165@\f/\u0003\u0019\u0005F\u0013\u001c\u0017H\u0015\u0011\u0001\u0004\f?5\u0004\u0005\u0016\f/\u0013\u0014&e-/\f\u0012+\u0019\u0013*0*7;&2\u0013\u0014\u000eE\u0005\u0016\f\u0012\u001e\u000b)-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\u000b\u000e:\t\u000b\u0015\u0011\u0001\u0004\f\u000f\u000e`\u0015\u0011\u0001\n\t\u000b\u0015`\u0013\u0014\u0017?\u0015\u0011\u0001@\fa\f\u0012\u001a\u001c\t\u000b+\u00195\n\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017X\u0013\u0014&$\tM\u0005\u0016\u0007\r\f\u0012%\u0012\u0003,\n%e-/\f\u0012+\u0019\u0013*0*7\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0010\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004->O$\u0005\u00165@%:\u0001;\t\u0014\u00053\t \u000b \u0000FjHA@C\"\fF\u0001\n\t3\u001a\u001f\f8+\u0019\f\u000f&\u00158\t\u0014\u0005\u0016\u0003\u00060\u001b\f\u0015\u0011\u0001\u0004\f.\u0003\u0019\u0005\u0016\u0005\u00165@\f.\u0013\u000b&6-?5\u0004\u0005\u0016\u0003\u0006%G95@\f\u000f\u000e\u001679O\u001fQR\u0001\u0004\u0003\u0019%J\u0001Y\u0003\u0006\u0005R\t[0\u0004\f\u0012<@\t\u000b\u0015\u0011\f30]\u0007\u0004\u000e\u0011\u0013\u0014<@+\u0019\f\u0012-\u0003\u0019\u0017Y-?5\u0004)\u0005\u0016\u0003\u0019%H\u000e\u0011\fJ\u0015\u0016\u000e\u0011\u0003\u0006\f\u000f\u001a\u001c\t\u000b+8%\u0012\u0013\u001c-/-[5@\u0017@\u0003,\u0015c7! \u00144A LG\f\u0012\u001a\u001f\fJ\u000e\u0016\u0015\u0011\u0001@\f\u0012+\u0019\f\u0012\u0005\u0016\u0005\u0012OD\u0015\u0011\u0001@\f\u0010\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005>\u0003\u0019\u0017\t\u000b5\u0004\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019%]\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017^-'\t37W\t\u0014\u0007\u0004\u0007@+,7\b\u0015\u0011\u0013>\u0015\u0011\u0001@\f]95@\f\u000f\u000e\u00167*)=\u0005\u0016\u0003\u00060\u0004\f/\u0005\u0016\u0003\u0019\u0017@%\u0012\f95@\fJ\u000e\u0011\u0003\u0006\f\u000f\u0005X-'\t\u00127\"\u0017@\f\u000f\f30\"\u0015\u0011\u0013H<\r\f]\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015\u0011\f\u00120N<\r\fJ&2\u0013\u0014\u000e\u0011\f]\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a*\u0003\u0019\u0017@\u001e\b-?5@\u0005\u0016\u0003\u0019%0\u001b\u00139%\u00125@-/\f\u0012\u00179\u0015\u0011\u0005\u0012O\u001f\t\u0014\u0017\n0-\u0002u\u001e\u001c\u00139\u0013*0 \u0004F\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017'\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@-/\u0005`Q\u0002\u0013\u001c5\u0004+\u00060?\u0001@\f\u0012+\u0019\u0007\u000e\u0011\f\u0012\u0007\u001b\u000e\u0011\f\u0012\u0005\u0016\f\u0012\u00179\u0015D-?5\u0004\u0005\u0016\u0003\u0006%.*5\u0004\f\u000f\u000e\u00167'\f\u000f#$\f\u0012%J\u0015\u0011\u0003\u0006\u001a\u001c\f\u0012+,79A\u0000\u0002\u0001\u0004\fH0\u001b\u0003\u0019\u0005\u0016%\u00125@\u0005\u0016\u0005\u0016\u0003\u0019\u0013\u001c\u0017x\u0013\u0014\u0017\u0015\u0011\u0001@\f\u00105@\u0005\u0016\f\u0010\u0013\u0014&E-/\f\u0012+\u0019\u0013*0*7V&I\u0013\u0014\u000e>\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017POD\u0003\u0019\u0005\u0005\u001d\u0015\u0011\u0003\u0019+\u0019+G\u0013\u0014\u0007\r\f\u0012\u0017z\t\u0014\u0017@0V\u0003\u0019\u0005]\t\u001c0\u00040\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30}\t\u000b\u0015Y0\u0004\u0003,#$\f\u000f\u000e\u0011\f\u0012\u00179\u0015'+\u0019\f\u0012\u001a\u001f\f\u000f+\u0006\u0005'\u0013\u000b&F-?5@\u0005\u0016\u0003\u0019%]\u000e\u0011\f\u000f)\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+e\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-U0\u0004\f\u000f\u0005\u0016\u0003\u0006\u001e\u0014\u0017PA\"\u0000\u0002\u0001@\u0003\u0019\u0005X\u0007\n\t\u000b\u0007\r\f\u000f\u000e[\u0007\u0004\u000e\u0011\u00134\u001a*\u00030\u001b\f\u0012\u0005[QR\u0003,\u0015\u0011\u0001N5@\u0005\u0016\fJ&25@+e\u0003\u0019\u0017\u001b)\u0005\u0016\u0003\u0019\u001e\u001c\u00019\u0015\u0011\u0005?\t\u0014<\r\u0013\u001c5\u001b\u0015[\u0015\u0011\u0001\u0004\f/\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u000f\u0017@\f\u0012\u0005\u0016\u0005E\u0013\u0014&D-/\f\u0012+\u0019\u0013*0\u001b79A\u001e\tP\u00139\u0013\u0014\u001b\u0003\u0019\u0017\u0004\u001e\u0010\t\u000b\u0015[\u0013\u0014\u0015\u0011\u0001@\fJ\u000e&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005\u0012Oa\u0005\u00165\u0004%J\u0001N\t\u000b\u0005[\u0015\u0011\u0003\u0019-?<\u001b\u000e\u0011\f/\u0013\u0014\u000e[\u000e\u0011\u0001*7*\u0015\u0011\u0001\u0004--'\t\u00127\"<\r\f'\u0013\u0014&D\u001e\u0014\u000e\u0011\f\u0012\t\u000b\u0015X\u001a\u0014\t\u0014+\u00195\u0004\f\u001cO<\u00045\u0004\u0015a\u0015\u0011\u0001\u0004\fG\u0005\u001d\u0015\u00115@0\u0004\u0003\u0019\f30'\u0005\u0011\t\u0014-/\u0007@+\u0019\fD\u0013\u0014&PQ\u0002\u0013\u0014\u000e\u0011*\u0005R\t\u000b\u0017\n0X\u0015\u0011\u0001@\fM\u0017\u001b5\u0004-?<\r\f\u000f\u000eg\u0013\u0014&P\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005Qa\u0013\u001c5@+\u00060Z\u0001@\t3\u001a\u001f\fG<\r\f\u0012\f\u000f\u0017Z-?5\u0004%J\u0001'+\u0006\t\u000b\u000e\u0011\u001e\u0014\f\u000f\u000e3Agwc\u0015R\u0005\u0016\u0001@\u0013\u00145@+\u00060Y<\r\fG\u0017@\u0013\u0014\u0015\u0011\f\u00120]\u0015\u0011\u0001\n\t4\u0015e\u0005\u0016\u0013\u001c-/\f\u000e\u0011\u000197*\u0015\u0011\u0001@- \u0003\u0019\u0017\u001b&2\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017;%3\t\u000b\u0017;\f\u000f(*\u0015\u0016\u000e:\t\u0014%\u000f\u0015\u0011\f30]&I\u000e\u0011\u0013\u001c- -/\f\u0012+\u0019\u0013*0*7 \u001d@\u0001@\u00133Q\u0002\f\u000f\u001a\u001f\f\u000f\u000e3O\u0004\u0003\u0019&\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e\u0002&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005aQa\f\u000f\u000e\u0011\fG<\r\f\u0012\f\u000f\u0017]%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u00060\u001b\f\u000f\u000e\u0011\f30]\t\u0014\u0017\n0/\t\u000b\u0017\n\t\u000b+\u00197\u001by\u0012\f\u00120\\O*\u0015\u0011\u0001@\fD\u0003\u0019\u0017*\u0015\u0011\fJ\u000e\u0016)\u0007\u001b\u000e\u0011\f\u000f\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017/\u0013\u000b&\r\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005`Q\u0002\u0013\u00145@+\u00060X<\r\fe%\u0012\u0013\u001c\u0017\u001b&25\u0004\u0005\u0016\u0003\u0006\u0017\u0004\u001e\u0004O9\f\u000f\u0005\u0016\u0007\r\f\u0012%\u0012\u0003\u0006\t\u0014+\u0019+,7/\u0003,&$\u0003,\u0015gQa\u0013\u001c5@+\u00060<\r\fG0\u0004\u0013\u001c\u0017\u0004\fM\u0003\u0019\u0017'\u000e\u0011\f\u0012+\u0006\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017'QR\u0003,\u0015\u0011\u0001]\u0015\u0011\u0001\u0004\fG\u0003\u0019\u00179\u0015\u0011\f\u000f\u000e\u0011\u0007\u0004\u000e\u0011\fJ\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017'\u0013\u0014&P\u000e\u0011\f\u0012\u0005\u00165@+,\u0015\u0011\u0005a\u000e\u0011\f\u000f\u001e\u001f\t\u000b\u000e:0*)\u0003\u0019\u0017@\u001e?-/\f\u0012+\u0019\u0013*0\u001b7]\t\u0014+\u0019\u0013\u0014\u0017@\f\u001cAa\u0000\u0002\u0001*5@\u0005\u0012O*Q\u0002\f8\u0007\u001b\u000e\u0011\f\u000f&2\f\u000f\u000e\u0016\u000e\u0011\f\u00120Y\u0015\u0011\u0013?%\u0012\u0013\u0014\u0017@%\u0012\f\u000f\u0017*\u0015\u0016\u000e:\t4\u0015\u0011\f8\u0013\u001c\u0017Y\u0013\u001c\u0017@\f&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\fD\u0015\u0011\u00138\u0003\u0019\u0005\u0016\u0013\u001c+\u0006\t4\u0015\u0011\fG\f\u000f(*\u0015\u0011\f\u000f\u000e\u0011\u0017@\t\u0014+\u0004&2\t\u0014%\u000f\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0002\t\u000b\u0017\n0[\u0015\u0011\u0013F\u0007\u0004\u000e\u0011\u00134\u001a*\u0003\u00060\u0004\fe\u0005\u0016\u0013\u0014-/\fD5\u0004\u0005\u0016\f\u000f&25\u0004+\u0003\u0019\u0017@\u0005\u0016\u0003\u0019\u001e\u0014\u0001*\u0015\u0011\u0005D\u0013\u001c\u0017Y-/\f\u0012+\u0019\u0013*0\u001b79A\t\\\u0003\u0006\u001c\fe-'\t\u0014\u001797F5@\u0005\u0016\f\u000f\u000e`\u0005\u001d\u0015\u00115@0\u0004\u0003\u0019\f\u0012\u0005\u0012O\u0014\u0015\u0011\u0001@\fe\u0005\u0011\t\u0014-/\u0007\u0004+\u0019\fe0@\t4\u0015:\tG5@\u0005\u0016\f30F\u0015\u0011\u0013.\f\u0012\u001a\u0014\t\u0014+\u00195\n\t4\u0015\u0011\f\u0002\u0015\u0011\u0001@\f\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017N\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-_\u000e\u0011\f\u0012*5\u0004\u0003,\u000e\u0011\f\u0012\u0005E\t>\u0017@\u0013\u001c\u0017\u001b)=+\t4\u000e\u0011\u001e\u001c\f]0@\t4\u0015:\t\u0014\u0005\u0016\f\u000f\u00153OhQR\u0001@\u0003\u0019+\u0019\f-[5@%:\u0001f+\u0006\t4\u000e\u0011\u001e\u001c\f\u000f\u000e]\u0013\u001c\u0017\u0004\f\u0012\u0005'\t\u000b\u000e\u0011\fZ\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015\u0011\f30N\u0015\u0011\u0013W%\u0012\u0013\u0014\u0017\n0\u00045\u0004%\u000f\u0015/+\u0006\t\u0014<\r\u0013\u000b\u000e:\t\u000b\u0015\u0011\u0013\u000b\u000e\u00167}\t\u000b\u0017\n0%\u0012\u0013\u0014-/\u0007@5\u001b\u0015\u0011\f\u000f\u000e?<@\t\u0014\u0005\u0016\f30\"\fJ(\u0004\u0007\r\fJ\u000e\u0011\u0003\u0006-/\f\u000f\u0017*\u0015\u0011\u0005E\u0003\u0019\u0017\"\u0015\u0011\f\u000f(*\u0015\u00115\n\t\u000b+e0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015F\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u001c\t\u000b+\f\u0012\u001a\u0014\t\u0014+\u00195@\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\\A\u0000\u0002\u0001\u0004\f\u0010%J\u0001\u0004\u0013\u001c\u0003\u0019%\u0012\f\b\u0013\u0014&[\t^\u0017@\u0013\u001c\u0017\u001b)=+\t4\u000e\u0011\u001e\u001c\f\u0010\u0005\u0011\t\u0014-/\u0007\u0004+\u0019\f\b\u0003\u0019\u0005Z\t\u0014+\u0019\u0005\u0016\u0013}0\u001b5@\f\u0015\u0011\u0013]\u0015\u0011\u0001@\fX\f\u000f(\u001b\u0007\r\f\u000f\u000e\u0011\u0003\u0019-/\f\u0012\u00179\u0015:\t\u000b+`0\u0004\f\u0012\u0005\u0016\u0003\u0019\u001e\u001c\u0017H%J\u0001\u0004\u0013\u001c\u0003\u0019%\u0012\fX\u0013\u000b&R\t\u0014\u0005\u0016*\u0003\u0019\u0017@\u001e]\u0015\u0011\u0001@\f[\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u00058\u0015\u0011\u0013\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015Dp\u001c2\u001b\u0015\u0011\u0001@\f\u0002\f\u000f(\u001b%\u0012\fJ\u000e\u0011\u0007\u0004\u0015\u0011\u0005`\t\u0014\u0017@0E\u0015\u0011\u0013.\u0007\u001b\u000e\u0011\u00134\u001a\u001b\u0003\u00060\u001b\fe95\n\t\u0014+\u0019\u0003,\u0015:\t\u000b\u0015\u0011\u0003\u0019\u001a\u001f\f\u0014O\u001f\f\u001cA \u001e\u0004A6&I5@+\u0019+,)\u0015\u0011\f\u000f(*\u0015F%\u0012\u0013\u0014-/-/\f\u0012\u00179\u0015\u0011\u0005G\u0013\u0014\u0017H\u0015\u0011\u0001@\f\u000f\u0003\u0019\u000eE\u00133QR\u0017W\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017\"0\u0004\f\u000f%\u0012\u0003\u0019\u0005\u0016\u0003\u0006\u0013\u0014\u0017@\u0005\u0012OiQR\u0001\u0004\u0003\u0019%J\u0001\u0001@\t\u0014\u0005h<\r\f\u000f\f\u0012\u00178\f\u000f(\u001b\u0007@+\u0019\u0013\u001c\u0003,\u0015\u0011\f3080\u001b5\u0004\u000e\u0011\u0003\u0019\u0017@\u001e\u0002\u0015\u0011\u0001\u0004\f6\u0007\u0004\u000e\u0011\u00139%\u0012\f\u000f\u0005\u0016\u0005i\u0013\u0014&9\u000e\u0011\f\u0012\u0005\u00165\u0004+,\u00156\t\u0014\u0017@\t\u0014+,7\u001b\u0005\u0016\u0003\u0019\u0005\u0012A1jZ\u0013\u0014\u000e\u0011\f\u000f)\u00134\u001a\u001f\f\u000f\u000e3O\u0004\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005eQ\u0002\f\u000f\u000e\u0011\fE\t\u0014\u0005\u0016\u001c\f30]\u0015\u0011\u0013X\u0007\u0004+\u0006\t37]\u0005\u0016\f\u0012\u001a\u001c\f\u000f\u000e:\t\u0014+$\u0015\u0011\u0003\u0019-/\f\u0012\u0005e\u0015\u0011\u0001@\f.-/\f\u0012+\u0019\u0013*0\u001b\u0003\u0006\f\u000f\u0005\u0005\u0016\u0013.\u0015\u0011\u0001@\t\u000b\u0015`\u0015\u0011\u0001@\fe0\u001b\f\u0012%\u0012\u0003\u0019\u0005\u0016\u0003\u0019\u0013\u001c\u0017X\u000e\u0011\f\u0012\u001e\u001c\t\u000b\u000e:0\u001b\u0003\u0006\u0017\u0004\u001e.QR\u0001@\f\u000f\u000e\u0011\f\u0002\u0015\u0011\u0013G\u0007@+\u0006\t\u0014%\u0012\fR<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u00056Qe\t\u000b\u0005\t\u000b\u0005e-/\u0013\u0014\u000e\u0011\fM&I\u0013\u001c5\u0004\u0017\n0\u0004\f\u00120Y\t\u000b\u0017\n0Z0\u001b\f\u000f\n\u0017\u0004\u0003,\u0015\u0011\u0003\u0006\u001a\u001c\fG\t\u000b\u0005e\u0007\r\u0013\u001c\u0005\u0016\u0005\u0016\u0003\u0019<@+\u0019\f\u001cAaM\u0005\u0002%\u000f\u0013\u001c\u0017@\u0005\u0016\f\u0012*5\u0004\f\u0012\u0017@%\u000f\f\u001cO\u0015\u0011\u0001\u0004\f.\u001a\u001c\t\u000b+\u00195@\fG\u0013\u000b&i\u0015\u0011\u0001@\fG0@\t4\u0015:\t\u0014\u0005\u0016\f\u000f\u0015R+\u0019\u0003\u0019\f\u0012\u0005R\u0003\u0019\u0017Y\u0015\u0011\u0001@\fG95@\u0003,\u0015\u0011\fG\u0001\u0004\u0003\u0006\u001e\u0014\u0001Z\u0017*5@-[<\r\f\u000f\u000e\u0002\u0013\u000b&60\u001b\u0003\u0019&)&I\f\u000f\u000e\u0011\f\u0012\u00179\u0015X\u0005\u0016\f\u0012\u001e\u0014-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017N<\r\f\u0012\u0003\u0019\u0017\u0004\u001e\b\u0007\u0004+\u0006\t\u0014%\u0012\f30N\t4\u0015X\f3\t\u000b%J\u0001N-/\f\u0012+\u0019\u0013*0\u001b7W<97\"\f\u0012\u001a\u001f\fJ\u000e\u00167\u0005\u00165\u0004<\u001bTu\f\u0012%\u000f\u00153O\u0004\t\u0014\u0017@0'\u0003\u0019\u0017/\u0015\u0011\u0001@\fM\t3\u001a\u001c\t\u000b\u0003\u0019+\t\u000b<@\u0003\u0019+\u0019\u0003,\u0015u7'\u0013\u0014&P%\u0012\u0013\u0014-/-/\f\u0012\u00179\u0015\u0011\u0005a\t\u000b\u0017\n0X\u0005\u00165@\u001e\u0014\u001e\u001c\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u0013\u0014\u0017@\u0005\u0015\u0011\u0001@\t\u000b\u0015D\u0001\n\t3\u001a\u001f\f.\fJ(\u0004\u0007\u0004+\u0019\u0003\u0006%\u000f\u0003\u0019\u0015\u0011+,7Y<\r\f\u0012\f\u0012\u0017Y\u0007\u0004\u000e\u0011\u00134\u001a*\u00030\u001b\f30Y<97'\u0015\u0011\u0001@\fG\u0005\u00165@<*T\u001d\f\u0012%\u000f\u0015\u0011\u0005\u0012A\u0000\u0002\u0001\u0004\fH\t\u000b+\u0019\u001e\u001c\u0013\u0014\u000e\u0011\u0003,\u0015\u0011\u0001\u0004-/\u0005]\u00134\u001a\u001f\fJ\u000e\u0016)c\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015\u0011\u0005/-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019\f\u0012\u0005\u0012ASCz\u0001\u0004\u0003\u0006+\u0019\f;\u0015\u0011\u0001@\u0003\u0019\u0005'\u0003\u0019\u0005Y\u0003\u0019\u0017\u001b)\n\u0015\u0011\fJ\u000e\u0011\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u0017@\u001e\u0010\u0003\u0019\u0017^\u0003\u0019\u0015\u0011\u0005[\u00133QR\u0017N\u000e\u0011\u0003\u0019\u001e\u0014\u0001*\u00153O6\u0015\u0011\u0001\u0004\f\u000f\u000e\u0011\f/Q\u0002\u0013\u00145@+\u00060\"\u0017@\u0013\u000b\u0015?<\r\f]\u0003\u0019-/\u0007@+\u0019\u0003\u0019%3\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005&I\u0013\u0014\u000eE\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+1\f\u000f#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017\u0004\f\u0012\u0005\u0016\u0005\u0012A'wc&a\u0015\u0011\u0001@\fX\u0005\u0011\t\u000b-/\f/\t\u0014+\u0019\u001e\u001c\u0013\u000b\u000e\u0011\u0003,\u0015\u0011\u0001@-_\u0003\u0019\u0005E5\u0004\u0005\u0016\f30\b\u0015\u0011\u0013\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015M<\r\u0013\u0014\u0015\u0011\u0001Z\u0015\u0011\u0001\u0004\fF0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015\u0011\u0005M\t\u000b\u0017\n0Y\u0015\u0011\u0001\u0004\fF5@\u0005\u0016\f\u000f\u000e\u0016)c95@\f\u000f\u000e\u001679O*\u0015\u0011\u0001\u0004\f\u0012\u0017;<\r\u0013\u0014\u0015\u0011\u00010\u001b\u00139%\u00125@-/\f\u000f\u0017*\u0015\u0011\u00051\t\u000b\u0017\n0/95@\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0005`QR\u0003\u0019+\u0019+\n<\r\fR\u0013\u000b\u001a\u001c\f\u000f\u000e\u0016)=\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015\u0011\f30$O\u001c<\u00045\u0004\u00151\u0005\u0016\u0003\u0019\u0017@%\u0012\fR\u0015\u0011\u0001@\f\u0003\u0019\u0017@0\u0004\f\u000f(\u001b\u0003\u0019\u0017@\u001e]\t\u0014\u0017@0Z\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+i\u0005\u00165\u0004<\u0004)=\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-/\u0005G\t4\u000e\u0011\f[%\u000f\u0013\u001c\u0017@\u0005\u0016\u0003\u0019\u0005\u001d\u0015\u0011\f\u0012\u00179\u0015MQR\u0003\u0019\u0015\u0011\u0001\u0010\f3\t\u000b%J\u0001\u0013\u000b\u0015\u0011\u0001@\f\u000f\u000e3Oh\u0015\u0011\u0001@\u0003\u0019\u0005F-'\t37\b\u0017\u0004\u0013\u0014\u0015E<\r\f'\tZ-'\t3T\u001d\u0013\u0014\u000eE\u0007\u001b\u000e\u0011\u0013\u001c<@+\u0019\f\u0012-U\t\u000b\u0005F&2\t\u000b\u000e?\t\u0014\u0005F\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+\fJ#$\f\u0012%\u000f\u0015\u0011\u0003\u0019\u001a\u001f\f\u0012\u0017\u0004\f\u0012\u0005\u0016\u0005R\u0003\u0006\u0005D%\u0012\u0013\u0014\u0017@%\u0012\f\u000f\u000e\u0011\u0017\u0004\f30\\A\n5.FUTURE WORK¬\u00045\u0004\u000e\u0016\u0015\u0011\u0001@\fJ\u000e'\u0005\u001d\u0015\u00115\n0\u001b7N%\u0012\t\u0014\u0017}<\r\fZ%\u0012\u0013\u001c\u0017@0\u00045@%J\u0015\u0011\f30V\u0003\u0019\u0017V\u0015\u0011\u0001\u0004\fZ&25\u001b\u0015\u00115\u0004\u000e\u0011\fZ\u0015\u0011\u0013H\f\u0012\u001a\u0014\t\u0014+\u00195\n\t4\u0015\u0011\f\u0015\u0011\u0001\u0004\f;\u000e\u0011\u0013\u001c+\u0019\f\u0010\u0013\u0014&F\u0015\u0011\u0003\u0019-?<\u0004\u000e\u0011\f\u0014O\u0002\u000e\u0011\u000197\u001b\u0015\u0011\u0001\u0004- \u0013\u0014\u000e3OD\u0001@\t\u000b\u000e\u0011-/\u0013\u001c\u001797f\u0013\u001c\u0017z\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017PO\u0003\u0019\u0017H\u0005\u0016\u0003\u0006-/\u0003\u0019+\u0006\t\u000b\u000e8Q\u0002\t\u00127;\u0015\u0011\u0013Y\u0015\u0011\u0001\n\t4\u001585@\u0005\u0016\f\u00120\b\u0003\u0019\u0017\u0010\u0015\u0011\u0001@\u0003\u0019\u0005.\u0007\n\t\u000b\u0007\r\f\u000f\u000e3A[G\u00133Q\u0002\f\u0012\u001a\u001c\f\u000f\u000e3O$\u000e\u0011\u0013\u001c<@5\u0004\u0005\u001d\u0015-/\fJ\u0015\u0011\u0001@\u0013*0\u0004\u0005\\&2\u0013\u000b\u000ei\u0015\u0011\u0001@\f\u0012\u0003,\u000eh\u0005\u0016\f\u0012\u001e\u001c-/\f\u0012\u00179\u0015:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017E\t\u0014\u0003\u0019-/\f30F\t\u000b\u00156-[5@\u0005\u0016\u0003\u0019%6\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+9\u0001\n\t3\u001a\u001f\f\u0015\u0011\u0013/<\r\fF\t\u00140@0*\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f30\\A1C\"\fMQR\u0003\u0006+\u0019+i\u0003\u0019\u0017*\u001a\u001f\f\u0012\u0005\u001d\u0015\u0011\u0003\u0019\u001e\u001c\t\u000b\u0015\u0011\f8\u0013\u001c\u0017>\u0015\u0011\u0001\u0004\f.\f\u000f#$\f\u0012%J\u0015M\u0013\u000b&`%\u0012\u0013\u001c\u0017\u0004\u0005\u0016\u0003\u00060\u001b)\fJ\u000e\u0011\u0003\u0006\u0017\u0004\u001e^-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%Z&I\f3\t\u000b\u0015\u00115\u001b\u000e\u0011\f\u0012\u0005Y\u0003\u0019\u0017z95\u0004\f\u000f\u000e\u00167N\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017POR\u0005\u0016\u0003\u0019\u0017\u0004%\u0012\f;-?5\u0004\u0005\u0016\u0003\u0006%95\u0004\f\u000f\u000e\u0011\u0003\u0019\f\u0012\u0005G\t4\u000e\u0011\f8-/\u0013\u0014\u000e\u0011\f8+\u0019\u0003\u0019\u001f\f\u0012+,7]\u0015\u0011\u0013X<\r\fF\t\u000b#$\f\u0012%J\u0015\u0011\f30Z<97]\f\u000f\u000e\u0016\u000e\u0011\u0013\u0014\u000e\u0011\u0005M\t\u0014\u0017@0>\t\u000b\u000e\u0011\f8\u0015u7\u001b\u0007\u001b)\u0003\u0019%3\t\u000b+\u0019+\u00197\u0010-[5@%:\u0001;\u0005\u0016\u0001@\u0013\u000b\u000e\u0016\u0015\u0011\f\u000f\u000eG\u0015\u0011\u0001@\t\u0014\u0017\u00100\u0004\u00139%\u000f5@-/\f\u0012\u00179\u0015\u0011\u0005\u0012AMwu\u0017>\u0015\u0011\u0001@\u0003\u0019\u0005DQ\u0002\u0013\u0014\u000e\u0011\rO\rQa\f?\u0001\n\t3\u001a\u001f\f\u0013\u0014<@\u0005\u0016\f\u000f\u000e\u0011\u001a\u001c\f30\b\u0015\u0011\u0001@\t\u000b\u00158\u0015\u0011\u0001@\fJ\u000e\u0011\fX\u0003\u0019\u0005[\tY\u001e\u001c\u00139\u0013*0\"\t\u000b\u001e\u0014\u000e\u0011\f\u0012\f\u0012-/\f\u000f\u0017*\u0015E\t\u000b-/\u0013\u001c\u0017\u0004\u001eZ\u0005\u00165@<*T\u001d\f\u000f%\u000f\u0015\u0011\u0005F\u0003\u0019&-'\t4\u000e\u0011\u001f\f\u000f\u000eD\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005G\t4\u000e\u0011\u0013\u001c5\u0004\u0017\n0>\u0017@\u0013\u000b\u0015\u0011\f\u0012\u0005D\t\u000b\u000e\u0011\fF%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u00060\u0004\f\u000f\u000e\u0011\f\u00120>\u0003\u0006\u0017\u0004\u0005\u001d\u0015\u0011\f3\t\u001c0>\u0013\u000b&`\f\u000f(*)\t\u000b%\u000f\u0015F\u0007\r\u0013\u001c\u0005\u0016\u0003,\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0004\u0005\u0012A'\u0000\u0002\u0001@\u0003\u0019\u00058\u0007\r\f\u0012%\u000f5@+\u0019\u0003\u0006\t\u000b\u000e\u0011\u0003,\u0015u7\b\u0013\u0014&a-/\f\u0012+\u0019\u0013*0\u001b7;\u0005\u0016\f\u000f\u001e\u001c-/\f\u0012\u00179\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017W\u0001\n\t\u000b\u0005\u0015\u0011\u0013[<\r\fG%\u0012\u0013\u0014\u0017@\u0005\u0016\u0003\u00060\u0004\fJ\u000e\u0011\f30Y\u0003\u0019\u0017Y\u0015\u0011\u0001\u0004\f.0\u001b\f\u0012\u0005\u0016\u0003\u0019\u001e\u001c\u0017Z\u0013\u000b&i\u0003\u0019\u0017\n0\u001b\f\u000f(\u001b\u0003\u0019\u0017@\u001eF\u0015\u0011\f\u0012%J\u0001\u0004\u0017@\u0003\u000695@\f\u0012\u0005a\u0013\u0014&i\u0005\u0016\f\u000f\u001e\u0014)-/\f\u000f\u0017*\u0015\u0011\f\u00120?-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019\f\u0012\u0005\u0012O9QR\u0001\u0004\u0003\u0006%:\u0001'\u0003\u0019\u0005\u0002\t\u0014\u0017]\t\u000b\u0005\u0016\u0007\r\f\u0012%\u000f\u0015\u0002<\r\fJ79\u0013\u001c\u0017@0?\u0015\u0011\u0001@\fD\t\u0014\u0003\u0019-/\u0005a\u0013\u000b&$\u0015\u0011\u0001@\u0003\u0019\u0005\u0005\u001d\u0015\u00115@0\u001b7Y\t\u0014\u0017@0]\u0015\u0011\u0001@\t\u000b\u0015RQR\u0003\u0019+\u0006+P<\r\f8\t\u001c0\u00040\u001b\u000e\u0011\f\u0012\u0005\u0016\u0005\u0016\f\u00120Z\u0003\u0019\u0017Z\u0015\u0011\u0001@\fG&I5\u0004\u0015\u00115\u001b\u000e\u0011\f\u001cA\n6.REFERENCES\u0019J \u0000?A\u0015\u000b\u0002\t\u0014\u0003\u0019\u0017*<\u0004\u000e\u0011\u0003\u00060\u001b\u001e\u001c\f\u001cO 2MA\n3A@LG\f\u0012\u001a*\u0003\u0019+\u0019+,)\u001dj>\t\u000b\u0017@\u0017\u0004\u0003\u0006\u0017\u0004\u001e\u0004O\u0004wJA [A*Cz\u0003,\u0015\u0016\u0015\u0011\f\u0012\u0017\\O \t`A EA~\u001b-/\u0003,\u0015\u0011\u0001\\Oe\t\u0014\u0017@0VdFA \u0000\u0004AejZ%\u000fL.\t\u0014<\\Ag\u0000h\u00133Qe\t\u000b\u000e:0\u001b\u0005Z\tH0\u0004\u0003\u0019\u001e\u001c\u0003,\u0015:\t\u000b+G+\u0019\u0003\u0019<\u0004\u000e:\t\u000b\u000e\u00167f\u0013\u000b&\u0007\r\u0013\u001c\u0007\u00045@+\u0006\t\u000b\u000ea-?5\u0004\u0005\u0016\u0003\u0006%\u0014A\u001fwu\u0017 \u001f\u0002q\u0016l3:s\u0011s\u0016o\u0014r2n9\u0014t8l\u001d\f,\r* .\u0002\u00018r\u0019\u0014r2=p\u001c\u0004\u00036rk\u000fq\u0016p\u0014q:r=sJt\u0005\u0001\u0006\u0003\b\u0007 *1l\u0014n\u00123sJq\u0011sJn\n:sJO\u0004\u0007@\t\u0014\u001e\u0014\f\u0012\u0005M\u0016\u0017\u001b\u001c«@\u0018\u0017\u0007\u001a\u001bO \u000bg\f\u000f\u000e\u0011\u001f\f\u0012+\u0019\f\u000f79O 2\u0002FO*M5\u0004\u001e\u001c5\u0004\u0005\u001d\u0015\u0018\u001a\u0007\u001a;\u001a*A 3 \u0000?A\u0001\u000bg\f\u0012\f\u000f&2\fJ\u000e\u0011-'\t\u0014\u0017PO.EA\u0001\u000bg\f\u000f\u000e\u0011\u001e\u001c\f\u000f\u000e3O8\t\u0014\u0017\n0\t\u0000\u001bA \th\t\u000b#$\fJ\u000e\u0016\u0015u79AD~9\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0003\u0019%3\t\u0014+-/\u0013*0\u0004\f\u000f+\u0006\u0005F&2\u0013\u000b\u000e?\u0015\u0011\f\u000f(*\u0015[\u0005\u0016\f\u0012\u001e\u001c-/\f\u000f\u0017*\u0015:\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017PA .Hp\u001f\u000b\n*rIn$s\f\u0003is\u0016p\u0014q:n@rIn*\u0014O1\u0005\u0016\u0007\r\f\u000f)%\u0012\u0003\u0006\t\u0014+@\u0003\u0006\u0005\u0016\u0005\u00165\u0004\fM\u0013\u0014\u0017/L.\t4\u0015\u00115\u0004\u000e:\t\u0014+ \th\t\u0014\u0017\u0004\u001e\u001c5@\t\u0014\u001e\u001c\f7\tP\f\u0012\t\u000b\u000e\u0011\u0017@\u0003\u0019\u0017\u0004\u001e\u0004O\u00132MA\u00112\u0002\t4\u000e:0\u0004\u0003\u0019\f.\t\u000b\u0017\n0dFA\rjZ\u00139\u0013\u0014\u0017@\f\u000f79O\u0004\f30\u0004\u0003,\u0015\u0011\u0013\u000b\u000e\u0011\u0005\u0012O\r\u0007\u0010\u0004\u001d\u000f)c\u001fJO\u0004\u0007\n\t\u000b\u001e\u001c\f\u0012\u0005F\u001c\u0012\u0007\u00129«**3*O6\u0018\u001a\u0007\u001a;\u001a\u001bA 47ªeA 2\u0002\t\u0014-[<\r\u0013\u001c5\u0004\u000e\u0011\u0013\u0014\u0007\r\u0013\u001c5\u0004+\u0006\u0013\u0014\u0005\u0012A1jZ5@\u0005\u0016\u0003\u0019%3\t\u000b+a\u000e\u0011\u000197*\u0015\u0011\u0001@->|[\tZ&2\u0013\u000b\u000e\u0011-'\t\u0014+\u0002-/\u0013*0\u001b\f\u0012+&2\u0013\u000b\u000eV0\u0004\f\u000f\u0015\u0011\f\u000f\u000e\u0011-/\u0003\u0019\u0017\u0004\u0003\u0006\u0017\u0004\u001e!+\u0019\u00139%3\t\u000b+Y<\r\u0013\u00145@\u0017\n0\u0004\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012A[wu\u0017 ªRA \tP\f\u000f-'\t\u0014\u0017PO]\f30*)\u0003,\u0015\u0011\u0013\u0014\u000e3O5.;m\u001bt\u0011r\u000e\r\u0010\u000fDsJt\u0011=p\u001c ;p\u0014n\ro *1l\u00143!\nm*Br2n9\u0014O/\u0007\n\t\u000b\u001e\u001c\f\u0012\u0005\";\u0012\u0007\u00129«\u001b:\u001a\u001c*A~\u001b\u0007\u001b\u000e\u0011\u0003\u0019\u0017@\u001e\u001c\fJ\u000e\u0016)\u0012\u00111\f\u000f\u000e\u0011+\u0006\t\u0014\u001e\u001bO \u000bg\f\u000f\u000e\u0011+\u0019\u0003\u0006\u0017\\Oh\u0016\u001a;\u001a\u001b\u00129A \u0010\u000b7ªeA 2\u0002\t\u000b-?<\r\u0013\u00145\u0004\u000e\u0011\u0013\u001c\u0007\r\u0013\u00145@+\u0019\u0013\u001c\u0005\u0012Ae\u0000\u0002\u0001@\f-\tP\u00139%3\t\u000b+ \u000ba\u0013\u001c5\u0004\u0017\n0\u0004\t\u000b\u000e\u00167 \u0000G\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017jZ\u0013*0\u0004\f\u000f+6 \t \u000b \u0000Fj;1\t\u0014\u0017@0'\u0003,\u0015\u0011\u0005aM\u0007@\u0007\u0004+\u0019\u0003\u0006%\u0012\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017'\u0003\u0019\u0017/\u0015\u0011\u0001@\fG~*\u0015\u00115\n0*7?\u0013\u000b&?ª6(*)\u0007\u0004\u000e\u0011\f\u000f\u0005\u0016\u0005\u0016\u0003\u0006\u001a\u001c\fF\u0000\u0002\u0003\u0006-/\u0003\u0019\u0017\u0004\u001e\u0004A\u0015\u001feq\u001dl3:s:s\u0016o\u0014rIn*\u0014tEl\u001d.\u0013\n@s\u0015\u0014:n@csJq:n\rp\u0014Brl\u000bn\rp\u001c *1l\u0014 F!\nm*csJq .\u0010m*t\u0011r *1l\u0014n\u00123sJq\u0011sJn\n:sJO\n.\t3\u001a\u0014\t\u0014\u0017@\t\u001bO 2a5@<\n\t*O\r\u000b\u001c*\u001cA \u00193\r2\u0002MLG\u0000i1\u0000\u001fªRA 2a\u0013\u001c-/\u0007\u00045\u0004\u0015\u0011\f\u000f\u000e\bD%\u0012%\u0012\f\u0012\u0005\u0016\u0005;\u0015\u0011\u0013LM\u0013\u0014\u0015:\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017\t\u0014\u0017\n0v\u0000\u0002\f\u000f(*\u0015\u0003\u0019\u0017jZ5\u0004\u0005\u0016\u0003\u0019% \tP\u0003\u0019<\u001b\u000e:\t\u000b\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012O\u0016\u0000\u001f\t\u0014\u0017\\AY\u0014\u001c\u001c*A\u0018\u0017\u001a\u0019\u001b\u0019\u001d\u001c\u001f\u001e! \u001b #\"$\"\u001b\"\u001f%'&#(\u001d)\u001f%+*-,\u001b \u001c\u001a.\u0004/$0$1\u00042#\u0019\u0004 \u001b2$3#*\u001a\u0019\u00043#\u0019\u001a1\u001a \u001a2$34*\u001a\u0019\u00043\u001d\u0019\u001a15%+\u0017\u001a\u001946\\A \u001747\u0000\u0004A ~$A8\u0000.\u00133QR\u0017@\u0003\u0019\f;\t\u000b\u0017\n0}jHAaLM\f\u0012+\u0019\u0005\u0016\u0013\u001c\u0017PANª`\u001a\u001c\t\u000b+\u00195\n\t\u000b\u0015\u0011\u0003\u0019\u0017\u0004\u001e^\tH\u0005\u0016\u0003\u0019-/\u0007@+\u0019\f;\t\u000b\u0017\n0\f\u000f#$\f\u0012%J\u0015\u0011\u0003\u0006\u001a\u001c\fW-?5\u0004\u0005\u0016\u0003\u0019%H\u0003\u0019\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017S\u000e\u0011\fJ\u0015\u0016\u000e\u0011\u0003\u0006\f\u000f\u001a\u001c\t\u000b+8-/\f\u000f\u0015\u0011\u0001@\u0013*0$A\u0002wu\u0017 \u001f\u0002q\u0016l\u0015F:s\u0011s\u0011o\u000br2n*\u0014tZl\u001d]\u0013\n\u0004s ,\r* .8\u0014:n\u0004csJqJn\np\u0014Brl\u0014n\np\u001c *1l\u0014n33sJq\u0016sJn\r:s>l\u0014n\u00109Ds Ft\u000fs\u0016p\u0014q\u0016\u000b\nVp\u000bn\ro:\u0001Es \u0000\u0014s\u000f\u0019l !\n/sJn@8r2n\u0016\u0014:n\u0012\u0012l\u0014q:/p\u0014Brl\u000bn\u00109RsJBqJrs \u0000\u000bp\u001c\n\u0005'-\u0014\u001dF\u000f;\u0014'9<\u00073O@\u0007@\t\u0014\u001e\u0014\f\u0012\u0005 \u0012\u000bP« \u0014\u001c\u001bO\re\u0015\u0011\u0001\u0004\f\u0012\u0017@\u0005\u0012O\n3\u000e\u0011\f\u0012\f\u000f%\u0012\f\u001cO$\u000b\u001c\u0014\u001bA \u00123FEA3¬\u0004\u000e\u0011\u0003\u0019<\r\f\u000f\u000e\u0011\u001e\u0004O*dFA \u000b1\u000e\u0011\f\u0012\u0005\u0016\u0003\u0019\u0017PO \t1A<¬\u0004\u000e\u00167\u00040<= \f\u0012\u0017\\O9\t\u0014\u0017@0>\u0000\u0004A\u001b~*5@\u0017*<\r\f\u000f\u000e\u0011\u001e\u0004A9jZ5@\u0005\u0016\u0003,)%3\t\u000b+\u0004¯`5@\u0017\u0004%\u000f\u0015\u00115\n\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017E\u0013\u001c\u0017F\u0015\u0011\u0001@\fejZ\u0003\u0019%\u000f\u000e\u0011\u0013\u0014+\u0019\f\u0012\u001a\u001f\f\u0012+B|`D5\u0004\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019%gwc\u0017\n0\u0004\f\u000f\u0017*\u0015\u0011\u0003,\u0004)%3\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017X\t\u000b\u0017\n07¯i\f\u000f\u000e\u0016&2\u0013\u000b\u000e\u0011-'\t\u0014\u0017\u0004%\u0012\fa\u0013\u000b&$~\u001b-'\t\u000b+\u0019+*jZ\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%\u0016VG\u0017\u0004\u0003,\u0015\u0011\u0005\u0012A\u001a?*l\u0014m*qJn\np\u001cl\u001d\u0006@FsBA .\u0010m*t\u0011rEq\u0016sJt\u0012s\u0016p\u0014q\u001d\u000b\n*O\\\u0007\u0012\u001b\u001fJ| \u0007\u0012*\u000f)u:\u001a\u001f*O6\u0016\u001a;\u001a\u0007\u0014 \u00144FEA\n3\u0001@\u0003\u0006\t\u000b\u0005\u0012OC\u0000\u0004A \tP\u0013\u001c\u001e\u001c\t\u0014\u0017\\O \u0000?A 2a\u0001\n\t\u000b-?<\r\f\u000f\u000e\u0011+\u0019\u0003\u0019\u0017POi\t\u0014\u0017\n0-\u000bMA 2MA6~*-/\u0003\u0019\u0015\u0011\u0001\\AD5@\fJ\u000e\u00167}<97V\u0001\u001b5\u0004-/-/\u0003\u0019\u0017@\u001e\u0004|HjZ5\u0004\u0005\u0016\u0003\u0019%3\t\u0014+.\u0003\u0019\u0017\u001b&2\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+M\u0003\u0006\u0017\t\u0014\u0017\"\t\u00145\n0\u001b\u0003\u0019\u0013;0@\t4\u0015:\t\u0014<\n\t\u000b\u0005\u0016\f\u001cA6wc\u0017\u0003\u001f\u0002q\u0016l3:s\u0011s\u0011o\u000br2n*\u0014t]l\u001d ,\r* .E\u00018r\u0019\u0014r2=p\u001cF\u0003`r Fk\u000fq\u0016p\u000bqJr=sJt\n\u0005\u0001G\u0003H\u0007 *1l\u0014n\u00123sJq\u0011sJn\nJsJOX\u0007\n\t\u0014\u001e\u0014\f\u0012\u0005N\u000b\u001b\u001c«*\u0014\u0007\u0017\u001bO'LG\fJQJIg\u0013\u0014\u000e\u0011\rOL\u0006I[O\u001bLG\u00134\u001a\u001f\f\u0012-[<\r\f\u000f\u000e.\u0018\u001a\u0007\u001a\u001b\u00199A \u001a48.Dd.j;GLMw&2\u0002FA$M%\u0012%\u000f\u0013\u001c-/\u0007\n\t\u000b\u001797\u0004\u0003\u0019\u0017\u0004\u001eYD%\u000f\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b\u0013\u001c\u0017WjZ5@\u0005\u0016\u0003\u0019%?wc\u0017\u0004&I\u0013\u0014\u000e\u0016)-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017\u0018\u0003\u0019\u0017 \tP\u0003\u0019<\u0004\u000e:\t4\u000e\u0011\u0003\u0006\f\u000f\u0005\u0012O>\u0000\u001f\t\u000b\u0017PAE\u000b\u001c\u001f9AK\u0017\u001a\u0019\u001b\u0019\u001d\u001c\u001f\u001e! \u001b #\"$\"\u001b\"\u001f%'&#(\u001d)\u001f%+*-,\u001b \u001c\u001a.\u0004/$0$1\u00042#\u0019\u0004 4\u0017-3\u001d.46L/4*NM\u001b2$3\u001b 4\u0017-3\u001d.46L/4*NM\u001b2$35%\u0012\u0017\u001b\u001946\\AAComparison ofManualandAutomaticMelodySegmentation,34 \u0000\u0004A'.\t4\u000e\u0011\u001a\u001f\f\u0012+\u0019+;\t\u000b\u0017\n0 2MA 2a+\u0006\t\u000b\u000e\u0011\rA]M\u0017@\t\u0014+,7\u001b\u0005\u0016\u0003\u0019\u0005\u0013\u0014&\u0010\u0015\u0011\u0001@\fv95\n\t\u000b\u0017*\u0015\u0011\u0003,\u0015:\t4)\u0015\u0011\u0003\u0019\u001a\u001f\f]0@\t\u000b\u0015:\t;\u0013\u000b&M\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012-_\u0007\r\f\u000f\u000e\u0016&I\u0013\u0014\u000e\u0011-'\t\u000b\u0017@%\u0012\f\u001cA \u0000.\f\u0012+\u0019\u0003\u0019\u001a\u001f\f\u000f\u000e:\t\u0014<\u0004+\u0019\f\u001e\u00124%\u001cO8\tPw \u000b1)\u0000!V\u0002\u00017ª \u000be\u0004\u0003\u0006\u0005:\u0010\u000b)\u00113\u0007\u0010;\u001a\u001b|VjZ5\u0004\u0005\u0016\u0003\u0006%HM%J\u000e\u0011\u0013\u001c\u0005\u0016\u0005\u001e\u000bg\u0013\u0014\u000e:0\u0004\fJ\u000e\u0011\u0005\u0012O[\u0016\u001a;\u001a;\u0017*AM~\u001b\f\u0012\f\t\u0014+\u0019\u0005\u0016\u0013\u0018\u0017\u001a\u0019\u001b\u0019$\u001c\u001f\u001e  \u001b #\"\u001b\"\u001b\"C%'&4)\u001f%!3$3\b\u0007\u001f%\n\t\f\u000b- \u000e\r\u000f\u0007\u0010\u000b-1#)-/\f\u0011\u0004 \u001b1\u0010\t\u0004M#\u0019\u0013\u0012-.\u001a1#\u001cN/\u001d.\u001b\u0019\u0014\u0012\u0016\u0015 %\u0017\u001a\u001946L,*O \u0000\u001f\t\u000b\u0017PA\n\u0014\u001c\u001c*A,\u001cJ[jHA EA\u0002M\f3\t\u000b\u000e\u0011\u0005\u001d\u0015Z\t\u0014\u0017@0\u000f2MA\u001f¯`+\u0006\t\u00145\u0004\u0017*\u00153Ae~\u001b5\u0004<\u0004\u0015\u0011\u0013\u001c\u0007\u0004\u0003\u0019%;\u0005\u001d\u0015\u0016\u000e\u00115@%\u000f\u0015\u00115\u001b\u000e\u0011\u0003\u0006\u0017\u0004\u001eW&2\u0013\u000b\u000e&25\u0004+\u0006+,)=+\u0019\f\u0012\u0017@\u001e\u000b\u0015\u0011\u0001x0\u0004\u00139%\u00125\u0004-/\f\u0012\u00179\u0015]\t\u0014%\u0012%\u000f\f\u0012\u0005\u0016\u0005\u0012Aewu\u0017}dEA\u0017\u00018\u0013\u0014\u000e\u0016&I\u0001\n\t\u0014\u001e\u0014\f\u001cO;ªRAadG\t\u000b\u0005\u001d)-?5\u0004\u0005\u0016\u0005\u0016\f\u0012\u0017PO\u0014\t\u0014\u0017@0@¯iA4Cz\u0003\u0019+\u0019+\u0019\f\u000f\u0015\u0016\u00153O\u001f\f30\u001b\u0003,\u0015\u0011\u0013\u0014\u000e\u0011\u0005\u0012O\u0011\u001f\u0002q\u0016l3:s\u0011s\u0016o\u0014r2n9\u0014tDl\u001d\u0002\u0013\n@s ,\r* .\u0014:n@csJq:n\rp\u0014BrBl\u0014n\rp\u001c *1l\u000bn33sJq\u0016sJn\r:s.l\u0014n 9RsJt\u000fs\u0016p\u0014q\u0016\u000b\nZp\u0014n\no \u0001Es \u0000\u0014s\u000f\u0019l !\n'sJn\u0004r2n \u0014:n3\u000fl\u0014qJXp\u0014Brl\u0014n 9RsJBqJrs \u0000\u000bp\u001c\n\u0005'-\u0014#\u000f\u001f\u0014'9 \u00073O\u0004\u0007\n\t\u000b\u001e\u001c\f\u0012\u00053\u0019\u0007\u001a9«\u0011\u0017;\u0014*Oi\u0018\u001a\u0007\u001a\u001c*A,43¬aA C\u0018A\u001b\tP\t\u0014\u0017\u0004%3\t\u0014\u0005\u001d\u0015\u0011\f\u000f\u000eg\t\u000b\u0017\n0EEA \u0000\u0004A\u000bC^\t\u000b\u000e\u0011\u0017\u0004\f\u000f\u000e3A\u001b\u0014:n3\u0012l\u000bqJXp\u0014Brl\u0014n 9RsJBqJrs \u0000\u000bp\u001c#\rl3o\u001fp\u0007\u0006\u000bA[wu\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017 dD\f\u0012\u0005\u0016\u0013\u00145\u0004\u000e\u0011%\u0012\f\u0012\u0005 ¯6\u000e\u0011\f\u0012\u0005\u0016\u0005\u0012OYD\u000e\u0011+\u0019\u0003\u0019\u0017@\u001e\u000b\u0015\u0011\u0013\u001c\u0017PO \u00116FO\u0018\u001a\u0007\u001a\u001c\u001bA,34¬aA\f\t\\\f\u000f\u000e:0\u0004\u0001@\t\u0014+M\t\u0014\u0017@0VdFA \u0000\u001f\t\u000b%J\u001c\f\u0012\u0017\n0\u001b\u0013\u0014#6A , 9sJn$sJq\u001dp\u0014Br \u0000\u0014s\b\u0013\n\u0004s\u0011l\u000bq \u0006fl\u001d=l\u0014n\rp\u0014P?m*t\u0011r\u000fA\njZw\u001d\u0000 ¯6\u000e\u0011\f\u0012\u0005\u0016\u0005\u0012O 2\u0002\t\u000b-?<\u0004\u000e\u0011\u0003\u00060\u001b\u001e\u001c\f\u001cO\nj>EO\\\u0018\u001a\u0007\u0014\u001c\u001bA,\u0016\u0010\u000bFdEA \u0000\u0004AMjZ%\u000fL.\t\u0014<\\O \t`A FAM~\u001b-/\u0003,\u0015\u0011\u0001POMwJA EA\u0002Cz\u0003,\u0015\u0016\u0015\u0011\f\u0012\u0017PO 2MA \t1ARG\f\u000f\u0017\n0\u0004\fJ\u000e\u0016)\u0005\u0016\u0013\u001c\u0017\\Oa\t\u000b\u0017\n0N~\rA \u0000\u001bA 2a5@\u0017\u0004\u0017@\u0003\u0019\u0017@\u001e\u0014\u0001\n\t\u0014->Ah\u0000h\u00133Qe\t4\u000e:0\u0004\u0005E\u0015\u0011\u0001@\f]0\u0004\u0003\u0019\u001e\u001c\u0003,\u0015:\t\u000b+e-?5@\u0005\u0016\u0003\u0019%+\u0019\u0003\u0006<\u001b\u000e:\t\u000b\u000e\u00167$|?\u0000h5@\u0017\u0004\fX\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u001c\t\u000b+g&\u000e\u0011\u0013\u001c-\t\u0014%\u0012\u0013\u00145@\u0005\u001d\u0015\u0011\u0003\u0019%'\u0003\u0006\u0017\u0004\u0007@5\u001b\u00153Ahwc\u0017 \u0001\u0006\u0003\u0019\u0018 \u001a\u000e\u001b\u001d\u001c\u001feq\u001dl3:s:s\u0016o\u0014rIn*\u0014tRl\u001d1\u0013\n@s\u0004\u001e\u000bt\u0011 ,\r* . \u0014:n@csJq:n\rp\u0014Brl\u000bn\rp\u001c *1l\u0014n\u00123sJq\u0011sJn\nJsRl\u0014n\u00018r\u0006\u0014rI=p\u001c \u00036rk\u000fq\u0016p\u0014q:r=sJt\u0011O.jZ5@+,\u0015\u0011\u0003\u0019-/\f30\u0004\u0003\u0006\t\b\u0000.\u0003\u0019\u001e\u001c\u0003,\u0015:\t\u000b+ \tP\u0003\u0019<\u0004\u000e:\t4\u000e\u0011\u0003\u0019\f\u0012\u0005\u0012OG\u0007@\t\u0014\u001e\u0014\f\u0012\u0005\u001c'«\n\u0018\u0014*Oi\u0018\u001a\u0007\u001a;\u0017*A,\u001c\u00193FdM\u0013*0\u001b\u001e\u001c\f\u000f\u000e \u0000\u001bAijZ%\u000fL.\t\u0014<\\O \t\\+\u0006\u001337\u00040\bEAP~\u001b-/\u0003,\u0015\u0011\u0001\\O1\u0000.\t4\u001a*\u0003\u00060 \u000b\u0002\t\u0014\u0003\u0019\u0017*<\u0004\u000e\u0011\u0003\u00060\u0004\u001e\u0014\f\u001cO\t\u0014\u0017@0\bwu\t\u0014\u0017WEA\rCz\u0003,\u0015\u0016\u0015\u0011\f\u0012\u0017PAP\u0000\u0002\u0001\u0004\f?LG\fJQ \u001f$\f3\t\u000b+\u0006\t\u0014\u0017\n0 \u0000.\u0003\u0019\u001e\u001c\u0003,\u0015:\t\u0014+8\tP\u0003\u0019<\u001b\u000e:\t\u000b\u000e\u00167$|j ª8\t\\\u0013*0\u001b7z\u0003\u0019\u0017\u0013\u0000,ª!\u0003EAR\u0000h\f\u0012%:\u0001@\u0017\u0004\u0003\u0019%3\t\u0014+.dD\f\u0012\u0007\r\u0013\u000b\u000e\u0016\u0015Z-'\t\u00127\u0013\u001a\u001b\u00123)=QR\u0003,\u0015\u0016\u0015\u0011\f\u0012\u0017\\O3\u0000M)\tP\u0003\u0019<\u0010j>\t\u000b\u001e\u001f\t\u0014y\u000f\u0003\u0006\u0017\u0004\f\u001cO\\j>\t\u00127\u0010\u001c\u00199Oh\u0018\u001a\u0007\u001a\u001b\u00129A,\u0018\u00174[jHAhjZ\f\u0012+\u00195@%\u0012%\u0012\u00031\t\u000b\u0017\n0\u0010L[APM\u000e\u0011\u0003\u0019\u0013\u0004AhjZ5@\u0005\u0016\u0003\u0019%3\t\u000b+`\u0003\u0006\u0017\u001b&2\u0013\u000b\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\b\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u001c\t\u000b+5@\u0005\u0016\u0003\u0019\u0017@\u001eH-/\f\u0012+\u0019\u0013*0\u0004\u0003\u0019%>\u0005\u00165\u001b\u000e\u0016&B\t\u0014%\u000f\f\u001cA1wu\u0017 \u001feq\u001dl3Js\u0011s\u0016o\u0014r2n9\u0014t\u0010l\u001d ,\r* . \u0001.r\u0006\u0014r2=p\u001c\u0003`rk\u000fq\u001dp\u0014q:r=sJt\n\u0005\u0001\u0006\u0003H\u0007 *1l\u0014n33sJq\u0016sJn\r:sJOX\u0007@\t\u0014\u001e\u0014\f\u0012\u0005V\u0018\u0019\u001c9«@\u0018\u0017\u0014\u001bO \u000ba\f\u000f\u000e\u0011\u001c\f\u0012+\u0019\f\u000f79O2\u0002EO@M5\u0004\u001e\u001c5@\u0005\u001d\u00158\u0016\u001a;\u001a\u0007\u001a\u001bA,\u001c\u00123[jHA$jZ\f\u0012+\u00195\u0004%\u0012%\u0012\u0003i\t\u000b\u0017\n0]L[A\nM\u000e\u0011\u0003\u0006\u0013\u001bA$~\u0004jZw \t{ªe|@\t?\u0005\u001d7\u001b\u0005\u001d\u0015\u0011\f\u0012- &2\u0013\u000b\u000eG%\u0012\u0013\u0014\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)<\n\t\u000b\u0005\u0016\f30Z-?5\u0004\u0005\u0016\u0003\u0019%3\t\u0014+\\\u0003\u0019\u0017\u0004&2\u0013\u000b\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017Y\u000e\u0011\f\u000f\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u001c\t\u000b+P\f\u000f\u0017\u001b\u001a*\u0003,\u000e\u0011\u0013\u001c\u0017\u0004-/\f\u0012\u00179\u0015\u0011\u0005\u0012A\nwu\u0017 \u001f\u0002q\u0016l3:s\u0011s\u0016o\u0014r2n*\u0014tl\u001d \u0014:n\u0004cs\u000f2 r\u00069sJn\u00045.\u0010m\u001b Br2's\u0016o\u0014rBp\t\u0014:n3\u000fl\u0014qJXp\u0014Brl\u0014n9RsJBqJr=s \u0000\u000bp\u0014 '\u0015\u0006\u000bt:csJ[tp\u000bn\ro\u000f.\bp\u0014n\rp39sJ/sJn@\n\u00059 \u0014&,#\" \u0007 *1l\u0014n\u00123sJq FsJn\r:sJO@\u0007\n\t\u0014\u001e\u0014\f\u0012\u0005F4:\u0010;\u0017P«@4\u0007\u0017\u0014\u001bO ¯6\t4\u000e\u0011\u0003\u0006\u0005\u0012O\u001b¬\u0004\u000e:\t\u0014\u0017\u0004%\u0012\f\u001cO@M\u0007\u001b\u000e\u0011\u0003\u0006+h\u0014\u0014\u001c*A,\u0016\u0014\u000b[jHA\\jZ\f\u0012+\u00195\u0004%\u0012%\u0012\u0003`\t\u0014\u0017@0>L[A\\M\u000e\u0011\u0003\u0019\u0013\u001bA$M\u0017 ª`\u001a\u001c\t\u000b+\u00195\n\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017W~9\u0015\u00115\n0*7Z\u0013\u0014\u0017WjZ5\u001b)\u0005\u0016\u0003\u0019%o¯i\f\u000f\u000e\u0011%\u0012\f\u0012\u0007\u001b\u0015\u0011\u0003\u0019\u0013\u001c\u0017?&I\u0013\u0014\u000eajZ5@\u0005\u0016\u0003\u0019% 2a\u0013\u001c\u00179\u0015\u0011\f\u0012\u00179\u0015\u0016)=<\n\t\u000b\u0005\u0016\f30[wc\u0017\u0004&I\u0013\u0014\u000e\u0011-'\t\u000b\u0015\u0011\u0003\u0019\u0013\u0014\u0017?dD\f\u000f)\u0015\u0016\u000e\u0011\u0003\u0019\f\u0012\u001a\u0014\t\u0014+BARwu\u0017 \u001f\u0002q\u0016l3:s\u0011s\u0011o\u000br2n*\u0014t\"l\u001d\b\u0013\n@s \u0014:n\u0004csJqJn\np\u0014Brl\u0014n\np\u001c *1l\u00143!@m\u001bcsJq.\u0010m*t\u0011r *1l\u0014n\u00123sJq\u0011sJn\n:sJO8\u0007\n\t\u000b\u001e\u001c\f\u0012\u0005W\u0016\u0017\u001f$«\u0018\u0018\u0017;\u0019*O \u000ba\f\u000f\u000e\u0011+\u0019\u0003\u0019\u0017PO\n3\fJ\u000e\u0011-'\t\u0014\u0017979OM5\u0004\u001e\u001c5\u0004\u0005\u001d\u0015M\u0014\u001c\u0014\u001bA,\u0016\u001a\u000b[jZ5@\u0005\u0016\u0003\u0019%3\t*Ag\u0000\u0002\u0001@\fZwc\u00179\u0015\u0011\f\u000f\u000e\u0011\u0017\n\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017\n\t\u000b+ \u00008\t\u000b\u0015:\t\u000b<\n\t\u0014\u0005\u0016\fY\u0013\u0014& 2a\u0001@\u0013\u000b\u000e:\t\u0014+RdM\f\u000f\u0007\r\f\u000f\u000e\u0016)\u0015\u0011\u0013\u001c\u0003,\u000e\u0011\f\u001cOL\u0000\u001f\t\u0014\u0017\\A\r\u000b\u001c\u001c*AL\u0017\u001a\u0019$\u0019$\u001c\u001f\u001e! \u001b 4\"\u001b\"\u001b\"\u001f%%$\u0010\u0007N&\u001bM\u001b2$3\u000f&\u00041$\u0019C%!/#.\u001d'\u0004 \u001bA \u000b\u000bªeA\u0014LG\t\u000b\u000e\u0011-/\u0013\u00145\u0004\u000e3A # \n@sRp\u0014n\np\u001c \u0006\u0014t\u0011r2tap\u0014n\noE\u0011l\u0011\u0014n\u0004r2Brl\u0014n]l\u001dek\u0011p\u0014t\u0011re's\u000f\u0019l3o\u000brt\u0011BqJm\u0004\u000fBm*q\u0011sJt\u0011A VG\u0017\u0004\u0003\u0019\u001a\u001f\f\u000f\u000e\u0011\u0005\u0016\u0003,\u0015u7f\u0013\u0014&02a\u0001@\u0003\u0019%3\t\u0014\u001e\u0014\u0013 ¯`\u000e\u0011\f\u0012\u0005\u0016\u0005\u0012O72a\u0001\u0004\u0003\u0019%3\t\u0014\u001e\u0014\u0013\u0004O.jZwJO\u0018\u001a\u0007\u001a\u001c*A 9\u000fFdDw\u0016~@jHANd = \f\u0012\u0007\r\f\u000f\u000e\u0016\u0015\u0011\u0013\u0014\u0003,\u000e\u0011\f wu\u00179\u0015\u0011\f\u000f\u000e\u0011\u0017@\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017@\t\u0014+x0\u001b\f\u0012\u0005 ~\u001b\u0013\u00145\u0004\u000e\u0011%\u0012\f\u000f\u0005 jZ5@\u0005\u0016\u0003,)%3\t\u000b+\u0006\f\u000f\u0005\u0012O\u0004\u0000\u001f\t\u0014\u0017\\A\u001c\u0014\u001c\u001c*A\u001d\u0017\u001a\u0019\u001b\u0019$\u001c\u001f\u001e  \u001b #\"\u001b\"\u001b\"C%\u0012.-M\u001b&\u000e6<%+\u0017-3\u001d.\u001b(\u001a3\u001d.\u001d\tC% 1\f\t\u0010\u0007- \u001d.-M\u001a&B6L (\n1\u001a,\u001b2$/ 6 1 % \u0017\u001a\u001946L,\u001bA \u00144\n3A\u001c~\u0004\t\u000b+\u0019\u0015\u0011\u0013\u0014\u0017PO\u001fEA\u0014~\u001b\u0003\u0019\u0017\u0004\u001e\u001c\u0001\n\t\u000b+BO 2MA\u001cEA:\u000ba5\u0004%J*+\u0019\f\u000f79O\u0014\t\u0014\u0017@0?jHA\u001fjZ\u0003,\u0015\u0016\u000e:\t\u001bA\u0014M5\u001b)\u0015\u0011\u0013\u001c-'\t4\u0015\u0011\u0003\u0019%G\t\u000b\u0017\n\t\u0014+,7\u001b\u0005\u0016\u0003\u0019\u0005\u0012O*\u0015\u0011\u0001@\f\u0012-/\fR\u001e\u001c\f\u000f\u0017@\f\u000f\u000e:\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017PO\u001b\t\u000b\u0017\n0/\u0005\u00165@-/-'\t4\u000e\u0011\u0003\u0006y\u0012\t\u000b\u0015\u0011\u0003\u0019\u0013\u001c\u0017\u0013\u0014&N-'\t\u000b%J\u0001\u0004\u0003\u0019\u0017@\f\u000f)\u000e\u0011\f3\t\u00140@\t\u0014<\u0004+\u0019\f\u0018\u0015\u0011\f\u000f(*\u0015\u0011\u0005\u0012A-'\n\u000fr=sJn\r:sJOf:\u0017\u0007\u0010@ \u00199\u0018\u0017:\u00109J|\u0019\u0016\u0010\u001f*\u001c«\u0016\u0010\u001f\u0007\u0017*Oi\u0018\u001a\u0007\u001a;\u0017\u001bA \u000b\u000bªeA ~*\f\u0012+,&I\u000e\u0011\u0003\u00060\u0004\u001e\u0014\f\u000f)h¬i\u0003\u0019\f\u0012+\u00060\\A Cz\u0001\n\t4\u0015 -/\u0013\u0014\u0015\u0011\u0003\u0019\u001a\u001c\t4\u0015\u0011\f\u0012\u0005 \t -?5\u0004\u0005\u0016\u0003\u0006%\u0012\t\u0014+95@\f\u000f\u000e\u00167*) wu\u0017 \u0014:n@csJq:n\rp\u000bBrl\u0014n\rp\u0014 '\u0015\u0006\u000b3!\u001bl\u0014t\u0011r2m* l\u0014n .\u0010m\u000eFt\u0011r \u0014:n\u0012\u0012l\u0014q:/p\u0014BrBl\u0014n 9DsJBq:r=s \u0000\u000bp\u001c O ¯`+\u00197\u001b-/\u0013\u00145\u0004\u0015\u0011\u0001POj>FO \u0014\u0014\u001c\u001bA\u0017\u001a\u0019\u001b\u0019\u001d\u001c\u001f\u001e! \u001b \u001b/#.\u00043#*\u000e'\u001a15%'2\u001b&F%%\u000746L3\u001a&\u001b&F%\u00121\u0010\t\u0010\u0007- 6+\u0007-&\u001bM\u001b2\u001d,\u0010-\u001d-\u0010-\u001a \u001d\u001c-3#\u001c\u00041\u001b.\u0004&$ M4*\u001a(\u0004M#\u0019\u001a1\u0004&$ \u001b&\u001d1\u001a,\f.\u001b.\u0004M\b\t\u001d'\u001a1\u000e/\u001bM4*\u001a(-M#\u0019\u001b1 %+\u001c\u000e\t\u001d.\nO5\u0000\u001f\t\u0014\u0017\\A\r\u000b\u001c\u001c*A \u001c\u0010\u0014E\u0000h\u0013\u0014\u0007@\u0003\u0019% \u0000.\f\u000f\u0015\u0011\f\u0012%\u000f\u0015\u0011\u0003\u0019\u0013\u0014\u0017 \t\u0014\u0017\n0\u0000P\u000e:\t\u0014%:*\u0003\u0006\u0017\u0004\u001e\u0004O ¯`\u0001\n\t\u000b\u0005\u0016\fv*A>\u0000\u001c\t\u0014\u0017PA/\u0014\u0014\u001f*A\u0017\u001a\u0019\u001b\u0019\u001d\u001c\u001f\u001e! \u001b 6 /\u001d.$\u001c\u001b\u0017\u001f% ,\f\t-2F%%\u0007$\u001c\u00041\u001d*\u001b*\u001f%\u00121\u0010\t\u0010\u0007- \b0\u0004.\u001a/\u001b0$1-2#\u0019-&\u001d \f1\u001d2\u000e1\u0014,\u001b  \u0007\u00194EEAAVG\u0003,\u0015:0\u0004\f\u000f\u0017\u001b<\r\u0013\u0014\u001e\u001c\f\u000f\u000e:0^\t\u000b\u0017\n0 \u0000\u001bA!\u001f$\u0013\u0014<\r\f\u0012+BA1j>\t\u0014\u0017\u0004\u0003\u0019\u0007@5@+\u0006\t4\u0015\u0011\u0003\u0006\u0013\u0014\u0017N\u0013\u0014&D-?5\u0004\u0005\u0016\u0003\u0006%&2\u0013\u000b\u000eG-/\f\u000f+\u0006\u0013*0*7]-'\t\u000b\u0015\u0011%:\u0001@\u0003\u0019\u0017@\u001e\u001bA@wc\u0017 \u001f\u0002q\u0016l3:s\u0011s\u0011o\u000br2n*\u0014t[l\u001d3,\r* . .\u0010m\u001b Br2's Fo\u0014rp *1l\u0014n\u00123sJq\u0011sJn\nJsJO@\u0007\n\t\u000b\u001e\u001c\f\u0012\u0005G\u0014;\u00199«\u001b\u001c\u0010\u001f*O \u000bg\u000e\u0011\u0003\u0019\u0005\u001d\u0015\u0011\u0013\u001c+BO\nV3\u0001XO\\\u0016\u001a;\u001a\u0007\u0014\u001bA"
    },
    {
        "title": "Opuscope - Towards a Corpus-Based Music Repository.",
        "author": [
            "Thomas Noll 0002",
            "Jörg Garbers",
            "Karin Höthker",
            "Christian Spevak",
            "Tillman Weyde"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417411",
        "url": "https://doi.org/10.5281/zenodo.1417411",
        "ee": "https://zenodo.org/records/1417411/files/NollGHSW02.pdf",
        "abstract": "Opuscope is an initiative targeted at sharing musical corpora and their analyses between researchers. The Opuscope repository will contain musical corpora of high quality which can be annotated with hand-made or algorithmic musical analyses. So, analytical results obtained by others can be used as a starting point for one’s own investigations. Experiments performed on Opuscope corpora can easily be compared to other approaches, since an unequivocal mechanism for describing a certain corpus will be provided.",
        "zenodo_id": 1417411,
        "dblp_key": "conf/ismir/NollGHSW02",
        "keywords": [
            "Opuscope",
            "musical corpora",
            "analyses",
            "researchers",
            "repository",
            "high quality",
            "annotated",
            "hand-made",
            "algorithmic",
            "investigations"
        ],
        "content": "Opuscope –Towar dsaCorpus-Based Music Repository\nOpuscope –TowardsaCorpus-Based Music Repositor y\nThomas Noll, J¨orgGarbers\nTechnische Universit¨atBerlin\nFranklinstr aße28/29\nD-10587 Berlin\n+493031473126\u0000\u0002\u0001\u0004\u0003\u0006\u0005\u0007\u0005\t\b \n\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0014\u0013\u0014\u0015 \u0016\u0018\u0017\u001a\u0019\u001c\u001b\u001e\u001d \u001f\u0018\u0005\u0007!\u0007\u0001\"\u0015 #$\u001dKarinH¨othker,Christian Spevak\nUniversit¨atKarlsruhe\nAmFasanengar ten5\nD-76128 Karlsruhe\n+497216084214\u0000&%\u001a\u0003\u000e\u001d \u0016\u0018%\u0004'\u0006\u001d \u001f\u0002\b \u0013)(\u001e\u001d\u0014*\u0006+,'\u0004\r\u0010\u000f\u0012!-\u001f.+$\u0015 \u0017\u0004'\u0010+$\u0015 #$\u001dTillman Weyde\nUniversit¨atOsnabr ¨uck\nNeuer Graben/Schloß\nD-49069 Osnabr ¨uck\n+495419694458\ntweyde@uos .de\nABSTRACT\nOpuscope isaninitiati vetargeted atsharing musical corpora and\ntheir analyses between researchers. TheOpuscope repository will\ncontain musical corpora ofhigh quality which canbeannotated\nwith hand-made oralgorithmic musical analyses. So,analytical\nresults obtained byothers canbeused asastarting point forone’s\nowninvestigations. Experiments performed onOpuscope corpora\ncaneasily becompared toother approaches, since anunequi vocal\nmechanism fordescribing acertain corpus willbeprovided.\n1.MOTIVATION\nWhen developing newmethods instructural music analysis ormusic\ninformation retrie val(MIR), thequestion arises whether theygen-\neralize topreviously unseen pieces orstyles ofmusic, andhowtheir\nresults compare torelated methods. Inother branches ofscience,\nrepositories with various experimental data setsarewidely used for\nmethod validation andcomparati vestudies (e.g. theUCI Repository\nofmachine learning databases [1]). Instructural music analysis,\nmost research groups usetheir individually deﬁned datasets. sothat\nmeaningful quantitati vecomparisons ofresults arenotpossible.\nThevoluminous collections ofdigital music scores available onthe\ninternet areill-suited forthepurpose: only fewofthem meet the\nquality standards required formusicological investigations (e.g. the\nMuseData library [7]). Moreo ver,theyfocus onproviding abroad\nvariety ofpieces, butextracting corpora according topredeﬁned\nmusical criteria andthedocumentation ofexperimental results is\nbeyond thescope ofthese collections.\nResearchers intheMIR community haverecently expressed the\nnecessity tocreate standardized benchmark collections toevaluate\nretrie valalgorithms [3,6].Opuscope aims atdeveloping acorpus-\nbased music repository which addresses these issues. Corpora of\nmusic pieces canbeextracted toprovide areproducible testbedfor\nthecomparison ofretrie valalgorithms.\nAnother motivation forcreating theOpuscope repository comes\nfrom theneed toexchange comple xmusic-analytical structures.\nThis need arises forexample when discussing analytical results in\nmathematical music theory ,orwhen calculating newlearning pat-\nterns from preprocessed datainmachine learning. Analytical results\ncanbedocumented intherepository referring totheunique musical\ncorpus used, thereby enhancing scientiﬁc transparenc yandcompa-\nrability .Asaside effect, working material covered inOpuscope\nwillfocus onmusical scores andperformance data.\nWebelie vethatOpuscope could constitute aninfrastructure from\nwhich manyresearchers andstructural music research ingeneral\ncould beneﬁt. Ourvision istoprovide anetw orkthatallowssharing\nmusical data ondifferent analytical levelstogiveamore complete\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feeprovided\nthat copies arenotmade ordistrib uted forprofitorcommercial\nadvantage andthatcopies bear thisnotice andthefullcitation on\nthefirstpage.\n\u0011/\n2002 IRCAM -Centre Pompidoupicture ofmusic –likeobserv atories allovertheworld share their\ndata tobuildamore complete image ofouter space.\n2.ORGANIZA TION\nOpuscope consists ofproject groups working onspeciﬁc corpora,\naservice team providing infrastructure andsupport, andendusers\nwho retrie vecorpora fortheir research purposes (Figure 1).\nMusical corpora arecreated andmaintained byautonomous project\ngroups inorder toaccommodate interests ofresearchers with differ-\nentbackgrounds. Each project group chooses acorpus ofmusical\npieces according tocriteria suiting their scientiﬁc needs anden-\ncodes them. Thedata cancomprise works ofaparticular composer\norgenre, variations orimpro visations onatheme, ordifferent inter-\npretations ofawork. Examples ofcorpora include Bach chorales,\n“Tr¨aumerei” performances or“pr´eludes non-mesur ´ees.”Theproject\ngroup isresponsible fordocumenting thetype ofinformation inthe\ncorpus andensuring that copyright isrespected. When acorpus\nhasreached areasonable state ofmaturity ,itispublished onthe\nOpuscope web platform.\nThe Opuscope web platform servesasacommunication turntable\nfortheparticipants. Inparticular ,alistofproject groups anda\nMUSITECH-based corpus browser willbeprovided (see nextsec-\ntion). Opuscope users (possibly belonging toaproject group) can\nretrie veacomplete corpus, useitfortheir research andrefer toit\nintheir publications. Typical users might beamusicologist who\nreturns amotivicanalysis ofthecorpus totherepository ,aresearch\ngroup thatrefers toOpuscope datainapublication, orastudent who\nuses Opuscope data forcourse work.\nTheadministrati vetasks aretakencare ofbytheOpuscope service\nteam. These include thedeﬁnition oftheXML-based corpus de-\nscription format andtheregistration ofongoing projects, sothat\nresearchers from different areas interested inthesame corpus can\njointheir efforts.\n3.TECHNICAL CONCEPT\nAcorpus consists ofacollection ofmusical data, analyses, and\nannotations, which canbeedited andupdated. Ingeneral, itwillbe\nagood idea torecord aﬁxedversion ofthecorpus before producing\nanyannotations. While thecorpus changes, thehistory ofdifferent\nversions willberegistered using aversion control system [2],where\nprevious versions remain accessible andcanbeidentiﬁed clearly ,\ne.g.bytheir URL.\nThe corpus structure andadditional annotations, e.g.analyses of\nthepieces, areencoded using anXML-based format. This format\nwillbederivedfrom MUSITECH, which iscurrently being devel-\noped atOsnabr ¨uckUniversity along with asetoftools foraﬂexible\nrepresentation ofmusical structure [4]. The MUSITECH format\nintegrates representations ofmusical dataondifferent structural lev-\nels.Onthelowest levelitcomprises sound dataandnote datawhere\nsymbolic data based onscores canbecombined with andrelated\ntoperformance data, forinstance from MIDI recordings. Structural\nandanalytical data may beadded tonote information, containing\nvoices, chords, motifs orother subsets representing musical struc-\ntureandanalytical annotations. Additionally ,metadata canprovideOpuscope –Towar dsaCorpus-Based Music Repository\ne.g. Bach chorales e.g. Preludes \nnon mesureese.g. Träumerei \nperformancese.g. Musicologist\ne.g. Studente.g. Research Group- list of project groups\n- corpus browser \n- publication list\n- sample corpus\n- documentation of data format\n- mailing list\n...\n... - encodes data corpus\n- chooses representation\n- documents corpus   \n- checks copyright- fixes format for \n   corpus representation \n- registers new projects\n- maintains web platform\n- supports project groups - retrieves data corpus\n- returns annotations \n- retrieves data corpus\n- submits publication related \n    to Opuscope data\n- retrieves data corpus \ne.g. Research Group\n- involved in project group \n- provides algorithmic\n\t   analysisUser\nUserUserUser\nOpuscope\nproject groupOpuscope\nproject groupOpuscope\nproject groupOpuscope\nweb platformOpuscope\nservice team\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0014\u0013\u0016\u0015\u0018\u0017\u0006\u0019\u001b\u001a\u001c\u0015\u001e\u001d\u001f\u0019! \"\u0017# \u0016$%\u000f\u0012&('\u000e)+*, \u0018&.-\ngeneral information about thepiece, thecomposer ,andtheper-\nformer aswell ashistorical structural andother annotations wich\nmay beuseful forselecting acorpus oracorpus subset foraspeciﬁc\npurpose.\nThe actual music data isstored either using existing formats such\nasMIDI orkern[5],orintheMUSITECH format. Theidea isto\nhaveanXML ﬁleforeach corpus, containing project information as\nwell asdataand/or links toexternal data. Aweb interf acetobrowse\nOpuscope corpora willbeintegrated intotheplatform. Inaddition,\ntheuser willbeable toselect data ontheserveraccording tocertain\ncriteria anddownload only theselected ﬁles.\n4.PILOTPROJECT\nCurrently three groups from Berlin, Karlsruhe, andOsnabr ¨uckare\ninvolvedinsetting upanOpuscope pilot project [8]. Itisbased\nontheinfrastructure provided bytheBerliOS open source softw are\ndevelopment platform, which includes aproject home page, public\nforums, mailing lists, aproject documentation manager ,andaCVS\nrepository .The idea istoimplement andtestaprototype ofthe\npresented concept incollaboration with potential Opuscope users.\n5.REFERENCES\n[1]C.L.BlakeandC.J.Merz. UCI repository ofmachine learn-\ningdatabases, 1998. /1020235476\u001e6\n8\u001e8\u001e859;:2<\u001e=\b9?>\"<2:\b9A@2B2>\u00166\u0018CED\u000eF2@\u001eG2H2I\u00186J\u001eK2L@M3\u0018N1=2:\u001c0\u0016NMH\u001eOP9?/10\nD\u000eF .[2]CVS. concurrent versions system./10\u001e02354A6\u001e6\u001c8\u001e8\u001e8Q9RD\u0018>\"=M@MB\u0016GM0\u0016GS9N\u001cH\u001eT.\n[3]J.S.Downie. Thinking about formal MIR system evaluation:\nSome prompting thoughts. In UWV;X,YEZ[Z;\\M]R^\u0016_M`aX7bdc\u0004efZdgE^\"chZ+V+^\u000ei2jck]kXM^\u0006i2lnmfo\u001cprq\u0018XM`[]Rs\u0016ptXM^vuws\u0016`[]xYygE^\u001fb\u001fXMVEpziMck]kXM^|{\u0002Z+ckVE]?Z+}\u001ci2l~RgEm\u000ejug;{W ,Plymouth, Massachusetts, Oct. 2000.\n[4]M.Gieseking andT.Weyde. The MUSITECH infrastructure\nforinternet-based interacti vemusical applications. InUWV;X,YEZ[Z;\\2j]\u0004^\u0016_M`\u0010X7b\u0010c\u0004efZgE^\"chZ+V+^\u000eiMck]xXM^\u000ei2l\u00025XM^,b,Z+V;Z+^\u0006YEZzXM^Z\u001f\u0010Z\u0014l ]\u0004}MZ+V+]R^1_XAb\u0012uws\u0016`[]xY~\nzn5Sum\u0016g\u001c( ,Darmstadt, German y,2002. In\npress.\n[5]D.Huron. Humdrum andkern: Selecti vefeature encoding. In\nE.Selfridge-Field, editor ,rZ+o2XM^\u0016_ug;g\u001fefZ\u0012iM^\u0006\\2EX,X,X7buds\u0018`[]xY[i2lQ5X,\\1Z+` ,pages 375–401. MIT Press, 1997.\n[6]efZuws\u0016`[]xYgE^,b\u0014XMV+piMck]xXM^w{Z+ckVE]?Z+}\u001ci2l \u0016uws\u0016`E]kY\u0012]!_M]Rc?iMl(Q]x\u0014V7iMVEoW}\u001ci2l s\"iMck]xX\u001c^dUWV;XA,Z[Y\u0014czze\u0016]RchZUiEq\"Z+V#5X2l\u0004l!Z[Y\u0014ck]kXM^fS\\\u001c]Rck]xXM^\f ,\nJuly 2002. /10\u001e02354A6\u001e6\u001fD\u0018>\"=2:\u001e<2\u000e:\nHQ97NMH\u001eT162@2 \u0016G\u001eF\n>\u0018GM0\u0018:2NMI\u00166\u001c8\u001e3(¡\n6 .\n[7]Musedata: Anelectronic library ofclassical music scores./\u001e0\u001e023547626\u001c8\u001e8\u001e859\u0004D\u0018>\"=M@2B1GM0\u0016GS97N\u001cH\u001eT .\n[8]Opuscope: aBerliOS project./10\u001e02354A6\u001e6MB1@2 \u001e@1F\u001eN\u001c3\u0016@MHQ9?¢\u0016@2H1F1:2N1=\b9B\u001e@16\u001c31H\u0016N2£2@\u0016<\u001c0\u0018=M6\u001eN\u001c3\u001e>\"=2<2N\u001c3\u0016@ ."
    },
    {
        "title": "Encoding Timing Information for Musical Query Matching.",
        "author": [
            "Bryan Pardo",
            "William P. Birmingham"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415776",
        "url": "https://doi.org/10.5281/zenodo.1415776",
        "ee": "https://zenodo.org/records/1415776/files/PardoB02.pdf",
        "abstract": "We compare representing note timing as Inter Onset Intervals (IOIs) and as the ratio of adjacent IOI values. A variety of log2 and linear quantizations of IOI and IOI ratios are considered for each representation. The utility of encoding with a particular quantization is measured by the ability of a simple string-matcher to differentiate between themes in a melodic corpus. Results indicate that time is best represented by IOI ratios quantized to a logarithmic scale.",
        "zenodo_id": 1415776,
        "dblp_key": "conf/ismir/PardoB02",
        "keywords": [
            "note timing",
            "Inter Onset Intervals (IOIs)",
            "ratio of adjacent IOI values",
            "log2 and linear quantizations",
            "melodic corpus",
            "string-matcher",
            "differentiate between themes",
            "time",
            "IOI ratios",
            "logarithmic scale"
        ],
        "content": "Encoding Timing Information for Musical Query Matching \nEncoding Timing Information for Musical Query Matching\nBryan Pardo \nUniversity of Michigan  \nEECS Department \n1101 Beal Avenue, Ann Arbor, MI, USA \n+1-734-369-3207  \nbryanp@umich.edu William Birmingham \nUniversity of Michigan  \nEECS Department \n1101 Beal Avenue, Ann Arbor, MI, USA \n+1-734-763-1561  \nwpb@eecs.umich.edu\n \nABSTRACT  \nWe compare representing note timing as Inter Onset Intervals \n(IOIs) and as the ratio of adjacen t IOI values. A variety of log2 \nand linear quantizations of IOI and IOI ratios are considered for each representation. The utility of  encoding with a particular \nquantization is measured by the ability of a simple string-matcher to differentiate between themes  in a melodic corpus. Results \nindicate that time is best represented by IOI ratios quantized to a logarithmic scale.  \n1. MUSIC IR AND STRING MATCHING \nFinding the best match between a melodic query and the melody \nof a target piece in a database has been a subject of great recent interest in the music IR world [1-5]. Both query and target are typically represented as a sequence of symbols drawn from a finite alphabet. Such sequences are commonly called strings.  \nStandard string matching [3, 6-8] measures the distance between \ntwo strings,  S\n1 and S2, as the number of operations required to turn \nS1 into S2, given a fixed set of allowable operations.  A typical set \nof edit operations would include deletion of a string element, insertion of an element, and matching between an element of S\n1 \nand an element of S2. Given a query string, Q, and a set of target \nstrings drawn from a database, the closest target is the one with \nthe lowest cost transformation into Q.  \n2. TEMPO SCALING WITH IOI RATIOS \nMelody matching has, in general,  concentrated on matching pitch \ncontour. Typically, rhythm is encoded implicitly. The number of contiguous, fixed-duration frames w ith the same pitch indicates \nnote length. Figure 1 shows three se quences as piano roll. Vertical \nlines indicate the boundaries of fixed-duration frames. A letter \nabove each frame indicates the pitch class present in that frame.  \nFrom the figure, it is clear that S\n1 and S2 represent the same \nmelody performed at two different speeds and S3, while \ncontaining all the notes in S1, is another melody entirely. A string \nmatcher, using the three edit operation from the previous section and the frame-based representation, finds the least-cost alignment between S\n1 and S2 is the same as the least-cost alignment between \nS1 and S3. The problem of dealing with  tempo scaling issues may \nbe resolved by explicitly representing durations and normalizing them to a reference duration, d\nref.  \nFigure 1. Three Sequences Ideally, dref would be the duration of a beat, as in written notation. \nUnfortunately, it is not always clear what the beat is in a sequence generated from a sung query. A simple substitute for the beat is the duration of the previous even t. If note durations are measured \nusing the Inter Onset Interval (IOI), the ratio between the current \nand next IOI may be used. We call this the IOI ratio (IOIr). Figure \n1 shows pitch interval (PI) as measured in half steps, IOI \n(measured in frames) and IOI ratio. Here, the final IOI ratios default to unity. Representing th e strings as <PI, IOIr> duples \nmake S\n1 and S2 identical, while S3 is distinctly different. \n3. ALPHABET SIZE \nAdvanced string matchers [6] a llow for inexact matches between \nstrings, based on the likelihood of  co-occurrence between element \ni of S1 and element j of S2. Allowing for inexact matches in \nmelody matching provides a way to  model singer error. This is \ndone by creating a probability distribution over the set of possible observations for each possible note. Given that the size of the alphabet of possible observations, n, is the same as the size of the \nalphabet of notes, the full obser vation-probability table is size n\n2. \nObservation-probability tables require many observation-state \npairs to provide good estimates of the probabilities. As the size of the alphabet increases, so does the difficulty of collecting sufficient data to fill the tabl e with good probability estimates; \nwithout good estimates, it is not po ssible to have a good matcher. \n4. EXPERIMENTAL SECTION \nIf we represent notes as <PI,  IOIr> duples, the size of the \nobservation-probability table incr eases quadratically as the \nnumber of duration values increases  linearly. Given this, we want \nto minimize the number of duration values used, without hurting a matcher’s ability to differentiate melodies based on rhythm. This section compares different encodi ngs of IOI and IOIr values, with \nthese goals in mind. \n4.1 Corpus Construction \nWe compiled 417 MIDI files created from themes in an index of themes of classical pieces [9]. Themes were entered into the music-typesetting program Fina le and saved as MIDI by \nundergraduate music students at our  university. For this corpus, \nthe largest IOI is 5100 millis econds. The smallest is 50 \nmilliseconds, 102 times shorter than th e largest. IOI values in the \ncorpus fall into 35 unique values, resulting in 82 unique IOI ratios.  \n4.2 Time Encoding \nWe encoded only timing informati on (i.e., no pitch information) \nfor each theme in the corpus as e ither IOI, or IOI ratios. Values \nwere encoded as integers using a 2n level quantization, with a \nsaturation point of s. Bins were spaced evenly, either in the linear \ndomain or in the log2 domain. Log2 bins were evenly distributed \nin the range – s and s. Linear bins were evenly distributed between \n0 and s. Values greater than s were put in bin number 2n-1. Log2 \nvalues less than –s were placed in bin 0. Linear values below 0 Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncommercial advantage and that copies bear this notice and the full \ncitation on the first page.  \n© 2002 IRCAM – Centre Pompidou   \nS1 G      A    B \nPI       2   2 \nIOI      1   1 \nIOIr     1   1 G     G     A    A    B    B S2 \nPI       2      2 IOI      1      1 IOIr     1      1 G     F     C    A    B    CS3 \n PI   -2  7  -3  2  1 \nIOI   1  1   1  1  1 \nIOIr  1  1   1  1  1 Encoding Timing Information for Musical Query Matching \nwere placed in bin 0. Linear vs. log2 bins and values for n and s \nwere varied as experimental parameters. \n4.3 Encoding Evaluation  \nGiven a number of bits for the encoding, n, and a saturation point, \ns, timing information for every theme in the corpus was encoded \nto an alphabet of size 2n. We encoded timing information in the \n417-theme corpus a total of 336 different ways. The data were encoded as either IOI or IOI ratio. Values were scaled either linearly or as log2. The number of bits, n, for the encoding was \nvaried between 2 and 8 (giving between 4 and 256 bins). Finally, the saturation point,  s, was varied between 1 and 12. For \nconsistency between the log2 and linear encodings, the saturation \npoint became the saturation exponent in linear encoding. Thus, saturation point of 5 in the log2 world is saturation point 2\n5in the \nlinear world.  \nA  query set of 30 themes was chosen at random from the corpus. \nThis query set was used consiste ntly through all trials. For each \ntrial, an encoding of the data was selected and a simple string \nmatcher was used to rank all themes in the corpus by similarity of timing information (no pitch information was used) to each of the 30 themes in the query set. Ideally, when theme i is the query, the \nsingle best-scoring theme is theme i. If the encoding makes some \nthemes indistinguishable, the numbe r of themes with the maximal \nscore increases. Thus, the average number of themes receiving a maximal similarity score provide s a relative measure of how well \nan encoding preserves timing information. \n4.4 Results \nThe best value possible for an enc oding is 1 (the query melody is \nalways the single best match). The worst possible value depends on how many themes in the corpus  have the same length as the \nquery. This is because the matche r can always distinguish themes \nbased on number of events, even if  all events are given the same \nlabel. For the query set, the average number of themes the same length as a query is 26. Figur e 2 show the mean number of \nmaximal-scoring themes returned in each trial, broken down by encoding, scaling, number of bits, and saturation point.  \nA2\nB3\nC4\nD5\nE6\nF7\nG8Bits\n135791113151719212325Me\nan Ma\nx CountAAAAAA\nA\nA\nA\nAAABBBBBB\nB\nB\nBBBBCCCCC\nC\nC\nC\nCCC CDDDDD\nD\nD\nD\nDDDDEEEEE\nE\nE\nE\nEEEEFFFFF\nF\nF\nF\nFFFFGGGGG\nG\nG\nG\nGGGGIOI      Linear IOIratio Linear\nIOI      Log2 IOIratio Log2AAAAAAAAAAAA\nBB BBBBBBBBBB\nCCC CCCCCCCCC\nDDDD DDDDDDDD\nEEEEE EEEEEEE\nFFFFFF FFFFFF\nGGGGGGG GGGGG\n123456789 1 0 1 1 1 2\nSaturation Exponent135791113151719212325Me\nan Ma\nx CountAAAAAAAAAAA\nABBBBBBB\nB\nB\nB\nBBCCCCCC\nC\nC\nC\nCCCDDDDDD\nD\nD\nDDDDEEEEE\nE\nE\nE\nEEEEFFFFF\nF\nF\nF\nFFFFGGGGG\nG\nG\nG\nGGGG\n123456789 1 0 1 1 1 2\nSaturation ExponentAAAAA AAAAAAA BBBBB BBBBB BB CCCCCCCCCC CC DDDDDDDDDDDDEEEEEEEEEEEEFFFFFFFFFFFFGGGGGGGGGGGG\n \nFigure 2. Count of themes indistinguishable from correct \ntheme by encoding and scaling  \nThe data show that, given a good choice of saturation point, it is possible to represent rhythmic valu es quite well using IOI with as \nfew as 2 bits to encode the data. In the case of this corpus, the best saturation exponent point is at 10. This corresponds to 2\n10, or 1024 \nmilliseconds. The data show that IO I is extremely sensitive to the \nchoice of saturation point. Log2 ba sed encoding of the IOI data \nhelps some when the saturation poi nt is set too high, but does not \nhelp when the saturation point is too low. Linear IOI ratios show the best  saturation exponent is around 1. \nAs the saturation point is placed further from it, more bits are needed in order to maintain sufficient resolution below 1. Log2 IOI ratios (LIRs) are a different story. The data show LIRs are extremely insensitive to variations in saturation point and number of bits. In fact, the worst result returned by any LIR encoding was an average of 1.3 themes per query. \n5. CONCLUSIONS \nThe use of explicit representa tion of time, coupled with \nexpressing timing variation as IO I ratios, normalizes rhythmic \ninformation between two perform ances at different tempos. \nExplicit encoding of time increases the alphabet size, which makes creating observation probability tables more difficult. \nEncoding timing as the log2 of IOI ratios preserves sufficient \ninformation to discriminate between  themes in our corpus using as \nfew as 2 bits (4 bins), minimizi ng the size of the state alphabet. \nThis allows the use of explicit re presentation of rhythmic relation \nthat is invariant with respect to tempo and still manageably small in alphabet size, indicating th is is a promising method for \nencoding timing information fo r musical query retrieval. \n6. ACKNOWLEDGMENTS \nWe gratefully acknowledge National Science Foundation grant \nIIS-0085945, and The University of Michigan College of Engineering seed grant to the Mu sEn project. The opinions in this \npaper are solely those of the authors and do not necessarily reflect the opinions of the funding agencies. \n7. REFERENCES \n[1] Mazzoni, D. and R.D. Da nnenberg. Melody Matching \nDirectly From Audio. IS MIR. 2001. Bloomington, IN. \n[2] Bainbridge, D., MELDEX: A Web-based Melodic Locator \nService, in Computing in Musi cology, W.B. Hewlett and E. \nSelfridge-Field, Editors. 1997, The MIT Press: Cambridge, MA. p. 223-229. \n[3] Lloyd Smith, R. McNab, and I. Witten, Sequence-Based \nMelodic Comparison: A Dynamic-Programming Approach, in Computing in Musicology, W. B. Hewlett and E. Selfridge-\nField, Editors. 1997, The MIT Press: Cambridge, MA. p. 101-117. \n[4] Shifrin, J., et al. HMM-Based Musical Query Retrieval. in JCDL. 2002. Portland, Oregon, USA. \n[5] Hoos, H., K. Renz, and M. Gorg. GUIDO/MIR - an Experimental Musical Informa tion Retrieval System based \non GUIDO Music Notation. In ISMIR. 2001. Bloomington, \nIN. \n[6] Gotoh, O., An improved algor ithm for matching biological \nsequences. Journal of Molecular Biology, 1982. 162: p. 705-708. \n[7] Gusfield, D., Algorithms on St rings, Trees, and Sequences. \n1997, New York, NY: The Press Syndicate of the University of Cambridge. \n[8] Needleman, S.B. and C.D.  Wunsch, A general method \napplicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 1970. 48: p. 443-453. \n[9] Barlow, H. and S. Morgenster n, A Dictionary of Musical \nThemes. 1975, New York , New York: Crown Publishers."
    },
    {
        "title": "Measuring the similarity of Rhythmic Patterns.",
        "author": [
            "Jouni Paulus",
            "Anssi Klapuri"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1414712",
        "url": "https://doi.org/10.5281/zenodo.1414712",
        "ee": "https://zenodo.org/records/1414712/files/PaulusK02.pdf",
        "abstract": "A system is described which measures the similarity of two arbi- trary rhythmic patterns. The patterns are represented as acoustic signals, and are not assumed to have been performed with similar sound sets. Two novel methods are presented that constitute the algorithmic core of the system. First, a probabilistic musical meter estimation process is described, which segments a continuous musical signal into patterns. As a side-product, the method outputs tatum, tactus (beat), and measure lengths. A subsequent process performs the actual similarity measurements. Acoustic features are extracted which model the fluctuation of loudness and brightness within the pattern, and dynamic time warping is then applied to align the patterns to be compared. In simulations, the system behaved consistently by assigning high similarity measures to sim- ilar musical rhythms, even when performed using different sound sets.",
        "zenodo_id": 1414712,
        "dblp_key": "conf/ismir/PaulusK02",
        "keywords": [
            "arbitrary rhythmic patterns",
            "measures similarity",
            "acoustic signals",
            "musical meter estimation",
            "tatum",
            "tactus",
            "measure lengths",
            "acoustic features",
            "loudness",
            "brightness"
        ],
        "content": "Measuring the Similarity of Rhythmic Patterns\nABSTRACT\nA system is described which measures the similarity of two arbi-\ntrary rhythmic patterns. The patterns are represented as acousticsignals, and are not assumed to have been performed with similarsound sets. Two novel methods are presented that constitute thealgorithmic core of the system. First, a probabilistic musical meterestimation process is described, which segments a continuousmusical signal into patterns. As a side-product, the method outputstatum, tactus (beat), and measure lengths. A subsequent processperforms the actual similarity measurements. Acoustic features areextracted which model the ﬂuctuation of loudness and brightnesswithin the pattern, and dynamic time warping is then applied toalign the patterns to be compared. In simulations, the systembehaved consistently by assigning high similarity measures to sim-ilar musical rhythms, even when performed using different soundsets.\n1.  INTRODUCTION\nMusic is composed, to an important degree, of patterns thatare repeated and transformed. Patterns occur in all ofmusic’s constituent elements, including melody, rhythm, har-mony, and texture . —Rowe, [1, p.168]\nPattern induction and matching plays an important role in music\nanalysis and retrieval. Especially melodic fragment matching hasreceived much attention in recent years [1,2,3,4]. However, mea-suring the similarity of rhythmic patterns has been almost a\nneglected problem. Work on the computation analysis of musicalrhythms has concentrated almost entirely on beat detection andtime quantization (see [5,6] for recent examples). Measuring thesimilarity of rhythmical patterns can be applied e.g. in musicaldatabase searches and in music context analysis in general [7].\nIt is intriguing to ask what makes two rhythms similar or dissimilar\nfrom a perceptual point of view. The problem has been addressedby musicologists in experiments, where rhythmic patterns werepresented for human listeners for similarity judgments or for repro-duction [8,9]. Obtained dimensions of dissimilarity have beeninterpreted to be e.g. “meter”, “rapidity”, “uniformity–variation”,“simplicity–complexity” etc. [8]. A problem with these ﬁndings isthat it is very difﬁcult to encode and quantify them into a computermodel. In following, a more pragmatic approach is taken.\nThe aim of this paper is to propose a method for measuring the\nsimilarity of two rhythmic patterns which are performed usingarbitrary drum/percussive sounds, and presented as two continuousacoustic signals. A preliminary process estimates the musicalmeter and ﬂags pattern boundaries. This is followed by the actual\nsimilarity measurements. No a priori knowledge of the rhythmic\npattern classes is involved in the comparison. Thus the method isnot conﬁned to e.g. Western music.\nThe task described above can be decomposed into a number of\nsmaller requirements. First, two identical rhythmic patterns have tobe recognized as similar even when played with different sounds.This has to do with the acoustic features used to describe the sig-nal. Secondly, the patterns have to be aligned in time and tempodifferences have to be reconciled. The common approach usinghidden Markov models (HMMs) is not appropriate here, since onlyone instance of both rhythms is given, i.e., we are not aiming torecognize predetermined rhythm classes, but to compare two indi-vidual data sets. Also, the duration model of conventional HMMsis very loose, basically allowing only exponentially decaying dis-tributions. For these reasons, dynamic time warping (DTW) wasemployed. DTW allows a certain amount of ﬂexibility in timealignment, and it has been successfully used to handle a third sub-problem of rhythmic pattern matching: musical variations. Similar-ity measurements must be robust to inserting, deleting, andsubstituting reasonable amounts of atomic elements.\nDynamic time warping is a dynamic programming algorithm that\nis based on sequental decision process. It has been originally usedin template matching in speech and image pattern recognition since1960’s. Later on it has been replaced by HMMs in speech recogni-tion. Dynamic programming has been successfully used in match-ing melodic patterns. Dannenberg used it for real-timeaccompaniment of keyboard performances [2]. The approach wasfurther developed by Stammen and Pennycook [3]. More recentsystems have sought mechanisms for pattern induction fromrepeated exposure, followed by pattern matching [1].\nOverview of the system to be presented is shown in Figure 1. The\ndifferent modules, preprocessing, pattern segmenting, featureextraction, and the DTW are now separately discussed.\n2.  METHODS\n2.1 Pre-processing\nAn optional preprocessing step in the system is preprocessing with\na sinusoidal model [11]. When analyzing percussive rhythms inreal-world musical signals, it is advantageous to suppress the otherPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-vided that copies are not made or distributed for proﬁt or commer-cial advantage and that copies bear this notice and the full citationon the ﬁrst page.(c) 2002 IRCAM – Centre Pompidou Figure 1. Overview of the system.Sin+noise model\npreprocessingFind recurringpatterns\nFeature extraction\nDTWx1(k)\nx2(k)y1(k)\ny2(k)b1(p)\nb2(p)\nF2(i,n)F1(i,n) Similarity\nmeasureMeasuring the Similarity of Rhythmic Patterns\nJouni Paulus\nTampere University of Technology\nInstitute of Signal Processing\nP.O. Box 553, FIN-33101 Tampere, FInland\n+358 3 3115 4790\npaulus@cs.tut.ﬁAnssi Klapuri\nTampere University of Technology\nInstitute of Signal Processing\nP.O. Box 553, FIN-33101 Tampere, FInland\n+358 3 3115 2124\nklap@cs.tut.ﬁMeasuring the Similarity of Rhythmic Patterns\n(pitched) musical instruments prior to rhythm processing. Drum\nsounds in Western music typically have a clear stochastic noisecomponent [10]. In addition, some drums have strong harmonicvibration modes and they have to be tuned. In the case of tom toms,for example, approximately half of the spectral energy is harmonic.Nevertheless, these sounds are still recognizable based on the sto-chastic component only.\nA sinusoids plus noise spectrum model was used to extract the sto-\nchastic parts of acoustic musical signals. The model, described in[12], estimates the harmonic parts of the signal and subtracts themin time domain to obtain a noise residual. Even though some non-drum parts of signal end up to the noise residuals y\n1(k) and y2(k),\nthe level of drums in relation to other instruments is considerablyenhanced. The amount of non-drum sounds in the residual does notcomplicate the distance measuring too much since we are not inter-ested in individual events, but in the entire rhythmic sensation.\n2.2 Pattern Segmenting\nAn essential step before similarity measurements is to segment thecontinuous time domain signal into chunks that represent patterns.A brute force matching of all possible patterns of all lengths wouldbe computationally too demanding.\nPattern segmenting is a part of a rather complicated musical meter\nestimation process, which is more or less independent of the subse-quent similarity measurements. Earlier algorithms for automaticmeter extraction have been developed e.g. by Brown and Temper-ley [13,14]. The estimator proposed here has not been previouslypublished and is therefore now brieﬂy introduced. The moduletakes the acoustic musical signal without preprocessing as input,and outputs the lengths of the tactus (beat) and the musical mea-sure. The latter is interpreted as the rhythmic pattern length. Also,pattern phase is estimated, in order to be able to list a vector of pat-tern boundary candidates b\n1(p) and b2(p).\n2.2.1 Mid-level Representation\nA signal model is used which retains the metric percept of most\nmusical signals while signiﬁcantly reducing the amount of parame-ters needed to describe the signal. Only amplitude envelopes of thesignal at eight sub-bands are stored. The general idea that rhythmicpercept is preserved with this signal model has been earlier moti-vated by Scheirer in [15].\nFirst, a bank of sixth-order Butterworth ﬁlters is applied to divide\nthe input signal into eight non-overlapping bands. The lowest bandis obtained by lowpass ﬁltering at 100 Hz cutoff, and the sevenhigher bands are distributed uniformly on a logarithmic frequencyscale between 100 Hz and half the sampling rate. Magnituderesponses of the ﬁlters sum approximately to unity, and groupdelays of the ﬁlters are compensated for.\nAt each subband, the signal is half-wave rectiﬁed, squared, and\ndecimated by factor 45 to 980 Hz sampling rate. Then a fourth-order Butterworth lowpass ﬁlter with 20 Hz cutoff frequency isapplied to obtain the amplitude envelope of the signal at each fre-quency channel. Finally, dynamic compression is applied to obtaincompressed amplitude envelopes v\nc(k) at channels c at time k:\n, (1)\nwhere zc(k) is the signal before compression and J=1000 is a con-\nstant. The value of Jis not critical, but merely determines the\ndynamic range after compression and ensures that numerical prob-lems do not arise. The amplitude of the original wideband inputsignal x(k) is controlled by normalizing it to have zero mean and\nunity standard deviation before any of the described processingtakes place.2.2.2 Periodicity Detection\nEnvelope signals vc(k) at each frequency channel are subject to\nperiodicity analysis. For this purpose, we employ an algorithmwhich has been originally proposed by de Cheveigné and Kawa-hara for fundamental frequency estimation [16]. First, a differencefunction is formed:\n, (2)\nwhere K=4900 is the size of the time frame. In the decimated sam-\npling rate, this corresponds to ﬁve seconds. The function is thenmean-normalized to obtain :\nforτ=0 (3)\notherwise\nThis function is closely related to the inverse of the autocorrelation\nfunction, but was found to behave much more nicely due to thenormalization. Minima of  indicate periods.\nThe bandwise functions  are then summarized over channels\n(4)\nwhere A\ncis the inverse of the minimum value of over τat\nchannel c. The value Accorrelates strongly with the strength of the\nperiodicity at channel c, and brings an important performance\nimprovement by implementing an adaptive weighting of differentfrequency channels.\nThe function s(τ) serves as the source of information for musical\nmeter estimation. Figure 2 illustrates a typical instance of s(τ) for a\npiece from soft rock genre (BeeGees: Alone ). The actual beat and\npattern periods are indicated with vertical lines. Please note thatdips in s(τ) indicate periods.\n2.2.3 Selecting Tatum, Tactus, and Measure Lengths\nMusical meter is estimated at three levels: tatum, tactus (beat), and\nthe musical measure. The term tatum , or, time quantum, refers to\nthe shortest durational values in a musical composition that are stillmore than incidentally encountered. The other durational values(with few exceptions) are integer multiples of the tatum. Tactus is\nperceptually the most prominent metrical level, also known as beat,\nor the foot tapping rate. Musical measure is a still higher metricallevel, correlated with the harmonic change rate, and most impor-tantly, can be used to deﬁne the rhythmic pattern length. All thethree levels are required to ﬁnd the musical measure boundaries,i.e., the patterns.\nSelection of the tatum period is done in a straightforward manner.\nWe denote by S(f) the discrete Fourier transform of s(τ). Tatum is\ndetermined according to the maximum of the functionin the range between 1.7 Hz and 20 Hz. Tatum period is the inversev\nck() 1Jzck()+[]ln=dc'τ() vck()vckτ+()–[]2\nk1=K∑=\ndcτ()\ndcτ()1=\ndcτ()dc'τ()\n1\nτ---dc'i()i1=τ∑---------------------------------=\ndcτ()\ndcτ()\nsτ() Acdcτ()c1=8∑=\ndcτ()00.5 11.5 22.5 33.5 44.5 50123\n Figure 2. A typical instance of the function s( τ). The actual\ntactus and measure periods are indicated with vertical lines.seconds\nfS f()×Measuring the Similarity of Rhythmic Patterns\nof the frequency corresponding to the maximum value. The ratio-\nnale behind weighting with is to implement a proper prefer-ence towards higher frequencies. Otherwise e.g. the tactus ormeasure period may be detected.\nTactus period is calculated in a probabilistic manner using three\nprobability distributions. The main likelihood function is obtainedfrom s(τ), it is, from the observation. The probability of a tactus\nperiodτ given s(τ) is deﬁned to be proportional to:\n. (5)\nThe concept of proportionality has to be used, since the integral\nover is not guaranteed to sum to unity. The above likeli-hood is then multiplied by a priori probabilities measured from\nactual data by several authors. As suggested by Parncutt [17], weapply log-normal distribution for tactus periods, written\n(6)\nwhere the average tactus period µwas set to 600 ms and σ=0.25.\nThe third probability distribution governing beat period probabili-ties comes from the tatum information. The conditional probabilitydistribution for tactus periods τgiven tatum period τ\n0is deﬁned as\na mixture of Gaussian distributions, written\n(7)\nwhereσ0=0.3 and the weights for different multiples mof tatum,\n. (8)\nThe exact weight values are not critical, but simply realize a ten-\ndency towards binary or ternary subdivisions of the tactus. Forexample, the tatum multiples m={1,2,4} have a weight 4/25, but\nm=5 is assigned a weight 1/25 only. The overall likelihood of dif-\nferent tactus periods τis then\n, (9)\nthe maximum of which indicates the most likely tactus period.\nFinally, the musical measure length which indicates the patterns, is\ncalculated in a manner analogous to tactus estimation. Likelihoodfor different measure lengths τgiven s(τ) is obtained directly from\nEq. (5). A priori probability distribution for measure lengths is cal-\nculated from Eq. (6) by substituting τ=2.2 and σ=0.4. Finally, the\nconditional probability of different measure lengths τgiven the\nestimated tactus period is calculated using Eq. (7), where tactusperiod is substituted for τ\n0, and P(τ| τ0) gives the conditional prob-\nability. The three probabilities are combined according to Eq. (9).\nFor pattern extraction, we need, not only the pattern length, but\nalso its phase. This turned out to be even more difﬁcult than ﬁndingthe pattern period. A simple yet satisfactory solution was to con-struct a signal, where impulses are placed at pattern length distanceapart. This signal is then correlated with the compressed amplitudeenvelope v\nc(k) at the lowest frequency channel, c=1. The highest\nmaximum in the resulting correlation function was used as a tem-poral anchor for pattern beginning points.\n2.3 Acoustic Features for Similarity Judgments\nAs shown in Fig. 1, the actual similarity measurement module getsas input the two noise residual signals y\n1(k) and y2(k), and a list of\npattern boundaries. Because the pattern segmenting stage is notguaranteed to be 100 % reliable, a couple of most probable patternlengths and phases are considered, one at a time, and two most\nsimilar patterns are used to determine the similarity measure. How-ever, in Simulations section the pattern segmentation and similaritymeasurement stages are separately evaluated.\nAfter we have isolated one pattern from both signals, acoustic fea-\nture extraction takes place in a series of consecutive 23 ms timeframes. There was no signiﬁcant performance difference between23 ms and 46 ms frame lengths, though. The frames are Hanning-windowed and adjacent frames overlap 50 %.\n2.3.1 Calculation of Features\nThe two most fundamental perceptual features of a individualrhythmic events (in addition to their timing) are their perceivedloudness and brightness. Most musical rhythms, if not all, can beidentiﬁably played using only these two dimensions. In addition tothese two, an attempt was made to utilize the timbre information.\nLoudness was modeled by calculating the mean square energy of\nthe signal in one frame, and then by taking a natural logarithm tobetter correspond to the perceived loudness. More exactly,\n, (10)\nwhere Kis the frame size and the value J=1000 is used for the\nsame practical purpose as in Eq. (1).\nSpectral centroid has been found to corresponds to perceived\nbrightness of sounds. It is deﬁned as the balancing point of thespectral power distribution, and is typically calculated as the ﬁrstmoment of the magnitude spectrum. As suggested by Eronen in[18], a more robust feature is obtained by using a logarithmic fre-quency scale. Spectral centroid is here calculated as follows. First,the short-time power spectrum of the signal is calculated. Then avector U(b) is formed which contains the energies at sixth-octave\nfrequency bands b, however, limiting the minimum bandwidth to\none spectral line at the low frequencies. Spectral centroid is then\n, (11)\nwhere f\nc(b) is the center frequency of band bin Hertz, and Bis the\nnumber of bands in U(b). Finally, centroid values at the linear fre-\nquency scale (Hz) are warped to a logarithmic scale simply byusing  as a feature.\nMel-frequency cepstral coefﬁcients (MFCC) were extracted by ap-\nplying the discrete cosine transform to the log-energy outputs of amel-scaling ﬁlterbank [20]. The ﬁlterbank was implemented bycalculating a discrete Fourier transform for the windowed wave-form, and then simulating 40 triangular bandpass ﬁlters havingequal bandwidth on the mel-frequency scale [19]. The zeroth ceps-tral coefﬁcient is discarded, and a the next 15 coefﬁcients are cate-nated to the feature vectors.\n2.3.2 Normalization of Feature Vectors\nThe above calculated features reﬂect the absolute “tone color” andloudness in each individual time frame. As such, the features arenot appropriate for rhythmic similarity measurement without nor-malization. Rhythmic events are perceived in their context and inrelation to each other. A sound may take the role of a “bass drum”just because it is lower than the neighbouring events, not becauseof its absolute timbre. This allows musicians to reproduce rhythmswith highly varying means, e.g. by tapping, scat-singing, or byplaying with drums. In following, we propose normalizationsf\nPτsτ()[]1\nsτ()----------∝\n1sτ()⁄\nP0τ()1\n2πσ--------------1\n2σ2---------log10τ\nµ---2\n–\nexp=\nPττ0()am\n2πσ0---------------- -1\n2---τmτ0–\nσ0------------------2\n–\nexpm1=9∑=\nam1\n25------443413132,,,,,,,,[]=\nPτ()Pτsτ()[]P0τ()Pττ0() ∝L1J\nK----yk()[]2\nk1=K∑+\nln=\nSCfcb()Ub()[]b1=B∑\nUb()b1=B∑-------------------------------------------------=\nSC()lnMeasuring the Similarity of Rhythmic Patterns\nwhich transform the absolute feature values to a relative represen-\ntation.\nThe energy feature L(n) over frames n=1,..., Nis normalized by\nsubtracting the minimum value of L(n) over time, and scaling the\nresulting curve to have a unity variance. This can be written\n(12)\nwhere L0is the minimum value of L(n)o v e r nandσLis the stan-\ndard deviation of L(n).\nThe normalized loudness feature L’(n) is used to weight the other\nfeatures, in order that the tone color of soft or quiet segmentswould not inﬂuence on the similarity judgments too much. Theweight vector λ(n) is equal to L’(n) divided by the sum over L’(n).\nThe normalization of the other features, spectral centroid and mel-\ncepstrum coefﬁcients, is performed as\n, (13)\nwhere is the standard deviation of λ(n)SC(n). The resulting\nnormalized features have zero mean and unity variance over time,and have been weighted with λ(n).\nEach individual feature over time has now been normalized to sim-\nilar mean and range, and can be later weighted in a controlled man-ner in relation to other features. Also, the absolute tone color isdiscarded, modeling only deviations up/down from the averagevalue. In this way, a tapped rhythm produces a feature vector whichis similar to that produced by playing drums.\nThe normalized feature vectors are collected to a matrix F(i,n)\nwhich contains feature vectors over the time range of the pattern,\n, (14)\nwhere i is the index of each feature, and n is the frame index.\n2.4 Dynamic Time Warping\nTwo feature vector sets F1(i,n) and F2(i,n) are matched using\ndynamic time warping (DTW). The DTW matches the two datasets by trying to ﬁnd an optimal path through a matrix of pointsrepresenting all the possible time alignments between the featurevector sets. The template feature vectors represent the row coordi-\nnates and the unknown feature vectors represent the column coordi-\nnates in the matrix. An example of a such matrix and the optimaltime alignment path is illustrated in Figure 3.\n2.4.1 Local Path Constraints\nThe main idea of DTW is that it allows some amount of adaptabil-ity when matching discrete data points of the two patterns. Theadaptability is achieved by allowing the path to vary the rate atwhich it goes through the two patterns. The non-warping methodwould always match feature vectors at the same indices from bothpatterns. DTW for its behalf, allows the patterns to differ in lengthand still it ﬁnds the best ﬁt for them. Some kind of path constraintis necessary, because it is not a good idea to allow the path to pro-ceed randomly to a next point. Usually the set of possible nextpoints is limited by local path constraints so that the local timewarps become smaller.\nThe three different local path constraint types that were tried in this\nimplementation are shown in Figure 4. They were chosen fromamong the eight different types presented in [19]. Type 1 local pathconstraint allows such transitions that path proceeds precisely one\ntime frame only in template features or only one time frame inunknown features or one in both at the same time. This constraintis the most loose of all, because it allows the path even e.g. to pro-ceed ﬁrst horizontally through the matrix and then continue to theend point with only vertical steps. Considering what this means interms of time warping, the path is quite inappropriate for templatematching, because it is very improbable situation that the ﬁrst fea-ture vector in template matches almost all of the unknown patternand all the rest of the template feature vectors are matched with justthe last one of the unknown pattern.\nType 3 allows only transitions which proceed in time frames both\nin template feature vectors and in unknown feature vectors. Thistype allows the matching to skip one feature vector either in tem-plate or in unknown, but not in both at the same time.\nThe third local path constraint type used here is called Itakura, ﬁrst\npresented in [21]. Every transition proceeds in time frames inunknown feature vectors. Transition does not have to proceed intime frames in template feature vectors, but two consecutive hori-zontal steps are forbidden. This way it is possible to avoid situationwhere path gets stuck on a horizontal direction.\nWith type 3 and Itakura local path constraints it is implied to use\nalso global constraints to limit the area which we must go throughwhile searching for the optimal path. This is because the minimumpath slope is 0.5 and the maximum slope is 2. Taking this fact andﬁxed start and end point of path in to consideration, the allowableregion on which the path can reside is a parallelogram. The aim ofthe area reduction is simply to reduce the amount of possible pathsand therefore the amount of needed calculations. It is possible touse similar global limitation with type 1 local path constraint, but itis not directly implied by the local path constraint.\n2.4.2 Path Length\nThe total length of the optimal time-warped path to a certain pointin the matrix is deﬁned recursively by Eq. (15) below. The localL'n()Ln()L\n0–[] σL⁄ =\nSC'n()λn()\nσSC'-----------SC n() λ m()SC m() []m1=N∑–\n=\nσSC'\nFin,()L'1SC'1MFCC '1\n…… …\nL'nSC'nMFCC 'n=010203040506070010203040506070Template feature vectors\nTest feature vectors\n Figure 3. An example of DTW matrix and an optimal path.\nType III Type I Itakura\n Figure 4. Local path constraints.Measuring the Similarity of Rhythmic Patterns\npath constraint used in this equation is type 3. The formulas for the\nother paths are analogous and can be found in [19] .The length of\nthe optimal path to point C(n,m) is deﬁned so that it has the small-\nest cumulative sum consisting of the feature vector difference costD(n,m) and the minimum of sum consisting of the path length to\nthe previous point and transition cost T from that point:\n(15)\n(16)\n(17)\nIn Eq. (16), vector Wdenotes the feature weight vector which con-\ntrols how much a certain feature weighs in determing the similarityof two feature vectors and Iis the number of different features. The\ntransition cost Tis here deﬁned to be the Euclidean distance\nbetween the start and end point of the transition. The absolute mag-nitudes of the values in Wdetermine the balance between the path\ncostT and feature vector difference cost D(n,m).\nThe ﬁnal similarity measure between two rhythmic patterns is\ngiven by\n. (18)\nIt is the theoretically shortest possible length divided by the cost of\nthe optimal path. This makes it possible to compare the similaritymeasures of patterns of different lengths. With two identical pat-terns the similarity measure is 1, and the more the patterns differthe smaller the measure gets, gradually approaching to zero, butnever actually reaching it.\n2.4.3 The Core DTW Algorithm\nThe algorithm sequentially goes through the whole globallyallowed area of the DTW matrix. For each point the optimal pathlength reaching it is stored as well as the link to the optimal previ-ous point. This leads to the situation that every time the algorithmcalculates the optimal path to a certain point, it already knows thepath length to all possible preceding points. From these it thenchooses the optimal one according to Eq. (15) and stores the result-ing path length and preceding point information. This kind of stepsare continued until the ﬁnal point (N,M) is reached and the total\npath length is known. If it is not enough to know the length of thepath, but also the actual path, it can be backtracked using the prede-cessor information stored in every point.\n3.  SIMULATIONS\n3.1 Meter Estimation and Pattern Segmenting\nTable 1 shows the statistics of the database used to evaluate the\naccuracy of the musical meter estimation and pattern segmentingalgorithm presented in Sec. 2.2. Acoustic music signals werestored as single-channel, 44.1 kHz, 16-bit, pulse code modulateddata. Tactus (beat) and musical measure (i.e. pattern) positionswere manually annotated for one-minute long representativeexcerpts selected from each piece. The annotations were made bytapping along with the musical pieces, recording the tapping sig-nal, and semiautomatically detecting the tapped time instants. Tac-tus and measures were separately annotated in different runs.Tactus could be more or less unambiguously judged for all thepieces. However, measure boundaries could be reliably marked by\nlistening for a subset of the pieces only. Tatums were not annotatedat all.\nIn the simulations, the algorithm was given one 10-second excerpt\nfrom the beginning of each annotated one-minute period. The esti-mated tactus and measure periods were then compared to the man-ually annotated value. The estimated tactus and measure periodswere deﬁned to be correct, if the values deviated less than 10 %from the correct one.\nThe tactus periods given by the proposed algorithm were correct\nfor 67 % of the 365 pieces. Most typical error was tactus perioddoubling. Estimated musical measure lengths (i.e., the patternlengths) were correct for 77 % of the 141 pieces for which the mea-sures was annotated, 17 % of the values were either half or doublethe pattern lengths, and 6 % were unclassiﬁed errors. However, itshould be noted that the pieces for which the measure informationcould be annotated represent metrically more clear cases. This atleast partly explains the performance difference between tactus andmeasure length estimation.\nThe estimated pattern phase was correct only in approximately half\nof the cases, suggesting a point of improvement in the system. Inpractice, this has more to do with computational efﬁciency, sincereliable pattern comparison can be achieved by taking a couple ofmost prominent pattern length and phase candidates, performingthe DTW for each candidate, and selecting the highest similarityvalue to the output.\n3.2 Similarity Measurements\n3.2.1 Similarity of Drum Patterns\nA database of rhythmic patterns was used to validate the describedsimilarity measurement approach. The database consisted of ninestandard rhythm patterns with a couple of variations, totalling to 14different patterns. The rhythms and the number of variations fromeach were: stomp, eight-note beat (x3), sixteenth-note beat (x2),triplet (x2), shufﬂe (x2), swing, waltz, samba, and songo. Varia-tions were in the bass drum pattern for the 8\nth-beat, triplet, and\nshufﬂe rhythms, and in the hi-hat pattern for the 16th-beat rhythm.\nEach of the 14 patterns was performed using three different sound\nsets, shown in Table 2. Swing rhythm makes an exception: it didnot make sense musically to perform it with Set 3. The soundswere selected according to the principle that they would be as dif-ferent as possible, yet exchangeable in different rhythmic roles.The differences between the bass drum sounds were small. How-ever, the snare drum sounds are quite different, depending onwhether played with stick or with brush, or at the rim of the drum.In the same manner, closed hi-hat, ride cymbal, and the shaker pro-Cnm,() Dnm,()minCn1–m1–,() T11,()+\nCn1–m2–,() T12,()+\nCn2–m1–,() T21,()+ \n+ =\nDnm,() Wi()F\n1in,()F2im,()– ()2\ni1=I\n∑=\nTpq,() p2q2+=\nSF1F2,()N2M2+\nCNM,()-------------------------=Table 1. Database for evaluating the meter estimation model.\nGenreTactus annotated\n(# of songs)Patterns annotated\n(# of songs)\nClassical 85 –\nElectronic/Dance 27 18\nHip Hop/Rap 12 8\nJazz/Blues 62 19\nRock/Pop 111 61Soul/RnB/Funk 44 27\nWorld/Folk 24 8\nTotal 365 141Measuring the Similarity of Rhythmic Patterns\nduce rather different sounds, but are typically used in same rhyth-\nmic roles. An amateur musician performed the rhythms usingRoland SPD-6 percussion pad together with two foot pedals.\nThe task of the system was to recognize the same rhythmic patterns\nas similar although performed using different sounds. Figure 5shows the estimated similarities for each pair of the 41 performedpatterns (3 ∗13+2∗swing). Each three consecutive samples repre-\nsent identical rhythms, played with the three sets (except only twofor swing). The whiter the area at the intersection of each two sam-ples, the higher their estimated similarity. The white areas with avalue missing are those for which the lengths of the two patternsdiffer by a factor greater than 2, in which case the local path con-straint does not allow the comparison.\nThe illustrated similarity matrix was calculated using only the nor-\nmalized spectral centroid as feature, and local path constraint 3.Preprocessing was not applied. In this experiment, similarity mea-surement was separately evaluated, taking the pattern boundariesfrom manually annotated time values.\nThe proposed system is successful in assigning a high similarity to\nsame rhythms, despite of being performed with different sounds.Bass drum variations have the effect that the patterns practicallyappear as different rhythms. On the other hand, hi-hat variation in16\nth-beat rhythm does not make a noticeable difference (a couple\nof hi-hat hits are omitted in the variation). This does not mean thatthe system would be purely bass-drum based, since among the 14different patterns, six patterns have identical “bass drum/snaredrum” patterns.\n3.2.2 Performance of Different Features\nSimulations were run to determine how well different features cor-relate with the rhythmic experience of a listener. In practice, theoptimal weights W(i) for different features in Eq. (16) were sought\nfor. The weight values were determined by trying out differentweight combinations and inspecting the resulting similarity matrixfor the described rhythm database, and for complex musical sig-\nnals, described in more detail in Sec. 3.2.3.\nNormalized spectral centroid turned out to be clearly the best per-\nforming feature. After all, the most consistent similarity measureswere produced by using this feature alone. However, it should benoted, that the normalized spectral centroid is actually an element-by-element product of the spectral centroid and loudness, as shownin Eq. (13). Loudness alone was somewhat successful, but using ittogether with the centroid only deteriorated the results.\nDifferent numbers of MFCC coefﬁcients were also evaluated as\nfeatures. However, even after the normalization, MFCCs assignedhigh similarities to the patterns performed with identical soundsets, not to the patterns that were rhythmically similar.\nAs another observation, the path cost in Eq. (15) had to be\nstrongly weighted in relation to the acoustic features in order todiscriminate between e.g. triplet and 16\nth-beat rhythms. With a\nhigh path cost weight, DTW allows the two patterns be of differentlengths, and compensated for slight deviations in pattern beginningtimes, but punishes paths that are not straight lines through thematrix. In other words, steady time is constrained. Veriﬁcation testswere performed which conﬁrmed that the absolute lengths of thetwo patterns do not have a noticeable effect on the similarity judg-ment as long as the ratio of the lengths is between 0.5 and 2,required by the local path constraints. The best performing localpath constraint in this experiment was the type 3.\n3.2.3 Experiments with Complex Music Signals\nIn the last experiment, patterns taken from real-world musical sig-nals were compared, using the database introduced in Sec. 3.1.Two patterns were taken from each of the annotated 141 songsusing the manually annotated pattern boundaries. Then in-song andinter-song similarity measures were calculated, producing a matrixof 141x141 values, where the in-song measures are at the diagonal.The underlying assumption was that two patterns taken from asame song should be more similar than patterns from differentsongs, despite musical variations and the interference of otherinstruments.\nThe problem with this kind of evaluation is that it is difﬁcult to\nknow if the similarity is due to rhythmic characteristics. For exam-ple, MFCCs without normalization would model the absolute tonalcolor of the piece, bringing high in-song similarities but not neces-sarily because of the rhythm. For this reason, only the normalizedspectral centroid was used as a feature, since we know that it doesnot retain any absolute features about the tonal color of a piece.Preprocessing with a sinusoidal model was applied.\nFigure 6 shows the in-song distance for each of the 141 pieces,\nalong with the average of inter-song distances calculated separatelyfor each piece. The in-song similarity is consistently higher, but thedifference is not large, most likely due to the other instruments andrhythmic variation.\n4.  SUMMARY AND CONCLUSIONS\nThe presented system was successful in extracting patterns fromactual musical signals, and in assigning consistent similarity mea-sures for drum patterns performed with different sound sets. Themost successful acoustic feature for describing rhythmic patternsturned out to be the spectral centroid weighted with the log-energyof the signal. This vector was further normalized to have zero meanand unity variance over time. Dynamic time warping reconciled fortempo differences and slight beginning point deviations. However,a relatively high cost had to be assigned to the length of the timeTable 2. Drum sets used in performing the rhythm patterns.\nDrum set Sounds involved\n1 bass drum snare hi-hat\n2 bass drum brush slap snare ride cymbal\n3 bass drum cross stick shaker\nst.8beat 16beat  tripletshuffle sw.wa.sa.so. stomp  8beat 16beat tripletshuffle swing  waltz  samba  songo \nPattern 2Pattern 1\nFigure 5. Calculated similarity measures for drum patterns.Tpq,()Measuring the Similarity of Rhythmic Patterns\nalignment path in order to constraint musical rhythms to steady\ntime and to discriminate between binary and ternary rhythms.\n5.  ACKNOWLEDGMENTS\nMatti Ryynänen performed the rhythmic patterns used inSec. 3.2.1. The acoustic feature extractors were provided by AnttiEronen, Tampere University of Technology. The MFCC analysiswas based on Malcolm Slaney’s original implementation (http://rvl4.ecn.purdue.edu/~malcolm/interval/1998-010/). Sinusoidalmodeling tools were provided by Tuomas Virtanen from TampereUniversity of Technology.\n6.  REFERENCES\n[1] Rowe, R, “Machine Musicianship,” MIT Press, Cambridge,Massachusetts, 2001.[2] Dannenberg, R. B., “An On-Line Algorithm for Real-TimeAccompanimen,” In Proc. of the 1984 International ComputerMusic Conference, 193-198.[3] Stammen, D.R., Pennycook, B. Real-time recognition ofmelodic fragments using the dynamic timewarp algorithm. ICMCProceedings 1993, 232-235.[4] Buteau, C. and Mazzola, G., “From contour similarity tomotivic topilogies,” Musicae Scientiae, V ol. 42, 2000.[5] Dixon, S. E., “Automatic Extraction of Tempo and Beat from\nExpressive Performances,” J. New Music Research, 30, 1, 2001,39-58.[6] Cemgil, A. T., Desain, P., Kappen, B. “Rhythm Quantizationfor Transcription,” Computer Music Journal, Summer 2000, V ol24:2.[7] Chen, A. L. P, Chen, J. C. C., “Query by Rhythm, An Approachfor Song Retrieval in Music Databases”. In Proc IEEE Workshopon Continuous-Media Databases and Applications, 1998.[8] Gabrielsson, A. Similarity ratings and dimension analyses ofauditory rhythm patterns I. Scand. J. Psychol 14, 1973, 138-160.[9] Powel, D.–J. and Essens, P., “Perception of Temporal Patterns,”Music Perception, Summer 1985, V ol. 2, No. 4, 411-440.[10] Fletcher, N. H. and Rossing, T. D., “The Physics of MusicalInstruments,” Springer–Verlag, New York, 1991.[11] Serra, X., “Musical Sound Modeling with Sinusoids plusNoise,” Roads, C. et al. (eds.) Musical Signal Processing, Swets &Zeitlinger Publishers.[12] Virtanen, T., “Audio signal modeling with sinusoids plusnoise,” MSc thesis, Tampere University of Technology, 2000.[13] Brown, J. C., “Determination of the meter of musical scoresby autocorrelation”. J. Acoust. Soc. Am. 94 (4), Oct. 1993.[14] Temperley, D., “The Cognition of Basic Musical Structures”.MIT Press, Cambridge, Massachusetts, 2001.[15] Scheirer, E. D. “Tempo and beat analysis of acoustic musicalsignals,” J. Acoust. Soc. Am. 103 (1), Jan. 1998, 588-601.[16] de Cheveigne, A. and Kawahara, H. “YIN, a fundamental fre-quency estimator for speech and music,” J. Acoust. Soc. Am.111 (4), April 2002.[17] Parncutt, R., “A Perceptual Model of Pulse Salience and Met-rical Accent in Musical Rhythms,” Music Perception, Summer1994, V ol. 11, No. 4, 409-464.[18] Eronen, A. “Comparison of features for musical instrumentrecognition”. In Proc. IEEE Workshop on Applications of SignalProcessing to Audio and Acoustics, 2001.[19] Rabiner, L., Juang B.H. Fundamentals of Speech Recognition.Prentice-Hall, New Jersey, 1993. 200-238.[20] Davis, S.B., Mermelstein, P. “Comparison of Parametric Rep-resentations for Monosyllabic Word Recognition in ContinuouslySpoken Sentences,” IEEE Transactions on Acoustics, Speech andSignal Processing, vol ASSP-28, No.4, 1980, 357-366.[21] Itakura, F. Minimum Prediction Residual Principle Applied toSpeech Recognition. IEEE Transactions on Acoustics, Speech andSignal Processing, vol ASSP-23, No.1, 1975, 67-72.\n0 50 100 1500.60.70.80.91Similarity value\nSong indexIn−song\nInter−song\nFigure 6. In-song and inter-song similarity measures for the\ndatabase of 141 real-world musical pieces."
    },
    {
        "title": "CubyHum: a fully operational &quot;query by humming&quot; system.",
        "author": [
            "Steffen Pauws"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415614",
        "url": "https://doi.org/10.5281/zenodo.1415614",
        "ee": "https://zenodo.org/records/1415614/files/Pauws02.pdf",
        "abstract": "'Query by humming' is an interaction concept in which the identity of a song has to be revealed fast and orderly from a given sung input using a large database of known melodies. In short, it tries to detect the pitches in a sung melody and compares these pitches with symbolic representations of the known melodies. Melodies that are similar to the sung pitches are retrieved. Approximate pattern matching in the melody comparison process compensates for the errors in the sung melody by using classical dynamic programming. A filtering method is used to save computation in the dynamic programming framework. This paper presents the algorithms for pitch detection, note onset detection, quantization, melody encoding and approximate pattern matching as they have been implemented in the CubyHum software system.   Since human reproduction of melodies is imperfect, findings from an experimental singing study were a crucial input to the development of the algorithms. Future research should pay special attention to the reliable detection of note onsets in any preferred singing style. In addition, research on index methods and fast bit- parallelism algorithms for approximate pattern matching need to be further pursued to decrease computational requirements when dealing with large melody databases.",
        "zenodo_id": 1415614,
        "dblp_key": "conf/ismir/Pauws02",
        "keywords": [
            "query by humming",
            "revealing song identity",
            "large database of known melodies",
            "detecting pitches",
            "comparing symbolic representations",
            "melodies similar to sung pitches",
            "approximate pattern matching",
            "dynamic programming",
            "filtering method",
            "CubyHum software system"
        ],
        "content": "CubyHum: A Fully Operational Query by Humming System \nCubyHum: A Fully Operational Query by Humming System \n Steffen Pauws \nPhilips Research Eindhoven \nProf. Holstlaan 4 (WY21) \nEindhoven, the Netherlands \n+31 40 27 45415 \nsteffen.pauws@philips.com  \nABSTRACT  \n'Query by humming' is an interaction concept in which the identity \nof a song has to be revealed fast and orderly from a given sung input using a large database of known melodies. In short, it tries to detect the pitches in a sung melody and compares these pitches with symbolic representations of the known melodies. Melodies that are similar to the sung pitches are retrieved. Approximate pattern matching in the melody comparison process compensates for the errors in the sung melody by using classical dynamic programming. A filtering method is used to save computation in the dynamic programming framework. This paper presents the algorithms for pitch detection, note onset detection, quantization, melody encoding and approximate pattern matching as they have been implemented in the CubyHum software system.   Since human reproduction of melodies is imperfect, findings from an experimental singing study were a crucial input to the development of the algorithms. Future research should pay special attention to the reliable detection of note onsets in any preferred singing style. In addition, research on index methods and fast bit-parallelism algorithms for approximate pattern matching need to be further pursued to decrease computational requirements when dealing with large melody databases. \n \n1. INTRODUCTION \nTypically, people listen to music separately from gaining \nknowledge about the name of the performer, the composer or the title of the song. Because song titles and melodies are not learnt associatively, recalling a song title from a given melody or vice versa is notoriously difficult [14]. Obviously, it is hard to find music without knowing it by heart. The interaction concept of query by humming makes it possible to retrieve a song when the user ponders a catchy tune without being able to name the song. It allows the user to sing any melodic passage of a song, while the system seeks the song containing that melody fast and orderly [6][11]. \nThe current implementation of query by humming in the \nCubyHum software system is a linked combination of speech signal processing, music processing and approximate pattern matching guided by empirical findings from singing experiments. \nIts algorithmic organization is illustrated in Figure 1 and forms the \nguide to this paper. Query by humming requires symbolic representations of the song melodies consisting of a sequence of musical notes (e.g., their pitch names) and the time onset and offset of each note. \nFirst, the pitch is estimated from the singing by a technique called \nsub-harmonic summation (SHS). In short time frames, SHS computes the sum of harmonically compressed spectra. In principle, the maximum sum result is chosen as the pitch estimate \nin that time frame. \nSecond, musical events and timing information are detected in the \nsinging such as note onsets, gliding tones and inter-onset-intervals. Standard signal processing techniques using short-time energy, pitch level shifts and amplitude envelopes are used for finding note onsets. \nFigure 1. The algorithmic framework of query by humming \nas implemented in CubyHum. \n \nBoth the pitch and timing information are combined and quantized \ninto musical notes and durations. Note quantization is based on the tuning and scale standards in Western music. This quantization transforms the singing input into its formal musical notation. From this musical score, a melody can be synthesized for auditory feedback by using standard MIDI\n1technologies. \nSince people are inaccurate in remembering and reproducing a \nmelody, a melody representation and comparison process are devised that are largely invariant to key, tempo and ornamentation (i.e., adding and leaving out notes). First, not the absolute musical pitches, but the intervals between notes are used, as they are insensitive to key. In particular, nine interval classes are defined that represent different interval sizes. Large intervals (i.e., larger that 6 semitones) are not further distinguished since they are hard to sing and occur rarely in musical melodies from all over the \n1MIDI stands for Musical Instrument Digital Interface, which is a \nstandard format to exchange music performance data between music synthesizers and sequencers. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM  Centre Pompidou CubyHum: A Fully Operational Query by Humming System \nworld. Likewise, duration ratios are used between successive \nnotes, as they are insensitive to tempo. \nApproximate pattern matching techniques allow the detection of \npatterns in tunes that are transformed and distorted in various ways. For that, we have defined a distance between any two variable-length melodies that depends on interval sizes and duration ratios.  In general, it comes down to the search of an optimal alignment between melodies by minimizing the number of changes that are needed to transform one melody into another. This problem can be easily solved in a dynamic programming framework. However, classical dynamic programming is impractical in terms of running time performance for large melody databases. Therefore, a filter mechanism has been implemented that quickly discards passages in the melody database that cannot contain an approximate match. \n2. THE ART OF SINGING \nIt is obvious that people have imperfect memories for melodies or \nmay lack any formal singing practice. Unfortunately, the literature does not provide clear-cut insights into the singing performance of the general public, the long-term memory and recollection of melodies and how these issues relate to experienced singers and real-world song material. This knowledge is indispensable for the development of query by humming algorithms. Hereunder, we summarize some of our findings of an experiment that examined the effects of singing experience, song familiarity and recent song exposure on singing popular rock song melodies\n2from memory. \nThese findings were a crucial input to the development of the algorithms for query by humming. \nPeople sing any part of the melody. A repetitive melodic passage \nin a song may represent the 'hook-line' of a song that 'gets stuck in people's head'. \nPeople sing at the wrong key. In our study, people chose a \nrandom pitch to start their singing. Only for their most favorite songs, people are thought to have a latent ability of absolute pitch [8]. \nPeople sing at a reasonably correct global tempo. In our study, \npeople knew or had a feeling, by previous hearings, what the correct tempo would be and were able to approach this tempo reasonably accurately. \nPeople sing too many or too few notes. Human memory is \nimperfect to recall all pitches in the right order. In our study, people sang just the line they remembered. They also added all kinds of ornaments (e.g., grace notes, filler notes, or thinner notes) to beautify their singing or to ease the muscular motor processes involved in singing. \nPeople sing the wrong intervals or confuse some with others.\nFrom our study, people sang about 59% of the intervals correctly, though there were differences due to singing experience, song familiarity and recent song exposure. Interval confusion seems to be symmetric; interchanging an interval with another was found to be equally likely as the other way around. A large interval (thirds and larger) tends to be more easily interchanged for another. \nPeople sing the contour reasonably accurately. In our study, \npeople largely knew when to go up and when to go down in pitch when singing; they did that correctly in 80% of the times. \nPeople with singing experience sing better on some aspects \nthan people without singing experience do. In our study, the non-experienced and experienced singers did not differ in singing the contour of a melody accurately. However, experienced singers \n2Twelve songs of the Beatles were used as experimental data. reproduced proportionally more correct intervals and sang at a \nbetter timing. \nPeople sing familiar melodies better than less familiar ones .I n  \nour study, less familiar melodies were reproduced with fewer notes and had proportionally fewer correct intervals than familiar melodies. Also, both experienced and non-experienced singers improved their singing of intervals when they had heard the melody very recently. \n3. PITCH DETECTION \nPitch is a percept that is defined as the characteristic of a sound \nthat gives the sensation of being high or being low. For a complex tone (as the human voice), the pitch corresponds mainly with the fundamental frequency of the signal. However, the correlation between sound frequency and pitch is not perfect, since pitch perception is influenced by the intensity, the duration, the surrounding sounds and the harmonics of the sound. Accurate pitch detection algorithms for normal speech have been developed, under the assumption that the voice has been well recorded under controlled conditions. \nTypical pitch in normal conversational speech is in the range of \n110 Hz in the male, 220 Hz in the female and 300 Hz in the child and may vary within one octave. When considering all voices and \nregisters, singing extends from 80 Hz (the \n2 /g40of a bass singer) to \n1400 Hz (the 6 /g38of a soprano singer) and may vary over two-and-\na-half octaves. We expect the pitch range for humming to lie in \nthe first one-and-a-half octave of the normal singing range. On the other hand, whistling extends from 700 Hz to 2800 Hz; the lowest whistling tone of person comes near to the persons highest reachable sung note. \nWe use the sub-harmonic summation  (SHS) method [7] to \nestimate the pitch in the singing.  This method stems from the theory that each spectral component (i.e., spectral peak) contributes to the perception of a pitch that corresponds to the frequency of the component. Also, elements that have a lower harmonic relationship with this component (i.e., that have an integral factor in frequency) contribute to the perception of a pitch, taking into account that higher components contribute less than lower components do. All these contributions add up in a sub-harmonic summation. The maximum of this sum result is the \nestimate of the pitch. Even when a fundamental frequency (i.e., the first harmonic) is missing in the signal, while other harmonics are present, this mechanism creates a (virtual) pitch of about that typically produced by that fundamental. Virtual pitch is common in human perception. The algorithm has been implemented as outlined in the original paper [7]. \nThe algorithm can be explained by a single expression,  \n/g12 /g79/g82/g74 /g11 /g12 /g79/g82/g74 /g11 /g12 /g11/g21 /g21\n/g20/g20/g81 /g86 /g54 /g81 /g86 /g58 /g75 /g86 /g43/g49\n/g81/g81+ + =∑\n=−(1) \nwhere /g73 /g86 /g79/g82/g74/g21= denotes the logarithmic frequency, /g12 /g11 /g86 /g43\nrepresents the sub-harmonic sum spectrum, N(e.g., 15) denotes \nthe number of harmonics, /g81is the compression rank, h(e.g., \n0.84) denotes the decreasing factor, /g12 /g11 /g86 /g58 is an arc-tangent \nfunction representing the transfer function of the auditory \nsensitivity filter, and /g12 /g79/g82/g74 /g11/g21 /g81 /g86 /g54+ denotes the compressed \namplitude spectrum representation. When a spectrum is \ncompressed, its frequency axis is squeezed by an integral factor, the compression rank n, and consequently its peaks come closer to \neach other. A pitch estimate is then the value of /g86/g73 /g21= for which \n/g12 /g11 /g86 /g43 is the maximum. CubyHum: A Fully Operational Query by Humming System \nRobust pitch estimation is difficult when relying on a single \nframe; octave errors or other erratic pitch estimates for creaky and hoarse voices are hard to circumvent without taking the necessary precautions. Since the sub-harmonic sum spectrum provides an array of pitch estimate candidates, a post-processing procedure is used to smooth the pitch contour. For that, we use a dynamic programming framework in which sudden pitch jumps larger than 50 Hz are prohibited by interchanging a deviant absolute maximum by one of the relative maximums in the sub-harmonic sum spectrum. \n4. EVENT DETECTION \nOnce the continuous pitches have been identified,  the time \nlocations at which a note starts (the onset time) and ends (the offset time) have to be found. To date, no algorithm has been developed that reliably detects the wide range of possible note onsets in performance data from different singing styles. For the current purpose, note onsets are defined as vowel onsets. Consequently, the singing is assumed to consist of short, relatively isolated syllables, preferably comprising a lengthened unvoiced fricative and a long mid vowel (e.g., /fa/-/fa/-/fa/). Note onsets are then characterized by an abrupt rise in energy over a broad frequency range. The sustained note has a relatively steady spectral shape representing the formants of the vowel used. Though note offsets can be identified to some extent by the fall of energy in especially the higher frequencies, they are less clearly defined. This is due to the exponential decay of the amplitude of a note making a note already inaudible while it is still physically present. \nWe have linked several standard signal processing techniques to \ndetect the rise and fall of energy in different frequencies to segment the signal into note onset and offset times. First, the short-term energy method is used to detect silent parts in the singing. Subsequently, each non-silent part is provided to the other three methods in succession. We assume a digital signal \n/g12 /g11 /g81 /g91 ,f o r  /g20 /g15 /g15 /g19− = /g49 /g81 /g21 that is pre-emphasized by the filter \n/g2095.01−− zto produce a 32 dB boost in the spectral magnitude, \nblocked into frames. \n4.1 Short-term energy method \nThe short-term energy method is a straightforward method to \ndetect note onsets and offsets and to distinguish singing from silence. For that, the signal is blocked into non-overlapping frames of 10 ms. \nThe short-term energy\nk /g40in frame kis estimated by  \n∑\n==/g49\n/g81/g78 /g81 /g91 /g40\n/g20/g21/g12 /g11 (2) \nThe short-time energy is normalized by a maximum short-term \nenergy found in a running window of the signal. Adaptive threshold values are used to determine note onsets and note offsets, in which the note onset threshold (e.g., 0.02) is defined to be higher than the note offset threshold (e.g., 0.01). This is done to avoid an on-off oscillation of a marginal signal (e.g., a weak fricative). First, the procedure looks for the first note onset to be detected. If the short-time energy exceeds the onset threshold, a note onset is detected. If, after a note onset has been detected, the short-time energy falls below the offset threshold, a note offset is detected. \n4.2 Surf method \nThe Surf onset detection algorithm has been adopted from the \ntechniques of Schloss [17]. The signal is passed through a first- \nFigure 2. The waveform produced by a male person singing \nthe first melody line of the Beatles song Yesterday of 12 \nnotes by using the syllables na-na.  The short-term energy \nmethod indicates regions of silence and singing for isolated \nnotes; it detects 9 note onsets. The Surf method indicates the \nsame 9 note onsets by looking at the positive zero-crossings in \nthe surf contour. The high-frequency content method can \ndetect all 12 note onsets by peak picking; there are some \nspurious peaks that can result in false alarms. The pitch \nmethod can detect gliding note onsets by looking how the pitch \ncontour fluctuates over time.  \n \norder high pass filter and blocked into frames of 20 ms, with a \nframe shift of 10 ms. The frames are used to compute a smoothed amplitude envelope of the signal that represents the higher \nfrequencies. This envelope \n/g12 /g11 /g78 /g92 is made out of the sequence of \naverage absolute values of the signal within each frame k. The \nslope of the envelope is found by a polynomial fitting procedure [15]. For that, the envelope sequence is fitted with a second-order polynomial over a M-point segment of the sequence. We denote a \nsmall finite envelope sequence around sample \nτ by \n{}/g48\n/g48 /g87t y−=+) (τ , where Mspecifies the width of the polynomial \ninterpolation. This small segment is fitted by a second-order \npolynomial /g21ctbta++  by minimizing some fitting error. \n \nA polynomial approximation of the slope at τis then given by \n∑∑−=−=+⋅=≅∂+∂/g48\n/g48 /g87/g48\n/g48 /g87tt ytbtt y\n/g21) ( ) ( τ τ(3) \nWe compute a 5-point approximation; M is fixed at 2. At the \nedges of the envelope sequence, no slope computation is done.  \nSince a note onset is characterized by an abrupt rise of higher \nfrequencies of the signal, we looked at positive zero-crossings of the slope contour to find these onset times. \n4.3 High-frequency content method \nThe high-frequency content method has been adopted from Masri \nand Bateman [9]. It aims at revealing both changes in overall energy and the energy concentration at higher frequencies. The CubyHum: A Fully Operational Query by Humming System \nsignal is blocked and Hanning-windowed into frames \n/g20 /g15 /g15 /g19− = /g46 /g78 /g21 of 20 ms, with a frame shift of 10 ms. \nAM-point FFT is used to produce a short-time DFT /g12 /g11 /g80 /g59k . The \nshort-time energy k /g40in frame kis computed as the sum of the \nsquared magnitude of each FFT bin, \n∑+\n==/g20 /g21 /g18\n/g21\n/g19)(/g48\n/g80 /g80/g78 /g78 mX E (4) \nwhere0 /g80denotes the lowest FFT bin that is taken into account. \nOnly the FFT bins are considered that fall within a frequency band \nof 400 Hz and higher. \nThe short-time higher frequency content k /g43 in frame kis a \nweighted version of k /g40, linearly biased to the higher frequencies, \n∑+\n=⋅=/g20 /g21 /g18\n/g21\n/g19)(/g48\n/g80 /g80/g78 /g78 mXm H (5) \nA detection function is computed that combines each pair of \nconsecutive frames; it is the product of the rise in high frequency energy between two frames and the current normalized high frequency content, \n/g78/g78\n/g78/g78\n/g78EH\nHHD ⋅=\n− /g20(6) \nThe first ratio represents the rise in high frequency content. The \nsecond ratio represents the normalized high frequency content for \nthe current frame k . Their product (i.e., k /g39) peaks prominently at \nabrupt increase in high frequency energy content. If it surpasses a \ngiven threshold, we say that the detection function has found a \nnote onset .\n4.4 Pitch method \nThe pitch  method is a way to segment gliding notes into note \nonset and offset times. Essentially, it groups and averages the pitches as found in 40 ms frames in different windows on a frame-by-frame basis. For this, we use a growing window of frames for which it has been concluded that they contain similar pitches. The growing window maintains the pitch of a current note.  A second window of fixed size is placed at the end of the growing window. \nThe median pitch is computed for both windows. If the median \npitch of the fixed-size window falls within 100 cents (a semitone) of the median pitch of the growing window, the growing window is extended with one frame and, consequently, the window of fixed size is moved one frame; the computation starts over again. \nIf, however, the median pitch of the fixed-size window equals or \ndiffers more than 100 cents (a semitone) from the median pitch of the growing window, it is concluded that the position of the fixed-size window notifies the start of a new note. \nThe offset time of the current note and the onset time of the new \nnote are determined by the starting frame of the window of fixed size. Now, the window of fixed size is the growing window and a new window of fixed size is placed at the end of it; the computation starts over again. \nThe minimal length of any window is determined at 3 pitch frames \n(120 ms), which also determines the minimal duration of a gliding note. Finding note onset and offset times on the basis of a changing pitch has the advantage that the user has some freedom in singing: notes do not need to be sung in an isolated manner but can be 'thread together'. However, a gliding note (or glissando) is segmented into a step-wise sequence of ascending or descending \nnotes of 120 ms each. \n5. QUANTIZATION \nQuantization  means the division of the pitch (tone) and time \ncontinuum into discrete steps. These steps are necessary to decide exactly how much the pitch has changed or what temporal units have elapsed.   Time quantization is not further discussed, since quantized duration is not used in the melody comparison. Instead, the inter-onset-interval (IOI) is used, which is the time difference between two adjacent note onsets. \nTo arrive at a discrete musical pitch, a pitch center of a continuous \npitch contour within a time interval is required. This pitch center should be the pitch perceived by the listener and correspond to the target pitch the singer intended to produce. Although there is ample evidence that the discrete pitch perceived in a pitch contour is that of the mean [2], the use of a mean is sensitive to octave errors or other deviant pitch estimates (e.g., due to vibrato, pitch overshooting or undershooting). We use the median pitch instead. If the contour is error free, the median and mean pitch in a time interval are reasonably close. \nThe median pitch between a note onset and offset is used to \nquantize the musical pitch value for each note using the equally \ntempered musical scale tuned at \n/g43/g93 /g36 /g23/g23/g194= (A-440). Musical \npitch is represented in categories along scales in terms of \nsemitones and cents. These categories are relative measures based on frequency ratios. Knowing that an octave is a frequency ratio of \n2:1, the semitone is one-twelfth of an octave (\n1.059462/g20/g21≈ ) and \na cent is 1/100 of a semitone.  Now, notes on the equally tempered \nscale relative to A-440 occur at multiples of 100 cents; they can be expressed as a distance in cents from 8.176 Hz and have a total \norder.  For instance, the middle C (\n/g43/g93 /g38 /g25/g22 /g17 /g21/g25/g204= ) is 6000 cents \n(or there about). To calculate the discrete musical pitch pof a \nmedian pitch f, we use the frequency ratio /g20/g26/g25 /g17 /g27 /g18 /g73 /g53= and \n  /g24 /g17 /g19 /g79/g82/g74 /g20/g212+⋅= /g53 /g83 (7) \nThe musical pitch is then represented by an integer value; its \ncorresponding pitch label is indexed in an array. By allowing an integer range between 0 and 127, we have essentially the MIDI convention to encode musical pitch. \nSinging is inevitably contained with deviations and changes from \nthe universal tuning standard at A-440. For instance, people tend to fall or rise in pitch (and key, consequently) due to large interval sizes, fatigue and inaccuracies in muscular control. To compensate for this, the tuning standard is adapted to the singing in due course. For the first note sung, it is assumed that singing starts at the universal tuning standard. For each next note, the (closest) musical pitch is calculated and the cent difference between the pitch sung and the musical pitch is computed. Using an inverse variant of Equation 7, the reference pitch (which is at 8.176 Hz, at first) is adapted, which changes all musical pitches relative to this reference pitch. \n6. MELODY REPRESENTATION \nA melody representation that is invariant to key is an interval \nrepresentation, that is, the sequence of distances between two succeeding notes expressed in semitones. In particular, a melody \nsequence \nN /g86 /g86 /g86 /g54 /g2121= comprising absolute pitches is \ntransformed in a sequence ) () )( (/g20 /g21 /g22 /g20 /g21/g10\n−− −−⋅=/g49 /g49s s ssss S \u0015 ,\nwhere the dot  ⋅ represents a special start element since no \ninterval is associated with the first note in a melody. \nIn order to transcend from mode and to use a diatonic scale \nstructure, intervals are grouped together. This has to be done CubyHum: A Fully Operational Query by Humming System \nwithout knowledge about the tonic since we do not know the key \nof the melody. Except for the unison, all intervals are categorized in groups of two intervals of 1 to 2 semitones. In addition, all intervals larger than 6 semitones are grouped in a single category for ascending and descending intervals. It is well-known that intervals larger than 5 semitones and greater are rare (only 10%) in musical melodies from all over the world [5]. In addition, large intervals are difficult to sing accurately. \nAs shown in Table 1, the interval categories are represented by the \nintegers \n/g23 /g15 /g22 /g15 /g22 /g15 /g23 /g21−− . The special start element  ⋅ is maintained. \nThe resulting melody representation is a 9-step contour and \nresembles the Dowling model of how melodies are assumed to be stored in human memory [4]. Temporal information is kept by storing the real value of the inter-onset-interval (IOI) of each note to each corresponding 9-step contour element. \nInvariance to global tempo is established by calculating the ratios \nof two IOIs, but this is done in the melody comparison. \nTable 1. The 9-step melody representation (st = semitone). \ninterval name interval size integer code \ndesc. dim. fifth and greater < -6 st -4 \ndesc. perfect/augm. fourth -5 or 6 st -3 \ndesc. minor/major third -3 or 4 st -2 \ndesc. minor/major second -1 or 2 st -1 \nunison 0 st 0 \nasc. minor/major second 1 or 2 st 1 \nasc. minor/major third 3 or 4 st 2 \nasc. perfect/augm. fourth 5 or 6 st 3 \nasc. dim. fifth and greater > 6 st 4 \n7. MELODY COMPARISON \nThe art of melody comparison is finding approximate similarities \nbetween finite sequences of elements drawn from a finite alphabet, though the sequences have different lengths. In our case, one sequence is a relatively small pattern and a second sequence that has a longer length.  The former sequence represent a transcription of what has been sung; the latter sequence represent a melody from the database. This sequence can also be interpreted as a concatenation of all melodies in the database. The pivot is defining an appropriate similarity metric (or distance measure) for melodic sequences that (1) assigns different costs to different local dissimilarities, (2) meets some invariance principles and (3) is psychologically plausible; it should provide an orderly representation of the melodies that fits human expectation and music theory. \n7.1 Notation \nWe adopt the following notation for comparing melodic \nsequences: let ∗Σ∈ =/g48q qqQ\u0015/g21 /g20 be a query pattern sequence of \nlength /g48 /g52= and ∗Σ∈ =/g49s ssS\u0015/g21 /g20 a sequence of length \n/g49 /g54=.Σis a finite alphabet of pitch intervals. Here, we use the \n9-step alphabet { }4,3,3,4/g28 \u0015−−=Σ− /g86/g87/g72/g83 .\nWe denote /g77sas the j-th element of S for an integer \n{} /g49 /g77 /g15/g15 /g20 /g21∈ . We denote /g77 /g76 /g77 /g76 ss S \u0015/g21= as a subsequence (or \nfactor) of S, which is the empty subsequence εif ji>.T h e  prefixes  of S are the 1 +N subsequences /g77 /g77 ss S \u0015/g21 /g20 /g20= for \nNj≤≤0 . Likewise, the suffixes of S are /g49 /g77 /g49 /g77 s s S \u0015/g21= for \n1 1 +≤≤ Nj . In addition, we define a tabular function S /g47,\nwhich is specific to the sequence S , that provides the IOI for a \ngiven j-th element of  S. In particular, /g77 /g77 /g54 t sL=:)( , where /g77tis the \nIOI for /g77s.\n7.2 Typical problem instances \nThe performance of an approximate pattern matching algorithm \ndepends on the length of the query pattern sequence M, the size of \nthe longer sequence N, the size of the melody database, the size of \nthe alphabet σ=Σ , the number of differences allowed k and \nconsequently the error level =αk/  M .\nPractical problem instances for our melody comparison can be \ndescribed by the following parameters. \n• The query pattern sequence Qhas a typical length Mof a \ndozen elements. For instance, singing the first phrase of the Beatles' song 'Yesterday' amounts to singing 12 notes (or 11 intervals). \n• The melodic sequence  S has a typical length N of a few \nhundred elements. The vocal melody of a popular rock song has about 300 notes. \n• The melody database can be as small as a few hundreds (for \nsmall-scale applications) and thousands and thousands (for full-scale applications). \n• The alphabet \n/g86/g87/g72/g83−Σ/g28 has 9 elements. These elements are \nintegers representing interval categories having total order. \n• In our singing experiment, we found that the percentage of \nerrors allowed is in the range of 20-40%. \n7.3 Edit distance \nThe traditional way to compare two sequences is to allow \nparticular differences (or errors) of elements to occur in the sequences while computing their distance, denoted as \n+∗∗ℜ→Σ×Σ/g19 :ed . Thus, /g12 /g15 /g11 /g54 /g52 /g72/g71 represents the distance \nbetween /g52and S. The type of differences can be deletions, \ninsertions and replacements of single elements that are necessary \nto transform one sequence into the other. A cost (or penalty) is associated with each transformation (or difference). A cost may be a constant (e.g., a unit cost for each transformation) or any value function that computes the difference between two elements in its context. By choosing appropriate costs, one can select those approximate matches that make sense in a particular domain and reject other which do not. When we restrict the costs to be unit, the match will be based on the unit-cost edit distance, that is, the minimal number of deletions, insertions and replacements to transform the sequence Q into the sequence S [20]. \nThe unit-cost edit distance model can be used in two different \nways. \n1. Minimal distance problem.  Finding an approximate match \nbetween Qand Sthat has minimal edit distance. \n2. k-difference problem. Finding an approximate match (or all \napproximate matches) in S that has (or have) at most \n+ℵ∈/g82 k\ndifferent elements with Q (i.e., at most an edit distance of k\nwith Q). CubyHum: A Fully Operational Query by Humming System \nThe computation of the edit distance can be easily solved by using \nclassical dynamical programming for sequences of the same length [20] and for sequences of different lengths [17]. \n7.4 Local melody differences \nThe edit distance model works fine for textual sequences and for \nmelody representations that abstract from tonal and timing structures such as a contour representation. For melodies decoded \nin \n/g86/g87/g72/g83−Σ/g28 and with timing information, we have to account for \nother types of differences and their effects. \nHereunder, we enumerate the most important local differences \nbetween melodies. The ones that have to do with musical pitch are shown in Figure 3. Some of these differences (or human errors in melody reproduction) have already been discussed in Section 2. \nFigure 3. Typical local differences between melodies. \nUnderneath each musical staff, the interval sizes in semitones \nare shown, not the interval categories from /g86/g87/g72/g83−Σ/g28 .\n1. Melodic sequences of variable length: the singing of any part \nof a melody. \n2. Amount of mistuning : the singing of an interval a little too \nsharp or flat should not be as bad as singing it far too sharply or flat. \n3. Modulation by interval replacement: the singing of a wrong \ninterval may result in a key modulation of the whole succeeding melody line. \n4. Note replacement: replacing one note for another note has \nimplications for the interval representation of the melody. The interval associated with the replaced note changes by a certain number of semitones, which is compensated by the very next interval. \n5. Note insertion : the singing of an additional note (a filler or \ngrace note) has implications for the interval representation of the melody. The sum of the sizes of the two new intervals introduced by the inserted note equals the size of the original interval. \n6. Note deletion : forgetting to sing or missing a particular note \n(a thinner note) has implications for the interval representation of the melody. The size of the new interval due to the deleted note equals the sum of the sizes of the two original intervals. \n7. Other note and interval insertions and deletions :s o m e  \ninsertions and deletions of small melodic fragments cannot be accounted by some of the above-mentioned schemes. For instance, short melody lines can be added or deleted. \n8. Duration error : the lengthening or shortening of a note \nwithout changing global tempo. \nIn addition to these local melody differences, one might think \nabout the concepts of consolidation  and fragmentation  as introduced by Mongeau and Sankoff [12]. A consolidation \nrepresents the replacement of several notes at the same pitch by a single note at that pitch. Likewise, a fragmentation represents the replacement of a single note by several ones at the same pitch. If these sequence differences were interpreted by a series of single insertions and deletions, it would cost more than counting them as a single transformation. However, these concepts occurred rarely in the singing data of the experiment and are computationally intensive. Therefore, we found them not in proportion to their added benefit. \n7.5 Dynamic programming solution \nSimilar to the edit distance model, the classical dynamical \nprogramming approach to compute the melody distance /g12 /g15 /g11 /g54 /g52 /g39\nbetween two melodic sequences M /g84 /g84 /g84 /g52 /g2121= and \nN /g86 /g86 /g86 /g54 /g2121= is done by filling a matrix ) (/g19 /g15 /g19 /g49/g48 D/g21 /g21 . The \nentry /g77 /g76D/g15holds the minimal melody distance between the two \nprefixes i /g52/g211and /g77S/g21 /g20. The algorithm to construct the matrix is \ndone by using the following recurrent formula \n \n\n\n\n⋅++>+=+\n− ⋅++− ⋅+−+>=+−+\n⋅++⋅++\n=\n−−−−−\n−−−− −−−−− −−\n−−−−\ninsertion) (interval) ()(\n1insertion) (note2 , if,) ()( ) (\n) ()(\n1(8) error)noor n (modulatio) ()(\n) ()(deletion) (note2 , if,) ()(\n) ()( ) (\n1deletion) (interval) ()(\n1\nmin\n/g20/g20 /g15/g20/g21/g20\n/g20/g21 /g15 /g20/g20 /g20/g20 /g15 /g20/g20/g20 /g21/g20\n/g20 /g15 /g21/g20/g15 /g20\n/g15\n/g77 /g54/g77 /g54\n/g77 /g76/g77 /g77 /g76/g77 /g54/g77 /g54 /g77 /g54\n/g76 /g52/g76 /g52\n/g77 /g76/g77 /g54/g77 /g54\n/g76 /g52/g76 /g52\n/g77 /g76 /g77 /g76/g77 /g76 /g76/g77 /g54/g77 /g54\n/g76 /g52/g76 /g52 /g76 /g52\n/g77 /g76/g76 /g52/g76 /g52\n/g77 /g76\n/g77 /g76\nsLsL\nK Djs sqsLsL sL\nqLqL\nK DsLsL\nqLqL\nK sqCDisq qsLsL\nqLqL qL\nK DqLqL\nK D\nD σ\nwhere /g28/g28=Σ=− /g86/g87/g72/g83σ denotes the size of the alphabet, and C\nand Kdenote constants that have to be determined empirically. \nWe use C= 1 and 2 . 0=K .\nThe following set of initial boundary conditions and special cases \nis used \n00) ()(\n110\n/g20 /g15 /g19 /g15 /g20/g15 /g19/g19 /g15 /g20 /g20 /g15/g20/g19 /g15 /g20 /g19 /g15/g19 /g15 /g20/g19 /g15 /g20 /g19 /g15 /g19\n====⋅++====\n−−−−\n/g77 /g77/g77/g76 /g76/g76 /g52/g76 /g52\n/g76 /g76\nD DDD DqLqL\nK D DDD D\n(9) \nThe rationale of the recurrent formulae is, first, that pitch intervals \nbetween melodies are penalized by their absolute difference, \n/g77 /g76 /g86 /g84− . If the pitch intervals are equal, there is no interval cost. \nIf they are not equal, we speak about an interval replacement that \nmay result in a modulation (key-change) of one melody in comparison to the other. The interval cost is normalized by the size of the alphabet so that it will never reach a cost of 1 or higher. Additional to this interval cost, there is a durational cost expressed CubyHum: A Fully Operational Query by Humming System \nby the absolute difference of duration ratios. The constant K\nrepresents the relative contribution of duration differences versus that of interval differences. A note replacement is not explicitly accounted for, but it can be interpreted as two modulations in series since it involves two succeeding intervals. \nSecond, if \n/g77 /g76 /g76 s q q =+− /g20 or /g77 /g77 /g76 s s q +=− /g20 , we speak of a note \ninsertion or note deletion, respectively. Recall that a note \ninsertions or deletions have special implications for the underlying intervals, expressed by the conditional summations. An interval cost of 1 is associated with these differences. The durational cost penalizes longer durations of inserted or deleted notes more than smaller durations; it thus favors grace notes for thinner and filler notes. In principle, the concepts fragmentation and consolidation can be worked out using the same scheme. \nThe two remaining differences are the insertions and deletions that \ncannot be accounted for by the other schemes. Their costs are 1 plus a varying durational cost. The duration cost is based on the motivation that the deletion of an interval can be seen as replacing a note with a nullified note of zero-length. Likewise, an interval insertion is similar to replacing a zero-length note with a note of a non-zero length. \nThe initial boundary conditions and special cases look rather \ncomplicated because (1) they express the possible start of Qat any \nposition in S , (2) the fact that the used duration ratios do not exist \nat the very start of a sequence and (3) the fact that the sequences start with a special start symbol. \nThe filling of the matrix ) (\n/g19 /g15 /g19 /g49/g48 D/g21 /g21 starts at /g19 /g15 /g19D and ends at \n/g49 /g48D/g15in either a column-wise top-to-bottom manner or a row-\nwise left-to-right manner. By keeping track of each local \nminimization decision in the matrix in a pointer structure, one can reveal the optimal alignment between P and a subsequence of S .\nThe entry in the column ) (\n/g19 /g15 /g49 /g48D/g21 holding the minimal distance \nvalue refers to the end of an optimal alignment. By tracing back \nthe pointers, one can recover all local minimization decisions in reverse order that resulted in this minimal value and, hence, the starting point of the optimal alignment. Likewise, one can find multiple optimal alignments, if there are several. Or, one can find the alignments (and positions) that have a distance that is lower than a pre-defined threshold. \nSince we have to compute all entries of the matrix and the \ncomputation of each entry \n/g77 /g76D/g15is a constant factor, the worst and \naverage case time complexity is still /g12 /g11 /g49/g48⋅Ο . Note that this \ncomputation has to be done for each melody in the database. A \nsignificant reduction in practical computing time without loss of performance can be obtained by leaving out the recurrent expressions for note insertion and deletion. \nIn principle, if we compute the matrix column-wise or row-wise, \nonly the current column (or row) and the previous two need to be \nstored; only \n/g12 /g15 /g80/g76/g81/g11 /g22 /g49/g48⋅ cells are required. Since NM<, the \nspace required is /g12 /g11 /g48Ο .\n7.6 An index method: Filtering \nChances are small that a query pattern Qhas a high approximate \nmelodic similarity with many melodic passages S in the database. \nLeaving out subsequences in Sthat cannot have a sufficiently high \nsimilarity with Q saves the computation of complete columns in \nthe dynamic programming matrix used to evaluate Equations (8) and (9). Index methods quickly retrieve parts in Sthat might be \nhighly similar to Q. When these parts in S are identified, they still \nneed to be evaluated by using Equation (8) and (9) to ensure whether or not they really match with Q.Current index methods are based on the k -difference problem \nbetween \n/g52and Susing the unit-cost edit distance model. One of \nthese index methods is known as filtering: parts in Sthat meet a \nwell-defined necessary (but not sufficient) filtration condition with respect to Q and a pre-defined error level \n=αk/  M  are \ncandidate for further evaluation; all other subsequences are \ndiscarded. It is conceivable that discarded parts in S can still have \na high melodic similarity with Q, as the filtering is based on the edit distance. To alleviate this discrepancy, the error level has to be set appropriately. \nThe used filtering method is the Chang and Lawlers LET (Linear \nExpected Time) algorithm [3]. It discards a subsequence of \nSwhen it can be inferred that it does not contain an approximate \nmatch with \n/g52. This can be done by observing that a region in S\nhaving a k-approximate match with a pattern Q of length Mis at \nleast of length M-  k  and is a concatenation of at most 1 +k\nlongest subsequences of Q with intervening (non-matching) \nelements.  So, the 'filtration' condition says that any subsequence \nin Sof 1+k concatenated longest subsequences of Q that is \nshorter than M-  k  can be discarded. The remaining subsequences \nare further evaluated using Equation (8) and (9). \nThe algorithm uses a suffix tree on Qto determine in linear time \nthe longest subsequences of Q in S. A suffix tree on a sequence Q\nis a special data structure that forms an ordered representation of \nall suffixes of Q. A suffix tree can be built in linear /g12 /g11 /g48Ο time \nand needs /g12 /g11 /g48Ο space [10][18]. \nThe algorithm works by traversing Sin a linear fashion (from left \nto right) and maintains the longest subsequence of Qat each \nelement in Susing the suffix tree on Q. When this subsequence \ncannot be extended any further, it starts a new subsequence of Q at\nthe next element. Note that there is an intervening element defined \nbetween any two longest subsequences of Q inS. These elements \nare called markers  in S.\nThe result is a partitioning of S that consists of the longest \nsubsequences of Q intervened by markers. Subsequences in S are \ndiscarded, if they are a concatenation of 1 +k longest \nsubsequences of Q (with kmarkers) of a length that is shorter than \nkM−.\nAn additional result of the partitioning of Swith respect to P is the \nnumber of markers in S. This quantity is also known as the \nmaximal matches  distance between S and P. This distance has \nbeen proven to be a lower bound for the unit-cost edit distance [19]. \nExample.  Let \n= /g52abcba (5 =M ) and =S adaaabdbadbbb \n(1 3=N ). The partition of S as a concatenation of longest \nsubsequences of Qintervened by markers is a-a-ab-ba-b-b, since \na,ab,ba and b are all (longest) subsequences of Q in S. The \nmarkers have been omitted at positions 2, 4, 7, 10 and 12 in S . The \nmaximal matches  distance between P and S equals 5. By \nallowing 1 =k difference, the regions a-a and b-b are discarded \nsince they are of length 3 <M    k  = 4. On the other hand, the \nregions a-ab,ab-ba  and ba-b  need to be further evaluated since \ntheir lengths are kM−.\n7.6.1 Heuristic adjustments \nFiltering methods are judged on their correctness, their time \ncomplexity and their filtration efficiency. \n1. Correctness.  The LET algorithm has been proven to \ncorrectly solve the k-difference problem, that is, it does not miss any approximate matches in Sin edit distance sense. CubyHum: A Fully Operational Query by Humming System \nHowever, regions in S that are filtered out by LET can still be \nsimilar to Q in our melody distance sense. \n2. Time complexity.  The identification of candidate regions in \nShappens in linear time /g12 /g11 /g49Ο by using a suffix tree on Q.\n3. Filtration efficiency.  The efficiency relates to the number of \nelements that can be discarded by the filter. Filtering works well on low error levels and bad or not at all on higher error levels; the filtration efficiency drops very quickly at a particular error level. \nBy using the maximal matches  distance between a region in S and \nQas a lower bound for their edit distance, we can further rule out \nregions in Son an heuristic basis by recognizing that Scan have \nrepetitive subsequences. Repetition in a melodic sequence is common; a melody can contain similar passages referring to the tune of the chorus or the individual phrases of a stanza. \nWe use two rule-out methods aiming at increasing the filter \nefficiency at a given error level. The heuristic is based on the observation that a region in S does not need to be evaluated again, \nif a similar one has already been evaluated. \n1. LET-H1 : all non-overlapping regions in S with equal \nmaximal matches  distances normalized by the length of the \nregion to Q are maintained. From this set, only one region is \nsubjected to further evaluation; all others are discarded. \n2. LET-H2 : only the region in S with the minimal maximal \nmatches  distance normalized by the length of the region to Q\nis chosen for further evaluation; all others are discarded. \nIt must be emphasized that these heuristic extensions make the \nfilter no longer working correctly, since approximate matches in S are discarded on purpose. To find a balance in correctness (heuristic) level, filtration efficiency and melody comparison \nperformance, we empirically set 25 . 0\n=α while using method \nLET-H1. As shown in Section 7.6.2, this provides us a 64% to \n89% reduction in computing columns during dynamic programming for pattern sequences Q with a length of 12. In \npractice, the LET algorithm was found to be too permissive in providing still too many similar regions for further evaluation. In contrast, LET-H2 was found to be far too stringent by discarding relevant regions. Some regions in S that were discarded by LET-\nH2 turned out to have a high melodic similarity with Q.\n7.6.2 Filtering experiment \nIn order to assess the filtration efficiency of the three filtering \nmethods for typical problem instances, we conducted experiments with a varying error level \nαusing a database with 510 popular \nmelodies3, each containing 285 notes on average. The filtering \nmethods were LET, LET-H1 and LET-H2. All sequences were \nmade out of our alphabet /g86/g87/g72/g83 /g16 /g28Σ of 9 interval elements.  The \npatterns Qwere constructed with varying lengths ( M=  10, 12, \n14). They were either randomly chosen excerpts from the database (the melodic sequences) or randomly compiled from the alphabet (the random sequences). \nA measure for filtration efficiency is the number of elements that \nare discarded divided by the total number of elements, \n/g49/g49 /g49/g72/g73/g73/g76/g70/g76/g72/g81/g70/g92 /g3 /g73/g76/g79/g87/g85/g68/g87/g76/g82/g81/g72−= (10) \n3The melody database contained 510 vocal monophonic melodies \nfrom songs of the Beatles (185), ABBA (73), the Rolling Stones (67), Madonna (38), David Bowie (34), U2 (33), Prince (23), Michael Jackson (20), Frank Sinatra (20) and the Police (17).  where Ndenotes the total number of elements and \ne /g49denotes the \nnumber of elements that need further evaluation. In order to \ndecrease random variations, we have determined the averages of 250 independent runs with different patterns. \nThe results are shown in Table 2. The random sequences are more \nstringently filtered since they show little resemblance with the structure in popular melodies. It is clear that the filtration efficiency of LET has a steep drop at an error level \nαbetween \n0.2 and 0.3. The use of the heuristics in LET-H1 and LET-H2 \nboosted the filter efficiency at each error level. An error level α\nof 0.25 is an appropriate parameter value when using one of the \nfilter approaches. \nTable 2. Filtration efficiency simulated for different parameters \nof a problem instance ( /g28/g28=Σ− /g86/g87/g72/g83 ). Parameter combinations \nthat resulted in zero filtration efficiency are not shown. \nM k α Melodic sequences Random sequences \n   LET LET-\nH1 LET-\nH2 LET LET-\nH1 LET-\nH2 \n10 1 0.10 0.99 1.00 1.00 1.00 1.00 1.00 \n 2 0.20 0.81 0.90 0.97 0.97 0.98 0.99 \n 3 0.30 0.22 0.37 0.76 0.35 0.49 0.85 \n 4 0.40 0.03 0.06 0.24 0.04 0.07 0.26 \n12 1 0.08 1.00 1.00 1.00 1.00 1.00 1.00 \n 2 0.17 0.93 0.96 0.98 1.00 1.00 1.00 \n 3 0.25 0.53 0.64 0.89 0.81 0.89 0.96 \n 4 0.33 0.11 0.16 0.45 0.14 0.23 0.52 \n 5 0.42 0.02 0.04 0.13 0.02 0.03 0.12 \n14 1 0.07 1.00 1.00 1.00 1.00 1.00 1.00 \n 2 0.14 0.98 0.99 0.99 1.00 1.00 1.00 \n 3 0.21 0.77 0.83 0.94 0.96 0.97 0.99 \n 4 0.28 0.28 0.38 0.70 0.45 0.58 0.86 \n 5 0.36 0.06 0.08 0.24 0.06 0.08 0.25 \n8. CONCLUSIONS \nCubyHum is a software system in which query by humming is \nrealized by linking algorithms from various fields: speech signal processing, music processing and approximate pattern matching. Empirical findings from singing experiments were a crucial input to the development of these algorithms. In short, it tries to detect the pitches in a sung melody and compares these pitches with symbolic representations of melodies in a large database. Melodies that are similar to the sung pitches are retrieved. Approximate pattern matching in the melody comparison process compensates for the errors in the sung melody (e.g., sharp or flat notes, wrong tempo) by using classical dynamic programming. A filtering technique saves much of the computing necessities involved in dynamic programming. \nCubyHum has been integrated in an in-house research \ndemonstrator, the Easy Access music jukebox, in which innovative user interface solutions supporting various user search strategies and intentions in music retrieval are demonstrated (see Figure 4). Besides query by humming, this Internet-connected jukebox incorporates \nspeaker identification for personalization purposes, collaborative filtering for recommending new music, navigation and playlist creation features by voice and \npointing gestures, and CubyHum: A Fully Operational Query by Humming System \nsystem feedback by text-to-speech synthesis and auditory \ncues. \nUsing this personalized jukebox, a user can simply name, sing and \npoint at songs to listen to or to collect them in a playlist. \nFigure 4. The Easy Access music jukebox. \n \nSome formal user studies and evaluations on query by humming \n(and the Jukebox as a whole) have already been finalized. Their findings guide further algorithmic improvement and tell what usability issues for a query by humming system are prevalent. As a conclusion, we would like to address the following recommendations for further research. \n8.1 Pitch detection \nThe current pitch detection algorithm (sub-harmonic summation) \nhas been developed for normal speech for which it works reliably. It has been made robust with respect to deviant pitch values by smoothing the pitch contour in a dynamic programming framework. Further research has to be pursued to detect pitch reliably for highly pitched tones, inharmonic sounds and severely degraded acoustical or channel conditions. \n8.2 Event detection \nThe current event detection is based on standard signal processing \nalgorithms. For best performances, users are recommended to sing the notes of their melody in an isolated manner using a non-sense syllable of an unvoiced fricative and a long vowel (e.g., /fa/-/fa/-/fa/).  Although this isolated way of singing can be easily taught and learnt, it takes away any opportunities for expressive and free-style singing. \nFurther research has to be pursued to detect musical events \nrobustly and reliably allowing users to sing in any preferred style.  For instance, the finding of note onsets can be helped by robust vowel onset detection mechanisms and the use of parametric models that detect abrupt signal changes or employ the presence of stationary signal segments. Moreover, singing contains all kinds of expressive means that largely go unnoticed by the current event detection method. If, for instance, vibrato or accentuation can be reliably detected, these cues can be used to extend the melody comparison process. \n8.3 Approximate pattern matching \nComputing requirements are dominated by approximate pattern \nmatching. It turned out that melody comparison by means of classical dynamic programming is impractical in terms of running time performances for large melody databases. Using the current filtering method, current response times take a few seconds for a database with only 510 melodies on a current platform. From a usability point of view, this has to be reduced to half a second for a database with many more melodies. Research on fighting the inherent \n/g12 /g11 /g49/g48⋅Ο time complexity of \ndynamic programming is pivotal. There are essentially two \napproaches to tackle this challenge. Index methods for approximate pattern matching allow a search to jump swiftly to candidate approximate matches in the database; this field is new and rather immature. In addition, some recurrent expressions for finding approximate matches can be evaluated by a fast bit-parallel implementation of dynamic programming. These algorithms exploit the intrinsic parallelism of bit-vector operations. If the length of the pattern P is smaller than the size of \nthe computer word, they can run in essentially linear time [1][13]. \n9. ACKNOWLEDGMENTS \nThanks go to Dik Hermes (Technische Universiteit Eindhoven) for \ntelling me everything about pitch perception, Sander van de Wijdeven for helping me C++debugging CubyHum and all members of the Philips Easy Access research project team. \n10. REFERENCES \n[1] Baeza-Yates, R., and Navarro, G. (1999). Faster approximate \nstring matching.  Algorithmica 23, 2 , 127-158. \n[2] Brown, J.C., and Vaughn, K.V. (1996). Pitch center of \nstringed instrument vibrato tones ,Journal of the \nAcoustical Society of America, 100 , 1728-1735. \n[3] Chang, W., and Lawler, E. (1994). Sublinear approximate \nstring matching and biological applications. Algorithmica, \n12, 4/5 , 327-344. \n[4] Dowling, W.L. (1978). Scale and Contour: Two components \nof a theory of memory for melodies. Psychological Review, 85, 4, 341-354. \n[5] Dowling, W.J., and Harwood, D.L. (1986). Music cognition .\nNew York: Academic Press. \n[6] Ghias, A., Logan, J., Chamberlin, D., and Smith, B.C. \n(1995). Query by humming: Musical information retrieval in an audio database. Proceedings of the ACM international \nMultimedia conference and exhibition, November 1995, San Francisco, California . New York: ACM, 231-236. \n[7] Hermes, D.J. (1988). Measurement of pitch by subharmonic \nsummation. Journal of Acoustical Society of America, 83, 1, \n257-264. \n[8] Levitin, D.J. (1994). Absolute memory for musical pitch: \nEvidence from the production of learned melodies. Perception & Psychophysics, 58 , 927-935. \n[9] Masri, P., and Bateman, A. (1996). Improved modelling of \nattack transients in music analysis-resynthesis.  Proceedings of International Computer Music Conference (ICMC 96), Hong-Kong, Aug 1996 , International Computer Music \nAssociation, 100-103. \n[10] McCreight, E. (1976). A space-economical suffix tree \nconstruction algorithm.  Journal of ACM, 23, 2 , 262-272. \n[11] McNab, R.J., Smith, L.A., Witten, I.H., and Henderson, C.L. \n(2000). Tune retrieval in the multimedia library, Multimedia Tools and Applications, 10, 113-132. \n[12] Mongeau, M., and Sankoff, D. (1990). Comparison of \nmusical sequences. Computers and the Humanities, 24, 161-175. \n[13] Myers, G. (1999). A fast bit-vector algorithm for approximate \nstring matching based on dynamic programming . Journal of \nthe ACM, 46, 4, 395-415. CubyHum: A Fully Operational Query by Humming System \n[14] Peynirçioglu, Z.K., Tekcan, A.I., Wagner, J.L, Baxter, T.L., \nand Shaffer, S.D. (1998). Name or hum that tune: Feeling of knowing for music, Memory & Cognition, 26, 6 , 1131-1137. \n[15] Rabiner, L.R. and Juang, B. (1993).  Fundamentals of Speech \nRecognition, Prentice-Hall Inc. \n[16] Schloss, W. (1985). On the Automatic Transcription of \nPercussive Music: From Acoustic Signal to High Level Analysis , PhD Thesis, Department of Music, Report No. \nSTAN-M-27, Stanford University, CCRMA. \n[17] Sellers, P.H. (1980). The theory and computation of \nevolutionary distances: Pattern recognition. Journal of \nAlgorithms, 1 , 359-373. [18] Ukkonen, E. (1995). Constructing suffix trees on-line in \nlinear time. Algoritmica, 14, 3 , 249-260. \n[19] Ukkonen, E. (1992). Approximate string matching with q-\ngrams and maximal matches. Theoretical Computer Science, 92, 1 , 191-211. \n[20] Wagner, R.A. and Fischer, M.J (1974). The string-to-string \ncorrection problem, Journal of the Association of Computing Machinery, 21, 1 , 168-173."
    },
    {
        "title": "PATS: Realization and user evaluation of an automatic playlist generator.",
        "author": [
            "Steffen Pauws",
            "Berry Eggen"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417971",
        "url": "https://doi.org/10.5281/zenodo.1417971",
        "ee": "https://zenodo.org/records/1417971/files/PauwsE02.pdf",
        "abstract": "A means to ease selecting preferred music referred to as Personalized Automatic Track Selection (PATS) has been developed. PATS generates playlists that suit a particular context- of-use, that is, the real-world environment in which the music is heard. To create playlists, it uses a dynamic clustering method in which songs are grouped based on their attribute similarity. The similarity measure selectively weighs attribute-values, as not all attribute-values are equally important in a context-of-use. An inductive learning algorithm is used to reveal the most important attribute-values for a context-of-use from preference feedback of the user. In a controlled user experiment, the quality of PATS- compiled and randomly assembled playlists for jazz music was assessed in two contexts-of-use. The quality of the randomly assembled playlists was used as base-line. The two contexts-of-use were ‘listening to soft music’ and ‘listening to lively music’. Playlist quality was measured by precision (songs that suit the context-of-use), coverage (songs that suit the context-of-use but that were not already contained in previous playlists) and a rating score. Results showed that PATS playlists contained increasingly more preferred music (increasingly higher precision), covered more preferred music in the collection (higher coverage), and were rated higher than randomly assembled playlists.",
        "zenodo_id": 1417971,
        "dblp_key": "conf/ismir/PauwsE02",
        "keywords": [
            "Personalized Automatic Track Selection (PATS)",
            "Dynamic clustering method",
            "Preference feedback",
            "Quality assessment",
            "Precision",
            "Coverage",
            "Rating score",
            "User experiment",
            "Jazz music",
            "Lively music"
        ],
        "content": "PATS: Realization and User Evaluation of an Automatic Playlist Generator \nPATS: Realization and User Evaluation o f an Automatic \nPlaylist Generator \nSteffen P auws \nPhilips Research Eindhoven \nProf. Holstlaan 4 (WY21) \n5656 AA Eindhoven, the Netherlands  \n+31 40 27 45 415 \nsteffen.pauw s@philips.comBerry Eggen \nPhilips Research Eindhoven, and Technische  \nUniversiteit Eindho ven / Faculty of Industrial Design \nEindhov en, the N etherlands \nj.h.eggen@ tue.nl \n \nABSTRACT \nAmeans to ease selectin g prefer red music r eferred to as \nPersonalized  Automatic Track  Selectio n (P ATS) has been  \ndevelop ed. PATS g enerates playlists that su it a particular context-\nof-us e, that is, the real-wo rld environment in w hich th e music is  \nheard. To create playlis ts, it uses a dynamic clustering method  in \nwhich s ongs are grouped based  on their attribu te similarity. The \nsimilarity measu re selectiv ely weighs attr ibute-v alues, as not all \nattribute-values are equally impo rtant in a con text-of-us e. An \ninductive learning algorithm is used to reveal the most impo rtant \nattribute-values for a context-of-use from prefer ence feedback of \nthe user. In a controlled  user experimen t, the q uality o f PATS-\ncomp iled and randomly assemb led playlists  for jazz music was  \nassessed  in two contexts-of-us e. The quality o f the rando mly \nassembled playlists  was used as base-line. Th e two contexts-of-us e \nwere listen ing to soft mus ic and  listen ing to lively music. \nPlaylist q uality w as meas ured b y precision (songs that su it the \ncontext-of-us e), coverage (songs that su it the context- of-use but \nthat were not already contain ed in prev ious playlis ts) and a rating \nscore.Results  showed that P ATS playlists con tained increasingly \nmore p referred mu sic (increasingly higher precision), covered \nmore preferr ed music in  the collectio n (higher coverage), and \nwere rated higher than randomly ass embled playlis ts.  \n1. INTRODUCTION \nSo far, music player fun ctionality that h as b een d esigned for \naccess ing and  exploitin g large personal music collection s aims at \nprovidin g fast and accurate ways to retr ieve r elevant mus ic. This  \ntype of acces s generally requires well-defined  targets. Music \nlisten ers need to instan taneou sly associate ar tists and  song titles  \n(or ev en CD and  track nu mbers) with mu sic. This is  not an easy \ntask to do, since titles and artists are not necess arily learnt together \nwith the music [8]. In o ur view, selecting music from a large \nperso nal mu sic co llection is better described  as a search for po orly \ndefin ed targ ets. These targ ets are poorly defin ed sin ce it is \nreason able to as sume th at music lis teners have no  a-priori mas ter \nlist of preferr ed songs for ev ery listen ing intentio n, lack precise \nknow ledge about the mu sic, an d canno t easily express  their music \npreference on-the-fly.Rath er, choice fo r music requires listening \ntobrief musical p assages to  recognize the mus ic before bein g able \ntoexpress a prefer ence for it. \nIf we take mus ic progr ammin g on current mus ic (jukebox) player s \nas an example, it allo ws playing a person ally created temp oral \nsequence of songs in one go, once th e playlist o r program has  \nbeen  created. The creation  of a playlis t, however,  can be a time-\nconsumin g choice task. It is hard to  arrive at an optimal p laylist as  \nmusic has person al appeal to the listener and  is jud ged on man y subjective criter ia. A lso, o ptimality requ ires a complete and \nthorough examination  of all available mus ic in a collectio n, which  \nisimpractical to do so .  Lastly, music progr ammin g consists of \nmultip le serial music choices that in fluence each  other; cho ice \ncriter ia pertain to  individual s ongs as w ell as already selected  \nchoices. A mean s to ease and  speed  up th is mus ic selection  \nproces s could be of much  help to the music listen er. PATS \n(Personalized Auto matic Track  Selection ) is a feature for mus ic \nplayers that automatically creates playlists  for a particular \nlisten ing occas ion (or context-of-use)with minimal user \nintervention  [7].  \nThis paper presents  the realizatio n of PATS an d the results of a \ncontrolled user experimen t to assess its performance.  PATS has \nbeen  realized by a d ecentralized and  dynamic clu ster alg orithm \nthat contin ually g roups so ngs usin g an attribute-value-based  \nsimilarity meas ure. A song refers to  a recorded  performance of an \nartist as can be foun d as a track on a CD. The clus tering on \nsimilarity adheres to the listen ers wis h of coherent music in a \nplaylis t. Since it is likely that this  coherence is  based  on particular \nattribute values of the so ngs, some attribute values  contribu te \nmore than  others in  the comp utation  of the similarity by the use of \nweights . At the same time, the clu stering allows  groups of songs \ntodissolve to form new  groups. This concept adheres to the \nlisten ers w ish o f varied mu sic with in a playlis t and over time. \nClusters are p resented to the mus ic listener as playlists from which  \nthe listen er can remove songs that do  not meet the expectatio ns of \nwhat a playlist should co ntain. A n inductive learnin g algorithm \nbased on decis ion trees is then employed  that tries to reveal th e \nattribute values that might ex plain the removal of songs. Weights  \nof attribute values are adjusted  accordingly,  and the clustering \ncontinues  with these new weights  aimin g at providing better future \nplaylis ts.  \n2. PATS: EASY WAY TO SELECT MUSIC  \nSome wid ely used terms such  as context-of-use and music \npreference n eed fu rther clarificatio n. Als o, we tell wh at we mean  \nwith minimal us er interventio n and  explain the requirements for \nPATS. \n2.1 Context-of-use \nWe defin e context-of-use as the real-wo rld environment in which  \nthe music is heard, bein g it a p arty, romantic evenin g or the \ntraveling by car  or train. The u se of th is con cept is th ought to  be a \npowerful s tarting point for creating a playlist or as an organizing \nprincip le for a music collectio n. \nIn every-day language, the terms music pr eference and  musical \ntaste are in tuitively meanin gful and apparently self-evident. They \nare interchan geably us ed to  refer  to the same concep t. We make a \ndistinction  between  the two , follow ing the definitio ns as given by \nAbeles [1] . \nMusical taste is defin ed as a persons slowly evolving long-term \ncommitmen t to a particular music idio m. Its developmen t is \nassumed to depend on the cultural environmen t, the major Permission to make digi tal or h ard copies of all o r part of this \nwork for p ersonal o r classroom use is g ranted without fee \nprovided that copies are not made o r distributed for p rofit or \ncommercia l advantag e and  that copies bear this notice and the \nfull citation on the first page .  \n© 2002 IRCAM  Centre Pomp idou PATS: Realization and User Evaluation of an Automatic Playlist Generator \nconsensus [3], peer approval, musical training [4], age as an \nindirect factor [5][11] and other personal characteristics. Personal \nmusic acquisition behavior over time is likely to represent the \ndevelopment of a persons musical taste. \nOnthe other hand, m usic preference is defined as a persons \ntemporary liking of particular music content in a particular \ncontext-of-use. It is instantaneous in nature and subordinate to the \nmusical taste of a person. Music is deemed to be preferred if its \nmusical features suit particular activities, moods or listening \npurposes. Therefore, the context-of-use is supposed to produce \nconstraints and opportunities for what music is preferred. It sets \nwhat kind of music should be selected and what kind of music \nshould be rejected. North and Ha rgreaves [10] showed that music \npreference is associated with the listening environment and that \npeople prefer to use different descriptors for music to be listened \nto in different environments. For instance, music for a dance party \nsets up desirable and undesirable criteria on tempo, rhythmic \nstructure, musical instrumentation and performers, which are \nlikely to be different for a romantic evening, for dull or repetitive \nactivities or for car traveling. \nHowever, an indefinite number of contexts-of-use may exist; they \nall produce different criteria for preferred music. In addition, the \nparticular experience to listen to given music does not need to be \nthe same in similar contexts-of-use or a given context-of-use is \nunlikely to be best provided with exactly the same music, over and \nover again. In other words, music preference changes over time. \n2.2 Interactive control of PATS \nWhe n using PATS, the link between a context-of-use and a  \nplaylist is established by c hoosing a single preferred song that is \nused to set up a complete playlist. Thu s, music listeners only have \nto select a song that they currently want to listen to or that they \nprefer in the given context-of-use. This selection requires minimal \ncognitive effort as it m ay be the result of habitual beh avior or \naffect r eferral.People may choose a song that is chosen always in \nasimilar context-of-use, that was selected last time in a similar \ncontext-of-use, or that was given much thought lately. \nAfter selecting a song, P ATS g enerates and p resents a playlist, \nwhich includes the selected song and songs that are similar to the \nselected one. While listening, a music listener indicates what \nsongs in the playlist do not fit the intended context-of-use. As \nonly a decision of rejection is needed for a small number of songs, \nthis task makes only a small demand on memory processes. This \nuser feedback is used by P ATS to learn about music preferences \nof the listener and to adapt its compilation strategy for future \nplaylists. If the system adapts well to a listeners music \npreferences, user feedback is no longer required. Moreover, PATS \ndoes not require any other user control actions. \n2.3 Requirements \nIdeally, PATS s hould make music choices that would have been \nmade by the music listener in case no PATS w as available. \nTherefore, it uses attribute information of music on which human \nchoice is largely based, and generates playlists that are both \ncoherent and varied. \nJazz was chosen as a music domain in this long-term research \nproject, as jazz contains a variety of well-defined styles or time \nperiods serving a diverse listening audience and its appreciation is \nlargely insensitive to temporarily prevailing m usic cultures and \nmovements. \n2.3.1 Attribute representation (m eta-data) of music \nMusic listeners use many different musical attributes for their \nmusic choice. Talking about and judgi ng popular and jazz music \nin terms of musicians, instruments, and music styles is common. It is therefore reasonable to represent songs as a collection of \nattribute-value pairs (meta-data). We have created and collected \nan attribute representation for jazz music of 18 attributes, in total. \nTheir values were primarily extracted from CD booklets, \ndiscographies, books on jazz m usic education and training, a nd \nsystematic listening. A listing of all attributes and an instance is \ngiven in Table 1. \nTable 1 . Attribute repres entation for jazz m usic. \nTitle Title of t he song All blues \nMain artist Leading perform er/band Miles Davi s \nAlbum Title of albu m Kind of blue \nYear Year of release 1959 \nStyle Jazz style or era  postbop \nTempo Global tempo in bpm 144 \nMusicians List o f musici ans Miles D avis, John  \nColtrane, C annonb all \nAdderley, Bill Evans, \nPaul Chamb ers, \nJimmy C obb \nInstrument s List o f instruments trumpet, te nor \nsaxophone, alt o \nsaxophone, p iano, \ndouble bass, dru ms \nEnsemble strengt h No. mu sicians 6 \nSoloists Soloing  musicians Miles D avis, John  \nColtrane, C annonb all \nAdderley, Bill Evans \nCompo ser Com poser of the song Miles Davi s \nProd ucer  Producer of the son g Teo Mac ero, Ray \nMoore \nStandard/C lassic Standard or classic jazz \nsong? Yes \nPlace Record ing place New York \nLive In front of a l ive \naudience? No \nLabel Record  company CBS \nRhythm Rhythmic foundation  6/8 \nProg ressi on Melodic/harmonic \ndevelop ment modal \nResults of a focus group study showed that the set of attributes \nand their values is sufficient to express reported preferences for \njazz music. In this study, participants were instructed to assort a \nset of 22 ja zz songs into a preferred and rejected category and \nverbalize their decisions. Many of the criteria elicited could be \nexpressed as a logical combination of attribute-value pairs. \n2.3.2 Wish for coherence \nCoherence of a playlist refers to the degree of homogeneity of the \nmusic in a playlist and the extent to which individual songs are \nrelated to each other. It does not solely depend on some similarity \nbetween any two songs, but also depends on all other songs in a \nplaylist and the conceptual description a music listener can give to \nthe songs involved. \nCoherence may be based on a similarity between songs such as the \nsharing of relevant attribute values. Whe n choosing music, music \nlisteners tend to focus on relevant attribute values for reducing the \navailable choice set of songs and for making different songs \ncomparable. This includes eliminating songs with less relevant \nattributes values and retaining o nly the ones with the more PATS: Realization and U ser Evaluation of  an A utomatic Playlist Generator \nrelevant attributes values. Choice on the basis of elimination is a \ncommon strategy in every-day choice tasks[13]. For instance, a \nmusic choice strategy is to first reduce the choice set by \neliminating those songs that do not belong to a particular music \nstyle or in which a particular musician did not participate, before \ncontinuing further search. \n2.3.3 Wish for variation \nVariation refers to the degree of diversity of songs in an individual \nplaylist and in successive playlists. It contradicts the requirement \nfor coherence. Variation is a p sychological requirement for \ncontinual music enjoyment by introducing new musical content \nand making t he outcome unpredictable. It produces surprise \neffects at the music listener such as the re-discovery of forgotten \nmusic. \nAs music preference changes over time, the most elementary \nrequirement is that not exactly the same music should be \nrepeatedly presented for a given context-of-use. Also, music \nwithin a playlist should be  varied as the experience of each \nadditional song in a playlist may decrease if it contains features \nthat are already covered by other songs in the list. \n2.4 R ealization \nPATS makes use of a two-step strategy in interaction with the \nuser. First, songs are clustered based on a similarity measure that \nselectively weighs attribute values of the songs.  Clusters are \npresented as playlists to be judged by the user on suitability for a \ndesired context-of-use. Second, an inductive learning algorithm is \nused to uncover the criteria on attribute values that pertain to this \njudgment. The weights of the attribute values involved are \nadjusted acco rdingly for adapting the clustering process. \n2.4.1 Si milarity measure \nIf it is  known that a set of songs is preferred (or fit a given \ncontext-of-use), then it is likely that preference can be generalized \nto other songs based solely on the fact that they are similar. \nAlthough a similarity measure may not provide all explanatory \nevidence for stating preference, it is an essential component for \nproviding some choice structure amongst songs. The used \nsimilarity between songs is based on a weighted sum of their \nattribute similarities. \nLet \n\u0000\u0001 \u0001 \u0001\n\u0002\n21 N\n\u0003 \u0003 \u0003\n\u0004 \u0005= denote the music collection containing N\nsongs. Each song \u0006\n\u0007\ni∈is represented by an arbitrary ordered set \nof Kvalued attributes \b\n\t\n\u000b\nik k \f \f\n\r\f \u000e= = where k\n\u000frefers to \nthe name of the attribute. A song is then represented by a vector \u0010\u0011 \u0011 \u0011\n\u0012\n2 1 iK ii i \u0013 \u0013 \u0013\n\u0007 \u0014= .In our case, the domain of an attribute can \nbe nominal, binary, categorical, numerical or set-oriented. For \nnotational convenience, the value of ) ,, ( \u0015\u0016\n\u0017 \u0018 \u0019\n\u0018 \u0019\u001b\u001a \u0018 \u0019 \u0018 \u0019 \u0018 \u0019v vv V \u0015 = is \nitself a vector of length ik\n\u001c.For most attributes, \u001d=ik\n\u001e,except \nfor set-oriented attributes since they represent the list of \nparticipating musicians or the instrumentation as found on a \nmusical recording. Likewise, no n-negative weight vectors \n) ,, ( \u001f \n! \" #\n\" #\u001b$ \" # \" # \" #w ww W \u0015 = are associated with each attribute \nk\n%and each song i\n&.These weights measure the relevance of an \nattribute value in the computation of the similarity between songs. \nFor nominal, binary or categorical attributes such as titles, person \nnames and music genres, the attribute similarity ),( ')(+*\n,(+*vvs is \neither 1 if the attribute values are identical, or 0 if the values are \ndifferent. More precisely, \n\n≠=\n= -).0/1\n.0/\n-).0/1\n.0/-).0/1\n.0/ 2 2\n2 22 23 45\n467\n48For numeric attributes such as the global tempo in beats per \nminute or year of release, the attribute similarity ),( ')(+*\n,(+*vvs is one \nminus the ratio between the absolute value and the total span of \nthe numerical attribute domain. More precisely, 9\n:\n9+;<\n9+;:\n9+;<\n9+; =?> >> >\n@−\n−= A\nBC\nD\nThe similarity measure ),( E\nFooS between song i\nGand 'ois then \nthe normalized weighted sum of all involved attribute similarities. \nIts value ranges between 0 and 1. More precisely, \n∑∑ ∑∑\n== === ⋅ =\nHI\nJKML\nINKO\nINKL\nINK\nHI\nJKML\nINKOL\nP Q P Q\nw vvs w ooS RSR RSR 1      with ),,( ),( ,\nwhere K is  the num ber of attribute s, ik\nTis the number of values \nfor attribute k\nU,and ) ,( ')(+*\n,(+*vvs denotes the attribute  similarity \nof attribute  k\nUbetween song i\nVand 'o.\nNote that the similarity between any song and itself is identical for \nall s ongs, a nd is  the  maximum possible (i.e., \n1= = ≤ )o,o(S)o,o(S)o,o(S W W\nX XW\nX ). This is evident since it is \nunlikely that a song would be mistaken for another. \nAlso, note that the similarity measure is asymmetric (i.e ., \n),( ),(\n,' '\n, ooS ooS ≠ )because each song ha s its own set of \nweights . Asymmetry in s imilarity refers to the observation that a \nsong i\nVis more similar to a song 'oin one context, wh ile it is  \nthe other way around in another context. It can be produced by the \norder in which songs are compared and wh at song a cts as a \nreference point. The choice of a reference point m akes attribute-\nvalues that are not part of the other song of less concern to the \nsimilarity computation.  Music that is more familiar to the listener \nmay act as such a reference point. Then, for instance, music from \nrelatively unknown artists may be judge d quite similar to music of \nwell-known artists, whereas the converse judgment may be not \ntrue. \n2.4.2 Cl uster method \nThe similarity measure governs the grouping o f songs in a cluster \nmethod. Cluster methods are traditionally based on optimizing a \nunitary performance index such as maximizing the mean with in-\ncluster similarity. We have however the two-edged objective to \ngroup songs adhering both to  the wish for coherence and to  the \nwish for variation. The wis h for coherence can be seen as \nmaximizing within-cluster similarity, whe reas the  wis h for \nvariation should rather decrease this within -cluster similarity. To \nmeet these contrasting requirements, a decentralized clustering \napproach is used in whic h the clustering is  established at the \nlocality of each individual song w ith little external main control of \nthe global clustering process. \nIn this  approach, songs are placed in a two-dimensional Euclidean \nspace of a finite size. The num ber of dimensions is arbitrary. \nSongs move around in discrete time steps at an initially randomly \nchosen velocity. For that, a song ha s been augmented with  \nposition and velocity coordinates. Basically, at each time step, a \nrandomly chosen song senses whether of not any other song is in \nits nearest vicinity. Vicinity is defined as the area that is contained \nina give n circle centered at a songs current position in Euclidean \ndistance sense. Vicinity checking has been realized by a constant \ntime algorithm based on a spatial elimination technique known as \nthe sector method.If the current song finds another song in its \nnearest vicinity, the similarity between the current song and th e \nother is computed. This similarity value is used as a probability PATS: Realization and User Evaluation of an Automatic Playlist Generator \nmeasure to determine whether or not the current song groups with \nthe other. Grouping can be seen as a one-way following relation: \neach song groups only with o ne other song tho ugh multiple songs \ncan group with th e same song.  I t means that the  current song \nadjusts its velocity to the velocity of the other song such that they \nstay close to each other in th e two -dimensional space. It also \nimplies that the grouping o f the current song w ith another can \nhave as side-effects that (1) a  previous grouping in w hich the \ncurrent song was involved will be broken and (2) the songs that \nfollow the current song are also indir ectly invo lved. \nFrom a global perspective, clusters are formed by the grouping \nmechanism and dis solved by  the breaking up of groups (see \nFigur e 1). Since the similarity measure selectively weighs different \nattribute  values of the songs, clusters of songs arise that have \nseveral distinct attribute values in common. This is deemed to \nadhere to the wish for coherence. Since the content of a cluster \nvaries continu ally in tim e, this  is deemed to adhere to the wish for \nvariation. \nEventually, whe n the user selects a preferred song, the cluster in \nwhich this  song is contained is presented as a playlist. Special \nmeasures in the clustering pr ocess are taken to preclude clusters \nfrom becoming too big. \nFigure 1. An ideal cluster result of songs that may represent a \nplaylist suiting a particular context-of-use for listening to \nvocal jazz, modern funky jazz or easy piano jazz (c luster \nlabels are added manually). Songs are rep resented by \ndifferently colored (or shaded) marbles. Similar songs have \nsimilar colors (shades). The lines connecting these marbles \nrepresent the grouping of songs in a cluster. The line width \ndenotes the similarity between two songs.  \n2.4.3 Induc tive learning \nUser feedback consists of the explicit indication of songs in a \nplaylist that do not fit the  intended context-of-use. In this  way, it \nisknown what songs in the  playlist are preferred and what songs \nare rejected. An ind uctive learning algorithm based on the \nconstruction of a decision tree is used to uncover the attribute \nvalues that assort songs into the categories preferred and rejected.\nAdecision tree is incrementally constructed by a greedy, non-\nbacktracking search algorithm in whic h the search is directed by \nan attribute  selection heuristic.  This heuristic is based on local \ninformation about how well an attribute partitions the set of songs (i.e., the current playlist) into the two categories under its values. \nOnly attributes that are not already present in the path from the \nroot to the current point of inve stigation are considered.  The \nincremental nature of the process is characterized by replacing a  \nleaf of the tree under construction by a new sub-tree of depth o ne. \nThis sub-tree consists of a node, whic h carries an attribute that \nprovides the  best po ssible categorization, and br anches that \nrepresent th e partitions along the  values of the attribute. This \nprocess is continue d until partitions contain only songs of one \ncategory or no more songs are left. If no more attributes are left \nwhile the current leaf still contains preferred and rejected songs, \nthe decision tree is indecisive for the songs involved. The \nconstructed tree then contains inte rior nodes and br anches \nspecifying attributes and the ir values along whic h the songs in the \nplaylist we re originally partitioned into  the categories preferred \nand rejected (see Figur e 2). \nFigure 2. Decision trees to uncover the attribute values that \nassort songs into the categories preferred and rejected  for \nfashionable dance mus ic and piano with a small ensemble. \nGive n a decision tree, the categorization of a song starts at the root \nof a tree. Attribute  values at the branches of the tree are compared \ntothe value of the corresponding attribute of the song. A branch is \nthen taken that is appropriate to the outcome of the comparison. \nThis comparison and branching process continu es recursively \nuntil a leaf is encountered at whic h time the predicted category of \nthe song is  known. \nDecision tree construction algorithms differ in the  type of heuristic \nfunction for attribute selection and the branching f actor on each \ninterior node. We  have experimented w ith f our different \nalgorithms: ID3 [9], ID3-IV [9], ID3-BIN that is a variant of ID3 \nwith a binary branching factor and INFERULE [ 12]. \nBasically, the ID3 family of algorithms uses a heuristic that is \nbased on minimizing the  entropy of the set of songs by selecting \nthe attribute  that makes the categories least randomly distribute d \nover the disjoint partitions of the set along its  values. In other \nwords, it selects the attribute that has the highest information gain \n(ratio) heuristic when used to partition a set of songs. On the other \nhand, the  INFERULE  algorithm uses a relative goodness heuristic \nthat selects an attribute value such that the category distribution in \nthe resulting partitions differs considerably from the original set. \nThis heuristic is especially useful if the available attributes are not \nsufficient to  discern category membership f or a given song [12]. \nThis is also typical for our categorization problem for it is very \nunlikely that the set of music attributes used will cover the whole \nrepertoire of music preferences. Since this  heuristic considers \nattribute  values instead of attribute s, the result is a binary decision \ntree. \nAll algorithms were augmented with  strategies to deal with \nattribute s that are not nominal such as num eric attribute s and set-\noriented attributes, strategies to deal with  missing attribute values, \ncases of equal evaluation of attributes (value) under the attribute \nselection heuristic and cases of inde cisive leaves. \nThe four algorithms we re assessed on the ir categorization \naccuracy and the compactness of the resulting d ecision tree using \ndata sets of 300 jazz songs pre-categorized by four participants PATS: Realization and User Evaluation of an Automatic Playlist Generator \nand using tr aining sets of different size to construct the tree. \nCategorization accuracy was defined as the percentage of songs in \nthe complete data set that we re correctly categorized as being \npreferred or rejected. Compactness was defined as the proportion \nof leaves that would be obtained by the least compact decision tree \nthat is possible. The least compact tree is a tree of depth one that \ncaptures each song in a separate leaf. Compact trees have been \ntheoretically proven to yield high categorization accuracy on \nunseen data in a  probabilistic and w orst-case sense [2]. This \nsuggests that it is wise to favor trees with fewer leaves, because \nthese trees are supposed to be better categorizers solely on the fact \nthat they have fewer leaves. \nIn short, the results showed tha t both ID3-BIN and INFERULE \nproduced the most accurate decision trees for categorizing th e data \nsets as being p referred or rejected under various training set sizes. \nIn additio n, INFERULE produced the most compact trees. ID3 \nproduced the  least accurate decision tree as it did no t even exceed \nthe categorization accuracy of a simple categorizer that randomly \nstated a given song as being pr eferred or rejected. \nObvio usly, the INFERULE algorithm was the best choice among \nthe four alternatives to be incorporated in the PATS system. The \ninput to INFERULE is  the playlist in w hich songs are indicated as \npreferred or rejected by the user. The output is a decision tree that \nseparates preferred and rejected songs on the basis of their \nattribute  values. Weights  of all songs in the  collection are now \nadjusted in two  stages, before the clustering is re-started. \nIn the  first stage, the  decision tree is used to categorize the \ncomplete music collection into the predicted categories preferred,\nrejected and indecisive.The latter category is required since there \ncan be indecisive leaves in the  tree. In the  second stage, weights  of \nattribute  values are multiplied by a factor in the  case of preferred \nsongs and divide d by this factor in the  case of rejected songs. The \nfactor is the multiplication of an arbitrary constant w ith \u00002/1−\n\u0001\n,\nwhere ldenotes the level in the tree at which the attribute value \noccurs. The root of the tree is at level 1. It is assumed that attribute \nvalues occurring higher in the tree are more relevant than attribute \nvalues at lower regions of the tree. The weights of indecisive \nsongs are left unc hanged. \n3. USE R EVALUATION \nAcontrolled user experiment examined the quality of PATS-\ncompiled playlists and randomly assembled playlists. Participants \njudged the quality of both type of playlists in two  different \ncontexts-of-use over four experimental sessions. Playlist quality \nwas measured by precision,coverage and a rating score.A post-\nexperiment inte rview wa s used to yield supplementary findin gs on \nperceived usefulness of automatic music compilation. \n3.1 Hy potheses \nThe qua lity of PATS-generated playlists should be  higher than \nrandomly assembled playlists irrespective of a given context-of-\nuse. It is hypothesized tha t \n1. Playlists compiled by PATS contain more preferred songs \nthan randomly assembled playlists, irrespective of a given \ncontext-of-use.  \n2. Similarly, PATS playlists are rated highe r than randomly \nassembled playlists, irrespective of a given context-of-use. \nPATS playlists should adapt to  a music preference in a given \ncontext-of-use. It is hypothesized tha t \n3. Successive playlists compiled by PATS contain an increasing \nnumber of preferred songs.  \n4. Similarly, successive PATS playlists are successively rated \nhighe r.  Finally, PATS playlists should cover more relevant m usic over \ntime of use than randomly assembled playlists. It is hypothesized \nthat \n5. Successive playlists compiled by PATS contain more distinct \nand preferred songs than randomly assembled playlists. \n3.2 Me asures \nThree measures for playlist quality were defined: precision,\ncoverage,and a rating score.\nPrecision was defined as the proportion of songs in a playlist that \nsuits the given context-of-use. Ideally, the precision curve should \napproach 1, meaning adequate adaptation to a given context-of-\nuse. \nCoverage was defined as the cumulative number of songs that \nsuits the given context-of-use and tha t was not already present in \nprevious playlists. Ove r successive playlists, the coverage measure \nisa non-decreasing curve. Ideally, this  curve should approach the \ntotal number of songs in a ll successive playlists, meaning ne arly \ncomplete coverage of preferred material given the num ber of \nplaylists. \nThe rationale of precision and coverage is that it is  very likely that \nmusic listeners wish a single playlist to adequately reflect their \nmusic preference as well as that successive playlists cover as much \ndifferent music reflecting the ir preference as possible. \nArating score was defined as the participants rating o f a playlist. \nThis score was defined on a scale ranging from 0 to 10 similar to \nthe traditional ordinal report-mark on Dutch elementary school (0 \n=extremely bad, 1 = very bad, 2 = bad, 3 = very insufficient, 4 = \ninsufficient, 5 = almost sufficient, 6 = s ufficient, 7 = fair, 8 = \ngood, 9 = v ery good, 10 = e xcellent). \nThe post-experiment inte rview po sed a single question concerning \nperceived usefulness of an automatic playlist generator (translated \nfrom Dutch): D o you find a feature that automatically compiles \nmusic for you a useful feature? \n3.3 Me thod \n3.3.1 In struction \nParticipants were not informed about the  actual purpose of the \nexperiment being a  comparison between two different playlist \ngeneration methods. Instead, the y were told tha t the research was \naimed at eliciting on wh at criteria people appraise music. They \nwere informed about the  global experimental procedures and the \ntest material, and p repared for the relatively high demands for \nparticipation in the  experiment since they had to return on four \nseparate days, preferably within  one week. \nThe two  contexts-of-use in the  experiment were described to the \nparticipants as a lively and loud atmosphere such as dance music \nfor a party and a soft atmosphere such as background music at a \ndinner. \nAt the first day, they were asked to imagine and de scribe personal \ninstantiations of the two  contexts-of-use, that is, the general \ncircumstances in which the music would be heard.  Three small \ntasks were inte nded to elicit some desirable properties of music \nsuited in  one of the two contexts-of-use. In the first task, \nparticipants completed a form in w hich they were asked to  \ndescribe wha t music would be appropriate in the given context-of-\nuse. In the second task, they were asked to compile a playlist by \npaper and pencil; they could select music from a list. Concluding, \nparticipants had to select a song from a list that they would \ndefinitely want to listen to in the given context-of-use. The list \nwas alphabetically ordered by musicians and contained all songs \ninthe collection. They had to do these tasks twice for each \ncontext-of-use separately. So, the  results of these tasks were PATS: Realization and User Evaluation of an Automatic Playlist Generator \npersonal instantiations of the two  different contexts-of-use, an \nelicitation of the music that wo uld fit the  contexts-of-use and a \nhighl y preferred song for each context-of-use. \nFor all four days, they were instructed to restrict their music \nlistening be havior to the instantiation of each context-of-use. \nAlso, the same highl y preferred song was used to set up a \nplaylist for a given context-of-use. \n3.3.2 In teractive system \nAn interactive computer application wa s implemented to listen \nand judge a playlist by using a standard mouse and a graphical \nuser interface. Title, and names of composers and artists of a song \nwere shown. Songs in a playlist were not displayed list-wis e, but \nwere presented one-by-one. Controls for common music play \nfeatures and for going through a  playlist we re provided. Also, \nbuttons for indicating pr eference in terms of good and bad per \nsong in  the playlist were provided. \nParticipants were instructed how to operate the interactive system. \nInformation about inte ractive procedures to follow dur ing an \nexperimental session was readily available to the participants \nduring the whole experiment. \n3.3.3 D esign \nAfactorial with in-subject design with  three independent variables \nwas applied. The first ind ependent va riable playlist generator \nreferred to the method used for music compilation, that is, PATS \nor random. The second in dependent variable context-of-use \nreferred to the two pre-defined contexts-of-use, that is, soft music \nand lively music. The order in whic h the levels of context-of-use \nand playlist generator were applied wa s counterbalanced. The \nthird ind ependent variable session referred to the four \nexperimental sessions in wh ich playlists were listened to in a \ngiven context-of- use. These sessions were intended to measure \nadaptive properties and long-term use of the compilation strategies \ninterms of changes in playlist quality as a function of time. \n3.3.4 Test material and e quipment \nAmusic database comprising 3 00 one-minute excerpts of jazz \nsongs (MPEG-1 Part 2 Layer II 128 Kbps  stereo) from 100  \ncommercial CD albums served as test material. The music \ncollection covered 12 popular jazz styles. These styles cover a \nconsiderable part of the whole jazz period. Each style contained \n25 songs. Pilot experiments showed that the shortness and sound \nquality of the excerpts did not negatively influence judgment. The \ntest equipment c onsisted o f a SUN Sparc-5 wo rkstation, \nAPC/CS4231 codec audio chip, and two Fostex 630 1 B pe rsonal \nmonitors (combined amplifier and loudspeaker system). \nParticipants were seated be hind a  desk in front of a 17-inc h \nmonitor (Philips Brilliance 17A ) in a sound-pr oof experimental \nroom. They could adjust the audio volume to a preferred level. \nBoth the  mouse pad and the monitor were positioned at a \ncomfortable working level. \n3.3.5 T ask \nThe task was to listen to a set of 11 songs (one-minute  excerpts) \nthat made up a playlist, while imaginin g a fixed and pre-defined \ncontext-of-use. Due to the size of a playlist, jud gments of the \nsongs were collected by presenting the m in series. The songs were \nshown one at the time. Participants only had to decide whic h song \ndid not fit the desired context-of-use, if at all. In the process of \nlistening, participants were allowed to compare songs freely in any \ncombination and cancel any judgement already expressed. There \nwere no time restrictions. 3.3.6 P rocedure \nParticipants took part in eight experimental sessions on four \nseparate days, preferably with in one week. The first session started \nwith instructions and a questionnaire to record personal data and \nattribute s. Use of the inte ractive system was explained and \ndemonstrated. At each session, pa rticipants were alternately \npresented a PATS and a randomly assembled playlist with a pause \ninbetween. In four consecutive  sessions, pa rticipants we re \ninstructed to perform music listening ta sks by considering a fixed \nand pre-defined context-of-use. At the start of every four sessions, \nparticipants completed a form in which they described their \ncontext-of-use and what music would be  appropriate in that \ncontext-of-use. In addition, they were asked to select a song from \nthe music collection that they definitely would listen to in th e \ngiven context-of-use. Both this song and the context-of-use had to \nbe recalled each tim e a new experimental session started. A PATS \nand a randomly assembled playlist was automatically generated \nround the selected song and pr esented to the participant. Then, a \nlistening and judgm ent task for the given playlist started. Whe n \nparticipants had completed a task, the inte ractive system was \nautomatically shut down. \nAfter completing e ach judgment task, participants were asked to \nrate the playlist just listened to, on a scale ranging from 0 to 10. \nAt the end of the experiment, a small interview was conducted. \n3.3.7 P articipants \nTwenty participants (17 males, 3 f emales) took part in the \nexperiment. They were recruited by advertisements and all got a \nfixed fee. All participants were frequent listeners to jazz music; for \nadmission to the experiment, they had to be able to freely recall \neight jazz m usicians, rank them on pe rsonal taste and m ention \nnumber of recordings (CD albums, tapes) owned for each \nmusician. The average age of the participants was 26 years (min.: \n19, max.: 39). A ll participants had completed higher vocational \neducation. Sixteen participants played a musical instrument. \n3.4 R esults \nPlaylists contained 11 songs from which one was selected by the \nparticipant. This song was excluded from the data as this song was \nnot determined by the system, leaving 10 songs per playlist to \nconsider for analysis. \n3.4.1 P recision \nThe results for the precision measure are shown in Figur e 3. \nFigure 3.  Mean precision  (and standard erro r) of the playlists \nin different contex ts-of- use.The left-hand panel (a) shows \nmean precision  for both playlist generators (PATS and \nrandom) in the soft music context-of-use. The right-hand \npanel (b) shows mean precision  for both generators in the \nlively music context-of-use.\nAMANOVA analysis with repeated measures was conducted in \nwhich session (4), context-of-use (2), and playlist generator (2) \nwere treated as within-s ubject independent variables. Precision PATS: R ealization and User Evaluation of an Automatic Playlist Generator \nwas dependent variable. A main effect for playlist generator was \nfound to  be signif icant (F(1,19) = 89.766, p < 0. 0001). Playlists \ncompiled by PATS contained more preferred songs than randomly \nassembled pl aylists (mean precision:0.69 (P ATS), 0.45 \n(random)). A  main effect for context-of-use was found to be \nsignif icant (F(1,19) = 13.842, p < 0. 005). Playlists for the soft \nmusic context-of-use contained m ore preferred songs (mean \nprecision:0.63 (soft music), 0.51 (lively music)). An interaction \neffect for playlist generator by session was just not signif icant \n(F(3,17) = 2 .675, p  = 0.08), whereas, in the univa riate test, it was \nfound to be signif icant (F(3,57) = 2.835, p < 0. 05). Fur ther \nanalysis of this interaction effect revealed a signif icant difference \ninmean precision between the fourth PATS playlist and mean \nprecision of preceding P ATS playlists in c ontrast to randomly \nassembled playlists (F(1,19) = 8 .935, p  < 0.01). In other words, \neach fourth PATS playlist contained more preferred songs than the \npreceding three PATS playlists (mean precision of fourth PATS \nsession: 0.76; mean precision of the first three PATS sessions: \n0.67). No other effects were found to be significant. \n3.4.2 Cov erage \nThe results for the coverage measure are shown in Figur e 4. \nFigure 4. Mean cover age (and standard erro r) of the playlists \nin different contexts-of-use. Recall that cover age is a \ncumulative measure. The left-hand panel (a) shows mean \ncover age for both playlist generators (PATS and random) in \nthe soft music context-of-use. The right-hand panel (b) shows \nmean covera ge for both generators in the lively music \ncontext-of-use. Note the maximally achievable cover age in four \nsuccessive playlists is 40. \nAMANOVA analysis with repeated measures was conducted in \nwhich session (4), playlist generator (2), and context-of-use (2) \nwere treated as within -subject independent va riables. Coverage \nwas dependent variable. A main effect for playlist generator was \nfound to be signif icant (F( 1,19) = 6 3.171, p < 0.001). More \ndistinct and preferred songs were present in  successive PATS \nplaylists than in successive randomly assembled playlists (mean \ncoverage at fourth session: 22.0 (PATS), 17.3 (random)). A main \neffect for context-of-use was found to be signif icant (F(1,19) = \n13.523, p < 0.005). It appeared tha t playlists for the soft music \ncontext-of-use contained more distinct and preferred songs (mean \ncoverage at fourth session: 21.8 (soft music), 17. 5 (lively music)). \nAmain effect for session was found to  be significant (F(3,17) = \n284.326, p < 0.001). More particularly, the coverage curves for all \nconditions showed a signif icantly linear course over sessions \n(F(1,19) = 8 52.268, p < 0.001). Also, an interaction effect for \nplaylist generator by session was found to be signif icant (F(3,17) \n=7.602, p < 0 .005). Successive playlists compiled by PATS \ncontained more varied preferred songs than randomly assembled \nplaylists. Likewise, the slopes of the coverage curves for PATS \nplaylists appeared to be signif icantly higher than for randomly \nassembled playlists (coverage slope: 5.2 (PATS), 4.3 (random)). \nFor each new playlist, PATS added five preferred songs that were not already contained in earlier playlists. For comparison, the \nrandom approach added four songs. No other effects were found \nto be signif icant. \n3.4.3 R ating score \nThe results for the rating score are shown in Figur e 5. \nFigure 5. Mean rating scor e (and standard erro r) of the \nplaylists in different contexts-of-use. The left-hand panel (a) \nshows m ean rating for both playlist generators (PATS and \nrandom) in the soft music context-of-use. The right-hand \npanel (b) shows mean rating scor e for both generators in the \nlively music context-of-use.  \nAMANOVA  analysis was conducted in whic h playlist generator \n(2), context-of-use (2), and session (4) we re treated as with in-\nsubject inde pendent variables. Rating score wa s dependent \nvariable. A signif icant main effect for playlist generator was \nfound (F(1,19 ) = 85.085, p  < 0.001). Playlists compiled by PATS \nwere rated higher than randomly assembled playlists (mean rating \nscore:7.3 (PATS), 5.3 (random)). In normative terms, PATS \nplaylists can be characterized as more than fair and randomly \nassembled playlists as almost sufficient. A signif icant m ain \neffect for context-of-use was found (F( 1,19) = 12.574, p < 0.005). \nPlaylists for the soft music context-of-use were rated highe r \n(mean rating score:6.6 (soft music), 6.1 (lively music)). N o other \nsignif icant effects were found. \n3.4.4 In terview \nThe post-experiment interview yielded relevant supplementary \nfindings about the perceived us efulness of automatic music \ncompilation. Of the 20 participants, twelve participants (60% ) told \nthat they would appreciate and us e an automatic playlist generator; \nthey commented tha t it would easily acquaint the m with  varying \nmusic styles and artists and w ould be a means to adequately cover \ntheir personal music collection. Two participants explained their \nappraisal by referring to  easy searching in an ever-increasing \nnumber of songs. The other eight participants rejected th e \nusefulness of such a system. Their main objection was a loss of \ncontrol in music selection, tho ugh o ne of these participants found \nautomatic playlist generation relevant for cafes and department \nstores. \n3.5 Discussion \nAuser experiment e xamined the  qua lity of PATS-generated \nplaylists and randomly assembled playlists. PATS playlists \nappeared to contain m ore preferred songs and were rated highe r \nthan randomly assembled playlists in both contexts-of-use (see \nHypothesis 1). In addition, PATS playlists appeared to contain \nmore preferred songs that were not already contained in pr evious \nplaylists than randomly assembled playlists (see Hypothesis 2). \nFor each new playlist, PATS found five preferred songs that were \nnot already contained in e arlier playlists. There we re no \nindications that PATS would deteriorate in findin g new preferred \nmusic for future playlists. PATS: Re alization and User Evaluation of an Automatic Playlist Generator \nIn contrast to what was stated in Hy potheses 1 and 2, soft music \nplaylists appeared to contain more preferred and more varied \nmusic than lively music playlists. Soft music playlists were \nalso rated higher than lively music playlists. As this context-of-\nuse effect bo th c oncerned PATS and randomly assembled \nplaylists, the two most likely explanations are that (1) m ore soft \nmusic was apparently available in the music collection than \nlively music or (2) a preference for soft music is apparently \neasier to satisfy than a preference for lively music. \nThe fourth PATS playlist appeared to contain one more preferred \nsong than the  first three PATS playlists, which indic ates that \nPATS playlists adapted to a given context-of-use (see Hypothesis \n3). Ho wever, successive PATS playlists we re not rated \nincreasingly highe r. This indic ates that im provement of the \nplaylists was objectively measurable, though it w as too small to \nget noticed by the participants in the current experimental design. \nParticipants were not told that the  experiment wa s actually a \ncomparison between two different playlist generation methods. It \nislikely that they observed the playlists as coming from one \nmethod.  In additio n, the  two methods were alternately presented \ntothe participants. To measure any perceived improvement, it is \nbetter to explicitly oppose the methods over time. \nIt was found that a more than half of the participants would use \nautomatic music compilation, though it is evident that user control \nshould be an essential property of any automatic feature. \n4. C ONC LUSION \nOnce music listeners have put tim e and effort to construct a large \npersonal collection of music, they should be provided with means \ntoorganize their music collection to ease selection later on. By \ngenerating c oherent and varied playlists for different contexts-of-\nuse, PATS can contribute to a new and pleasant interactive means \ntoexplore and organize the ample music selection and listening \nopportunities of a large personal music collection. The automatic \n(pre-)creation and saving o f playlists can also be seen as a way to \norganize your music collection suited to each possible listening \noccasion. \nMusic listeners may use various strategies when choosing music \nfrom a wide assortment of songs by inspecting various sources and \npresentations of information. Kn owing on wh at grounds and in  \nwhat ways music listeners like to organize and select their music is \nessential to the making of usable and viable products and services \nfor music listening. \n4.1 PA TS applications \nFor demonstration purposes, several research prototype music \nsystems have been implemented that have the PATS functionality \ninside. We will discuss three of them. \nAversion of the open source FreeAmp MP3 jukebox player has \nbeen extended with  the PATS playlist creation feature (see Figur e \n6). PATS playlists can be generated (by selecting a single song \nand pressing a  single button), adjusted and saved to establish a \nmusic organization based on the  concept of context-of-use. This \nplayer also provides acce ss to a free on-line service for meta-data \nof CD albums. Interactive forms for the input o f additio nal meta-\ndata information are implemented as well. \nAmulti-modal inte raction style based o n a slotmachine \nmetaphor[6] presents songs on four rollers that can be \nmanipulated by a force feedback trackball (see Figure 7). By \nrolling the  trackball laterally, one can hop from one roller to \nanother. By rolling the  trackball forwards or backwards, one can \nmanipulate a single roller. A press on the trackball provides \nspoken information  about th e music and the playback being \ntoggled on or off. Double-pressing the trackball means adding or removing a  song to or from a personally created playlist located at \nthe first, left-most roller. Each time a song on the third roller is at \nthe front, a small PATS playlist is generated on the basis of that \nsingle song and shown o n the fourth, right-m ost roller. \n \nFigure 7. The PATS slotmachine jukebox. The PATS \ngenerated playlists are shown on the right-hand roller on the \nbasis of the currently selected song on the high-lighted roller. \nAPhilips Pronto remote control device with  a modified touch \nscreen interface provides direct and remote access to a music \nserver. This server incorporates PATS, essential features for music \nplayback and spoken information feedback about the music by \nusing text-to-speech and langua ge generation from the music \nmeta-database (see Figur e 8). \nFigure 8. The PATS pronto device. \n5. ACKNOWL EDGE MENTS \nThanks go to Dunja Obe r for running the experiment and to all \nparticipants in the  experiment. Figure 6. The PATS-enhanced FreeA mp MP3 player. PATS: Re alization and User Evaluation of an Automatic Playlist Generator \n6. RE FERENCES \n[1] Abeles, H.F. (1980). Responses to music. In: Hodges, D.A. \n(Ed.), Handbook of music psychology,Lawrence, KS : \nNational Association of Music Therapy, 105 -140. \n[2] Fayyad, U., and Irani, K. (1990). Wha t should be minimized \nin a decision tree? In: Dietterich, T., and Swartout, W. (Eds.), \nProceedings of the Eighth National Conference on Artificial \nIntelligence, Volume 2, AAAI-90, Boston, Mas sachussets, \nUSA, July 29  August 3, 1990,Menlo Park: AAAI Press / \nMIT Press, 749-75 4. \n[3] Furman, C.E., and D uke, R.A. (1988). Effects of majority \nconsensus on preferences for recorded orchestral and popular \nmusic. Journal of Research in Mu sic Education,36, 4, 220-\n231. \n[4] Geringer, J.M. (1982). Verbal and operant music listening in \nrelationship to  age and musical training, Psychology of music \n(special issue),47-50.  \n[5] Holbrook, M.B., and S chindler, R.M. (1989). Some \nexploratory findin gs on the  development of musical tastes. \nJournal of Consumer Research, 20,119-12 4. \n[6] Pauws, S., Bouwhuis, D., and Eggen, B.  (2000).  \nProgramming and enjoying m usic with  your eyes closed. In: \nTurner, T., Szwillus, G., Czerwinski, M., and P aterno, F \n(Eds.), CHI 2000 C onference Proceedings,1-6 A pril, 2000, \nthe Hague, the Netherlands, 376 -383. [7] Pauws, S.C., and Egg en, J.H. (1996). New functionality for \naccessing digita l media: Personalised Automatic Track \nSelection. In: Bl andford, A ., and Timbleby, H. , (Eds.), \nHCI96, Industry day & Adjunct Proceedings,London, UK,  \nAugust 20-23, 19 96, Lo ndon: M iddlesex Unive rsity, 127-\n133. \n[8] Peynircioglu, Z.F., Tekcan, A.I., Wa gner. J.L., Baxter, T.L., \nand Shaffer, S.D. (199 8). Name or hum that tune: Feeling o f \nknowing for music. Memory & C ognition, 26, 6, 1131-113 7. \n[9] Quin lan, J.R. (19 86). Induction of decision trees. Machine \nLearning, 1,81-106 . \n[10] North, A.C., and Hargreaves, D.J.  ( 1996). Situational \ninfluences o n reported musical p references. \nPsychomusicology, 1 5,30-45.  \n[11] Rubin, D .C., Rahhal, T.A., and Poon, L.W. (19 98). Things \nlearned in early adulthood are remembered best. Memory & \nCognition, 26, 1,3-19. \n[12] Spangler, S., Fa yyad, A .M., and Uthu rusamy, R. (1989). \nInduction of decision trees from inconclusive data. In: Segre, \nA.M. (Ed. ), Proceedings of the Sixth International Workshop \non Machine Learning, Ithaca, New York, USA, June 26-27, \n1989,San Mateo, CA: Morgan Kaufmann, 146 -150. \n[13] Tversky, A. (1972) . Elimination by aspects: a theory of \nchoice.Psychological Review, 76,31-48."
    },
    {
        "title": "Toward Automatic Music Audio Summary Generation from Signal Analysis.",
        "author": [
            "Geoffroy Peeters",
            "Amaury La Burthe",
            "Xavier Rodet"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417885",
        "url": "https://doi.org/10.5281/zenodo.1417885",
        "ee": "https://zenodo.org/records/1417885/files/PeetersBR02.pdf",
        "abstract": "This paper deals with the automatic generation of music audio sum- maries from signal analysis without the use of any other information. The strategy employed here is to consider the audio signal as a suc- cession of “states” (at various scales) corresponding to the structure (at various scales) of a piece of music. This is, of course, only applicable to certain kinds of musical genres based on some kind of repetition. From the audio signal, we first derive dynamic features representing the time evolution of the energy content in various frequency bands. These features constitute our observations from which we derive a representation of the music in terms of “states”. Since human seg- mentation and grouping performs better upon subsequent hearings, this “natural” approach is followed here. The first pass of the pro- posed algorithm uses segmentation in order to create “templates”. The second pass uses these templates in order to propose a structure of the music using unsupervised learning methods (K-means and hidden Markov model). The audio summary is finally constructed by choosing a represen- tative example of each state. Further refinements of the summary audio signal construction, uses overlap-add, and a tempo detection/ beat alignment in order to improve the audio quality of the created summary.",
        "zenodo_id": 1417885,
        "dblp_key": "conf/ismir/PeetersBR02",
        "keywords": [
            "automatic generation",
            "music audio summarization",
            "signal analysis",
            "audio signal",
            "state representation",
            "musical genres",
            "repetition",
            "human segmentation",
            "unsupervised learning",
            "audio quality"
        ],
        "content": "TowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\nTowardAutomaticMusic AudioSummary Generation\nfromSignal Analysis\nGeoffroyPeeters\nIRCAM\nAnalysis/Synthesis Team\n1, pl. Igor Stravinsky\nF-75004Paris - France\npeeters@ircam.frAmaury La Burthe\nIRCAM\nAnalysis/SynthesisTeam\n1,pl. Igor Stravinsky\nF-75004 Paris - France\nlaburthe@ircam.frXavierRodet\nIRCAM\nAnalysis/Synthesis Team\n1,pl. Igor Stravinsky\nF-75004 Paris - France\nrod@ircam.fr\nABSTRACT\nThispaperdealswiththeautomaticgenerationofmusicaudiosum-\nmariesfromsignalanalysiswithouttheuseofanyotherinformation.\nThestrategyemployedhereistoconsidertheaudiosignalasasuc-\ncessionof“states”(atvariousscales)correspondingtothestructure\n(at various scales) of a piece of music. This is, of course, only\napplicabletocertainkindsofmusicalgenresbasedonsomekindof\nrepetition.\nFromtheaudiosignal,weﬁrstderivedynamicfeaturesrepresenting\nthetimeevolutionoftheenergycontentinvariousfrequencybands.\nThese features constitute our observations from which we derive a\nrepresentation of the music in terms of “states”. Since human seg-\nmentation and grouping performs better upon subsequent hearings,\nthis “natural” approach is followed here. The ﬁrst pass of the pro-\nposed algorithm uses segmentation in order to create “templates”.\nThesecondpassusesthesetemplatesinordertoproposeastructure\nof the music using unsupervised learning methods (K-means and\nhiddenMarkovmodel).\nThe audio summary is ﬁnally constructed by choosing a represen-\ntative example of each state. Further reﬁnements of the summary\naudio signal construction, uses overlap-add, and a tempo detection/\nbeat alignment in order to improve the audio quality of the created\nsummary.\n1. INTRODUCTION\nMusic summary generation is a recent topic of interest driven by\nbothcommercialneeds(browsingofonlinemusiccatalogues),doc-\numentation (browsing over archives) as well as music information\nretrieval (understanding musical structures). As a signiﬁcant factor\nresulting from this interest, the recent MPEG-7 standard (Multime-\ndiaContentDescriptionInterface)[10],proposesasetofmeta-data\nin order to store multimedia summaries: the Summary Description\nScheme (DS). This Summary DS provides a complete set of tools\nallowingthestorageofeithersequentialorhierarchicalsummaries.\nHowever, while the storage of audio summaries has been normal-\nized,fewtechniquesexistallowingtheirautomaticgeneration. This\nis in contrast with video and text where numerous methods and ap-\nproachesexistfortheautomaticsummarygeneration. Mostofthem\nassessthat the summary can be parameterized at three levels[8]:\nThe type of the source (in the case of music: the musical\ngenre) to be summarized. In this study, we are addressing\nmusic audio summary without any prior knowledge of the\n“music”. Hence, we will only use the audio signal itself and\ninformationwhich can be extractedfrom it.\nThe goal of the summary Thegoalisnotaprioridetermined.\nPermission to make digital or hard copies of all or part of this\nworkforpersonalorclassroomuseisgrantedwithoutfeeprovided\nthat copies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on\nthefirst page. c°2002IRCAM - Centre PompidouA documentalist and a composer for example do not require\nthe same information. We therefore need to get the “music”\nstructure, to be able to select which type of information we\nwantforthesummary. Itisimportanttonotethatthe“perfect”\nsummary does not exist since it at least depends directly on\nthe type of information sought.\nThe output format Itconsistsmainlyofanaudioexcerpt. Ad-\nditionalinformationcanalsobeprovidedasisthecaseinthe\nrealm of video where manytechniques [1, 5, 13] propose ad-\nditional information, by means of pictures, drawings, visual\nsummary,etc... Thesameisfeasibleinaudiobyhighlighting,\nfor example, parts of the signal or its similarity matrix [7] in\norder to locate the audio excerptin the piece of music.\n2. AUTOMATIC AUDIO SUMMARY GEN-\nERATION\nVarious strategies can be envisioned in order to create an audio\nsummary: time-compressed signal, transient parts signal (highly\ninformative), steady parts signal (highly representative), symbolic\nrepresentation (score, midi ﬁle, etc ...). Our method is based on\nderiving musical structures directly from signal analysis without\ngoing into symbolic representations (pitch, chords, score, ...). The\nstructures are then used in order to create an audio summary by\nchoosing either transient or steady parts of the music. The choice\nof this method is based on robustness and generality (despite it is\nrestricted to certain kind of musical genre based on repetition) of\nthe method.\n2.1 State of the art\nFewstudiesexistconcerningtheAutomaticMusicAudioSummary\nGeneration from signal analysis. The existing ones can be divided\ninto twotypes of approache.\n2.1.1 “Sequences” approach\nMost of them start from Foote’s works on similarity matrix .\nFoote showed in [7] that a similarity matrix applied to well-chosen\nfeatures allows a visual representation of the structural information\nof a piece of music. The signal’s features used in his study are\nthe Mel Frequency Cepstral Coefﬁcients (MFCC) which are very\npopular in the ASR community. The similarity s(t1; t2)of the\nfeature vectors at time t1andt2can be deﬁned in several ways:\nEuclidean, cosine, Kullback-Leibler distance, ... The similarity of\nthe feature vectors over the whole piece of music is deﬁned as a\nsimilarity matrix S= [s(ti; tj)]i; j= 1; :::; I. Since the distance\nis symmetric, the similarity matrix is also symmetric. If a speciﬁc\nsegmentofmusicrangingfromtimes t1tot2isrepeatedlaterinthe\nmusicfrom t3tot4,thesuccessionoffeaturevectorsbetween [t1; t2]\nissupposedtobeidentical(closeto)theonesbetween [t3; t4]. This\nisrepresentedvisuallybyalower(upper) diagonal inthesimilarity\nmatrix. An example of a similarity matrix estimated on a popular\nmusicsong(Moby“NaturalBlues”)isrepresentedinFigure1[top].\nThe ﬁrst 100 s of the music are represented. In this ﬁgure, we see\ntherepetitionofthesequence t= [0 : 18] att= [18 : 36] ,thesame\nis true for t= [53 : 62] which is repeated at t= [62 : 71] . Most\nof works on Automatic Music Audio Summary Generation startsTowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\nfromthissimilaritymatrixusingeitherMFCCparameterization[3],\npith or chromagram [4] features. They then try to detect the lower\n(upper)diagonalsinthematrixusingvariousalgorithms,andtoﬁnd\nthemost representativeor the longest diagonals.\n2.1.2 “States” approach\nAstudyfromCompaq[9]alsousesthisMFCCparameterizationin\norder to create “key-phrases”. In this study, the search is not for\nlower(upper) diagonal(successionofevents)butforstates (collec-\ntion of similar and contiguous states). The song is ﬁrst divided into\nﬁxedlengthsegmentswhicharethengroupedaccordingtoacross-\nentropymeasure. Thelongestexampleofthemostfrequentepisode\nconstitutesthe“key-phrase”usedforthesummary. Anothermethod\nproposedby[9],closetothemethodproposedby[2],isbasedonthe\ndirect use of a hidden Markov model applied to the MFCC. While\ntemporalandcontiguitynotionsarepresentinthislastmethod,poor\nresultsare reported by the authors.\n2.1.3 Conclusion\nOneofthekeypointsofalltheseworksstandsintheuseofstaticfea-\ntures(MFCC, pitch, chromagram) as signal observation. ‘Static”\nfeaturesrepresentthesignalaroundagiventime,butdoesnotmodel\nany temporal evolution. This implies, when looking for repeated\npatternsinthemusic,thenecessitytoﬁndidenticalevolutionofthe\nfeatures(throughthesearchof“diagonals”inthesimilaritymatrix),\nor the necessity to averages features over a period of time in order\ntoget states.\n3. EXTRACTIONOFINFORMATIONFROM\nTHE SIGNAL\nThe choice of signal features used for similarity matrix or sum-\nmary generation plays an essential role in the obtained result. In\nour approach, the features used are “dynamic” , i.e. they model\ndirectly the temporal evolution of the spectral shape over a ﬁxed\ntimeduration. Thechoiceofthedurationonwhichthemodelingis\nperformed, determines the kind of information that we will be able\ntoderivefrom signal analysis.\nThis is illustrated on Figure 1 for the same popular music song\n(Moby “Natural Blues”) as before. On Figure 1 [middle], a short\nduration modeling is performed which allows deriving sequence\nrepetitionthroughupper(lower)diagonals. Comparedtotheresults\nobtained using MFCC parameterization (Figure 1 [top]), we see\nthatthemelodysequence t= [0 : 18] isinfactrepeatednotonlyat\nt= [18 : 36] butalsoat t= [36 : 54] ,t= [71 : 89] ,... Thiswasnot\nvisible using the MFCC because at time t= 36the arrangement of\nthe music changes which masks the repetition of the initial melody\nsequence. Note that the features sample rate used here is only 4\nHz (compared to 100 Hz for the MFCC). On Figure 1 [bottom], a\nlongdurationmodelingisusedinordertoderivethestructureofthe\nmusic such as introduction/verse/chorus/... In this case, the whole\nmusic(250s)isrepresented. Notethatthefeaturessamplerateused\nhereis only 1 Hz.\nInFigure2,weshowanotherexampleoftheuseofdynamicfeatures\non the title “Smells like teen spirit” from artist Nirvana. The [top]\npanel shows the similarity matrix obtained using MFCC features.\nThe [middle] panel shows the same using dynamic features with a\nshort duration modeling. We see the repetition of the guitar part (at\nt= 25andt= 30), the repetition of the verse melody (at t= 34\nandt= 42), the bridge, then the repetion of the chorus melody (at\nt= 67,t= 74,t= 82) and ﬁnally the break at t= 91. The\n[bottom] panel, illustrates the use of a long duration modeling for\nstructurerepresentation.\nSeveral advantages come from the use of dynamic features: 1) for\nan appropriate choice of the modeling’s time duration, the search\nfor repeated patterns in the music can be far easier, 2) the amount\nof data (and therefore also the size of the similarity matrix) can be\n102030405060708090102030405060708090\n−0.8−0.6−0.4−0.200.20.40.60.8\n 13  25  38  50  63  75  88 100 13 25 38 50 63 75 88100\n00.10.20.30.40.50.60.70.80.9\n 50 100 150 200 50100150200\n00.10.20.30.40.50.60.70.80.9Figure 1: Similarity matrix computed using [top]\nMFCC features, [middle] Dynamic features with\nshort duration modeling, [bottom] Dynamic fea-\ntures with long duration modeling, on title “Natural\nBlues” from artist MobyTowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\n 10 20 30 40 50 60 70 80 90100 10 20 30 40 50 60 70 80 90100\n−0.8−0.6−0.4−0.200.20.40.60.8\n 12.5  25  37.5  50  62.5  75  87.5 100 12.5 25 37.5 50 62.5 75 87.5 100\n00.10.20.30.40.50.60.70.80.9\n50 100 150 200 25050100150200250\n00.10.20.30.40.50.60.70.80.9\nFigure 2: Similarity matrix computed using [top]\nMFCC features, [middle] Dynamic features with\nshort duration modeling, [bottom] Dynamic features\nwith long duration modeling, on title “Smells like\nteen spirit” from artist Nirvanagreatlyreduced: fora4minutelongmusic,thesizeofthesimilarity\nmatrix is around 24000*24000 in the case of the MFCC, it can be\nonly 240*240 in the case of the “dynamic” features.\nIn the following, we will concentrate on the use of dynamic fea-\ntures for structural representation. Since the information derived\nfrom signal analysis is supposed to allow the best differentiation\nof the various structures of a piece of music, signal features have\nbeen selected from a wide set of features by training the system\non a large hand-labeled database of various musical genres. The\nfeatures selected are the ones which maximize the mutual informa-\ntion between 1) feature values and 2) manually entered structures\n(supervised learning).\nThe selected signal features, which are also used for a music “ﬁn-\ngerprint” application which we have developed [14], represent\nthe variation of the signal energy in different frequency bands. For\nthis,theaudiosignal x(t)ispassedthroughabankof NMelﬁlters.\nTheevolutionofeachoutputsignal xn(t)ofthe n2Nﬁltersisthen\nanalyzedbyShortTimeFourierTransform(STFT),noted Xn;t(!).\nThewindowsize LusedforthisSTFTanalysisof xn(t)determines\nthe kind of structure (short term or long term) that we will be able\nto derive from signal analysis. Only the coefﬁcients (n; !)which\nmaximize the Mutual Information are kept. The feature extrac-\ntion process is represented in Figure 3. These features constitute\nthe observations from which we derive a state representation of the\nmusic.\nf\n............\n...\n...signal\n STFT\nfilter−banktt\nxn(t) x(t)Xn;t(!)L\n!\nFigure 3: Features extraction from signal. From left\nto right: signal, ﬁlter bank, output signal of each\nﬁlter, STFT of the output signals\n4. REPRESENTATIONBYSTATES:AMULTI-\nPASSAPPROACH\nThe summary we consider here is based on the representation of\nthe musical piece as a succession of states (possibly at different\ntemporal scales) so that each state represents a (somehow) similar\ninformation found in different parts of the piece. The information\nis constituted here by the dynamic features (possibly at different\ntemporal scale L) derivedfrom signal analysis.\nThestateswearelookingforareofcoursespeciﬁcforeachpieceof\nmusic. Therefore no supervised learning is possible. We therefore\nemploy unsupervised learning algorithms to ﬁnd out the states as\nclasses.\nSeveral drawbacks of unsupervised learning algorithms must be\nconsidered:\n²usuallyapreviousknowledgeofthe number of classes is\nrequired for these algorithms\n²thesealgorithmsdependsonagood initialization of the\nclasses\n²most of the time, these algorithms do not take into account\ncontiguity (spatial or temporal) of the observations.TowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\nA new trend in video summary is the “multi-pass” approach\n[15]. As for video, human segmentation and grouping performs\nbetterwhenlistening(watchinginvideo)tosomethingforthesecond\ntime[6]. A similar approach is followedhere.\n²The ﬁrst listening allows the detection of variations in the\nmusicwithoutknowingifaspeciﬁcpartwillberepeatedlater.\nInouralgorithmtheﬁrstpassperformsasignalsegmentation\nwhich allows the deﬁnition of a set of templates (classes) of\nthemusic [see part 4.1].\n²The second listening allows one to ﬁnd the structure of the\npiece by using the previously mentally created templates. In\nour algorithm the second pass uses the templates (classes)\nin order to deﬁne the music structure [see part 4.2]. The\nsecond pass operates in three stage: 1) the templates are\ncompared in order to reduce redundancies [see part 4.2.1],\n2) the reduced set of templates is used as initialization for a\nK-meansalgorithm(knowingthenumberofstatesandhaving\na good initialization) [see part 4.2.2], 3) the output states of\nthe K-means algorithm are used for the initialization of a\nhidden Markov model learning [see part 4.2.3]. Finally, the\noptimalrepresentationofthepieceasaHMMstatesequence\nisobtained by application of the Viterbialgorithm.\nThis multi-pass approach allows solving most of the unsupervised\nalgorithm’s problems. The global ﬂowchart is depicted into Figure\n4.\nstates grouping\nk−means algorithmsegmentation\nlearning: Baum−Welch\nstate sequenceinitial states\nmiddle states\nfinal states\ndecoding: Viterbi algorithmHMMaudio signal\ncoding\nSegmentation\nStructuringpotential statesfeature vector\nFigure 4: States representation ﬂowchart\n4.1 First pass: segmentation\nFromthesignalanalysisofpart3,thepieceofmusicisrepresented\nbya set of feature vectors f(t)computed at regulartime instants.\nTheupperandlowerdiagonalsofthesimilaritymatrix Soff(t)(see\nFigure5[top])representtheframetoframesimilarityofthefeatures\nvector. Therefore it is used to detect large and fast changes in the\nsignalcontent and segmentit accordingly (see Figure 5 [middle]).\nA high threshold (similarity ·0:99) is used for the segmentation\nin order to reduce the “slow variation” effect. The signal inside\neach segment is thus supposed to vary little or to vary very slowly.\nWeusethevaluesof f(t)insideeachsegmenttodeﬁne“potential”states sk. A “potential” state skis deﬁned as the mean value of the\nfeaturesvectors f(t)overthedurationofthesegment k(seeFigure\n5 bottom panel).\ntimetimeSEGMENTATION\n50 100 150 200 25050100150200250\n0 50 100 150 200 250 3000.60.81\ntimesimilarity\npotential statedimension\n2 4 6 8 10 12 14 16 1824681012\nFigure 5: Feature vectors segmentation and “po-\ntential” states creation [top:] similarity matrix of\nsignal features vectors [middle:] segmentation based\non frame to frame similarity [bottom:] “potential”\nstates found by the segmentation algorithm\n4.2 Second pass: structuring\nThe second pass operates in three steps:\n4.2.1 Groupingor “potential” state reduction\nThe potential states found in [4.1] constitute templates. A sim-\nple idea in order to structure the music would be to compute the\nsimilarity between them and derive from this the structure (similar-\nity between values should mean repetition of the segment over the\nmusic).\nHowever,weshouldinsistonthefactthatthesegmentsweredeﬁned\nas the period of time between boundaries deﬁned as large and fast\nvariations of the signal. Since the “potential” states skare deﬁned\nasthemeanvalueoverthesegments,ifthesignalvaryslowlyinside\na segment, the potential states may not be representative of the\nsegment’scontent. Therefore no direct comparison is possible.\nInstead of that, the “potential” states have been computed in order\ntofacilitatetheinitializationoftheunsupervisedlearningalgorithm\nsince it provides 1) an estimation of the number of states and 2)\na “better than random” initialization of it. Before doing that, we\nneedtogroupnearlyidentical(similarity ¸0:99)“potential”states.\nAftergrouping,thenumberofstatesisnow Kandarecalled“initial”\nstates. This grouping process is illustrated in Figure 6.\n4.2.2 K-means algorithm\nK-means is an un-supervised classiﬁcation algorithm which allows\nat the same time to estimate class parameters1and to assign each\nobservation f(t)to a class. The K-means algorithm operates in\nan iterative way by maximizing at each iteration the ratio of the\nbetween-classinertiatothetotalinertia. Itisasub-optimalalgorithm\nsince it strongly depends on a good initialization. The inputs of the\nalgorithm are 1) the number of classes, given in our case by the\nsegmentation/grouping step and 2) states initialization, also given\nby the segmentation/groupingstep.\nK-means algorithm used:\nLet us note Kthe number of required classes.\n1In usual K-means algorithm, a class is deﬁned by its gravity\ncentre.TowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\nstatedimensionSTATES GROUPING\n2 4 6 8 10 12 14 16 1824681012\nstatestate\n2 4 6 8 10 12 14 16 1851015\nstatedimension\n0.5 11.5 22.5 33.5 44.5 55.524681012\nFigure 6: “Potential” states grouping [top:] poten-\ntial states [middle:] similarity matrix of potential\nstates features vectors [bottom:] “initial” states fea-\ntures vectors\n1.Initialization: each class is deﬁned by a “potential” state sk\n2.Loop: assigntheobservation f(t)totheclosestclass(accord-\ningto an Euclidean, cosine or Kullback-Leiblerdistance),\n3.Loop: update the deﬁnition of each class by taking the mean\nvalueof the observation f(t)belongingto each class\n4.loopto point 2.\nWenote s0\nkthestatesdeﬁnitionobtainedattheendofthealgorithm\nandcall them “middle” states.\n4.2.3 Introducing time constraints: hidden Markov\nmodel\nMusichasaspeciﬁcnature,itisnotjustasetofeventsbutaspeciﬁc\ntemporal succession of events. So far, this speciﬁc nature has not\nbeentakenintoaccountsincetheK-meansalgorithmjustassociates\nobservations f(t)tostates s0\nkwithouttakingintoaccounttheirtem-\nporal ordering. Several reﬁnement of the K-means algorithm have\nbeenproposed in order totakecontiguity (spatial or temporal) con-\nstraints into account. But we found more appropriate to formulate\nthis constraint using a Markov Model approach. Since we only ob-\nserve f(t)and not directly the states of the network, we are in the\ncaseof a hidden Markovmodel (HMM) [11].\nHidden Markov model formulation: A state kproduces\nobservations f(t)represented by a state observation probability\np(fjk). The state observation probability p(fjk)is chosen as a\ngaussian pdf g(¹k; ¾k). A state kis connected to other states jby\nstatetransition probabilities p(k; j).\nSince no priori training on a labeled database is possible we are in\nthecase of ergodicHMM.\nTheresulting model is represented in Figure 7.\nTraining: The learning of the HMM model is initialized using\ntheK-means“middle”states s0\nk. TheBaum-Welchalgorithm\nis used in order to train the model. The outputs of the train-\ning are the state observation probabilities, the state transition\nprobabilitiesand the initial state distribution.\nDecoding: Thestatesequencecorrespondingtothepieceofmu-\nsicis obtained by decoding using Viterbialgorithm given the\nhiddenMarkovmodel and the signal feature vectors f(t).\n... ...=g(¹k; ¾k)p(fjk)sk sj\nf(t) f(t)p(sk; sj)\np(fjj)\n=g(¹j; ¾j)\nFigure 7: Hidden Markov model\n4.2.4 Results:\nThe result of both the K-means and the HMM algorithm is a set\nof states sk, their deﬁnition in terms of features vectors and an\nassociation of each signal features vector f(t)toa speciﬁc state k.\nIn Figure 8, we compare the results obtained by the K-means algo-\nrithm [middle] and the K-means + HMM algorithm [bottom]. For\nthe K-means, the initialization was done using the “initial” states.\nFortheHMM,theinitializationwasdoneusingthe“middle”states.\nIn the K-means results, the quick state-jumps between states 1, 2\nand 5 are explained by the fact that these states are close to each\nother. These state-jumps do not appear in the HMM results since\nthese jumps have been penalized by state transition probabilities,\ngivingtherefore a smoothest track.\nTheﬁnalresultusingtheproposedmethodisillustratedinFigure9.\nThe white line represents the state belonging of each observations\nalong time. The observations are represented in background in a\nspectrogram way.\nObservation\n50 100 150 200 250012345StateK Means\n50 100 150 200 2500123\nTimeStateHMM\nFigure 8: Unsupervised classiﬁcation on title “Head\nover Feet” from artist Alanis Morisette [top:] signal\nfeatures vectors along time [middle:] state number\nalong time found using K-Means algorithm [bottom:]\nstate along time found using hidden Markov model\nresult of initialization by the K-Means Algorithm\n5. AUDIOSUMMARYCONSTRUCTION\nSo far, from the signal analysis we have derived features vectors\nused to assign, through unsupervised learning, a class number toTowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\ntimedimension\n50 100 150 200 2502468101214\nFigure 9: Results of un-supervised classiﬁcation us-\ning the proposed algorithm on title “Head over Feet”\nfrom artist Alanis Morisette\neach time frame. Let us take as example the following structure:\nAABABCAAB.Thegenerationoftheaudiosummaryfromthis\nstaterepresentation can be done in severalways:\n²providing audio example of class transitions (A !B, B!\nA,B!C,C!A)\n²providing an unique audio example of each of the states (A,\nB,C)\n²reproducing the class successions by providing an audio ex-\namplefor each class apparition (A, B, A, B, C, A, B)\n²providing only an audio example of the most important class\n(intermsofglobaltimeextendorintermofnumberofoccur-\nrencesof the class) (A)\n²etc...\nThis choice relies of course on user preferences but also on time\nconstraintson the audio summary duration.\nIn each case, the audio summary is generated by taking short frag-\nments of the state’s signal. For the summary construction, it is\nobvious that “coherent” or “intelligent” reconstruction is essential.\nInformation continuity will help listeners to get a good feel-\ningand a good idea of a music when hearing its summary.\nOverlap-add: The quality of the audio signal can be further im-\nprovedbyapplyinganoverlap-addtechniqueoftheaudiofragment.\nTempo/Beat: For highly structured music, beat synchronized\nreconstruction allows improving largely the quality of the audio\nsummary. Thiscanbedone1)bychoosingthesizeofthefragments\nasintegermultipleof4or3bars,2)bysynchronizingthefragments\naccording to the beat position in the signal. In order to do that, we\nhaveusedthetempodetectionandbeatalignmentproposedby[12].\nThe ﬂowchart of the audio summary construction of our algorithm\nisrepresented on Figure 10.\n6. CONCLUSION\nMusic audio summary is a recent topic of interest in the multime-\ndia realm. In this paper, we investigated a multi-pass approach\nfor the automatic generation of sequential summaries. We in-\ntroduced dynamic features which seems to allow deriving power-\nfull information from the signal for both -detection of sequence\nrepetion in the music (lower/upper diagonals in a similarity matix)\nmoduleoverlap−addtempo\nbeat alignmentsong\nstructureoveralp-addoveralp-add\nbeat alignment\nFigure 10: Audio summary construction from class\nstructure representation; details of fragments align-\nment and overlap-add based on tempo detection/\nbeat alignment\nand -representation of the music in terms of “states”. We only in-\nvestigated the latter here. The representation in terms of “states”\nis obtained by means of segmentation and unsupervised learning\nmethods (K-means and hidden Markov model). The states are then\nusedfortheconstructionofanaudiosummarywhichcanbefurther\nreﬁned using an overlap-add technique and a tempo detection/ beat\nalignment algorithm.\nExamples of music audio summaries produced with this approach\nwill be givenduring the presentation of this paper.\nPerspectives: towardhierarchicalsummaries\nAs for text or video, once we have a clear and ﬁne picture of the\nmusicstructurewecanextrapolateanytypeofsummarywewant. In\nthisperspective,furtherworkswillconcentrate onthedevelopment\nofhierarchical summaries. Dependingonthetypeofinformation\nwished, the user should be able to select some kind of “level” in a\ntree structure representing the piece of music. Of course tree-like\nrepresentation may be arguable, and an efﬁcient way to do it has to\nbe found. Further works will also concentrate on the improvement\nof the audio quality of the output results. When combining\ndifferent elements from different “states” of the music a global and\nperceptivecoherence must be ensured.\nAcknowledgment\nPartofthisworkwasconductedinthecontextoftheEuropeanI.S.T.\nproject CUIDADO[14] http://www.cuidado.mu .\n7. REFERENCES\n[1]P.Aigrain,P.Joly,andAl.Representation-baseduserinterface\nfortheaudiovisuallibraryofyear2000.In IST-SPIE95 Mul-\ntimedia computing and networking ,pages 35–45, 1995.\n[2]J.-J. Aucouturier and M. Sandler. Segmentation of musical\nsignalsusinghiddenmarkovmodels.In AES 110th Conven-\ntion,2001.\n[3]J.-J.AucouturierandM.Sandler.Findingrepeatingpatternsin\nacousticmusicalsignals: applicationsforaudiothumbnailing.\nInAES 22nd International Conference , 2002.TowardAutomaticMusic AudioSummary GenerationfromSignal Analysis\n[4]R.Birmingham,W.Dannenberg,G.Wakeﬁeld,andal.Musart:\nMusic retrieval via aural queries. In ISMIR, Bloomington,\nIndiana, USA, 2001.\n[5]S. Butler and A. Parkes. Filmic spacetime diagrams for video\nstructurerepresentation. Image Communication ,Specialis-\nsue on Image and Video Semantics: Processing, Analysis,\nApplication, 1995.\n[6]I. Deliege. A perceptual approach to contemporary musical\nforms. In N. Osborne, editor, Music and the cognitive sci-\nences, volume 4, pages 213–230. Harwood Academic pub-\nlishers, 1990.\n[7]J. Foote. Visualizing music and audio using self-similarity.\nInACM Multimedia , pages 77–84, Orlando, Florida, USA,\n1999.\n[8]K. S. Jones. What might be a summary ? In K. Womser-\nHackerandK.and,editors, Information Retrieval 93: Von\nder Modellierung zur Anwendung , pages 9–26. University\nKonstanz,Konstanz,DE, 1993.\n[9]B.LoganandS.Chu.Musicsummarizationusingkeyphrases.\nInICASSP, Istanbul,Turkey,2000.[10]MPEG-7. Information technology - multimedia content de-\nscription interface - part 5: Multimedia description scheme,\n2002.\n[11]L. Rabiner. A tutorial on hidden markov model and selected\napplicationsinspeech. Proccedings of the IEEE ,77(2):257–\n285, 1989.\n[12]E. Scheirer. Tempo and beat analysis of acoustic musical sig-\nnals.JASA,103(1):588–601, 1998.\n[13]H. Ueda, T. Miyatake, and S. Yoshizawa. Impact: An inter-\nactive natural-motion-picture dedicated multimedia authoring\nsystem. In ACM SIGCHI , NewOrleans, USA, 1991.\n[14]H. Vinet, P. Herrera, and F. Pachet. The cuidado project. In\nISMIR,Paris, France, 2002.\n[15]H. Zhang, A. Kankanhalli, and S. Smoliar. Automatic par-\ntitioning of full-motion video. ACM Multimedia System ,\n1(1):10–28, 1993."
    },
    {
        "title": "Polyphonic Score Retrieval Using Polyphonic Audio Queries: A Harmonic Modeling Approach.",
        "author": [
            "Jeremy Pickens",
            "Juan Pablo Bello",
            "Tim Crawford",
            "Matthew J. Dovey",
            "Giuliano Monti",
            "Mark B. Sandler"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418091",
        "url": "https://doi.org/10.5281/zenodo.1418091",
        "ee": "https://zenodo.org/records/1418091/files/PickensBCDMS02.pdf",
        "abstract": "This paper extends the familiar “query by humming” music retrieval framework into the polyphonic realm. As humming in multiple voices is quite difficult, the task is more accurately described as “query by audio example”, onto a collection of scores. To our knowledge, we are the first to use polyphonic audio queries to re- trieve from polyphonic symbolic collections. Furthermore, as our results will show, we will not only use an audio query to retrieve a known-item symbolic piece, but we will use it to retrieve an entire set of real-world composed variations on that piece, also in the sym- bolic format. The harmonic modeling approach which forms the basis of this work is a new and valuable technique which has both wide applicability and future potential. 8",
        "zenodo_id": 1418091,
        "dblp_key": "conf/ismir/PickensBCDMS02",
        "keywords": [
            "query by audio example",
            "polyphonic realm",
            "multiple voices",
            "polyphonic audio queries",
            "polyphonic symbolic collections",
            "harmonic modeling approach",
            "wide applicability",
            "future potential",
            "known-item symbolic piece",
            "real-world composed variations"
        ],
        "content": "Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\nPolyphonic Score Retrie valUsing Polyphonic Audio\nQueries: AHarmonic Modeling Appr oach\nJerem yPickens\n\u0000,Juan PabloBello\n\u0001,Giuliano Monti\n\u0001,\nTim Crawford\n\u0002,Matthe wDovey\n\u0003,MarkSandler\n\u0001,Don Byrd\n\u0004\u0000\nCenter forIntelligent Information Retrieval\nDepar tment ofComputer Science ,Univ ersity ofMassachusetts ,Amherst\u0005\u0007\u0006\t\b\n\u0006\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0010\u0013\u0015\u0014\u0017\u0016\u0018\u000b\u001a\u0019\n\u0013\u0010\u0013\u0015\u0014\u001b\u0006\t\u001c\t\u0016\u0001\nDepar tment ofElectronic Engineer ing,Queen Mary,Univ ersity ofLondon\u0005\u0018\u0016\u001d\u0019\u001f\u001e \u0014\u0017!\n\u0006\n\"\u0010\"\u0010#\u0010$\u0010\u0011\t#\u0007\b\u0010\b\n\u0006\u0010\u0019\t\u000f\u0010\u0006\n\"\t\u0006\n\u0011\u0015\u0014\u001b%\u0018\u000b\u001d\u0016\u0012\"&\u0014'\u0019\u0012\u0011\u0015\u0014\u0017\u0016\u0007()\u001d*\n\u0016\u001d\"*\n\u0019\u001f\u001e\u001d#&\u0014+\u000b,#\u001f\u001e\n-*\n\u000f\t\u0006\n\"\t\u0006\u0012\u0011\u0015\u0014\u0017%\u0018\u000b\u001d\u0016\u001d\"&\u0014.\u0019\n\u0011\u0015\u0014\u0017\u0016\t(\u000b\u001a\u0019\u0007\b\u0007( \u0014/\u0013\t\u0019\u001f\u001e\n\u001c\n\"\t\u0006\t\b\n\u000f\u0010\u0006\u0010\"\t\u0006\u0012\u0011\u0015\u0014\u001b%\f\u000b\u001d\u0016\u001d\"&\u0014'\u0019\u0010\u0011\u0015\u0014\u0017\u0016\t(\u0002\nMusic Depar tment, King' sCollege ,London-*\n\u000b0\u0014/\u0011\u001f\b\u0012\u0019\u00181\n2\u0012#\u0007\b\u0010\u001c\u0010\u000f\u001f(,\u0011\t\"&\u0014.\u0019\n\u0011\u0015\u0014\u0017\u0016\t(\u0003\nOxfordUniv ersity\u000b\u001a\u0019\u0007-\t-\t3\u0012\u0006\u00071 \u0014\u0017\u001c\u0012#\u00074\n\u0006\t\r\u0010\u000f\n\"\u0010\u0019\n\u0013\u0015\u0014.#\u000756\u0014'\u0019\n\u00117\u0014\u0017\u0016\t(\u0004\nIndiana Univ ersity ,Bloomington\u001c\n#\u001f\u001e\u0010!\n\r\u0010\b\t\u001c\n\u000f*\n\u001e\n\u001c*\n\u0019\u001f\u001e\u001d\u0019&\u0014\u001b\u0006\t\u001c\t\u0016\nABSTRACT\nThis paper extends thefamiliar “query byhumming” music retrie val\nframe workinto thepolyphonic realm. Ashumming inmultiple\nvoices isquite difﬁcult, thetask ismore accurately described as\n“query byaudio example”, onto acollection ofscores. Toour\nknowledge, wearetheﬁrsttousepolyphonic audio queries tore-\ntrievefrom polyphonic symbolic collections. Furthermore, asour\nresults willshow,wewillnotonly useanaudio query toretrie vea\nknown-item symbolic piece, butwewilluseittoretrie veanentire\nsetofreal-w orldcomposed variations onthatpiece, alsointhesym-\nbolic format. The harmonic modeling approach which forms the\nbasis ofthisworkisanewandvaluable technique which hasboth\nwide applicability andfuture potential. 8\n1.INTRODUCTION\nMusic information retrie valisarapidly growing ﬁeld. Asmore mu-\nsiccollections come online, thedemand tosearch these collections\nincreases. Music collections, orsources, existinoneoftwobasic\nformats: audio andsymbolic. Tocomplicate matters, music queries\nexistinboth formats aswell. Acomprehensi vemusic retrie val\nsystem should beable toallowqueries ineither format toretrie ve\nmusic pieces ineither format. Theproblem liesinthefactthatthe\nfeatures readily available from audio ﬁles (MFCCs, energy)donot\ncorrespond well with thefeatures available from symbolic ﬁles(note\npitches, note durations) Itisa“vocabulary mismatch” problem.\nOur system will bridge thegapbetween audio andsymbolic mu-\nsicusing transcription algorithms together with harmonic modeling\ntechniques. Inthismanner weallowusers topresent queries inthe\naudio format andretrie vepieces ofmusic which existinthesym-\nbolic format. This isoneoftheearliest goals ofmusic retrie val,and\nuntil nowithasonly been possible within themonophonic domain.\nWeextend therealm ofpossibility intotheremarkably more difﬁ-\ncultpolyphonic domain, andshowthisthrough successful retrie val8:90;\u000e<>=@?BA\u0007CEDF?0G\u0007=H=/I\u000eJ,JKA\u001fC/LEM\fNF<>OPJ\u001aG\u0018C/LHQ\nRSLE;,MUTBMVO\nLEMWCYX+A\u0007CYZ.O\nLEMV[>[><]\\^MVO\nLHZ\u001bO\u000eX+A\u001fCE_`G\u001fLE<>A\tOFaYMWL/CE<>MVb\u0007G\u0007[\u0015G\u0007O,NF<>OPJ\u001aG\u001fC/L@Q\nRdcfe\u000eg\n^C:G\u0007O\nLHhiZ/Z/e\n\\j\u0007j\tk\u0010l\u001fm\u0007n\u0010o\u0012pdqO\nRPA\u0007J,<>O,<>A\tO\u000e=Vr7s,O\u001aN\u001d<tO\n^=uG\u001fO\u001aNwvVA\tO\u000evV[>I,=/<>A\tO\u000e=UA\u001fCfCEMVvVA\u0007_x\\_yMVO,N,G\u0018LE<tA\u0007O,=0M{z\u000eJ\u001dCEMV=/=/M\fN`<>OSLE;\u000e<t=|_`G\u0018LEMWCE<tG\u0007[}G\u0018CEMHLE;,MHG\u0007I\u000eLE;\u000eA\u0007C\u0018~={G\u001fO\u001aNN\u001dAxO,A\u001fLYO\u000eMVvVMV=/=EG\u0018CE<>[>RSCEMW,MVvWLLE;,A\u0007=/MfA\u0007X&LE;\u000eMf=/JKA\tO,=/A\u001fC\np\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.\nv\n\n2002IRCAM-CentrePompidouexperiments forboth known-item andvariation queries. Theability\ntousepolyphonic audio queries toretrie vepieces ofmusic from a\npolyphonic symbolic collection isamajor stepforwardintheﬁeld.\nTheremainder ofthispaper proceeds asfollows:InSection 2we\ngiveabrief reviewoftheproblem domain andexisting literature.\nSection 3locates this paper within thelargerframe workofthe\n“language” modeling approach toInformation Retrie val.Section 4\ncontains anovervie wofoursystem. InSection 5weexplain our\naudio music transcription techniques. InSection 6weexplain our\nharmonic modeling techniques, while insection 7weshowhowtwo\nmodels arecompared fordissimilarity .Finally ,Sections 8and9\ncontain ourexperimental design, results, discussion andconclusion.\n2.BACKGROUNDANDRELATEDWORK\nTodate, research intheﬁeld ofadhocmusic retrie valhasexpe-\nrienced twofundamental divisions. The ﬁrst division isoneof\nrepresentation. Music may either bepresented asaperformance or\nasinstructions totheperformer .Aperformance isanaudio ﬁle,\ninaformat such asWAVorMP3. Instructions totheperformer\nexistinasymbolic format, either asaMIDI ﬁle(www .midi.or g)or\ninConventional Music Notation (CMN) format [1],both ofwhich\nexpress some manner ofinstructions about what notes should be\nplayed, when, forhowlong, andwith what instrument ordynamic.\nThis division between actualized performance andinstructions for\naperformance manifests itself inthetypes offeatures readily ex-\ntractable from digital forms ofaudio andsymbolic music. Those\nretrie ving audio tend toworkwith features such asMFCCs, LPCs,\ncentroids, orenergy,while those retrie ving symbolic sources use\nactual note pitch and/or duration, asthese values areknown.\nThesecond division inmusic IRisoneofcomple xity,ormonophon y\nversus polyphon y.Monophonic music hasatmost onenote playing\natanygiventime; before anewnote starts theprevious note must\nhaveended. Polyphonic music hasnosuch restrictions. Anynote\norsetofnotes may beginbefore anyprevious note orsetofnotes\nhasended, which provesdifﬁcult foranyclear ,unambiguous sense\nofsequentiality .Therefore, techniques which workformonophonic\nmusic, such asstring matching orn-gramming, aremore difﬁcult to\napply tothepolyphonic domain. Furthermore, reasonably accurate\nconversions from audio tosymbolic music isgenerally seen asa\nsolved(oratleast manageable) problem formonophonic music, but\nstillafairly inaccurate, unsolv edproblem forpolyphonic music.Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\nPolyphonic music ingeneral ismore comple xanddifﬁcult towork\nwith. Indeed, some oftheearliest works inmusic retrie valremained\nentirely within themonophonic domain [16, 25]. These “query by\nhumming” systems allowthequery tobepresented inaudio for-\nmat, andthen converted tosymbolic format tobeused forquery\nonamonophonic symbolic collection. Gradually ,systems which\nallowed monophonic queries upon apolyphonic collection, amore\ndifﬁcult prospect, were introduced [5,21,35]. The query isstill\nmonophonic, soconversion ofthequery between audio andsym-\nbolic formats remains possible. Thecollection tobesearched may\ntherefore beaudio orsymbolic, asthequery may easily beconverted\nineither direction tomatch. Butagain, thisisonly possible because\nthequery ismonophonic.\nMost recently ,polyphonic queries upon apolyphon iccollection have\nbecome possible. Yetbecause ofthecomple xnature ofpolyphonic\nmusic andthedifﬁculty ofaccurate conversion, researchers tend not\ntomixtheaudio andsymbolic domains. Research haseither focused\nonpolyphonic audio queries upon polyphonic audio collections [14,\n31,34],orpolyphonic symbolic queries upon polyphonic symbolic\ncollections [6,11,10,26,29]. Weknowofnoprior workwhich\ntackles polyphon y,audio, andsymbolic music allinthesame breath.\nOfthepapers mentioned above,theonethatmost closely resem-\nbles ourworkisPurwins etal[31]. These authors havedevised a\nmethod ofestimating thesimilarity between twopolyphonic audio\nmusic pieces byﬁtting theaudio signals toavector ofkeysignatures\nusing real-v alued scores, averaging thescore foreach keyﬁtacross\ntheentire piece, andthen comparing theaverages between twodoc-\numents. Asdowe,these authors useKrumhansl distance metrics\n[20] toassist inthescoring. One ofthemain differences, however,\nisthatthese authors attempt toﬁtanaudio source toa12-element\nvector ofkeys,while weﬁtasymbolic source toa24-element vector\nofmajor andminor triads. Furthermore, byaveraging their key-ﬁt\nvector across theentire piece, their representation isanalogous to\nour\nk\u0001\u0000\u0003\u0002-order Mark ovmodels. Ourpaper utilizes notonly\nk\u0004\u0000\u0003\u0002-order\nmodels, but\n\u0005\u0007\u0006\b\u0000and\no\u0001\t\u0001\n-order models aswell. Moreo ver,thePur-\nwins paper wasnotspeciﬁcally developed asamusic retrie valtask,\nandthushasnoretrie val-related evaluation. Wepresent comprehen-\nsiveknown-item aswell asrecall-precision results.\nFinally ,apaper byShmule vich etal[33]alsouses some ofthesame\ntechniques presented here, such asKrumhansl’ sdistance metrics\nandthenotion ofsmoothing, thelatter which willbepresented in\nsection 6.2. The domain towhich these techniques areapplied\naremonophonic, butShmule vich’ sworknevertheless demonstrates\nthatharmonic analysis andprobabilistic smoothing canbevaluable\ncomponents ofamusic retrie valsystem.\n3.LANGUAGEMODELING APPROACH\nLanguage Modeling (LM) hasrecei vedmuch attention recently in\nthetextinformation retrie valcommunity .Itisonly natural thatwe\nwish toleverage some oftheadvantages ofLMandapply ittomusic.\nPonte explains some ofthemotivations forthisframe work:\n[Alanguage model is]aprobability distrib ution over\nstrings inaﬁnite alphabet (page 9)... Theapproach to\nretrie valtakenhere istoinfer alanguage model foreach\ndocument andtoestimate theprobability ofgenerating a\nquery according toeach model. Thedocuments arethen\nrankedaccording tothese probabilities (page 14)...The\nadvantage ofusing language models isthatobserv able\ninformation, i.e.,thecollection statistics, canbeused\ninaprincipled waytoestimate these models anddonot\nhavetobeused inaheuristic fashion toestimate the\nprobability ofaprocess thatnobody fully understands\n(page 10)...When thetask isstated thisway,theview\nofretrie valisthatamodel cancapture thestatisticalregularities oftextwithout inferring anything about the\nsemantic content (page 15).”[30]\nEventhough ourretrie valtaskispolyphonic music rather than text,\nweareduplicating theLMframe workbycreating statistical models\nofeach piece ofmusic inacollection andthen ranking thepieces by\nthose statistical properties. Thus, while itmight bemore appropriate\ntoname thiswork“statistical music modeling”, westillsaythatwe\naretaking thelanguage modeling \u000b\r\f\u000e\f\u0010\u000f\b\u0011\u0007\u000b\u0004\u0012\r\u0013 toinformation retrie val.\nSorather than attempting aformal analysis oftheharmonic structure\nofmusic, weinstead “capture thestatistical regularities of[music]\nwithout inferring anything about thesemantic content”.\nNothing illustrates thismore than ourchoice, explained insection 6,\ntocharacterize theharmon yofapiece ofmusic atacertain point asa\f\u0014\u000f\b\u0011\u0016\u0015\u0017\u000b\u0016\u0015\u0019\u0018\u001b\u001a \u0018\u0003\u001c\u0003\u001d\u001f\u001e\u000e\u0018\u0003 !\u001c\u0003\u000f\r\u0018\u001b\u0015\u0019\"#\u001c\u0003\u0018\u0003\u0011\u000e$ overchords, rather than asasingle chord.\nSelecting asingle chord isakin toinferring thesemantic meaning\nofthepiece ofmusic atthatpoint intime. While useful forsome\napplications, wefeel that forretrie val,thissemantic information\nisnotnecessary ,perhaps evenharmful iftheincorrect chord is\nchosen. Rather ,weletthestatistical patterns ofthemusic speak for\nthemselv es.\nToourknowledge, theﬁrstLMapproach tomusic IRwasdone in\nthemonophonic domain [28]. Other recent techniques, which also\ntaketheLMapproach (though without alwaysexplicitly stating it),\napply\n\u0005\n\u0006\b\u0000-order Mark ovmodeling tomonophonic note sequences\n[32, 17]. Further workextends themodeling tothepolyphonic\ndomain, using both\nk\n\u0000\u0003\u0002and\n\u0005\n\u0006\b\u0000-order Mark ovmodels ofrawnote\nsimultaneities torepresent scores [4].\n4.SYSTEM OVERVIEW\nThe goal ofthissystem istotakepolyphonic audio queries and\nreturn polyphonic symbolic pieces ofmusic, highly ranked,which\narerelevanttothegivenquery .This isdone inanumber ofstages,\nasoutlined inFigure 1.\n%'&\u0003(\u0014)+*-,/.\u001402143\u00145\u00176\u00197\u000e8:9<;=7\u000e>\u0019;4?@7BA\nOfﬂine andprior toquery time, theentire source collection (theset\nofpolyphonic scores which aretobesearched) ispassed through\ntheharmonic modeling module, described inSection 6.Each piece\nofmusic, each document, isthen \"inde xed\", orstored, asamodel.\nAtquery time, thesystem ispresented with polyphonic audio, suchPolyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0014\u0013\u0016\u0015\f\u0017\u0006\u0018\u001a\u0019\u001b\u0018\u001a\u001c\u001e\u001d \u001f\"!$#&%\u0016'(\u0019\u001b'*)\u001a\u0011\u0014+-,.\u00130/1%2\u001c\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f3\u000e\r\u0010\u000f\u0012\u0011\u0014\u0013\u0016\u00154\u0017.\u0018\u001a\u0019\u001a\u0018\u001a\u001c5\u001d \u001f\"!567%2/\u001b8:9;/\u001b+(<.=>\u0015\u001a/\u001b).'?\u0013A@B%2\u00111).CD\u0013\n%\u0016'*=\u001aE2'(/\u001a) FDFG\u0011\u0014+?\u0019\u0014/1%\u0016'HE0\u0015\u00068\nasadigitized recording ofapiano piece from anoldLP.Thequery\nisﬁrst passed through theaudio transcription module, described\ninSection 5.The transcription from thismodule ispassed tothe\nharmonic modeling module, andamodel forthequery iscreated.\nFinally ,ascoring function isused tocompare thequery model with\neach ofthedocument models, andgiveeach query-document paira\ndissimilarity value. Documents arethen sorted, orranked,bythat\nvalue, with theleast dissimilar atthetopofthelist.\n5.AUDIOTRANSCRIPTION\nAutomatic music transcription istheprocess oftransforming a\nrecorded audio signal intoarepresentation ofitsmusical features.\nWewilllimit ourdeﬁnition totheestimation ofonset times, durations\nandpitches ofthenotes being played. This task becomes increas-\ningly complicated when dealing with polyphonic music because of\nthemultiplicity ofpitches, inconsistent durations, andvaried tim-\nbres. Most monophonic transcription techniques aretherefore not\napplicable. Infact,despite severalmethods being proposed with\nvarying degrees ofsuccess [9,19,23,24], automatic transcription\nofpolyphonic music remains anunsolv edproblem.\nWeoffertwoﬁgures asanexample ofthistranscription procedure.\nFigure 2istheoriginal score ofBach’ sFugue #10from Book Iof\ntheWell-tempered Clavier,presented here inpiano-roll notation. A\nhuman musician then performs thispiece, andtheaudio signal is\ndigitized. Figure 3isthetranscription ofthisdigitized audio from\noneofouralgorithms. Itiswith thisimperfect transcription thatwe\nstillachie veexcellent retrie valresults.\nWelocate theaudio transcription taskwithin theconte xtofCompu-\ntational Auditory Scene Analysis (CASA). Inthisconte xt,systems\ntrytoexplain theanalysed signal following asetofperceptual rules\nandsound models. These rules suggest howtogroup theelements\nfrom thesignal time-frequenc yrepresentation intoauditory objects\n(i.e. musical notes). Inpolyphonic music, events overlap both\ninthetime andthefrequenc ydomain, meaning thattranscription\nsystems should beable toanalyse thesignal inboth domains in\norder toreturn anaccurate representation ofthescene. From thisapproach wepropose twodifferent methods. Both techniques will\nbeused, separately ,toproduce queries, andretrie valresults foreach\ntranscription technique willbegiven.Wedothistoshowthatour\nharmonic modeling algorithm isrobusttovarying transcriptions and\ntheir associated errors.\n5.1Polyphonic Transcription I\nOurﬁrstmethod isanextension andreworking ofatechnique used\nformonophonic transcription inMonti [27]. Fourier analysis is\nused torepresent thesignal inthefrequenc ydomain. Anauditory\nmasking threshold iscalculated using aperceptual model. Only\nspectral maxima abovesuch athreshold arechosen torepresent\nthesignal. The Phase-V ocoder technique isused tocalculate the\ninstantaneous frequencies ofthepeaks, byinterpolating thephase\noftwoconsecuti veframes. Theanalysis isoptimised forthesteady\nstate partofthenotes.\nOnce therepresentation ofthesignal isgivenasasetofspectral\npeaks, thesystem groups thepeaks according totheir frequenc y\nposition andtime evolution. The grouping rules are: harmonic\nrelation inthefrequenc ydomain andcommon onset inthetime\ndomain. Fortheimplementation ofthese rules, which group peaks\nintoobjects (notes) weused theBlackboard model [12]. This model\nhasshowngreat ﬂexibility andmodularity ,which isimportant when\nimplementing additional rules.\nThesystem starts selecting thelowest available frequenc ypeak and,\nassuming ittobeanote’ sfundamental, looks forharmonic support\namong theother peaks. Thesupport ofanote hypothesis isgiven\nbyafuzzy ratedepending onthefundamental frequenc yposition\nandenergy,andtheharmonic support inthespectrum. Ifthenote is\nconﬁrmed asanhypothesis, itsharmonic peaks areeliminated from\nthehypothesis space sotheycannot bechosen asnewfundamental\nhypotheses. However,theystill may contrib utetoother notes’\nhypotheses since thepartials ofthenotes composing achord often\noverlap inwestern music.\nThealgorithm iterates while there arepeaks inthespectrum. Hy-\npotheses qualify asnote objects, only iftheylastintime foramini-Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\nmum number of(activation) frames. Once anote isrecognised the\nsystem predicts itsevolution inthespectrum, andinfuture analysis\ntheexisting notes areveriﬁed before searching fornewnotes. Ifthe\nspectrum revealschange inthefrequencies’ positions oramplitude\nthesystem formulates newnotehypotheses corresponding tothenew\neventsdetected. Using thismethod, octaveerrors areeliminated, but\natthecostoffailing todetect octaveinterv alswhen played simulta-\nneously .Thesystem extracts onsets, offsets andMIDI pitches from\ntheaudio andwrites them inaMIDI ﬁleforlistening andretrie val\ntasks.\n5.2Polyphonic Transcription II\nOur second system isanextension ofworkfound inBello [2,3].\nWeagain beginbyapply Fourier analysis onoverlapping frames\ninthetime-domain. The phase-v ocoder technique isalso used to\nestimate theexact instantaneous frequenc yvalue foreach binin\nthefrequenc y-domain representation. Howeverinthisapproach all\nfrequenc ypeaks areused, regardless oftheir perceptual conditions.\nTwolevelsofhypotheses areconsidered here. Oneach analysis\nframe, allmusical notes within theevaluated range (from 65to2kHz)\nareconsidered tobe‘frame’ hypotheses. Associated with each of\nthese frame hypotheses aﬁlter isdeveloped inthefrequenc ydomain.\nTodothisweassume thatanote with fundamental frequenc y \u0000\u0002\u0001\nmust (theoretically) present frequenc ypartials located according to:\u0000\u0004\u0003\u0006\u0005 \u0001\b\u0007\n\t\f\u000b\r\u0000\u0004\u0001\u000f\u000e\n\u0005\u0011\u0010~\n\t\u0013\u0012\u0015\u0014\n\u0005\n\u000b\u0017\u0016\u0018\u0001~\n\u0005\nwhere \u0016\u0018\u0001istheinharmonicity factor (note andinstrument depen-\ndent) [13], and\t\u0019\u0007\n\u0005\u001b\u001a\u001c\u001a\u0017\u001a\u001e\u001d,with\n\u001dsuch that\u0000 \u001f\n\u0005 \u0001\"!\u0000\n\u0006$#\no.The\nﬁlter associated with \u0000\r\u0001behaveslikeacomb ﬁlter with lobes cen-\ntered attheexpected partials’ frequencies andbandwidths equal to\nhalfthetone-distance between thehypothetical note anditsclosest\nneighbour (aquarter orhalfatone depending onthenote).\nTheframe’ sfrequenc y-domain isprocessed through thisﬁlter -bank,\nproducing agroup ofspectrums associated with each oftheframe-\nhypotheses. Thehypotheses arerated according totheratio between\ntheﬁltered spectra energyandtheenergyoftheoriginal spectrogram.\nHypotheses with high ratings areclassiﬁed as‘note’ hypotheses\nandfollowed overtime. Ifcontinuity andenvelope conditions are\nsatisﬁed, then thenote isrecognised asanote-object ofthesignal.\nNote thatinthisapproach, noonset detection isperformed onthe\naudio signal. Timing information depends onthebehaviour ofthe\ninstantaneous rating ofeach possible note. Asmoothing windo wis\nused togroup eventsthatareveryclose intime.\nAnimportant difference from theprevious approach isthatframe\nhypotheses areevaluated independently ,allowing anyinterv alto\nbedetected. This brings asaconsequence thedetection ofoctave\ninterv alsandtheproliferation ofoctave-related errors. Aswith the\nprevious transcription algorithm, thesystem extracts onsets, offsets\nandMIDI pitches from theaudio andwrites them inaMIDI ﬁlefor\nlistening andretrie valtasks.\n6.HARMONIC MODELING\nAharmonic model isourterm foraMark ovModel inwhich thestates\nofthemodel aremusically salient, harmonic entities. Theprocess of\ntransforming polyphonic music intoaharmonic model divides into\nthree stages. Intheﬁrststage,\u0013 \u000b\u000e\u000f\u001e% \u0011\u000e$#\u0018\u001b\u0012 \u001e'&\u0019 \r\u0012 \u000f!\u0018 \f \u001c\u0003\u0018\u0003\u0011\u000e$,themusic\ndocument tobemodeled isbrokenupintosequences ofnote sets,\nandeach ofthose note setsareﬁttoaprobability vector .Each of\nthese note setsisassumed tobeindependent oftheneighboring sets.\nThis assumption, while necessary forthemodeling, isnotalways\naccurate, inparticular because harmonies inapiece ofmusic are\noften deﬁned bytheir conte xt.The second stage oftheharmonic\nmodeling process istherefore a \u001e% \u0011\u0007\u0011\u0004\u001c \u0013=\u0018\u0003$'( procedure, designed to\naccount forthisconte xt.Finally ,thethird stage istheprocess bywhich) \u000b\u0001\u000f+*\u0001\u0011 ,-% \u0011\u0007\u001e.& \u001a  arecreated from thesmoothed harmonic\ndescriptions. Stages oneandthree arecovered ingreater detail in\n[29], while stage twoisanewtechnique ﬁrstdescribed inthispaper .\n6.1Harmonic Description\nRecall from Section 1thatpolyphonic music hasnoinnate, one-\ndimensional sequence. Arbitrary notes orsetsofnotes may start\nbefore thecurrent note orsetofnotes hasﬁnished playing. Itthere-\nfore becomes necessary forustoartiﬁcially impose sequentiality .\nThis isaccomplished byignoring theplayed duration foreverynote\ninascore, andthen selecting ateach newnote onset allthenotes\nwhich also beginatthat onset. These event-based sets arethen\nreduced, mod 12,tooctave-equi valent pitch classes andgiventhe\nname \u0018/% \" \u001a \u001c\u001b\u000b\u000e$\u0018&\u0019\u0018\u001b\u001c\u0003\u001d .\nWedeﬁne a \u001a0&+1\u0001\u0018\u001b\u0012 \u000b\u0004\u001a \u0012\r\u0013 \u0011\u000e\u000f \u001easacodiﬁed pitch template. Ofthe12\noctave-equi valent (mod 12)pitches intheWestern canon, weselect\nsome 2-sized subset ofthose, callthesubset a \u0012\r\u0013 \u0011\u000e\u000f \u001e,givethatchord\naname, andaddittothelexicon. Notallpossible chords belong\ninalexicon; with3\n8\n\u0012\t54possible lexical chords ofsize2,and12\ndifferent choices for 2,wemust restrict ourselv estoamusically-\nsensible subset. The chord lexicon will furthermore makeupthe\nstate space ofourMark ovmodel, inaddition toproviding thebasis\nfortheharmonic description.\nThechord lexicon used inthispaper isthesetof24major andminor\ntriads, oneeach forall12members ofthechromatic scale: CMajor ,\ncminor ,C6Major ,c6minor\n\u001a\u0017\u001a\u001c\u001aB7Major ,b7minor ,BMajor ,\nbminor .Nodistinction ismade between enharmonic equivalents\n(C6/D7,A6/B7,E6/F,andsoon). Assuming octave-invariance, the\nthree members ofamajor triad havetherelati vesemitone values 2,2\n\u0010 nand2\n\u001098;those ofaminor triad2,2\n\u0010;:and2\n\u0010;8.\nDuring the1970s and1980s themusic-psychologist Carol Krum-\nhansl conducted aground-breaking series ofexperiments into the\nperception andcognition ofmusical pitch [20]. Byusing thesta-\ntistical technique ofmulti-dimensional scaling ontheresults ofex-\nperiments onlisteners’ judgements ofinter-keyrelationships, she\nproduced atable ofcoordinates infour-dimensional space which\nprovides thebasis forthelexical chord distance measure weadopt\nhere. The ‘distance’ between triads <and =canbeexpressed as\nthefour-dimensional Euclidean distance between these coordinates.\nWedonotreproduce these distances here, butdenote thedistance\nas >@?.ACB\u001cD~\n<\u0018EF=.\nNowthatthese deﬁnitions areclear ,wemay proceed with thehar-\nmonic description algorithm. Thebasic ideaisthatwhen calculating\nthescore ofasimultaneityBonalexical chordG,thisscore isinﬂu-\nenced byalltheother lexical chords Hinwhich Bparticipates. Thus,\neverylexical chord hasaneffectoneveryother lexical chord.\nAnanalogy might help: Theamount ofgravitational force thattwo\nbodies (such astheearth andmoon) exertoneach other ispropor -\ntional totheproduct oftheir masses, andinversely proportional to\nafunction ofthedistance between them. Byanalogy ,each ofour\n24lexical chords isabody inspace, andeach exertssome inﬂuence\nonallothers. Thus, ifthenotes ofaGmajor triad areobserv ed,\nnotonly does Gmajor getthemost mass, butwealso assign some\nprobability mass toEminor andBminor ,abitlesstoCmajor and\nDmajor ,evenlesstoAminor andF\n\u0002\nminor ,andsoon.\nSotheamount ofinﬂuence exerted byeach chord inthelexicon on\nthecurrent chord isproportional tothenumber ofpitches shared\nbetween thesimultaneity Bandeach lexical chord H,andinversely\nproportional totheinter-triad distance from each Hto G.Since,\ningeneral, ‘contrib utions’ ofnear neighbors interms ofinter-key\ndistance arepreferred, weusethatfactasthebasis forcomputing a\nsuitable conte xt:Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\u0000\u0002\u00012 D\u0004\u0003\u0006\u0005 D~\nB'E G\n\u0007\n\u0007\b\n\t\f\u000b\u000e\r\u0004\u000f\u0006\u0010\u000e\u0011\u0013\u0012\t\n\u0014B\u0016\u0015\bH\n\u0014>@?.A B\u001cD~\nH E G\n\u0010 \u0005 ~\no\nThis conte xtscore iscomputed foreverychordGinthelexicon\n(each point inthedistrib ution), andthen theentire distrib ution is\nnormalized bythesum total ofallconte xtscores. While itisclear\nthat theharmon yofallbutthecrudest music cannot bereduced\ntoamere succession ofmajor andminor triads, asthischoice of\nlexicon might bethought toassume, webelie vethatthisisasound\nbasis foraprobabilistic approach toharmonic description, asmore\ncomple xchords (such as\n8\u0001\u0000\u0003\u0002chords) areinfactaccounted forbythe\ncontrib utions oftheir notes totheoverall probabilistic conte xt.\n6.2Smoothing\nWhile themethod abovetakesintoaccou ntcontributions from neigh-\nboring triads, itonly does sowithin thecurrent simultaneity ,the\ncurrent timestep. Harmon y,asmusicians percei veit,isahighly\nconte xtual phenomenon which depends notonly ontheharmonic\ndistances atthecurrent timestep, butisalsoinﬂuenced bytheprevi-\noustimesteps: theharmonies present intherecent pastareassumed\ntobeagood indication ofthecurrent harmon y.Thus, asimultaneity\nwith only onenote might provide arelati velyﬂatoruniform dis-\ntribution across thelexical chord set,butwhen thatsimultaneity is\ntakeninhistorical conte xt,thedistrib ution becomes more accurate.\nWehavedeveloped anaive,yeteffective,technique fortaking into\naccount thisevent-based conte xtbyexamining awindo wof2si-\nmultaneities andusing thevalues inthatwindo wtogiveabetter\nestimate forthecurrent simultaneity .This isgivenbythefollowing\nequation, whereB\n\u0000isthesimultaneity attimestepD:\u0017\t\n\u0001\f\u0001D\u0004\u0018\u0019\u0003$?~\nB\n\u0000E G\n\u0007\n\t\u0007\u0010\u001b\u001a8\n\u0005A\n\u001c\u001d\u0007\b\u001e\t\f\u000b\u000e\r\u0004\u000f\u0006\u0010\u001f\u0011 \u0012\t\n\u0014B\n\u0000 !\n\u0010\u001b\"8\n\u0015 H\n\u0014>@?.ACB\u001cD~\nH E G\n\u0010 \u0005$# %~\n:\nWhen thesmoothing windo w2isequal to1,thisequation degen-\nerates into theonefrom theprevious section. When 2isgreater\nthan one, thescore forthelexical chordGatthecurrent timestep\nisinﬂuenced byprevious timesteps inproportion tothedistance\n(number ofevents) between thecurrent andprevious timestep. As\nintheunsmoothed version, thesmoothed conte xtscore iscomputed\nforeverychord Ginthelexicon andthen theentire distrib ution is\nnormalized bythesum total.\n6.3MarkovModeling\nItshould beclear bynowthattheprimary difference between our\nharmonic description algorithm andmost other such algorithms is\nthechoice tocreate probabilistic \u001e\u000e\u0018\u0003 !\u001c\u0003\u000f!\u0018 \u0015\u0019\" \u001c\u0003\u0018\u001b\u0011\u000e$# across thelexical\nchord set,rather than\u000f &\u0017\u001e\u000e\" \u0012\u0007\u001c\u0003\u0018\u001b\u0011\u000e$# ofeach simultaneity toasingle,\nmost salient lexical chord. Theﬁgure belowisatoyexample ofa\nharmonic description, using anexample lexicon ofthree chords,&,',and (.Withthisprobabilistic harmonic description, wenow\ncreate aMark ovmodel.90<>_yMV='LEMVJ ~\u0017e\u0012<>_ I,[]L:G\u001fO,MV<]L.R\u001a)M{z\u000e<>v\fG\u001f[&TB;\u000eA\u0007C:N\n\u0005 o : n l*k\u001dp o k\u001dp \u0005 k\u0012p 8 k\u001dp l k+k\u001dp l k\u001dp \u0005 k\u0012p \u0005 k\u001dp l k\u0012p \u0005a\nk\u001dp : k\u001dp m k\u0012p o k k\u0012p j\nMark ovmodels areoften used tocapture statistical properties of\nastate sequence overtime. Wewanttobeable topredict future\noccurrences ofastate bythepresence ofsequences ofprevious\nstates. Inourharmonic approach, wehavechosen lexical chords as\nthestates ofthemodel. Foran 2\n\u0000\u0003\u0002-order model, a\no\u0018n\u0004\t\u0005\no\u0018nmatrix\nisconstructed, with the\no\u001fn\n\trowsrepresenting the\f\u0014\u000f &\u0017,\u0007\u0018\u001b\u0011\u000e\"=  \r\u001c\u001b\u000b\u000e\u001c &\nspace, andthe\no\u0018ncolumns representing the \u0012 \"=\u000f!\u000f &\u0019$\u0014\u001c  !\u001c\u001b\u000b\u0004\u001c &space.An~\n2\n\u0010 \u0005sized windo wslides overthesequence oflexical chord\ndistrib utions andMark ovchains areextracted from that windo w.\nThecount ofeach chain isadded tothematrix, where thecross of\ntheﬁrst 2states istheprevious state, andthe~\n2\n\u0010 \u0005\n\u0000\u0003\u0002state is\nthecurrent state. Finally ,when theentire observ able sequence has\nbeen counted, each rowofthematrix isindividually summed and\ntheelements ofeach rownormalized bythesum total forthatrow.\nOneproblem isthatMark ovmodeling only works on1-dimensional\nsequences ofobserv able states, while ourharmonic description\nisasequence of24-point probability distrib utions. Our solution\nistoassume independence between points ineach distrib ution at\neach timestep, sothatanexhausti venumber ofindependent, one-\ndimensional paths through thesequence may betraced. (This ex-\nhausti vepaths approach isabstractly similar toonesuggested by\nDoraisamy andR¨uger [10].) Each path, thus constructed, isnot\ncounted asafullobserv ation. Instead, observ ations arepropor -\ntional; thedegree towhich each path isobserv edisafunction of\ntheamount bywhich allelements ofthepath arepresent. Since in-\ndependence between neighboring simultaneities wasassumed, this\nbecomes theproduct ofthevalues ofeach state which comprises\nthepath. Forexample, suppose weconstruct a\no\u000e\tB\n-order model\nfrom thesequence ofdistrib utions, above.Then oneofthemany\nobserv edstate sequences wewould seeintimesteps 1to3is“QRR”.\nThecount ofthisobserv ation is0.08 =(0.5*0.8*0.2).\n7.SCORING FUNCTION\nOurgoal istoproduce arankedlistforaquery across thecollection.\nWewish torank those pieces ofmusic atthetopwhich aremost\nsimilar tothequery ,andthose pieces atthebottom which areleast\nsimilar .This isthetask ofthescoring function. Wehavechosen\nasthisfunction theKullback-Liebler (KL) divergence, ameasure of\nhowdifferent twodistrib utions are,overthesame eventspace. The\ndivergence isalwayszero iftwodistrib utions areexactly thesame,\norapositi vevalue ifthedistrib utions differ.Wedenote theKL\ndivergence between query model,andmusic document model?as-~\n,\n\u0014\u000e\u0014?.“The KLdivergence between [ ,]and[ ?]istheaverage\nnumber ofbitsthatarewasted byencoding eventsfrom adistrib ution\n[ ,]with acode based onthenot-quite-right distrib ution [ ?]”[22].\nInourMark ovmodel, each previous state, each rowinthe\no\u001fn\t\u0005\no\u001fn\nmatrix, isacomplete distrib ution. Wetherefore compute adiver-\ngence score foreach rowinthemodel, andaddthevalue tothetotal\ndivergence score forthatquery-document pair.This isgivenbythe\nfollowing equation, where ,\n\u0010and ?\n\u0010represent each previous state.\nItisimperati vethatthesame modeling procedure andsize thatis\nused forthedocument models isalsoused forthequery model.-~\n,\n\u0014\u000e\u0014?\n\u0007\n\u0007.\u0013/0\t\f.\u0005\n\n/\u0004\t\n1\u0007\u000f2\t\f3\n,\n\u0010~\n\u0005\n[>A\n^\n,\n\u0010~\n\u0005?\n\u0010~\n\u000554\n~\nn\nHowever,there isaproblem inthatsometimes adocument model can\nhaveestimates ofzero probability .This isespecially trueofshorter\nmusic documents, inwhich alotofthepossible transitions arenever\nobserv ed. The divergence score insuch cases ( ,\n\u0010~\n\u0005\u0010[tA\n^\n.\u0013/06\u000e\u000f879)\nautomatically goes toinﬁnity .This small problem injustasingle\nvalue could therefore throwoffourentire score forthatdocument.\nWetherefore must create some small butprincipled non-zero value\nforeverydocument model zero value. There aremanywaystodo\nthis, butwehavedone soby“backing off”toageneral music model,\nusing thevalue ofthatprevious state node from thegeneral model\nwhene verweencounter azero value inanyparticular document\nmodel.\nAgeneral music model iscreated byaveraging themodels overthe\nentire setofdocument models inthecollection. Inprinciple, there\ncould stillremain zero values inthegeneral music model, depending\nonthesize andproperties ofthecollection. Inourexperiments,Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\u0000\u0002\u0001\u0004\u0003\u0006\u0005\n\t\b\u0007.\r\n\t\f\u000b\u0006\t\u0014\u0007\u0001\n\u0003.\t\u000e\r\u0001\u0010\u000f\u0012\u0011\u0012\u0013\u0015\u0014\u0017\u0016\n\u0007\u0000\n\u0007\u0001\u0018\u000f\u0019\u0013\u001b\u001a\n\u0007\"\u0001\u001d\u001c\u0012\u001e0\u0001\u0016\u001f\u000f! \"G\u001fv{;\n*CEMV[>I,N\u000eMV=_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u000eA\f?\n\u0005 n\u001dp m : o\r:\u001dp \u0005\u000e\u0005 o\u0016\u0005\fj\u001dp n \u0005#<>O\u001aN\u000eA\f?\no n\u001dp m : n\u001dp m : \u0005$:\u001dp j\u0007m#<>O\u001aN\u000eA\f?\n: n\u001dp 8%$ :\u0012p l\to n\u000ep :\tk#<>O\u001aN\u000eA\f?\nn n\u001dp m : :\u0012p \u0005\u00048\n3'&)(\u0010*aHG\u001fO\u001aN\u001dA\t_\n\u0007\n\u0005\u0018l\u00028\tl\n\"G\u0007v:;Pg,I\n^I,MV=_y_\nk_y_\n\u0005_y_\no#<tO,N\u000eA\f?\n\u0005 n\u001dp k\u001fn :\u0010l\np k\tm \u0005\fj\to\u0012p k\tm#<tO,N\u000eA\f?\no :\u0012p $\u0002: l\np $\u0007j \u0005Vk\u001dp l\u0007m#<tO,N\u000eA\f?\n: :\u0012p :=\u0005 l\np@\u0005Vj :\u001dp l\to#<tO,N\u000eA\f?\nn :\u0012p o\r: n\u001dp k\to\n\u000b'&*3\u0018+aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\n\u0005\u0018l\u00028\tl\u0000\u0002\u0001\u0010\u0003\u0006\u0005\n\t \u000b\u000e\r\n\t\f\u000b\u0006\t1\u0007\u0001\n\u0003.\t\u000e\r\u0001\u0018\u000f\u0012\u0011,\u0013-\u0014.\u0016\n\u0007\u0000\n\u0007\u0001\u0010\u000f\u0019\u0013\u001b\u001a\n\u0007 \u0001\u001d\u001c\u0019\u001e0\u0001\u0016\u0018\u000f! \u001b \"G\u0007v:;\n*CEMV[>I\u001aN\u000eMW=_y_\nk_y_\n\u0005_y_\no#<>O,N\u000eA\f?\n\u0005 m\u001dp j=\u0005 o\u0007m\u0012p 8\u0007o o\to\r:\u001dp m'8#<>O,N\u000eA\f?\no 8\u0012p m\tl l\u0012p k\u001fn \u0005/$\u001dp 8\to#<>O,N\u000eA\f?\n: 8\u0012p l\u0018n\n3'&0+\u001a3$\u0012p m\u0010l#<>O,N\u000eA\f?\nn 8\u0012p :\tl n\u000ep m 8 8\np j1$aYG\u0007O,N\u000eA\u0007_\n\u0007\n\u0005\u0018l 8\u0007l\n\"G\u0007v:;Pg,I\n^I\u000eMV=_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\u0018?\n\u0005 $\u001dp k\tm o\u0018n\u000ep m\tm \u0005Vn\no\np j\u0010o#<>O\u001aN\u001dA\u0018?\no l\u0012p : : n\u000ep 8 8 \u0005\fk\u0012p o\r:#<>O\u001aN\u001dA\u0018?\n: $\u001dp \u0005\fk :\u001dp 8\tl :\u0012p $\u0002:#<>O\u001aN\u001dA\u0018?\nn l\u0012p 8\u0007j :\u001dp l\u0007m\n\u000b2&03\u0010(aYG\u0007O\u001aN\u001dA\t_\n\u0007\n\u0005\fl 8\u0007l\nhowever,wefound thisalmost nevertobethecase. Also, itshould\nbeobserv edthatwhen thequery model hasazero probability in\nanycell, there isnoproblem. TheKLdivergence forthatpoint isk[>A\n^\n9\n/\n6\u000e\u000f87,which iszero.\n8.EXPERIMENT DESIGNANDRESULTS\nForourretrie valexperimentation, weadopt theCranﬁeld evaluation\nmodel\n\u0012[8]. This requires three crucial components: (1)Source\ncollection, (2)Query ,and(3)Relevance judgements which label\neach item inthesource collection aseither relevantornotrelevant\ntothequery .Inallourexperiments, thesource collection remains the\nsame. However,wevarythequeries andtherelevance judgements,\nasdescribed below.\n8.1SourceCollection\nThebasic testcollection onwhich wetested ourretrie valmethod was\nassembled from data provided bytheCenter forComputer Assisted\nResearch intheHumanities (CCARH) [18]. Itcomprises around\n3000 ﬁles ofseparate movements from polyphonic fully-encoded\nmusic scores byanumber ofclassical composers (including Bach,\nBeetho ven,Handel, andMozart) ofvarying keys,textures (i.e. av-\nerage numbers ofnotes inasimultaneity) andlengths (numbers of\nsimultaneities). Tothisbasic collection weadd, forthepurposes\nofthepresent paper ,three additional setsofpolyphonic music data,\nforatotal collection ofapproximately 3,150 pieces ofmusic. Col-\nlectively,wedenote these Twinkle, Lachrimae andFoliavariations\nastheTLF sets:\u000026individual variations onthetune knowntoEnglish speak ers\nas‘Twinkle, twinkle, little star’ (infactamixture ofmostly\npolyphonic andafewmonophonic versions);475versions ofJohn Dowland’ s‘Lachrimae Pavan’,collected as\npartoftheECOLM project (www .ecolm.or g)from different\n16th and17th-century sources, sometimes varying inquality\n(numbers of‘wrong’ notes, omissions andother inaccuracies),\ninscoring (forsolo lute, keyboard orﬁve-part instrumental\nensemble), insectional structure andinkey;\u000050variations byfour different composers onthewell-kno wn\nbaroque tune ‘Les Folies d’Espagne’.\n8.2Experiment One:KnownItem\nThe idea fortheﬁrst experiment comes from adesire totestthe\nrobustness ofourharmonic modeling. Wetherefore assembled\nfrom theNaxos audio collection the24Preludes andFugues of\u0012e\u0012MVM G\u0007[>=/A;\nL/LEJ,5 616\u0018vV<><>C\npvV=\npI\u000e_`G\u0007=/=\npM\fN\u000eI\u00186\u001f_iI,=/<>v\no\u0007k\u0007k\tk6\u001fMVb\u0007G\u0007[>I\u001aG\u0018LE<>A\tO\np;\nLE_y[Book IofBach’ sWell-tempered Clavier.The score versions of\nthese piano-based, human-played audio ﬁles arepresent within our\nsource collection, from theCCARH data. Soeach audio-transcribed\nPrelude orFugue becomes aquery ,andthescore from which the\naudio ﬁlewasostensibly played becomes theone“kno wnitem”\nrelevantdocument inthecollection.\nThequestion iswhether thisdegraded, transcribed query (Figure 3)\ncanretrie ve,atahigh rank relati vetoallother music inthecollection,\ntheoriginal “perfect” score (Figure 2).Forthisparticular example,\nFigure 2wasretrie vedatarank of\n\u0005\u0006\b\u0000,from ourcollection of3,150\npieces ofmusic.\nAsgood asthisresult is,accurate evaluation deals with averages\ntogetatrueindication ofsystem performance. Theresults ofthis\nexperiment arefound inTables 1and2.Foreach setofqueries\n(either the24Preludes or24Fugues) theknownitem wasretrie ved\natsome rank, where ﬁrstisthebestpossible value. These ranks were\nthen averaged across allqueries intheset.Results aregivenfor\nk\u0000\u0003\u0002\nto\no\u0001\tB\n-order Mark ovmodels, each ofwhich hasbeen smoothed over\nawindo wofsize 2\u0013\u0007\n\u0005to 2 \u0007\nn.Forcomparison, asystem which\nperformed random ranking would place theknownitem, onaverage,\napproximately\n\u0005E\nl\u00028\tl\n\u0000\u0003\u0002.7 \u0001\u0013\u001b\u001a\n\u0005\u0013\u001b\u0013\n\u0001\u0016\u001f\u000f\n\rOurresults showthattheknownitem searches areex-\ntremely successful. Through acombination ofhigher -order Mark ov\nmodels andlargersmoothing windo ws,wewere able toretrie ve\nthetruesymbolic version ofthepiece using theaudio-transcribed,\ndegraded query atanaverage rank ofalittle over3fortheBach\nPreludes, andalittle over2fortheBach Fugues. While there is\nstillroom forimpro vement, itshould provedifﬁcult toproduce an\u000b\u0002,\r&\u0019\u000f\b\u000b$(.& which isbetter than 2ndor3rd.\nThough results varyslightly from theTranscription ItotheTran-\nscription IIalgorithms, equally good results were achie vedusing\neach. Ourharmonic modeling technique isrobustenough tohandle\ntwosigniﬁcantly different transcription algorithms.\n8.3Experiment Two:Variations\nForthesecond experiment, wewish todetermine whether ourhar-\nmonic modeling approach isuseful forretrie ving variations ona\npiece ofmusic, rather than justtheoriginal. Recall thatinaddi-\ntion totheCCARH data, oursource collection contains three sets\nofvariations. Forthisexperiment, theaudio version onevariation\nisselected andthescore versions ofallthevariations arejudged\n“rele vant” totheaudio query ,eventhough their actual  \u0018/% \u0018\u001b\u001a \u000b\u0001\u000f\r\u0018\u001b\u001c\u0003\u001d\nmay varyconsiderably .Agood retrie valsystem would therefore\nreturn allvariations towardthetopofthe3,150 item list,andallPolyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\nnon-v ariations further down. This isrepeated forallaudio pieces\nintheset. Forexample, Figure 4contains afewofthe“Twinkle”\nvariations. When theaudio version ofVariation 3isused asthe\nquery ,weexpect notonly thescore version ofVariation 3tobe\nrankedhighly ,butthescore version ofVariation 11andthescore\nversion oftheTheme toberankedhighly aswell. (The “Theme” is,\nofcourse, oneofthemanyvariations.)\n§\nê&\n?24\n24Theme\nÏ\nÏÏ\nÏÏ\nÏÏ\nÏÏ\nÏÏ\nÏÏ\nÏÏÏ Ï\nÏÏ\nÏÏ\nÏÏ\nÏÏ\nÏÏ\nÏú\nú\n§\nê&\n?9 £Variation 3\nÏ\nÏÏÏÏ\nÏÏÏÏ\nÏúÏÏÏ\nÏÏÏÏ\nÏú#ÏÏÏ\nÏÏÏÏ\nÏú#ÏnÏÏ\nÏÏÏÏ\nÏÏnÏÏÏ\nÏÏÏÏÏ\nÏÏÏÏÏ\nÏÏÏÏ\n§\nê&\n?15 Ï\nÏÏÏÏÏ\nÏÏÏÏÏ\n£Ï\nÏÏÏÏÎVariation 11\nÏ.\n·ÏÏ.ÏÏJ\nÏ.Ï\nÏÏ.ÏJ\nÏÏÏÏJ\nÏ.Ï.ÏÏÏ\nÏÏ.ÏÏÏJ\nÏÏÏÏJ\nÏ.ÏÏÏ\nÏÏ.ÏÏÏJ\nÏ\n§\nê&\n?21Ï\nÏÏÏJÏÏ\näÅ\nÏÏÏJÏÏÏ\näÏÏÏÏ.\nÏÏÏJ#Ï\näÏJ\nÏÏÏJÅÏÏ\näÏÏnÏÏÏ.\nÏÏÏÏÏÏÏÏ\nÏÏÏ\nÏÏ\nÏÏÏÏ\nÏÏ\nÏÏjÏä\nä\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000e*\b\r\u0001\u0000\u0003\u0002.\u00130\u001c\"%\u0016=\u001aE\u0016CG67% /\u001a8 E0\u0015\u001a\u001c\u0005\u0004 @\u0007\u0006&' )\t\b\u001b+?\u001c\u000b\n\r\f \u0011\u0014%2'?\u0011 E2'?/\u001b).C\nBecause ofthesize ofthese sets andourlimited resources, we\nwere notable togethuman performances ofallthese variations.\nInstead, weconverted thequeries toMIDI andused ahigh-quality\n(30Megabyte) piano soundfont tocreate anaudio “performance”.\nThis apparent weakness inourevaluation iscountered bytwofacts:\n(1)These audio queries arestillpolyphonic, evenifsynthesized,\nandautomatic transcription ofoverlapping andirregular -duration\ntones isstillquite difﬁcult. (2)Manyofthevariations onapiece\narethemselv esquite different from apotential query ,asweseein\nFigure 4,andgood retrie valisstill adifﬁcult task. Evenifthe\nperfect score ofavariation were used asaquery ,rather than the\nimperfect (though perhaps slightly better because ofthesynthesized\naudio), quality retrie valisnotguaranteed. While wehope towork\nwith ahuman-produced audio collection forthisretrievalexperiment\nsomeday ,aswehavedone with theknown-item Naxos data above,\nwefeelthegistoftheevaluation hasnotbeen compromised.\nPresentation oftheknown-item results were straightforw ard. With\nonerelevantdocument intheentire collection, oneneed only report\ntherank (oraverage rank across allqueries) ofthisdocument. The\nproblem with multiple relevantdocuments ishowbest tovisualize\ntherankedlist.Typically thisisdone using 11-pt interpolated recall-\nprecision graphs, with \f\u0014\u000f &\u0017\u0012 \u0018\u0003 \u0018\u001b\u0011\u000e$(number ofrelevantdocuments\novertotal retrie vedatapoint intherankedlist) givenatvarious\nlevelof \u000fF&\u0017\u0012!\u000b\u000e\u001a\u0003\u001a (number ofrelevantdocuments retrie vedoverthe\ntotal number ofrelevantdocuments inthequery set). However,\nspace constrains us.Instead, wepresent twovalues which hopefully\ncharacterize thedata: mean average precision andmean precision\natthetop5retrie veddocuments.\nAverage precision iscomputed bycalculating theprecision fora\nsingle query (retrie vedrelevantovertotal retrie ved)everytime an-\nother variation (relevantdocument) isfound, then averaging overallthose points. This score isthen averaged overallqueries intheset,\ntocreate themean average precision. Itisasingle value popular in\nInformation Retrie valstudies because itallowseasy comparison of\ndifferent systems.\nHowever,some users aremore interested intheprecision ofasystem\natthetopoftherankedlist. Iftheuser does notcare about ﬁnding\neverysingle variation butonly cares about ﬁnding anyvariation, then\ntheaverage precision isnotasimportant astheprecision atthetop\noftherankedlist. Wetherefore compute theprecision forasingle\nquery after retrie ving thetop5documents. If1ofthose documents\nisrelevant(avariation), then theprecision is0.2,or20%. Ifnone\nofthem are,theprecision is0%. Ifallofthem are,theprecision is\n100%. Wethen average thisvalue overallqueries intheset,toget\nthemean precision atthetop5retrie veddocuments.\nTables 3and4contain themean average precision results, while\nTables 5and6contain theaverage precision atthetop5retrie ved\ndocuments. These values aregivenforthethree TLF query sets, fork\u0001\u0000\u0003\u0002to\noB\t\u0001\n-order Mark ovmodels, each ofwhich hasbeen smoothed\noverawindo wofsize2\u0013\u0007\n\u0005to2-\u0007\nn,averaged overallqueries in\neach oftheTLF query sets. Unlik etheknown-item results, where\nthelower numbers were better because theyrepresented average\nrank, thevalues forthese variations experiments represent precision.\nHigher numbers arebetter .\nForeach query setwegive,asabaseline, theexpected value a\nrandom ranking algorithm would produce, foradocument collection\nofsizeandwith relevantdocument count equal tothose ofthevarious\nquery sets. Forexample, theTwinkle setonly has26variations, soa\nrandom ranking ofthecollection yields amean precision atthetop\n5documents of0.0077. TheLachrimae sethas75variations, soit\nisonly natural thatwith more relevantdocuments inthecollection,\narandom ranking ofthose documents will include more relevant\ndocuments towardthetopofthelist.Indeed, themean precision at\n5docs oftherandom algorithm ontheLachrimae setis0.0213.7 \u0001\u0013\u001b\u001a\n\u0005\u0013\u001b\u0013\n\u0001\u0016\u001f\u000f\n\rUsing anaudio-transcribed query toretrie vevari-\nations onapiece ofmusic isamuch harder problem. Wedonot\nconsider thisasolvedproblem byanymeans, butweareencouraged\nbytheresults wesee. First, itisclear thatourharmonic modeling\nalgorithm isdoing something correctly ,asityields signiﬁcant im-\nprovement overtherandom algorithm. Second, weonce again\nseethetrend thathigher order Mark ovmodels andmore harmonic\nsmoothing yield better results. Higher andlonger does notmono-\ntonically indicate better performance, butthetrend isnonetheless\napparent.\nWealso note thatsome query setsaremore difﬁcult than others.\nNotonly didwehavemore success ontheFoliavariations than on\ntheTwinkle variations, butafter listening totheactual pieces, itis\nclear than human judges would havemore difﬁculty picking outthe\nTwinkle variations than theywould theFoliavariations. Neverthe-\nless, evenforthese more difﬁcult Twinkle variations, almost 3of\nthe5toprankeddocuments are,onaverage, relevantvariations. We\nfeelthisisarespectable result.\n9.CONCLUSION\nItisnowclear thatretrie valofpolyphonic scores using polyphonic\naudio ispossible. By\"taking apart\" (transcribing) anaudio music\nquery andharmonically modeling themusically-salient pitch fea-\ntures wearebridging thegapbetween audio andsymbolic music\nretrie val,anddoing sowithin thedifﬁcult polyphonic domain.\nThat wehaverestricted ourselv esinthispaper topiano (asingle\ntimbre) isnotalimitation asmuch asitisanindication offuture\npotential. Wedidnothavetoperfectly recognize everysingle note\ninapiece ofmusic inorder fortheharmonic modeling tobesuccess-\nful.Therefore, future audio transcription methods which attempt toPolyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\u0000\u0002\u0001\u0004\u0003\u0006\u0005\n\t43\u000e\r\u0001\u0000\u0001\n\u0007 \u0001\u0001\n\u001e0\u0001\u0016\u001f\u000f\u0012\u0013-\u0000\n\u0007\u0001\u0018\u000f\u0019\u0013\u001b\u001a\n\u0007 \u0001\u001d\u001c\u0019\u001e\u0016\u0001\u0016\u001f\u000f! \u0003\u0002\u0005\u0004\n\t\u0001\u0018\u000f\n\t\f\u000b\u0006\t\u0014\u0007\u0001\n\u0003.\t\u0007\u0006G\u0007\n\t\u001a\n\u0001\u0013\n\u0001\u0016\u001f\u000f9B?<>O\u000eD\u001d[>M_y_\nk_y_\n\u0005_y_\no#<tO,N\u000eA\f?\n\u0005 k\u0012p@\u0005/$\u0007n k\u0012p \u0005$:\tk k\u0012p@\u0005/$\tm#<tO,N\u000eA\f?\no k\u0012p@\u0005/$\tm k\u0012p \u0005\u001b$ :\n('& \u0007\t\b\u000b\n#<tO,N\u000eA\f?\n: k\u0012p@\u0005/$\tm k\u0012p \u0005\u0018o\to k\u0012p@\u0005\u001c: \u0005#<tO,N\u000eA\f?\nn k\u0012p@\u0005$8\to k\u0012p \u0005$:\u0010l k\u0012p@\u0005Vk \u0005aYG\u0007O\u001aN\u001dA\t_\n\u0007\nk\u0012p k\u0007k\u0010l\u0007o\n)G\u001fv{;\u001dCE<t_`G\u001fM_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\f?\n\u0005 k\u001dp \u0005\u001b$\u0007m k\u001dp k%$\u0007n k\u001dp k :\u0002:#<>O\u001aN\u001dA\f?\no k\u001dp \u0005\u001b$\u0007m k\u001dp \u0005Wn\u0010k k\u001dp k\tj\u001fn#<>O\u001aN\u001dA\f?\n: k\u001dp \u0005\u001b$\u001fn k\u001dp \u0005$8\to k\u001dp \u0005\u0018l\u001fm#<>O\u001aN\u001dA\f?\nn k\u001dp \u0005\u001b$\to k\u001dp \u0005$8\u0007j\n(,& \u0007\f\n2\u0007aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\nk\u001dp k=\u0005\u000e\u0005\u0018o\ng,A\t[><tG_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\u0018?\n\u0005 k\u001dp :'8\u0007l k\u0012p o=\u0005\u001b$ k\u0012p \u0005$:1$#<>O\u001aN\u001dA\u0018?\no k\u001dp :'8\u001fj k\u0012p :1$\u0010l k\u0012p o=\u0005\fj#<>O\u001aN\u001dA\u0018?\n: k\u001dp :'8\u001fm\n(,& *\r\b\u000b\nk\u0012p : :\u0007n#<>O\u001aN\u001dA\u0018?\nn k\u001dp :\tm\u001fn k\u0012p n\tn\nl k\u0012p :\tj\tkaYG\u0007O,N\u000eA\t_\n\u0007\nk\u001dp k\tk\u0007m'8\u0000\u0002\u0001\u0010\u0003\u0006\u0005\n\t *\b\r\u0001\u0000\u0001\n\u0007 \u0001\u0001\n\u001e0\u0001\u0016\u001f\u000f\u0019\u0013-\u0000\n\u0007\u0001\u0010\u000f\u0019\u0013\u001b\u001a\n\u0007 \u0001\u001d\u001c\u0019\u001e0\u0001\u0016\u001f\u000f  \u001b \u000e\u0002\u000f\u0004\n\t\u0001\u0018\u000f\n\t \u000b\u0006\t\u0014\u0007\u0001\n\u0003\u001a\t\u0007\u0006G\u0007\n\t\u001a\n\u0001\u0013\n\u0001\u0016\u001f\u000f9B?<>O,D\u0012[>M_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\f?\n\u0005 k\u001dp \u0005Vn\u0010l k\u001dp \u0005\u000e\u0005\u0001\u0005 k\u001dp \u0005\u0018l\u001fk#<>O\u001aN\u001dA\f?\no k\u001dp \u0005Vn\u0010l k\u001dp \u0005Vn\tj\n(,& \u0007\f\u0010\u00103#<>O\u001aN\u001dA\f?\n: k\u001dp \u0005Vn\u0010l k\u001dp k\tj\tl k\u001dp \u0005\u000e\u0005$8#<>O\u001aN\u001dA\f?\nn k\u001dp \u0005$:\u0007k k\u001dp \u0005\fk\u001fn k\u001dp k\tm\u0002:aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\nk\u001dp k\tk\tl\to\n)G\u0007v:;\u000eCE<>_`G\u0007M_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u000eA\f?\n\u0005 k\u0012p \u0005\u00048\to k\u001dp k\u0010l $ k\u0012p k :\tk#<>O\u001aN\u000eA\f?\no k\u0012p \u0005\u00048\u001fn k\u001dp \u0005$:%$ k\u0012p k\tj1$#<>O\u001aN\u000eA\f?\n: k\u0012p \u0005\u00048\u0002: k\u001dp \u0005\u00048\u00028 k\u0012p \u0005\u001b$\u0010o#<>O\u001aN\u000eA\f?\nn k\u0012p \u0005\u00048\to k\u001dp \u0005\fm=\u0005\n(,& \u0007\f\n\u0011\u0010aYG\u0007O,N\u000eA\t_\n\u0007\nk\u0012p k \u0005\u000e\u0005\fo\ng\u000eA\t[><tG_y_\nk_y_\n\u0005_y_\no#<>O,N\u000eA\f?\n\u0005 k\u001dp :\u0002: : k\u001dp \u0005\u00048\u0007o k\u001dp \u0005Vk\u0010l#<>O,N\u000eA\f?\no k\u001dp :\u0002:'8 k\u001dp : \u0005\fl k\u001dp \u0005$8\u0007m#<>O,N\u000eA\f?\n: k\u001dp :\u0002: \u0005\n(,& *\u0006\u000b.\u000bk\u001dp o\u001fm\u0007n#<>O,N\u000eA\f?\nn k\u001dp :\to\u0007m k\u001dp :\tm\u0007j k\u001dp :\to\u0007jaHG\u001fO\u001aN\u001dA\t_\n\u0007\nk\u0012p k\u0007k\tm 8\u0000\u0002\u0001\u0004\u0003\u0006\u0005\n\t\u0012\u0010\u000e\r\u0013\u0000\u0001\n\u0007 \u0001\u0001\n\u001e0\u0001\u0016\u0018\u000f\u0019\u0013-\u0000\n\u0007\u0001\u0018\u000f\u0019\u0013\u001b\u001a\n\u0007\"\u0001\u001d\u001c\u0012\u001e0\u0001\u0016\u001f\u000f  \u0003\u0002\n\u0006G\u0007\n\t\u001a\n\u0001\u0013\n\u0001\u0016\u001f\u000f \u0001\n\u001e\u0015\u001e\u0016\n\u001c\u0014\u00104\u0007 \t \u001e\u0016\u0007 \u0001\u0004\t \u000b\u0006\t\f\u0015!\u001cB\u0001\u0004\t\u001a\n\t\u00139B?<>O\u000eD\u001d[>M_y_\nk_y_\n\u0005_y_\no#<tO,N\u000eA\f?\n\u0005\n(,&\u0016\u0010\u000b\n.\u000bk\u001dp :\to\u0002: k\u0012p n1$\u0010o#<tO,N\u000eA\f?\no k\u001dp l\u00028 8 k\u001dp l\u001fk\tk k\u0012p l\u0016\u0005\u0018l#<tO,N\u000eA\f?\n: k\u001dp l\u00028 8 k\u001dp n : \u0005 k\u0012p n\tm\u0010l#<tO,N\u000eA\f?\nn k\u001dp l\u001fm\u0010l k\u001dp n\tm\u0010l k\u0012p n \u0005\u0018laYG\u0007O\u001aN\u001dA\t_\n\u0007\nk\u0012p k\u0007k'8\u00028\n)G\u001fv{;\u001dCE<t_`G\u001fM_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\f?\n\u0005 k\u001dp n\u0010j%$ k\u001dp k%$'8 k\u001dp k\u0010l $#<>O\u001aN\u001dA\f?\no k\u001dp l\u0007k=\u0005 k\u001dp :=\u0005\u00048 k\u001dp k\tj%$#<>O\u001aN\u001dA\f?\n: k\u001dp n.8\u00028 k\u001dp l\u0007o\u0007k k\u001dp n\nl\u0016\u0005#<>O\u001aN\u001dA\f?\nn k\u001dp n\nl $ k\u001dp l\r: \u0005\n(,&03'\u0007 3aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\nk\u001dp k\to=\u0005$:\ng,A\t[><tG_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\u0018?\n\u0005 k\u001dp $\tj\to k\u0012p \u0005\fk\u0007n k\u0012p o=\u0005\u0018o#<>O\u001aN\u001dA\u0018?\no k\u001dp $\tm\u0007k k\u0012p n\tn\tn k\u0012p o\u0007k\tk#<>O\u001aN\u001dA\u0018?\n: k\u001dp 8\u0007k\u001fn\n(,&0+\u0018+\u0004*k\u0012p l\u001fn\tn#<>O\u001aN\u001dA\u0018?\nn k\u001dp 8\u001fn\tk k\u0012p m\tk\u0007n k\u0012p m \u0005\u001b$aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\nk\u001dp k\u0010o\u0000\u0002\u0001\u0010\u0003\u0019\u0005\n\t 3\u000e\r\u0001\u0000\u0001\n\u0007\"\u0001\u0001\n\u001e0\u0001\u0016\u001f\u000f\u0019\u0013 \u0000\n\u0007\u0001\u0010\u000f\u0019\u0013\u001b\u001a\n\u0007 \u0001\u001d\u001c\u0019\u001e0\u0001\u0016\u0018\u000f  \u001b \u000e\u0002\n\u0006G\u0007\n\t\u001a\n\u0001\u0013\n\u0001\u0016\u001f\u000f \u0001\n\u001e-\u001e\u0016\n\u001c\u0017\u0010 \u0007\n\t \u001e\u0016\u0007 \u0001\u0004\t \u000b\u0006\t\f\u0015 \u001cB\u0001\u0004\t\u001a\n\t\u00139B?<>O,D\u0012[>M_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u001dA\f?\n\u0005 k\u001dp n\u0010m\tl k\u001dp o\u0007m\tl k\u001dp n\u0010k\u0007m#<>O\u001aN\u001dA\f?\no k\u001dp l=\u0005\fl\n(,&\u0016\u0010.3\u0011\nk\u001dp n':=\u0005#<>O\u001aN\u001dA\f?\n: k\u001dp l\u0002:=\u0005 k\u001dp :\u0007n1$ k\u001dp n\tn1$#<>O\u001aN\u001dA\f?\nn k\u001dp n':\u0007j k\u001dp :\tj\to k\u001dp : :=\u0005aHG\u001fO\u001aN\u000eA\u0007_\n\u0007\nk\u001dp k\tk 8 8\n)G\u0007v:;\u000eCE<>_`G\u0007M_y_\nk_y_\n\u0005_y_\no#<>O\u001aN\u000eA\f?\n\u0005 k\u0012p n $ \u0005 k\u001dp k\u0007n\tk k\u0012p k :\u0010o#<>O\u001aN\u000eA\f?\no k\u0012p n\tn\u0010k k\u001dp o=\u0005/$ k\u0012p k\u0010l\u0007j#<>O\u001aN\u000eA\f?\n: k\u0012p n\no 8 k\u001dp l\to\r: k\u0012p n#\u0005\fj#<>O\u001aN\u000eA\f?\nn k\u0012p n\tn\u0010k k\u001dp n\u0010j\u0007j\n(,&03'\u0007\f\naYG\u0007O,N\u000eA\t_\n\u0007\nk\u0012p k\u0010o=\u0005\u001c:\ng\u000eA\t[><tG_y_\nk_y_\n\u0005_y_\no#<>O,N\u000eA\f?\n\u0005 k\u001dp $\to\u0007m k\u001dp k\u0010l $ k\u001dp \u0005\u0001\u0005\u0018o#<>O,N\u000eA\f?\no k\u001dp $ 8\to k\u001dp n\u0010k\u001fn k\u001dp \u0005Wn\tn#<>O,N\u000eA\f?\n: k\u001dp $\to\u0007m\n(,&\u0018\b\u0004+\u0018+k\u001dp n\tm\tk#<>O,N\u000eA\f?\nn k\u001dp $\u0007k\tm k\u001dp 8\to\u001fm k\u001dp 8\r:\u0010oaYG\u0007O,N\u000eA\t_\n\u0007\nk\u0012p k\u0010o\ntranscribe theevenmore difﬁcult polytimbral, polyphonic domain\nmay dosowith theconﬁdence thatthetranscription need notbe\nperfect inorder togetgood retrie valresults.\nThesame technique which givesusrobust,error -tolerant retrie valof\nknown-item queries (Section 8.2)isalsouseful forretrie ving varia-\ntions (Section 8.3). Indeed, atonelevelofabstraction, acomposed\nvariation canbethought ofasan“errorful transcription” oftheorigi-\nnalpiece. Ourharmonic modeling approach succeeded incapturing\nadegree ofinvariance, adegree ofsimilarity ,across such “transcrip-\ntions”. Thetechnique, though farfrom perfect, isanimportant ﬁrst\nstepforpolyphonic (audio andsymbolic) music retrie val.\n10.FUTURE WORK\nWefeel oneuseful direction forthisworkistobypass thetran-\nscription phase andgodirectly from audio features toaharmonic\ndescription. This willmakethemodeling phase slightly more difﬁ-\ncult, butthere might beadvantages tobypassing thetranscription, as\nthetranscription isonly used tocreate harmonic descriptions. This\nwould bring uscloser tosome harmonic-recognition workbeing\ncarried outbyothers inthepure audio domain such asbyCarreras\netal[7],orFujishima [15].\nAsecond direction istomodify theharmonic description smoothing\nalgorithm. Wepropose inthefuture toadopt either a(millisecond)\ntime-based ora(rhythmic) beat-based windo wsmoothing approach,\nrather than theevent-based approach weuseinthispaper .Wewillsum theharmonic contrib utions inthewaydescribed aboveacross\nsimultaneities within thewindo wininverseproportion totheir time\norbeat-based distance from thecurrent simultaneity ,with additional\nweightings provided according tometrical stress, note duration or\nother factors thatmight beconsidered helpful. Indeed, harmonic\nsmoothing, properly executed, might beawayofintegrating the\nproblematic, not-quite-orthogonal dimensions ofpitch andduration\nwithin apolyphonic source. Better time-based smoothing might also\nyield aricher harmonic description, because itgiveslessweight to\ntransient changes inharmon yarising from non-harmonic notes such\naspassing tones orappoggiaturas.\nAthird direction deals with passage levelretrie val.Rather than\nmodeling entire documents, itmight beuseful tomodel portions\nofdocuments, particularly ifthose portions aremusically salient.\nFinally ,theissue ofstandardized testcollections remains important.\nWeareinterested inparticipating insuch experiments, tocompare\noursystem with others thatwillbedeveloped inthefuture.\n11.ACKNOWLEDGEMENTS\nWewould liketothank Eleanor Selfridge-Field, Craig Sapp, and\nBret Aarden fortheir patient assistance with theCCARH data, which\nweused asourprimary source collection. Wewould liketothank\nNaxos fortheuseoftheir Bach Prelude andFugue audio recordings.\nFinally ,Samer Abdallah deserv escredit asanearly inspiration for\nsome oftheharmonic description assumptions made inthispaper .Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\n12.REFERENCES\n[1]AMNS. Nightingale music notation softw are, 2001.\nhttp://www .ngale.com.\n[2]J.P.Bello, L.Daudet, andM.B.Sandler .Time-domain poly-\nphonic transcription using self-generating databases. In\u0000 \u000f\b\u0011\u0002\u0001\u0012\u001e&\u001e&\u0017\u001e\u000e\u0018 $.(\u000e \u0011\u0004\u0003 \u001c \u0013 &\u0006\u0005\u0002\u0005\b\u0007\u0016\u001c \u0013\n\t \u0011\u000e$\u000f,\r&\u0019$4\u001c\u0003\u0018\u001b\u0011\u000e$\u001f\u0011\u0004\u0003 \u001c \u0013 &\f\u000b'\" \u001e\u000e\u0018\u001b\u0011\u000e\r $.(\u000e\u0018\u0003$ &+&\u0019\u000f\u000f\u0001\u0018\u0003$.(\u0011\u0010\u0014\u0011\u0007\u0012 \u0018 & \u001c\u0003\u001d,Munich, German y,May 2002.\n[3]J.P.Bello andM.B.Sandler .Blackboard system andtop-\ndownprocessing forthetranscription ofsimple polyphonic\nmusic. In\u0000 \u000f \u0011\u0007\u0012 &+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u0011\u0004\u0003 \u001c \u0013 &\u0012\t\u0014\u0013\u0015\u0010\u0017\u0016\u0019\u0018\u0014\u0001\u001b\u001a\u001c\t \u0011\u000e$\u001d\u0003\u0017&\u0019\u000f &\u0019$\u0014\u0012\u001e& \u0011\u000e$\u001e\u00180(\u000e\u0018\u0003\u001c\u001b\u000b\u0004\u001a\u001f\u000b'\" \u001e\u000e\u0018\u001b\u0011 \r\"! &\u0017\u0012\u0007\u001c\u0003 $#\n\u001e\u000b\u000e%51\u0002\u0001'&\u0002&\u000f( ,Verona, Italy,December\n7-92000.\n[4]W.Birmingham, R.B.Dannenber g,G.H.Wakeﬁeld,\nM.Bartsch, D.Bykowski, D.Mazzoni, C.Meek, M.Mel-\nlody,andW.Rand. Musart: Music retrie valviaaural queries.\nInJ.S.Downie andD.Bainbridge, editors,\u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u0004\u0003\u001c \u0013 &)\u0007\u000e$\u0014\u001e*\u000b'$4$#\"#\u000b\u000e\u001a,+\r$4\u001c &\u0019\u000f\r$4\u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$4\u000b\u0004\u001a-\u0010#\u001d\u0002%'\f \u0011\u000e \u0018\u0003\" % \u0011\u000e$ ) \"= \u0018\u001b\u0012.+\r$,\u0001\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$\u001c/\u0006& \u001c\u0003\u000f!\u0018 &\u0017,-\u000b\u0004\u001a0#1+2\u0010 )\n+\u0004/\u0015( ,pages 73–81, Indiana Uni-\nversity ,Bloomington, Indiana, October 2001.\n[5]W.Birmingham, B.Pardo, C.Meek, andJ.Shifrim. Themusart\nmusic-retrie valsystem. In\n\u001e\u000143 \u0018 \u0015 ) \u000b$(\u0004\u000b65\u0019\u0018\u0003$ &,February 2002.\nAvailable at:www .dlib.org/dlib/february02/02contents.html.\n[6]J.Bloch and R.Dannenber g.Real-time accompaniment of\npolyphonic keyboard performance. In \u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u001b\u0003/\u001c \u0013 &\u000587\u000298:;+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$\u0014\u000b\u0004\u001a0\t \u0011\u0002% \f\u0010\" \u001c &\u0019\u000f ) \"= !\u0018\u0003\u0012<\t \u0011\u000e$\u001d\u0003\u0017&\u0019\u000f &\u0019$\u0014\u0012\u001e&,pages\n279–290, Vancouv er,1985.\n[7]F.Carreras, M.Leman, andM.Lesaf fre.Automatic harmonic\ndescription ofmusical signals using schema-based chord de-\ncomposition.==\u0011\u0001\" \u000f!$\u0014\u000b\u000e\u001a \u0011\u0004\u0003.>\b&\u000f? ) \"= \u0018\u001b\u0012@/\u0006&\u0019 &\u0017\u000b\u000e\u000f\b\u0012\r\u0013 ,28(4):310–\n333, 1999.\n[8]C.W.Cleverdon, J.Mills, andM.Keen. % \u000b\u000e\u0012\u0007\u001c\u001b\u0011\u000e\u000f\r \n\u001e& \u001c &\u0019\u000f % \u0018\u0003$,\u0001\u0018\u0003$.( \u001c \u0013 &\u0011\u0000 &\u0019\u000f1\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0001$\u0014\u0012\u001e& \u0011\u0004\u0003A+\r$4\u001e.&+1\u0001\u0018\u0003$.(\u0012\u00104\u001dB \r\u001c &\u0017%  \u001bB<C \u0011\u0004\u001a \" % &D+E\u0001\u001e&\u0019 \u00180(\u000e$FB.C \u0011\u0004\u001a \" % &G+\u0004+H\u0001\u0015\u0016 &\u0019 !\u001cI/ &\u0019 \" \u001a \u001c\u0003 .ASLIB Cranﬁeld Project,\nCranﬁeld, 1966.\n[9]S.E.Dixon. Onthecomputer recognition ofsolo piano music.) \u0018 *B\u000f \u0011\r\f \u0011\u0004\u001a \u001d \f4\u0013 \u0011\u000e$#\u0018 &,6,2000.\n[10] S.Doraisamy andS.M.R¨uger.Anapproach towardapoly-\nphonic music retrie valsystem. InJ.S.Downie andD.Bain-\nbridge, editors,\u0000 \u000f\b\u0011\u0007\u0012\u001e&+& \u001e\u0001\u0018\u0003$.(\u000e \u0011\u0004\u0003 \u001c \u0013 &\n\u0007\u000e$\u0014\u001e\u0012\u000b'$#$4\" \u000b\u0004\u001aH+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0002\u0001\u001c\u0003\u0018\u001b\u0011\u000e$4\u000b\u0004\u001a-\u0010#\u001d\u0002%'\f \u0011\u000e \u0018\u0003\" % \u0011\u0001$\u0013) \"  \u0018\u001b\u00120+\r$\u001d\u0003\u0019\u0011\u0001\u000f % \u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$\n/\u0006& \u001c\u0003\u000f\r\u0018 &\u0017,-\u000b\u000e\u001a\f#1+2\u0010,\u0001)\n+\u0004/\u0015( ,pages 187–193, Indiana University ,Bloomington, In-\ndiana, October 2001.\n[11] M.Dovey.Analgorithm forlocating polyphon icphrases within\napolyphonic piece. In \u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u0004\u0003\f\u000b\u000e+2\u0010KJL\u00104\u001d\r% \f \u0011\u000e \u0018\u0003\" % \u0011\u000e$) \"= \u0018\u001b\u0012 \u000b\u0004\u001aG\t \u000fF& \u000b\u000e\u001c\u0003\u0018/,\u0007\u0018\u001b\u001c\u0003\u001d ,pages 48–53, Edinb urgh,April 1999.\n[12] R.S.Engelmore and A.J.Morgan.J \u001a \u000b\u0004\u0012 *\u000e\u0015\u0017\u0011\u0007\u000b\u000e\u000f\b\u001eM\u00104\u001dB \r\u001c &\u0017%  .\nAddison-W esleyPublishing, 1988.\n[13] N.Fletcher andT.Rossing. \u0016 \u0013 &\u0015\u0000 \u0013=\u001dB \u0018\u001b\u0012  \u0011\u0004\u0003 ) \"= \u0018\u001b\u0012 \u000b\u0004\u001aF+\r$# !\u001c\u0003\u000f\r\"N\u0001% &\u0019$4\u001c\u0003 .Springer Verlag, 1991.\n[14] J.Foote. Arthur: Retrie ving orchestral music bylong-\nterm structure. In\u0000 \u000f \u0011\u0007\u0012 &+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u0011\u001b\u00032\u001c \u0013 &O\u0005\u0001 !\u001cP+\r$4\u001c &\u0019\u000f\r$4\u000b8\u0001\u001c\u0003\u0018\u001b\u0011\u000e$4\u000b\u0004\u001aQ\u0010#\u001d\u0002%'\f \u0011\u000e \u0018\u0003\" %R\u0003\u0019\u0011\u000e\u000f ) \"= \u0018\u001b\u0012S+\r$\u001d\u0003\u0019\u0011\u0001\u000f % \u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$T/\u0006& \u001c\u0003\u000f\r\u0018 &\u0017,-\u000b\u0004\u001a#1+2\u0010 )\n+\u0004/\u0015( ,Plymouth, Massachusetts, October 2000. See\nhttp://ciir .cs.umass.edu/music2000.\n[15] T.Fujishima. Realtime chord recognition ofmusical sound:\nasystem using common lispmusic. In \u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u0004\u0003 \u001c \u0013 &\u0005878787\n+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$\u0014\u000b\u0004\u001a0\t \u0011\u0002% \f\u0010\" \u001c &\u0019\u000f ) \"= !\u0018\u0003\u0012<\t \u0011\u000e$\u001d\u0003\u0017&\u0019\u000f &\u0019$\u0014\u0012\u001e&,pages\n464–467, Beijing, China, 1999.[16] A.Ghias, J.Logan, D.Chamberlin, andB.Smith. Query by\nhumming -musical information retrie valinanaudio database.\nIn\u0000 \u000f \u0011\u0007\u0012 &+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u001f\u0011\u0004\u0003)\u000b)\t )U+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u000e\u001c\u0003\u0018\u001b\u0011\u000e$\u0014\u000b\u000e\u001a\u001b) \" \u001a \u001c\u0003\u0018/% & \u001e\u0001\u0018\u001b\u000b\u001c\t \u0011\u000e$,\u0001\u0003\u001c&\u0019\u000fF&\u0019$\u0014\u0012\u001e&V#W\u000b)\t ) ) )X(,pages 231–236, San Francisco, CA,\n1995.\n[17] H.H.Hoos, K.Renz, andM.G¨org.Guido/mir -anexper-\nimental music information retrie valsystem based onguido\nmusic notation. InJ.S.Downie andD.Bainbridge, editors,\u0000 \u000f\b\u0011\u0007\u0012\u001e&+& \u001e\u0001\u0018\u0003$.(\u000e \u0011\u001b\u0003 \u001c \u0013 &\u0015\u0007\u0004$\u0014\u001e\u0006\u000b'$#$4\" \u000b\u0004\u001a\b+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$\u0014\u000b\u0004\u001aF\u00104\u001d\r% \f \u0011\u0001 !\u0018 \" %\u0011\u000e$ ) \"= \u0018\u001b\u0012D+\r$Y\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$Z/ & \u001c\u0003\u000f!\u0018 &\u0017,-\u000b\u0004\u001a\u0006#W+2\u0010 )\n+\u0004/\u0015( ,pages 41–50,\nIndiana University ,Bloomington, Indiana, October 2001.\n[18] http://www .musedata.or g.The musedata collection, 2000.\nCenter forComputer Assisted Research intheHumanities\n(Stanford, CA).\n[19] A.P.Klapuri. Automatic transcription ofmusic. Master’ sthe-\nsis,Tampere Univeristy ofTechnology ,1998.\n[20] C.L.Krumhansl. \t \u0011+(\u000e$4\u0018\u001b\u001c\u0003\u0018/,\r&\u0015% \u0011\u000e\"=$\u0014\u001e\u000e\u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$# \u0011\u0004\u0003\u0011) \"= !\u0018\u0003\u0012!\u000b\u000e\u001a8\u0000 \u0018\u001b\u001c\u001b\u0012\r\u0013.\nOxford University Press, NewYork,1990.\n[21] K.Lemstr ¨omandJ.Tarhio. Searching monophonic patterns\nwithin polyphonic sources. In \u0000 \u000f\b\u0011\u0007\u0012\u001e&+& \u001e\u0001\u0018\u0003$.(\u000e \u0011\u0004\u0003/\u001c \u0013 &</[+\\\u000b)\u0013\t \u0011\u000e$Y\u0003\u001c&\u0019\u000f &\u0019$4\u0012 &,volume 2,pages 1261–1278, Colle geofFrance,\nParis, April 2000.\n[22] C.D.Manning andH.Sch¨utze. % \u0011\u000e\"=$4\u001e\u0004\u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$# \u0011\u0004\u0003*\u00104\u001c\u001b\u000b\u0004\u001c\u0003\u0018\u0003 !\u001c\u0003\u0018\u001b\u0012 \u000b\u0004\u001a> \u000b\u0004\u001c\u0003\"=\u000f\b\u000b\u0004\u001a]3 \u000b\u000e$.(\u000e\" \u000b$(.&@\u0000 \u000f\b\u0011\u0007\u0012\u001e&\u0019  \u0018\u0003$.(.MIT Press, 2001.\n[23] M.Marolt. Transcription ofpolyphonic piano music with neu-\nralnetw orks. In +\r$Y\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$ \u001c & \u0012\r\u0013\u0016$\u0014\u0011\u0004\u001a \u0011+(\u000e\u001d \u000b\u000e$\u0014\u001e & \u001a0&\u0017\u0012\u0007\u001c\u0003\u000f\b\u0011\u0004\u001c &\u0017\u0012\r\u0013N\u0001$\u0014\u0011\u000e\u001a \u0011+(\u000e\u001d*\u0003\r\u0011\u000e\u000f \u001c \u0013 & ) &\u0017\u001e\u000e\u0018\u001b\u001c &\u0019\u000f!\u000f\b\u000b\u0001$\u0018&\u0017\u000b\u000e$ \u0012 \u0011\u000e\"=$\u0014\u001c\u0003\u000f!\u0018 &\u0019 2^XC \u0011\u0004\u001a_^\u0014\u0007\bB0\u00104\u00180(\u000e$4\u000b\u0004\u001a\u000b\u000e$4\u001e \u0018 % \u000b$(.& \f\u0010\u000f \u0011\u0007\u0012 &\u0019  \u0018\u0003$'(a`b\u0000 \u000f \u0011\u0007\u0012 &+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u001bB\u0013)\n\r \u001a0&\u0002\t \u0011\u0001$c\u0007F&8&8&\u001dB\u00058&\u000e\u001c \u0013 ) &\u0017\u001e\u000e\u0018\u001b\u001c &\u0019\u000f!\u000f\b\u000b\u0001$\u0018&\u0017\u000b\u000e$\u0011\r \u001a0&\u0017\u0012\u0007\u001c\u0003\u000f\b\u0011\u0004\u001c &\u0017\u0012\r\u0013=$#\u0018\u001b\u0012 \u000b\u0004\u001ad\t \u0011\u000e$Y\u0003\u001c&\u0019\u000f &\u0019$4\u0012\u001e&,Cyprus,\nMay 29-31 2000.\n[24] K.Martin. Ablackboard system forautomatic transcription of\nsimple polyphonic music. Technical Report Technical Report\nNo.385, Perceptual Computing Section, MIT Media Labora-\ntory,1996.\n[25] R.J.McNab, L.A.Smith, D.Bainbridge, and I.H.\nWitten. The newzealand digital library melody in-\ndex.In\n\u001e\u000143 \u0018 \u0015 ) \u000b\u001c(\u0004\u000b65\u0019\u0018\u0003$\u0018& ,May 1997. Available at:\nwww .dlib.org/dlib/may97/melde x/05witten.html.\n[26] D.Meredith, G.Wiggins, andK.Lemstr ¨om. Pattern induc-\ntion and matching inpolyphonic music and other multi-\ndimensional datasets. In \u001c \u0013 &\n:\u0004\u001c \u0013fe \u0011\u000e\u000f\r\u001a \u001e\u0013) \"#\u001a \u001c\u0003\u0018W\u0001g\t \u0011\u000e$Y\u0003\u001c&\u0019\u000f &\u0019$4\u0012\u001e&\u0011\u000e$E\u00104\u001dB \r\u001c &\u0017% \u0018\u001b\u0012  \u001bB[\t \u001d\u000e\u0015\u001e&\u0019\u000f!$\u0018& \u001c\u0003\u0018\u0003\u0012  <\u000b\u000e$4\u001e@+\r$\u001d\u0003\r\u0011\u000e\u000f % \u000b\u0004\u001c\u0003\u0018\u001b\u0012  ,pages 61–66,\nOrlando, 2001.\n[27] G.Monti andM.Sandler .Pitch locking monophonic music\nanalysis. In\u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u0011\u0004\u0003\u001f\u001c \u0013 &;\u0005\u0002\u0005\b\u0007\u0016\u001c \u0013V\t \u0011\u000e$ ,\r&\u0019$\u0014\u001c\u0003\u0018\u001b\u0011\u0001$ \u0011\u0004\u0003\u001f\u001c \u0013 &\u000b'\" \u001e\u000e\u0018\u001b\u0011\u000e\r $.(\u000e\u0018\u0003$ &+&\u0019\u000f\r\u0018\u0003$'( \u00104\u0011\u0007\u0012 \u0018 & \u001c\u0003\u001dA#W\u000b[\r\u0014\u0010\u0002( ,Munich, German y,May\n10-13 2002.\n[28] J.Pickens.Acomparison oflanguage mode lingandproba bilis-\ntictextinformation retrie valapproaches tomonophonic music\nretrie val.In\u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u0004\u0003 \u001c \u0013 &X\u0005\u0001 !\u001cG+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$\u0014\u000b\u0004\u001a\u0017\u0010#\u001d\u0002%'\f \u00118\u0001 \u0018\u0003\" %h\u0003\u0019\u0011\u0001\u000f ) \"= \u0018\u001b\u0012\u0006+\r$\u001d\u0003\u0019\u0011\u0001\u000f % \u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$D/ & \u001c\u0003\u000f!\u0018 &\u0017,-\u000b\u0004\u001a\u0017#1+2\u0010 )\n+\u0004/\u0015( ,October\n2000. Seehttp://ciir .cs.umass.edu/music2000.\n[29] J.PickensandT.Crawford. Harmonic models forpolyphonic\nmusic retrie val.In \u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u0011\u001b\u0003 \u001c \u0013 &i\u000b)\t )j\t \u0011\u000e$Y\u0003\u001c&\u0019\u000f &\u0019$4\u0012\u001e&\u0018\u0003$Q+\r$Y\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0004\u001c\u0003\u0018\u0003\u0011\u000e$lk $4\u0011\u0002? \u001a0&\u0017\u001e$(.& \u000b\u000e$\u0014\u001e ) \u000b\u0001$\u0014\u000b$(.&\u0017% &\u0019$\u0014\u001cE#\\\t-+\u0004k@)\u0011( ,\nMcLean, Virginia, November 2002.\n[30] J.M.Ponte. \u000bm3 \u000b\u0001$.(\u000e\" \u000b$(.&-) \u0011\u0007\u001e'& \u001a \u0018\u0003$.(n\u000b \f\u0004\f\u0014\u000f\b\u0011\u0007\u000b\u0004\u0012\r\u0013 \u001c\u001b\u0011n+\r$Y\u0003\u0019\u0011\u000e\u000f2\u0001% \u000b\u000e\u001c\u0003\u0018\u001b\u0011\u000e$n/ & \u001c\u0003\u000f!\u0018 &\u0017,-\u000b\u0004\u001a.PhD thesis, University ofMassachusetts\nAmherst, 1998.Polyphonic ScoreRetrievalUsingPolyphonic AudioQueries: AHarmonic Modeling Approach\n[31] H.Purwins, B.Blank ertz, andK.Obermayer .Anewmethod\nfortracking modulations intonal music inaudio data format.\nciteseer .nj.nec.com/purwins00ne w.html.\n[32] W.Rand andW.Birmingham. Statistical analysis inmusic in-\nformation retrie val.InJ.S.Downie andD.Bainbridge, editors,\u0000 \u000f \u0011\u0007\u0012\u001e&\u001e&\u0017\u001e\u000e\u0018 $.(\u000e \u0011\u0004\u0003 \u001c \u0013 &[\u0007\u000e$\u0014\u001e\u0006\u000b'$4$#\" \u000b\u0004\u001a\b+\r$\u0014\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u001b\u0011\u0001$\u0014\u000b\u0004\u001aF\u00104\u001d\r% \f \u0011\u000e \u0018\u0003\" %\u0011\u000e$ ) \"= \u0018\u001b\u0012D+\r$Y\u0003\u0019\u0011\u000e\u000f\u001e% \u000b\u0004\u001c\u0003\u0018\u001b\u0011\u0001$Z/ & \u001c\u0003\u000f!\u0018 &\u0017,-\u000b\u0004\u001a.#1+2\u0010 )\n+\u0004/\u0015( ,pages 25–26,\nIndiana University ,Bloomington, Indiana, October 2001.\n[33] I.Shmule vich, O.Yli-Harja, E.Coyle,D.Povel,andK.Lem-\nstr¨om.Perceptual issues inmusic pattern recognition -com-\nplexityofrhythm andkeyﬁnd. \t \u0011 %'\f\u0010\" \u001c &\u0019\u000f\r '\u000b\u000e$4\u001e \u001c \u0013 &\u0001\u0000<\" % \u000b\u000e$I\u0001\u0018\u001b\u001c\u0003\u0018 &\u0019 ,35(1):23–35, 2001. Appeared alsointheProceedings of\ntheAISB’99 Symposium onMusical Creati vity.[34] G.Tzanetakis, G.Essl, andP.Cook. Automatic musical genre\nclassiﬁcation ofaudio signals. InJ.S.Downie andD.Bain-\nbridge, editors,\u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$.(\u000e \u0011\u0004\u0003 \u001c \u0013 &\n\u0007\u0004$\u0014\u001e\u0012\u000b'$#$4\" \u000b\u0004\u001aH+\r$4\u001c &\u0019\u000f\r$4\u000b8\u0001\u001c\u0003\u0018\u001b\u0011\u000e$4\u000b\u0004\u001a-\u0010#\u001d\u0002%'\f \u0011\u000e \u0018\u0003\" % \u0011\u000e$ ) \"= !\u0018\u0003\u0012@+\r$\u001d\u0003\r\u0011\u000e\u000f % \u000b\u0004\u001c\u0003\u0018\u001b\u0011\u000e$\n/\u0006& \u001c\u0003\u000f\r\u0018 &\u0017,-\u000b\u0004\u001a\f#1+2\u0010,\u0001)\n+\u0004/\u0015( ,pages 205–210, Indiana University ,Bloomington, In-\ndiana, October 2001.\n[35] A.Uitdenbogerd andJ.Zobel. Melodic matching techniques\nforlargemusic databases. In\u0000 \u000f\b\u0011\u0007\u0012\u001e&+&\u0017\u001e\u000e\u0018\u0003$'(\u000e \u0011\u0004\u0003l\u000b)\t )R+\r$,\u0001\u001c &\u0019\u000f!$\u0014\u000b\u0004\u001c\u0003\u0018\u001b\u0011\u0001$\u0014\u000b\u0004\u001a ) \" \u001a \u001c\u0003\u0018/% &\u0017\u001e\u000e\u0018\u0003\u000b;\t \u0011\u000e$\u001d\u0003\u001c&\u0019\u000fF&\u0019$\u0014\u0012\u001e&D#4\u000b)\t ) ) )X(,Orlando\nFlorida, USA, Oct. 1999. ACM, ACMPress."
    },
    {
        "title": "Indexing Music Databases Using Automatic Extraction of Frequent Phrases.",
        "author": [
            "Anna Pienimäki"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416632",
        "url": "https://doi.org/10.5281/zenodo.1416632",
        "ee": "https://zenodo.org/records/1416632/files/Pienimaki02.pdf",
        "abstract": "The Music Information Retrieval methods can be classified into online and offline methods. The main drawback in most of the offline algorithms is the space the indexing structure requires. The amount of data stored into the structure can however be reduced by storing only the suitable index terms or phrases instead of the whole contents of the database. Repetition is agreed to be one of the most important factors of musical meaningfulness. Therefore repetitive musical phrases are suitable for indexing purposes. The extraction of such phrases can be done by applying an existing text mining method to musical data. Because of the differences between text and musical data the application requires some technical modification of the method. This paper introduces a text mining-based music database indexing method that extracts maximal frequent phrases from musical data and sorts them by their length, frequency and personality. The implementation of the method found three different types of phrases from the test corpusconsistingof Irish folk music tunes. The suitable two types of phrases out of three are easily recognized and separated from the set of all phrases to form an index data for the database. \u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007 \u000e\r\u0010\u000f music retrieval, indexing, text mining.",
        "zenodo_id": 1416632,
        "dblp_key": "conf/ismir/Pienimaki02",
        "keywords": [
            "Music Information Retrieval",
            "online and offline methods",
            "space indexing structure",
            "reducing data storage",
            "suitable index terms",
            "repetition in music",
            "text mining method",
            "musical data indexing",
            "technical modification",
            "maximal frequent phrases"
        ],
        "content": "IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrases\nIndexingMusicDatabases UsingAutomatic Extraction of\nFrequent Phrases\nAnnaPienim¨aki\nDepartmentofComputer Science,UniversityofHelsinki\nPOBox26,FIN-00014 UniversityofHelsinki,Finland\n+358408389350\nAnna.Pienimaki@cs .Helsinki.FI\nABSTRA CT\nTheMusicInformation Retrievalmethods canbeclassiﬁed into\nonlineandofﬂinemethods. Themaindrawbackinmostofthe\nofﬂinealgorithms isthespacetheindexingstructure requires. The\namountofdatastoredintothestructurecanhoweverbereducedby\nstoringonlythesuitableindextermsorphrasesinsteadofthewhole\ncontentsofthedatabase.\nRepetition isagreedtobeoneofthemostimportant factorsof\nmusicalmeaningfulness. Therefore repetitivemusicalphrasesare\nsuitableforindexingpurposes. Theextraction ofsuchphrasescan\nbedonebyapplying anexistingtextminingmethodtomusical\ndata.Becauseofthedifferencesbetweentextandmusicaldatathe\napplication requiressometechnical modiﬁcation ofthemethod.\nThispaperintroduces atextmining-based musicdatabaseindexing\nmethodthatextractsmaximal frequentphrasesfrommusicaldata\nandsortsthembytheirlength,frequencyandpersonality .The\nimplementation ofthemethodfoundthreedifferenttypesofphrases\nfromthetestcorpusconsisting ofIrishfolkmusictunes.Thesuitable\ntwotypesofphrasesoutofthreeareeasilyrecognized andseparated\nfromthesetofallphrasestoformanindexdataforthedatabase.\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000fmusicretrieval,indexing,textmining.\n1.INTRODUCTION\nTheMusicInformation Retrieval(MIR)methodshavedeveloped\nrapidlysincetheintroduction oftheQuerybyHumming system\n[6].Sincethenthemaininteresthasmovedfromthesignalpro-\ncessingtaskstorepresentational andalgorithmic issues.Alsosome\naspectsofmusicology andcognitivepsychology havebeentaken\nintoconsideration.\nTheMIRmethodscanbeclassiﬁed inmanywaysusingdifferent\ncriteria.Oneofthemostfundamental waystoclassifyMIRmethods\nistodividethemintothosethatprocessdigitalized audiosignals\nusingdigitalsignalprocessing methodsandthosethatprocessdigital\nsymbolic representations. Becauseofthecomplexityofpolyphonic\nsignals,mostoftheMIRsystemsusesymbolic representations of\nmusicaldataastheirinnerrepresentations. Suchrepresentations\nusuallyareprocessed byusingvariationsofexistingstringmatching\nalgorithms.\nThereareseveraldifferentsymbolic representations usedasinner\nrepresentations inMIRsystems. TheﬁrstMIRmethodsusedthe\nthreeletteralphabetdescribing themelodiccontourofthemono-\nphonicmelody[6].Eventhoughmostoftheexistingrepresentations\ndescribeeachnotewithoneormoresymbols, therearealsosome\ncompressed representations inwhichtheobjectdescribed isacom-\nbinationofsequential notes[4,3].Thefunctionality ofmostof\nthemethodsuseddependsstronglyonthestructureoftherepresen-\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage. \u0011\n\u0012\n2002IRCAM-CentrePompidoutationandapplying themtoprocessotherrepresentations requires\nmodiﬁcations tothemethod. Howevertherearefewmethodsthat\narealsocapabletoprocessasetofdifferentrepresentations [9,10].\nAnotherquitefundamental classiﬁcation ofMIRmethodsisbased\nontheuseofindexing.Intheonlinemethodsthewholedatabaseis\nprocessed whenexecutingaquerywhileintheofﬂinemethodsthe\nqueryisexecutedusingtheindex.Eventhoughthereareefﬁcient\nindexstructures likesufﬁx-tree[12],inmanycasesthewholeindex\nrequirestoomuchspace.Therefore thereisaneedfornewkinds\nofmethodsthatalsoreducetheamountofthedatastoredintothe\nindexstructure.\nTherearecertainsimilarities intheuseoftextandmusicaldata\nwhichallowalsoapplying textminingmethodstoprocessmusical\ndata.Whenapplyingsuchtextprocessing methods, onehastotake\ncareofthetechnical differencesbetweentextandmusicaldata.The\nmaindifferencesincludepolyphony,multidimensionality ofdata\nitems,andtransposition invariance.\nThispaperintroduces atextmining-based indexingmethodforsym-\nbolicrepresentation ofmusicaldata.Themethodextractsmaximal\nfrequentpatternsfrommusicaldataandsortsthembytheirlength,\nfrequencyandpersonality .Thefoundphrasesorsomesubsetof\nthemcanbestoredintotheindexstructure andusedasindexterms\nintheMIRsystem.\n2.INDEXING MUSICDATABASES\nAsmentioned above,MIRmethods canroughlybecategorized\nintoonlineandofﬂinemethods. Theofﬂinemethodscanalsobe\ndividedintotwogroupsonthebasisofthedatastoredintotheindex\nstructure. Mostoftheofﬂinemethodsuseindexingwhichstoresthe\nwholecontentsofthedatabase intoamoreefﬁcientstructure like\nasufﬁx-tree[12]oritsvariants[4].Themaindrawbackofthis\napproach istheamountofspacesuchanindexstructure requires\n[7].Therefore thesizeofthestructure isoftenreducedbyusing\nsimplealphabets [4,3].\nAnotherapproach toindexingistousesimilarstructures butstore\nonlysomepartsofthedataintoit,sothereduction ofindexsize\ndoesnotlimitthealphabet [8].Inthisapproach themostdescrip-\ntivemusicalpatternsareextractedfromthedataandusedasindex\nterms.Eventhoughthedatamightincludemanylongmeaningful\npatterns,thereduction oftheamountofdataisconsiderable. Ifthe\ndataincludesmanymeaningful patterns,thesemayalsobesortedby\nsomeoftheirproperties. Becausetherepetition iswidelyagreedto\nbeoneofthemostimportant factorsthatindicatethemusicalmean-\ningfulness ofthephrase,theextraction ofthemeaningful phrasesis\ndoneonthebasisofrepetition inthiswork.\nTheextraction ofrepetitivepatternsisofinterestalsointhearea\noftextmining.Whentheanalysed dataissemi-orunstructured,\ntheimportant indexwordsorphrasescannotbeidentiﬁed usingthe\nknowledgeofthetextstructure. Becausethemosttypicalsymbolic\nrepresentations ofmusicaldatacanbeseenasunstructured data,the\nmethodsdevelopedfortheneedsoftextminingmayalsobeapplied\ntomusicalcontexts.IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrases\nAhonen-Myka [1]hasintroduced atextminingalgorithm, which\nextractsmaximal frequentsequences fromunstructured textdata.\nBecauseoftheproperties oftheAhonen-Myka method,itissuit-\nableforindexingunstructured musicaldata,aswell.However,the\ndifferentcontextrequiressomemodiﬁcations andaddedpartstothe\nalgorithm.\n3.APPLYINGTEXTMINING METHODS\nTOMUSICAL DATA\nTheideaofapplying thetextminingmethodtomusicalcontextsis\nbasedonsomesimilarities betweenmusicalandtextdata.Musical\ndataiscomparable totextformanyreasonsinspiteofthedifferences\nbetweenmusicandlanguage asphenomena. First,bothmusicaland\ntextdataareconsidered tobehierarchical –thesentences orterms\nandmusicalphrasesconsistofsmallerunitslikeletters,wordsor\nnotes.Second,inbothmusicalandtextdata,theorderofunitsis\nofimportance. Third,itispossiblethattherearesomeadditional\nunitsbetweentheunitsofthemeaningful phrase.Inmusicsuch\nadditional unitsareornaments forexample.\nThewaysmusicandlanguage areusedalsohavesomesimilarities.\nRepetition isofhighimportance inconversationbutalsoinwritten\ntext.Theimportant ideas,opinionsorfactsarestressedbyrepeating\nthemwithsomevariation. Eventhoughinmusicthereareno\nsuchdirectopinionsoffacts,repetition stillhasquiteanimportant\nmeaning–repeating patternsareusedasthemesorhooks.Themes\nandhooksusuallyoccuralsowithorwithoutvariationandareeasily\nremembered [2].\nInadditiontorepetition thevariationandcontrasts arealsoused\nwhendrawingone’sattentiontoimportant ideas.Burns[2]stresses\nthatrepetition actuallyisquiteinsigniﬁcant withoutitscounterpart\nvariation.Themorecontrasting thevariationis,themorelikelyitis\ntobenoticed.However,thepurevariationwithoutanyrepetition is\ncorrespondingly meaningless.\nWhenapplying atextminingmethodtomusicaldata,onehasto\ntakecareoftheabove-mentioned technical differencesbetweentext\nandmusicaldata.Twodifferentapproaches havebeenintroduced\ntotheprocessing ofthepolyphonic data.Thesimplerwayisto\nprocesseachvoiceseparately [11].Thisapproach, however,does\nnotﬁndpatterns, whichoccurbetweendifferentvoices.Another\napproach istogiveatotalordering onallthenotes[9].Inthis\napproach asuitablegapsouldbeallowedbetweenthenoteitems\nwhenmatching thepattern.\nThemultidimensional datacanberepresented usingsetsorvectors\n[9]thatdescribedataitems.Inpatternmatching thematcheditems\narewholesetsorvectorsinsteadofindividuallettersornumbers.\nEventhoughsomedimensions insetsorvectorsmightrepresenttime\nevents,theireffectonpatternmatching caneasilybeeliminated\ncalculating thedistinction betweentwovectorsorsetsinsteadof\nusingabsolutevectorsorsets.\nUsingsuchcalculated relativesetsorvectorsalsosolvestheproblem\noftransposition invariance–thesimilarthemesstartingfromdiffer-\nentpitchesshouldberecognized tobethesame.Eventhoughthe\nabsolutepitchvaluesofnotesarenotthesame,therelationbetween\npitchesstayssimilar.Becausetherelativevectorsrepresent these\nrelations, themesarerecognized tobetransposes ofeachother.\n4.EXTRACTINGPHRASES FROMMUSI-\nCALDATA\nTheindexingmethoddescribed inthispapermanages bothhomo-\nphonicandpolyphonic data.Intheinnerrepresentation theprop-\nertiesofnotesarerepresented asvectordimensions. Therefore the\nalgorithm iscapableofhandlingmanykindsofnumerical symbolic\nrepresentations ofmusic.Thebasicideaofthealgorithm istoextractmaximal frequentse-\nquencesfromthegivenmusicalpieceandsortthembytheirprop-\nerties.Thesequences, whicharenotsubsequences ofalreadyfound\nmaximal sequences, areexpandedtobemaximal. Amaximal fre-\nquentsequence isthenasequence, whichappearsatleast\u0013times\ninapieceandwhichisnotcontained inanothermaximal frequent\nsequence.\nThesequences arerepeatedly constructed fromshortersequences\nuntilallthesequences ofthesamelengthareeithersubsequences\nofsomemaximal frequentsequence ornotfrequent. Theremay\nbeatmost \u0014dataitemsbetweenitemsofsequence, enabling the\ntreatment ofthepolyphonic data.Both\u0013(frequenc y)and\u0014(gap)\nvaluesaredeﬁnedbytheuser.\nTheextraction algorithm isbasedontheAhonen-Myka [1]algo-\nrithmwithsomeextraphases.Thetechnical requirements ofthe\nmusicaldatahavealsobeentakencareof.Thealgorithm consistsof\nthreeparts:thetechnical preprocessing phase,thediscoveryphase,\nandthesortingphase.\nThealgorithm takessymbolic representations ofmusicaldata,such\nasMIDI,asitsinputandusestheabove-mentioned relativevectors\nasitsinnerrepresentation. Theproperties ofthedataitemsthatare\nusedasvectordimensions aredeﬁnedbytheuser.\n4.1Deﬁnitions\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c1.$&%('\f)+*-,/.102)435.!6\u0010798:,;7<65=4)+>5,?>5,:@;=A358CB\u001bD\u001e)+0C=\u001dEF,G'5)+02%=4)\u001d.!@;=4)H35.\bB\u0010IFJ\u0017B\u0004KML2,/=4NO,P,/.Q=4NR3MS\u001e%('\f)+*-,/.102)435.!6\u00107\u0019>5,:@;=A358/0TB\u0004KU65.!'VB9I\u001bW\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c2. XYEF,\b8:,;7<65=4)+>5,Z'965=A6Q0[,/=]\\D\n)+0\b6\f.&358^'\u001b,/8P,:'\u00020;,/=3(_`8:,;7<65=4)+>5,`>5,:@;=A35820aBD\nW\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c3.b?E\u001b8:650;,?c\u0006)+0`65.d358:'9,/8P,:'e0;,/=V3^_`8P,;7<65=4)+>5,`>5,P@[%=A3\f8/0aBDHf\nE\u001965>g)+.\u0004hQ6-*Z6gi5)+*Z6\u00107\nh962jk3^_G\u0014lL/,/=4N?,P,/.\u0006,/>5,/82m-0;,Pn;oF,/.1=4)H6\u00107>5,:@;=A358MBD\nW\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c4.b?E\u001b8:650;,?pq)+0V6`02o1L(j1E\u001b8:650;,r3^_Cj1E\u00048:6\f0[,\u000ecs) _a,:69@2E8:,;7<65=4)+>5,\b>5,:@;=A358aB\u001bDO)+.-ps62j9j1,:658`)+.t=\u001dEF,`0/65*q,\u0017358^'\u001b,/8\u00176\u00107 0/3-)\u001d.`cTW\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c5. b?E\u001b8:650;,\u0017cu)+0v_/8P,:n;oF,/.F=Z)\u001d.w=\u001dEF,s8:,;7<65=4)+>5,x'\u001065=A60;,/=T\\D\n) _Ccy6/j9j1,:65820\u001765=O7 ,:650P=O)\u001d.-\u0013z7<3g@P65=4)H35.10r)+.e\\DAf\n\u0013wL2,/)+.\u0004hq=\u001dEF,h5)\u001d>5,/.{_28P,:n;oF,/.!@;mQ=\u001dE\u00048P,/0^E13\u00107<'9W\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u001a \u001f!\u001a\u001d\"#\u001c6.b?E\u001b8:650;,Gc|)+0\u00176e*q6[i5)+*q6\u00107\u0004_28P,:n;oF,/.F=]j1E\u00048:6\f0[,Z)+.\\D\nf\n) _O=\u001dEF,/8:,v'93},/0V.!35=T,:i5)+0P=T65.FmVj1E\u001b8:650;,?pZN\u000eE\u0004)H@2EZ)+0V6\u00107 0/3?_28P,:n;oF,/.F=)\u001d.\u0002\\ZDV6\f.!'Vc\u0006)\u001d0r6q02o1L(j1E\u001b8:650;,Z3^_GpFW\n4.2ConversionPhase\nTheconversionphaseincludesconversionfromMIDIdatatoinner\nrepresentation whichpresentseverynoteasavector.Inthisphase\ntheneededrelativevectorswithinthelimitofthegapvaluearealso\ncalculated andstoredintothetablestructure. Aftertheconstruction\nofthetable,theunfrequent relativevectorsareremovedbecause\ntheycannotformanyfrequentphraseseither.Thetableoffrequent\nrelativevectorsisthenusedinseveralsituations inthemainphase.~r\u0010\"#!\u001a\u001f\n#1.]35.1>5,/8/0P)H3\f.ej1E\u0019650;,.\u0010jFo\u0019=V\n:U\u000e7 ,o\u0004=j\no\u0004=VrF,:@;=A35820_[358\u0017,:69@2E\u0002.!3\f=,\b)+.\u0002\n:MC7 ,@P35.F>5,/82=O.\n35=,\b=A3Q6->5,:@;=A358`>F,:@;=A358/0\u0017\u001dF,:@;=A35820v>0/3582=ZF,P@;=A3\f8/0_[358\u0017,:69@2EZB5M)+.wF,:@;=A358/0IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrases_[358\b,:69@2EqB[e)\u001d.w\n,:@;=A35820f\nB\n]B;\b\u0006\u0004\fc\bzrF,:@;=A35820\u001dB\u0010\u001d^\u001dB\n\bzB\nJxB53g@:\u001dB;UJB\n\b+\n3g@:\u001dB;MJdB\nU )f ¡_[358\b,:69@2E¢>\u001e£\u001b)+.\n3g@) _r¤ BD\n¤\n_/8:,:n;o\n,/.\n@;m8P,/*Z35>5,\b)+.10P=A65.!@2,/0\b3^_r>\u001e£2_/8:35*¥vF,P@;=A3\f8/08P,/=4o\u000482.sv\n,:@;=A35820\n4.3DiscoveryPhase\nThediscoveryphaseisverysimilartotheAhonen-Myka algorithm\n[1].Theﬁrstpreprocessing phaseusestherelativevectortableto\nformfrequentpairswithinthelimitofthegapvalue.Thefound\npairsandtheirlocations arestoredintothe ¦-Gramsset.~`5\"#!\u001a\u001f!\u001e2. bO8P,j\n8^3g@/,/0P0P)+.\u001bh.\u0010j\no\u0004=GvF,P@;=A3\f8/0o\u0004=j\no\u0004=M§\u001b%P¨O8:65*\u00170_[358v)?+ª©Z=A3e¤ «r¬`­g®/¯(°5±\u0010²\u001b¤\fJd¦_[358¡\n+¥³H)V´uµ2¶{=A3s³H)V´|h96/jy´uµ/¶f\u0019¡l¤ «v¬`­g®2¯^°\f±9²\u001b¤\fJy) _MEF,:69'e+·rF,:@;=A35820(A) ^¡\n\b,:i5)+0P=40_[3\f8M¸Q\u001d¹³¡\n´ºµ/¶{=A3t³¡\n´|h96/j»´ºµ/¶f\n¸l¤ «r¬\b­g®2¯^°\f±9²\u001b¤) _U=A65)+7C+·rF,:@;=A35820¡\n^H¸(\b,:i5)+0P=40j\u001965)+8\u0017\u001d¹³4E1,P6\u0010'f\n=A65)47 ¶§\u001b%P¨O8:6\f*{0j\u001965)+8\u001d\u0017+º)f ¡;f\n¸_[358\b,:69@2EZj\u001965)+8r)+.t§9%2¨O8^65*{0) _r¤ c\n\u0004¼H±F¤\n_/8:,:n;o\n,/.\n@;m8P,/*Z35>5,Vj\u001965)\u001d8]_/8:35*§9%2¨O8^65*{08P,/=4o\u000482.x§\u001b%P¨O8^65*{0\nThemainphasetakesthe¦-Gramssetasitsinput.Foreachround\nofthemainloopeachgraminthe S-Gramssetisﬁrstcompared to\nthephrasesinthemaximal frequentphrasesetMax.Ifthegramis\nnotasubphrase ofanyofthefoundmaximal frequentphrasesitis\nthenexpandedtobemaximal. Thefoundmaximalphraseandallof\nitslocations arestoredintothesetMax.\nIfthegramwasalreadymaximal beforetheexpansion phaseitis\nremovedfromtheS-Gramsset.Theremaining gramsarejoined\ntoeachothertoform S`½sizedgramswhicharethenstoredinto\nthenewk-Grams set,thevalueofSbeingnowincreased. When\nstoringthegramsintotheset,theunfrequent newgramsarebeing\nremoved.Thealgorithm stopswhenallofthegramsareremoved\nfromthe S-Gramssetbecauseoftheirunfrequenc yormaximality .\nThe S-Gramssetcanalsobeprunedbeforethejoiningphase.~`5\"#!\u001a\u001f!\u001e3. x65)\u001d.qj1E1650;,.\u0010j\no\u0004=a§9%2¨O8^65*{0o\u0004=j\no\u0004=Vx6gi¸e+l§x6gie+¥³¶N\u000eE\u0004)47 ,M¸5%2¨O8^65*{0r)\u001d0U.!3\f=a,/*Uj\n=4m_[358\b,:69@2Ee¸5%+h58^65* h) _Vh-)\u001d0U.!3\f=G6q02o1L(j1E\u001b8:650;,Z3^_`65.1mq*)+.¢x6gi*q6[i\u0002+½¾]i/j\u001965.!'1³\u001dh;¶x6gie+·x6gi{*q6[i) _v*q6[i\u0002½h8P,/*Z35>5,vha_/8:35*¥¸5%P¨O8^65*{0³+¸5%2¨O8^65*{0\u0017\u001d·bO82o\u0019.!,5³+¸5%2¨O8^65*{0A¶P¶¸5%2¨?8:65*\u00170\u0017+À¿\u000435)+.Á³+¸5%P¨O8:6\f*{0A¶¸Q\u001dl¸¢´ºµ8P,/=4o\u000482.sx6[iTheexpansion phasetakesagramcasitsinputandrepeatedly\nsearchesforthenewfrequentgram p, ¤ p1¤being ¤ c\u000e¤g½.Thegramccanbeexpandedbyaddingonevectoratthebeginning,endorin\nthemiddleofthegram.Thepossibleexpansion vectorsarefound\nfromthefrequentrelativevectortable.~r\u0010\"#!\u001a\u001f\n#4.¾]i/j\u001965.F0P)H3\f.ej1E\u0019650;,.\u0010jFo\u0019=Rjo\u0004=j\no\u0004=RjG£8P,j1,:65=7 ,/=O7YL2,\u0017¤ cC¤.!'Uj\u0019E\u00048:650;,OjG£f\n¤ c\nD¤9ÂwÃ\f&f\nj¢)+0U6\u00170Po1L(j1E\u00048^650;,3^_OjG£) _?jG£\u001b)+0C_/8P,:n;oF,/.F=jd+|jG£o\u0004.F=4)475_/8:,Pn;oF,/.1=\u000ejG£1@26\f..\n35=GL2,]_[35o\u0004.!'8P,/=4o\u000482.ej\nThejoiningphasejoins Ssizedgramstoform S{·sizedgrams.\nPereachtwogramscQÂz\nK[Ä[Å<Å<Å<ÄFÆandprÂzÇ\nKgÄ[Å<Å<Å<ÄÇgÆthealgorithm\nexaminesifthesubgrams cD\nÂ·1I\nÄ;Å Å<Å<ÄÆand pD\nÂwÇ5K\nÄ[Å<Å<Å<ÄÇÆ\fÈYKare\nsimilarandthenjoinsthemtoformanewgramcFprÂz!K\nÄgÅ<Å Å<ÄÆ\nÄÇÆ.\nThefrequencyofthenewgramsisexaminedattheendofthephase.~r\u0010\"#!\u001a\u001f\n#5. ¿\u000435)\u001d..\u0010jFo\u0019=V¸5%P¨O8^65*{0o\u0004=j\no\u0004=\u0017³+¸t´Àµ/¶}%2¨?8:65*\u00170_[358\u0017,:69@2E-h58:6\f*É\u0010YÂw!K\nÄ[Å<Å<Å<ÄÆ_[358`,P6\u0010@2Eeh58:6\f*É[UÂzÇ\nKgÄ[Å<Å Å<ÄÇ[Æ) _O1I\nÄ[Å<Å<Å<ÄÆ\nÂwÇ\fK\nÄ[Å<Å<Å<ÄÇÆ\fÈÁKh58^65*Ê+z\u001eK\nÄgÅ Å<Å<ÄÆ\nÄÇÆ³+¸\u0017´yµ/¶}%P¨O8:6\f*{04h58:6\f*GM+y7<3g@P65=4)H3\f.Á³\u001dh58:65*U¶_[358\u0017,:69@2E-h58:6\f*Ë)\u001d.k³4¸¢´uµ2¶\f%P¨O8^65*{0) _r¤ \u0004±\f\u0019Ìd¤\n_/8:,Pn;oF,/.\n@;m8P,/*Z35>5,Uh58^65*º_/8:35*Í³+¸¢´Àµ2¶\f%P¨O8^65*{08P,/=4o\u000482.»³+¸¢´uµ/¶}%P¨O8:6\f*{0\nBecausesomeofthegramsmayexpandtomanymaximalfrequent\nphrases,allofthephrasesmaynotbefoundduringtheﬁrstroundof\nthemainloop.Therefore onlytheunfrequent andalreadymaximal\ngramscanberemovedfromthesetofgramsineveryround.Without\nthepruningphase,thealgorithm takesÃPJ{rounds,Ãbeingthelength\nofthelongestfoundmaximalphrase.Whenusingthepruningphase,\ntheamountoftheneededroundsismuchsmaller.\nThepruningphasegenerates pereachgram ZÂz\nK[Ä[Å<Å<Å<ÄÆtwosets\nofphrases,LMaxandRMax.ThesetLMaxconsistsofthemaximal\nphrasesthathavegram \u0004D#Âl\nK[Ä;Å Å<Å<ÄÆ\fÈÁKasitssubphrase. Theset\nRMaxisgenerated similarlynowusingthesubgram D\u001dD\nÂz1I\nÄ[Å<Å Å<ÄÆ.\nForeachphraseinLMaxthepossiblesufﬁxesforthegram\u0004Dare\nextractedandstoredintosetLStr.ThesetRStrisgenerated similarly\nbyextractingpreﬁxesforthegram\u0004D\u001dD.Eachcombination ofsufﬁxes,\ngramandpreﬁxesaretestedagainstthefoundmaximal phrases.If\nanewfrequentmaximal phraseisfound,allofitssubphrases are\nexamined andthe S-gramsofthosethatarenotsubphrases ofany\nmaximalphrasearemarked.Whenallthegramsoftheset S-Grams\nareprocessed, gramsthatarenotmarkedareremovedfromtheset.~r\u0010\"#!\u001a\u001f\n#6. bO8/o\u0004.!,.\u0010jFo\u0019=V¸5%P¨O8^65*{0o\u0004=j\no\u0004=V¸5%P¨O8:65*\u00170-³ jF8/o\u0004.\u001e,:';¶_[358\u0017,:69@2E-h\u0002z!K\nÄ[Å<Å<Å<ÄÆ\n)\u001d.\u0002¸5%2¨O8^65*{0IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrasesx6[i\u0002·ÎPcC¤ ck)+.\u0002x6[ie65.\n'\b\nK[Ä[Å<Å<Å<ÄFÆ}ÈÁK`)+0`60Po1L(j1E\u00048^650;,Z3^_Oj1ÏVx6giQºÎ:cC¤ c\u0006)\u001d.\u0002x6gie6\f.!'`\nI}Ä[Å<Å Å<Ä1Æ\u0002)+0r60Po1L(j1E\u00048^650;,Z3^_Oj1Ï_[358\b,:69@2EZjs)+.\nx6[iTÐ=48^+h^{lÎ}Ç\nK[ÄgÅ<Å Å<ÄÇ\n¤ \nKgÄ[Å<Å<Å<ÄÆ}ÈÁK\n,:i5)+0P=40)+.qj0P=A658/=4)\u001d.\u0004hU_/8^35*Ñ7<3g@P65=4)H35.eÇ[Ò\u000eK/Ï_[358\b,:69@2EZjs)+.\u0002Vx6gi\nÐ=48(4h^\u0017lÎgÇ[ÓGÒ\u000eK\nÄ[Å<Å<Å<ÄÇ;ÔY¤ 1I\nÄ[Å<Å<Å<ÄÆ\n,:i5)\u001d02=40)+.qj&,/.!'5)\u001d.\u0004he6\f=O7<3g@P65=4)H35.QÇ\nÓÏ_[358\b,:69@2E¢0;µq)+.\nCÐ=48_[358\u0017,:69@2E\u00020P§e)\u001d.t\nÐ=4802%A.!,/Nlº0[µQWOheWV0P§) _U02%H.\u001e,/Nw)+0M.!35=V6Z0Po1L(j1E\u00048^650;,Z3^_65.Fmq*)+.\u00026gi_[358\b,:69@2Er_/8P,:n;oF,/.F=?02o1L(j1E\u001b8:650;,\b03^_r02%A.!,/N) _U0v)+0v.\n35=G6q0Po1L(j1E\u00048:6\f0[,\u00173^_65.1mq*)+.¢x6gi*q658:¸Q,:69@2EQ¸5%\u001dh58:65*Ë)+.¢0_[358\b,:69@2EQh) _Vh-)\u001d0U.!3\f=?*q6\f8P¸9,:'8P,/*Z35>5,vha_/8:35*¥¸5%P¨O8^65*{08P,/=4o\u000482.s¸5%P¨O8:6\f*{0\n4.4SortingPhrases\nWhenallofthemaximalfrequentphrasesarefound,thephrasesare\nsortedbytheirlength,frequencyandpersonality .Thepersonality\npropertymeasures theaveragedivergencebetweenthesequential\nnotes.Theideaofpersonality isbasedoncontrasts –themore\ncontrasting thesequential notesonaveragearethemorepersonal\nthephraseisconsidered tobe.\nThepersonality valueforeachphraseiscalculated usingthescaled\nvaluesofeachvectordimension. Thepersonality ofonephraseisthe\naverageofthepersonality ofitsvectors:Õ Ö\f××gÕ\nÒCØ Ø Ø ÒÕ Ö×Ù\nÕ\nÒ\u000eØ Ø Ø ÒÕ Ö[ÚÙ\nÕÔ\nwhere Ûisthelengthofthephrase, Sthecardinality ofthevector\nand 1isthescaledvalueofthe Üthdimension ofthe ¼thvector.\nThetotalrateforeachphraseiscalculated usingtheformula:Ý-ÞÃ+­gÛ!\u001b¯:ßvxàUÞRáF±5­}p\fâ!­gÛ\u001e®;àMdãVÞ!c!­g±9²[°\fÛ\u001e1Ã¼H¯(à , Ý, à,and ãbeingthe\nuserdeﬁnedparameters forsorting.~`5\"#!\u001a\u001f!\u001e7.\nÐ3\f8/=.\u0010j\no\u0004=G6gio\u0004=j\no\u0004=Vx6gi50/3582=,:'_[358\b,:69@2EZj1E\u00048^650;,ajx)\u001d.\u0002x6gi7 ,/.\u0004h5=\u001dEs+u¤ cC¤_/8P,:n;oF,/.!@;m\u0002+u¤ Ã\u001d°\f®;\u001b¯^¼H°\fÛY²\u0004¤_[358\b,:69@2E¢>5,:@;=A358`>-z!K\nÄ[Å<Å<Å Ä\u0019Ôx)+.qjj1,/820;3\f.!6\u00107 )+=4m\u0017\u001dj1,/8/0/35.\n6\u00107 )+=4mZ´y¤ !K\f¤\nÅ Å<ÅQ¤ 1Ô\u001e¤j1,/820;3\f.!6\u00107 )+=4m¢+»j1,/820/35.!6\u00107 )\u001d=4m\u0017ä7 ,/.\u0004h5=\u001dE8:6\f=,Z+¥³\u001di\u001eå27 ,/.\u0004h5=\u001dE&´lm!å_/8:,Pn;oF,/.\n@;m´æåHj1,/820/35.!657 )+=4m;¶x6[i50;3\f8/=,:'[8^65=,+Z\u001dwx6gi50/3582=,:'[8:6\f=,+rlj8P,/=4o\u000482.sx6[i50;3\f8/=,:'\nBecausethenumberoffoundphrasescanbequitehigh,depending\nontheanalyseddata,itmaybeusefultochooseonlythebestphrases\nfromthesetofallfoundphrases. Thechosenphrasescanthenbe\nstoredintotheindexstructure.\n4.5RequirementsfortheSearchAlgorithms\nThefoundphrasesorsomesubsetofthemcanbestoredintoasufﬁx-\ntreestructure [12]forexample.Aquerycanbeexecutedusingthe\nexistingstringmatching algorithms. Theformofthefoundphrases,\nhowever,setssomerequirements forthesearchalgorithms.Asmentioned above,thepolyphonic structure ismanaged byal-\nlowinggapsbetweenthenotesofthephrase.Therefore someof\nthefoundphrasesmayalsoincludechordsinadditiontomelodic\nstructure. Becausethephrasesarestoredintheirentiretythesearch\nalgorithm shouldbeabletoprocesssuchpartlypolyphonic struc-\ntures,aswell.\n4.6OpenProblems\nOneofthemainprinciples ofthealgorithm istosupportdifferentnu-\nmericalsymbolic representations ofmusicaldata.Inthisapproach\ntheresulting phrasesandalsothemanaging ofsomepolyphonical\nsituations arehighlydependent onboththegivenrepresentation of\nthedataandtheparameter values.Theorderofthefoundphrases\ndependsontheuser-givenparameters, aswell.Themaindrawback\ninthisapproach isthatitdoesnotuseanyknowledgeoftheorder\nortypeofthegivenvectordimensions andthatitishighlyuser\ndependent.\nEventhoughthepersonality propertyaddssomesemantics intothe\nsortingphase,theideaofcontrastororiginality hastobeconsidered\nfurtherandsomenewproperties addedintothesortingformula.\nSomeothersortingformulas couldbeaddedtothealgorithm, as\nwell.\n5.IMPLEMENT ATIONANDRESULTS\nThetwoversions,withandwithoutpruning,ofthealgorithm were\nimplemented inPerlandexperiments werecarriedoutwiththeMIDI\ndatabaseconsisting of130polyphonic Irishfolkmusicpieces.The\npiecesconsisted of300–3500 notesandtheyweredividedinto11\nsizegroups(Table1).Twoofthegroups(ç5è\u0010èdÉÝ\n¹éè\u0010èandéè\u0010è|ÑÝ\ngè\u0010è5è)wereexperimented systematically ,theothers\nwereusedascomparison material. Theinnerpresentation was\nformedusingthepitchandstarting-time values.Thehardwareenvi-\nronmentwasAMDAthlon1.33GHz with512MBofmainmemory.êOëFìCí\n\u0001&îF\u000f`ïUðFñZò;ó<ô\fñ{õ1ö/÷Fø\nù!ògúûMü5ýPþ[ÿ \u0000\u0002\u0001<þ\u0011\nþ[ÿÝ\nç\u0010è5è \u0003ç5è\u0010èZyÝ\n|éè\u0010è\nééè\u0010èZyÝ\n[è\u0010è5è \u0004\u0003[è\u0010è\u0010èZyÝ\n}¦\fè\u0010è \u0004\u0003g¦5è\u0010èZyÝ\n\u0004\u0005\u0010è\u0010è ¦5¦\u0006\u00059è\u0010èZyÝ\ngç5è\u0010è 5[ç\u0010è\u0010èZyÝ\n\néè\u0010è [ç\néè\u0010èZyÝ\n¦5è5è\u0010è\né¦\fè\u0010è\u0010èZyÝ\n¦\u0010¦\fè\u0010è \u0007¦5¦5è\u0010èZyÝ\n¦\b\u0005\u0010è\u0010è \u0005Ý\n\t|¦\b\u0005\u0010è\u0010è \u0007\nTheexperiments werealsodividedintothreesets.Theﬁrstset\nofexperiments measured theeasilyrecognizable featureslikethe\ndirecteffectofamountofstartingpairsandparameters (Figures 1\nand2).Thesecondsetmeasured theeffectsofbothparameters\nandthedatacontentsandthethirdsettheeffectofpruningonthe\nefﬁciencyofthealgorithm. Theresulting phrasessortedbytheir\nrateandthetimeanalysisdatawerecollected intoaresultﬁle.\nTheﬁrstsetofexperiments wasconducted usingtheversionofthe\nalgorithm withoutpruningphaseandﬁvedifferentsetsofparameter\nvalues.Theexperiments provedthattheparameters hadadirect\neffectontheamountofstartingpairs(Figure 2)buttheexact\namountofnotesdidnot.However,theaverageexecutiontimerose\nwhenexecutingpiecesofthenextsizegroup(Figure3).Themost\ninteresting observation,however,wasthatthealgorithm hadtwo\ndifferentbehaviormodels. Incaseofsomepiecesandparameter\nvalues,especially whentheamountofstartingpairswasquitesmall,IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrases\nforexamplelessthan200,theamountofthegramsintheS-Grams\nsetstartedtodecrease fromtheﬁrstround.Inothercases,the\namountofgramsﬁrstincreased andthendecreased.\n020406080100120140160180\n2 4 6 8 10 12 14 16Time (sec)\nGapThe Effect of Gap Value on Time (Frequency 20)\n\"parameters\"\u000b\r\f\u000f\u000e\u0011\u0010Y\t\f\u0001\u0006î\n\u000f\u0017ïMðFñqñ\b\u0012Yñ\u0014\u0013\u0004\u0015`÷\u0017\u0016?õ\u0017\u0018\u0004ù\u001a\u0019\u0014\u0018\u001c\u001bøFñq÷\u001e\u001d\u001f\u0015gó! eñqó!\u001d\u001a\u0015[ðFñ-ò2ñ\"\u0013g÷#\u001d%$ò/ó ô}ñqõ\u0019ö;÷1ø\nù\n050100150200250300\n2 4 6 8 10 12 14 16Starting Pairs\n&\nGapThe Effect of Gap Value on Starting Pairs (Frequency 20)\n\"parameters\"\u000b\r\f\u000f\u000e\u0011\u0010Y\t\f\u0001('#\u000fMïMðFñrñ\b\u0012Yñ\u0014\u0013\u0004\u0015a÷\u0017\u0016Áõ\u0017\u0018\u0004ù)\u0019\u0014\u0018\u001c\u001b\u001døFñr÷#\u001dQò*\u0015+\u0018\u0004ö+\u0015[ó\u000f\u001dFõ\bù%\u0018\u0019óö[òOó\u000f\u001d)\u0015[ðFñòPñ\u0014\u0013g÷#\u001d%$dò/ó ô}ñZõ\u0019ö;÷1ø\nù\n05101520253035\n400 600 800 1000 1200 1400 1600 1800Time (sec)\nNotesThe Effect of the Amount of Notes on Time (Gap 3, Frequency 20)\n\"sizegroups\"\u000b\r\f\u000f\u000e\u0011\u0010Y\t\f\u0001-,#\u000frïUðFñqñ.\u0012Áñ\"\u0013\u0004\u0015`÷\u0017\u0016/\u0018\u001c Q÷Fø\u0011\u001d0\u0015U÷\u0017\u00161\u001dF÷0\u0015;ñ5òU÷#\u001d\u001f\u0015[ó\u000f Qñ\nThesecondsetofexperiments wasalsoconducted usingthesim-\nplerversionofthealgorithm. Inthisset,bothdecreasingly and\nincreasingly behavingpiecesofsecondandthirdsizegroupswere\nprocessed usingthesameparameter valuesandtheresultingphrases\nwereanalysed. Theresultsprovedthatthedecreasing behavior\nendedupwithalargesetofsmallphrasesconsisting ofonlyacou-\npleofnotes.Whenthebehaviorwasincreasing thesetofphrases\nincludednoticeably longerphraseswithadditional smallerphrases\naswell(Table2).êOë1ìCí\n\u00012'#\u000fê43\n\u0001í\n\u0001\u001c56\u000e%73\n\u0007%8973\n\u0001;:3\n\të\n\r[\u0001\u0019\r<\f\u000f5=73\n\u0001Q\r[\u0001\u001c>\u0010\u0007\u00115\u000e\u000bë\n5\u000e\u000b73\n\f\u001d\t\f\u000bw\r?\f\u000f@9\u0001\u001a\u000e\n\t}\u0007\u0011\u00106:C\rq\u0005;\f!73\n:ë\n\të%A\n\u0001\u00177[\u0001\u0004\t)BëFí\n\u0010Á\u0001\u0019\r\nC/î\u0017DFE9'\u001eDHG.I\u0000\u0002J#KML\fÿ:þ[ÿ NPO\u0011\nKPþ?L\fÿQ\u0001RO\u001eS\u001eTVU#\u0001<þ\u0011\nþ[ÿMW Xvþ\u0011\nKPþ?L5ÿQ\u0001RO\u001eS#TVU\u001e\u0001<þ\u0011\nþ;ÿMWY[Zþ\u0014\\/]#þ\u0004O\u001eS\fý*Jé\nT\u0010\nW\u0005\nT[ç\nW]#ü\u0014O#S\u0010þ[ÿ^ý¦\fè \u0007Thethirdsetofexperiments measured theefﬁciencyofthetwo\nversionsofthealgorithm. Ontheaveragethealgorithm withpruning\nphasewastwiceasefﬁcientastheonewithoutpruning. Incaseof\nthedecreasing behavior,however,theversionwithoutpruningwas\nconsiderably moreefﬁcient(Table3).êOë1ìCí\n\u0001^,\u001e\u000fê43\n\u0001ë\nB!\u0001\u0004\të\n\u000eF\u0001¹\u00010_`>a\f\u0001#56>\u0010\u0003Í\u0007%8b73\n\u0001ë\ní\n\u000e\n\u0007F\t\"\f!73cA\f\u000f5d73\n\u0001\u0006\r[\u0001\u001c>\u0010\u0007\u00115\u000e\u000bº\r?\f\u000f@9\u0001`\u000e\n\t}\u0007\u0011\u00106:À\u0005e\f\u000f73Àë\n5Á\u000bl\u0005;\f\u000f73\n\u0007\u0011\u001067\u001a:\u000e\t\b\u0010f5f\f\u000f56\u000e:3Áë\n\r[\u0001s\u0005;\f\u000f73\n:ë\n\të%A\n\u0001\u00177[\u0001\u0004\t2Bë\ní\n\u0010Á\u0001\u0019\r\nCQ,FEGî\u0017D\u0011G.INPO\u0011\nKPþ\u0004L5ÿQ\u0001RO\u001eS XUþ\u0011\nKPþ?L\fÿQ\u0001gO#Shi\u0001ý*JFü\"j1ý[\u0000cK*j\u001eO#\u0001gO#S\n\\ kml;\u0001RO n\u0017ÿ:þ\u0011hi\u0001ý*J\n\u0000cK*j\u001eO#\u0001RO\u001eSè\n\\ k.l;\u0001gOç\nÿ:þ\u0011\nEventhoughthepruningphasewasrelativelyslowwhenthealgo-\nrithmbehaveddecreasingly andalsoinsituations, whentherewere\nmanymaximalphraseswithcommonsubphrases, therearestillrea-\nsonsforusingthephase.Ontheaveragethepruningphasespeeded\nupthealgorithm considerably .Alsothephrasesproduced bytheal-\ngorithmwhenbehavingdecreasingly wereveryshortandtherefore\nuseless.\nWhencomparing totheoriginaltextminingmethodthebehavior\nofalgorithm wasslightlydifferent.Thetextphrasesaretypically\nmuchshorterthanthephrasesusedinmusicwhichwasalsoseen\nwhenexpanding thephrases. Further,thereseemedtobemuch\nmorerepetition inthemusicaldatausedascorpusthaninthetextin\ngeneral.\nThefoundphraseswereofthreedifferenttypes.Thephrasesof\ntheﬁrsttypewereveryshortconsisting ofthreetoﬁvenoteswhich\ntypically represented chordswithoneortwoextranotes.When\nsearching amelodytheinformation consisting mainlyofchords\nisirrelevant.Therefore thesephrasesareunsuitable forindexing\npurposes. Typicallythisgroupofphraseswasgenerated whenthe\ngivengapvaluewastoosmall.\nThephrasesofthesecondgroupwereconsiderably longerandcon-\nsistedofbothmelodicandharmonic parts.Themostcommonform\nofthiskindofaphrasestartedwithachordfollowedbyoneortwo\nsequential melodicnotes.Asimilarstructure wasrepeatedseveral\ntimesinthephrase.Thethirdgroupconsisted ofpurelymelodic\nphraseshavingapproximately ﬁvetotennotes.\nWhenexamining thephrasesofthesecondandthirdgroupcloser,\nitwasnoticeable thatmostofthephrasesconsisted ofsuchrepeat-\ningstructures thatcouldalsobeheardwhenlisteningtothepiece.\nTherefore thesephrasesalsoformthesetofthephrasestobestored\nintotheindexingstructure.\nInmostcasesthephrasesofthesecondandthirdgrouphadthe\nhighestratevalues,aswell.Stillthereisaneedforﬁndingbetter\nsortingformulas forstressingthefactorsoftheratevalue.\nBecausethebehaviorofthealgorithm andtherefore alsotheform\nofthefoundphrasesdepended partlyontheparameters givenbythe\nuser,thesuitableparameter valuesshouldbedetermined foreach\ncorpusindividually.Therefore thereisalsoaneedforanalgorithm\nwhichwouldanalysethedataanddecidethebestparameter values\nforeachdata.\n6.CONCLUSIONS\nThispaperpresented atextmining-based indexingalgorithm, which\nextractsrepeating patternsfromasymbolic representation ofpoly-\nphonicmusicaldata.Thepatternsfoundareﬁrstsortedtoseparate\nusefulandunuseful phrases. Thebestphrasescanthenbestored\nintotheindexstructure.IndexingMusicDatabases UsingAutomaticExtractionofFrequentPhrases\nAlthough thephenomena ofmusicandlanguage arequitedifferent\ntoeachother,especially theirusehavesimilarfeaturesinmany\ncases.Further,themusicalandtextdatahaveenoughsimilarities\nforalsoapplying thetextprocessing algorithms tomusicaldata.\nTherefore theknowledgegainedfromthetextminingandlanguage\ntechnology researchcanalsobeusefulforfurtherdevelopment of\nMIRmethods.\n7.ACKNOWLEDGEMENTS\nTheworkreported inthispaperhasbeensupported byFinnish\nAcademy grant#68657.\n8.REFERENCES\n[1]H.Ahonen-Myka. FindingAllFrequentMaximal Sequences in\nText.InbO8:3g@2,P,P'\f)+.\u0004h50M3^_O=\u001dEF,Mµao\f=\u001dE\n.F=,/82.!65=4)H3\f.!6\u00107Á]3\f.g_g,/8:,/.!@2,35.¢x69@2E\u0004)\u001d.\u001e,\n,P6\f8/.1)+.\u0004h\nÁ\n%Ppap,Ljubljana, pp.11–17\n[2]Burns,G.,ATypologyof’Hooks’inPopularRecords.bV3/j\no\u0019%7<658Uso\u00040P)H@ 6,1(1987),pages1–20.\n[3]J.ChenandA.L.Chen.QuerybyRhythm: AnApproach\nforSongRetrievalinMusicDatabases.\n¾]¾R¾\n.F=,/82.!6\u0010%=4)H35.\n6\u00107rq\u0002358:¸\f0:E\u00193/j\u000635.¢V,/0[,:658^@2E\n0P02oF,/0r)+.\n65=A6Z¾O.\u0004h5)+.!,P,/8;%)+.\u0004h,pages139–146, Orlando, Florida,USA,February 23–24,\n1998.\n[4]T.C.Chou,A.L.Chen,C.C.Liu.MusicDatabases: In-\ndexingTechniques andImplementation.\n¾]¾R¾\n.1=,/8/.\n65=4)H35.\n6\u00107q¢3\f8P¸\f0^E13/j|35.dso\u00197 =4)4%+d,:'5)H6\n65=A69LP650;,qx6\f.!6gh\u001b,/*q,/.F=\nÐm502%=,/*{0,pages46–53,BlueMountain Lake,NewYork,USA,\nAugust14–16,1996.\n[5]J.Hsu,C.Liu,andA.L.Chen.Discoveringnontrivialrepeating\npatternsinmusicdata.\n¾]¾]¾uX18:65.10;6\u0010@;=4)H35.F0r35.Qso\u00197 =4)+*q,:'5)H6,\nVolume3,3(2001),pages311–325.[6]A.Ghias,J.Logan,D.Chamberlin, andB.C.Smith.Query\nByHumming –MusicalInfromation RetrievalinanAudio\nDatabase.bO8:3g@2,P,P'\f)+.\u0004h50-3(_\u0017=\u001dEF,4s\bÁ so\u00197 =4)+*q,P'\f)H6Á£ pat,1995,\npages231–236.\n[7]K.Lemstr¨om.InSearchofaLostMelody.Computer Assisted\nMusic:Identiﬁcation andRetrieval. uR)+.F.1)+0^Edso\u00040P)H@\u001avOo1658;%=,/8/7 m,3–4(2000),pages40–45.\n[8]K.Lemstr¨om,G.A.WigginsandD.Meredith. AThree-Layer\nApproach forMusicRetrievalinLargeDatabases. In bO8:3\u0010%@2,P,P'\f)+.\u0004h50M3^_\n2Ð\nz§\u0019©9©9µr§9.!'wsa.F.1o16\u00107\n.1=,/8/.\n65=4)H35.\n6\u00107\nÐm\f*Z%j\u0019350P)+o\u0004* 35.·so\u000402)4@\n.[_[3582*q65=4)435.·V,/=48/)H,/>\f6\u00107 ,pages13–14,\nBloomington, Indiana,2001.\n[9]D.Meredith, G.A.WigginsandK.Lemstr¨om.Patterninduc-\ntionandmatching inpolyphonic musicandothermultidimen-\nsionaldatasets. InbO8:3g@2,P,:'5)+.\u0004h50x3^_¢=\u001dEF,xuR) _/=\u001dEyq\u0002358;7<'xso\u00197%=4)H@P35.[_g,/8P,/.\n@/,Q35.\nÐm50P=,/*\u0017)H@;0\u0002Cm\u0010L2,/8/.!,/=4)H@;0q65.\n'\n.g_[3\f8/*Z65=4)H@;0³\nÐ\n§\u0019©\u0010©9µ/¶,pages61–66(volX),Orlando, FL,2001.\n[10]D.Meredith, K.Lemstr¨om,andG.A.Wiggins.SIATECand\nSIA:Efﬁcientalgorithms fortranslation-in variantpattern-\ndiscoveryinmultidimensional datasets,prep.\n[11]Rolland, P.-Y.Discoveringpatternsinmusicalsequences.¿\u000435o\u000482.!6\u00107C3(_wz`,/N»so\u00040P)H@va,/0;,:658:@2E ,28,4(1999),pages334–\n350.\n[12]E.Ukkonen.On-lineconstruction ofsufﬁx-trees.sM7 h93582)+=\u001dE\u0019%*{)4@2614(1995),pages249–260."
    },
    {
        "title": "Some Considerations About Processing Singing Voice for Music Retrieval.",
        "author": [
            "Emanuele Pollastri"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416494",
        "url": "https://doi.org/10.5281/zenodo.1416494",
        "ee": "https://zenodo.org/records/1416494/files/Pollastri02.pdf",
        "abstract": "The audio processing and post-processing of singing hold a fundamental role in the context of query-by-humming applica- tions. Through the analysis of a sung query, we should perform some kind of meta-information extraction and this topic deserves the interest of the present paper. Considering the raw output of a pitch tracking algorithm, the issues of note estimation and the study of singing accuracy have been addressed. Further, we report an experiment on the deviations from pure tone intonation in performances of untrained singers.",
        "zenodo_id": 1416494,
        "dblp_key": "conf/ismir/Pollastri02",
        "keywords": [
            "Query-by-humming",
            "Singing voice processing",
            "Music retrieval",
            "Pitch tracking",
            "Note estimation",
            "Microintonation",
            "Macrointonation",
            "Intonation accuracy",
            "Vibrato",
            "Pitch contour analysis"
        ],
        "content": "Some Considerations about Processing Singing Voice for Music Retrieval  \nSome Considerations About Processing \nSinging Voice for Music Retrieval\n Emanuele Pollastri \nDipartimento di Scienze dell’Informazione \nUniversità degli Studi di Milano \nvia Comelico, 39/41 \n20135 Milan – Italy \n+39-02-50316297 \npollastri@dsi.unimi.it  \nABSTRACT  \nThe audio processing and post-processing of singing hold a \nfundamental role in the context of query -by-humming applica-\ntions. Through the analy sis of a sung query , we should perform \nsome kind of meta-information extr action and this topic deserves \nthe interest of the present paper. Considering the raw output of a \npitch tracking algorithm, the issues of note estimation and the \nstudy of singing accuracy  have been addres sed. Further, we report \nan experiment on the deviations  from pure tone intonation in \nperformances of untrained singers.  \n1. INTRODUCTION \nThe need for content-bas ed music retrieval tools  has been already  \nstressed by  a number of publications ; in this context, one of the \nmost appealing applications s eems to be query ing-by -humming \n[1, 2, 5, 7] . The singing voice as another possible channel of inter-\naction with m achineries  is cons idered eas y, natural and enjoy able \nfor both music professionals and casual users. Nonetheless many  \nproblems remain open in building an effective system for \nquery ing-by -humming. In partic ular, the singing voice should be \nbetter investigated. The aim  of this  paper is to highlight useful \naspects of singing voice to improve the translation from acoustic \nevents  into a note-like repres entation.  Figure 1. Exam ple of pitch events related to the \ntranslation from microintona tion to macrointonation. \n2. NOTE ESTIMATION AND SINGING \nVOICE ARTICULATION \nA raw pitch tracker output will produce an accurate pitch contour \nin the “ microintonation” sense. In music retrieval we are \ninterested in the “ macrointonation”  aspect, which is the sequence \nof notes sung by  a singer. Determ ining the latter contour starting \nfrom  the form er one is  a difficult tas k especially  when we are \ndealing with untrained voices, for which the degree of pitch \nvariability  is affected by  a num ber of factors like the vocal range \nand singer’s emotions. The actua l pitch of a note is only  an \nabstraction of the microinto-nation. In order to be able to \nreconstruct the intended sequence of notes, some “hard” smoo-\nthing, like low-pass or m edian filtering, is surely  necessary  but it \nis not s ufficient.  \nIn our experimentation, we are using a set of heuristic rules \nobtained by study ing the singing se quences contained in a dataset \nmade of forty  tunes sung by  non- professional singers. The quasi-\nstationary  conditions are m et for fram es 5 m illiseconds long; both \npauses and notes could not be less than 120 m illiseconds in \nduration. The pitch-tracker employ ed has been pres ented in [6] . Aiming to avoid local discontinuities, thus im proving note \nrecognition, the frame-level pitch contour is post-processed with a \nmedian filter and a running m ean block with windows 100 m sec. \nlong, overlapped by  95 m sec (i.e. shifting each window of 5 \nmsec). Each note hy pothesis (duration and pitch), is evaluated to \nmatch one of the following situations: \n1- Spike : it is a tone, often short (around 100 msec), character-\nrized by a monotonically  increasing sequence of pitches \nfollowed by a monotonically  decreasing one. Spikes are often \nencountered in short note repetiti ons; som etimes they  are used \nas ornamental notes. The value of pitch for this kind of event \nis the local m aximum. \n2- False Spike : event 60 msec. long with a mean pitch value in a \nrange of ±0.6 semitones within the next tone. A false peak \nshould be merged with the following note. \n3- Ascending/Descending Intervals : in a sequence of two or more \nascending/descending notes (not separated by  silence) is \ncommon to have an unstable region during the first 70/120 \nmsec. in the shape of a glissando. Usually , it does not happen \nwith short notes, so the soluti on consists in discarding the \nunstable fragment and taking the mean or the m edian of the \nremaining pitch values. \n4- Vibrato : an event corresponding to a quasi-sinusoidal \nmodulation of pitch [8] . The center of the oscillation is the \npitch. Permission to make digital or  hard copies of all or  part of this \nwork for per sonal or  classr oom use is granted without fee \nprovided that copies ar e not m ade or distr ibuted for  profit or \ncommercial advantage and that copies bear this notice and the f ull \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou In a practical im plem entation, the s ituation indicated as  num ber \nthree is  the rule while the others  are the exceptions . In Figure 1, a \ncurve indicating a microintonation contour is commented to \nhighlight spikes, vibrato regions and local instability . Some Considerations about Processing Singing Voice for Music Retrieval  \n3. ACCURACY OF INTONATION \nIn the previous section we re viewed the problem related to \ntransform a sequence of frame-level pitch contour to a note-level \ncontour. We did not mention that we are operating in the \nfram ework of the equal tem pered m usical scale, so the reference \nunit is the sem itone and the fractions of semitone (or cents). While \nthis choice is debatable, it is worth to be adopted for two main \nreasons. The precision of a query -by-singing sy stem depends on \nthe understanding of the query . Therefore, a nearly  perfect \ntranslation of the input at the semitone level remains an important \ngoal, even for sy stem based on si mpler representation like 3 or 5 \nlevel interval contour. Moreover,  previous works on the question \nare still based on equal tem pered m usical scales, so for the sake of \ncomparison we will continue  with this convention. \nIn a previous paper [3], the author stressed the importance of \nadopting a m usical scale relative to the singer. This solution is \nneeded becaus e singers  very  rarely  have a tone-abs olute pitch. It \nhas been confirmed that singers  made constant-sized errors, \nregardless of note distance in time or in frequency  [4]. Thus the \nrelative scale coincides  with a s hifted equal tem pered m usical \nscale. The amount of the shifti ng can be calculated out of the \nperform ance of the s inger. An open ques tion regards  the accuracy  \nof intonation in untrained singers. W e can speculate that the \noverall deviations from pure intervals on average tend to go to \nzero. In other words, singers are able to adjust their intonation \nduring singing. It should confirm  Sundberg’s intuition for which \nsingers must hear the next target  pitch before starting to change \nthe pitch [9] . From our experience, we should add that trained \nsingers are able to do this opera tion in their mind before giving \nany sound, while untrained singers need to phy sically  hear their \nvoice. However, what about the accuracy  of intonation of every  \nsingle interval?  \n-0.08-0.06-0.04-0.0200.020.040.060.08\n-6-5-4-3-2-10123456\nintervalsemitones\n050100150\n-11\n-9\n-7\n-5\n-3\n-1\n1\n3\n5\n7\n9\n11\nintervalnumbe r of intervals\nEstimated Frequenc y Distribution of  Intervals\nActual Frequenc y Distribution of  IntervalsUsing our dataset (40 tunes), we collected som e data try ing to \nanswer this question. A word of warning is needed since in this experiment singers were  not asked to repeat a set of tone with \nintervals studied to cover all the possible configurations. Instead, \nthey sung well-known tunes prepared for a music retrieval test. \nWithin sung intervals, we meas ure the average “flatness” or \n“sharpness” without taking care if that interval was the right one. \nFor instance, if an interval was  estimated to be 3.75 s emitones , we \nconsidered it as 4 semitones with 0.25 semitones flat, no matter if \nthe right interval for the tune was 3 semitones. The amount of \ncorrect intervals  can be inferred by  the difference between the \nexpected and the es timated dis tribution of intervals .  \nEven with all the given precautions , untrained s ingers  did not \nshow considerable deviation for any interval (s ee Figure 2, top). \nThes e results contras t with the accepted notion that there is a trend \nto tune narrowly  minor intervals and widely  major intervals [26] . \nFurtherm ore, the curves  of es timated and expected dis tributions  \nare quite sim ilar with the excepti on of the interval of zero size \n(note repetition) (see Figure 2, bottom ); the segm entation algo-\nrithm employ ed in the audio analy sis did not probably  track that \ninterval correctly  [6]. A deeper investigation, in fact, has shown \nthat m ost m issing notes were repetitions. W e over-estim ated \nintervals of minor second (inter val=1 semitone) but we missed \nintervals of major second a nd unison (interval=0 and 2 \nsemitones). Thus we can argue that som e difficulties actually  exist \nin tuning repetitions and second major intervals, which are often \ntuned respectively  sharp and flat. In the light of these findings, \none could debate that the average measure of deviation tends to be \nwider than a s emitone. Therefore, the pres ented s tatistics are at \nleast questionable. Lacking formal  investigations on this topic, \nfurther experiments are needed to dispel all doubt. \n4. REFERENCES  \n[1] Ghias, A., Logan, D ., Cham berlin, D ., Smith, S .C. Q uery by \nhumming – musical information retrieval in an audio \ndatabase. In Proc. of ACM Mu ltimedia’95, San Francisco, \nCA., Usa, Nov. 1995. \n[2] Haus, G. and Pollastri, E. A m ultimodal fram ework for music \ninputs. In Proc. of ACM Multim edia 2000, Los Angeles, CA, \nUsa, Nov. 2000. \n[3] Haus,  G. and Pollastri,  E. An Audio Front End for Query -by-\nHumming Sy stems. In Proc. of ISMIR 2001, Bloomington, \nIN, Usa, Oct. 2001. \n[4] Lindsay , A. Using contour as a mid-level representation of \nmelody . M.I.T. Media Lab, M.S. Thesis, 1997.  \n[5] McNab, R.J., Sm ith, L.A., Witten, C.L., Henderson, C.L., \nCunningham, S.J. Towards the digital music library : tune \nretrieval from acoustic input. In Proc. of the 1st ACM Int. \nConf. on Digital Libraries, Bethesda, USA, March 1996. \n[6] Pollastri, E. A pitch tracking sy stem dedicated to process \nsinging voice for m usic retrieval. To appear in Proc. of IEEE \nInternational Conf. on Multim edia and Expo 2002, Lausanne, \nSwitzerland, Aug. 2002. \n[7] Prechelt, L. and Ty pke, R. An interface for m elody input. \nACM Trans. On Computer Huma n Interaction, Vol.8, 2001. \n[8] Rossignol, S., Rodet, X., Soum agne, J., Colette J.L. and \nDepalle P., Autom atic characterisation of m usical signals: \nfeature extraction and tem poral  segmentation. Journal of \nNew Musical Research, Vol. 28, N. 4, Dec. 1999. \n[9] Sundberg, J. The science of the singing voice. Northern \nIllinois University  Press, Dekalb, IL, 1987. Figure 2.  Deviations of intervals from pure tones in our dataset \n(top). Num ber of es timated and actual intervals  (bottom )"
    },
    {
        "title": "Automatic Transcription of Piano Music.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1414952",
        "url": "https://zenodo.org/records/1414952",
        "ee": "https://zenodo.org/records/1414952/files/Raphael02.pdf",
        "abstract": "A hidden Markov model approach to piano music transcription is presented. The main difficulty in applying traditional HMM techniques is the large number of chord hypotheses that must be considered. We address this problem by using a trained likelihood model to generate reasonable hypotheses for each frame and construct these arch graph out of these hypotheses. Results are presented using a recording of a movement from Mozart's Sonata18,K.570",
        "zenodo_id": 1414952,
        "dblp_key": "conf/ismir/Raphael02",
        "keywords": [
            "Automatic transcription",
            "Piano music",
            "Hidden Markov Model (HMM)",
            "MIDI representation",
            "Audio features",
            "Chord hypotheses",
            "Statistical learning",
            "Probabilistic models",
            "Musical score alignment",
            "Recognition accuracy"
        ],
        "content": "AutomaticTranscription ofPianoMusic\nAutomatic Transcription ofPiano Music\nChristopher Raphael\nUniv.ofMassachusetts ,Amherst\nDepar tment ofMathematics andStatistic\nAmherst, MA01003-4515\nraphael@math.umass .edu\nABSTRACT\nAhidden Mark ovmodel approach topiano music transcription is\npresented. Themain difﬁculty inapplying traditional HMM tech-\nniques isthelargenumber ofchord hypotheses thatmust beconsid-\nered. Weaddress thisproblem byusing atrained likelihood model\ntogenerate reasonable hypotheses foreach frame andconstruct the\nsearch graph outofthese hypotheses. Results arepresented using a\nrecording ofamovement from Mozart’ sSonata 18,K.570.\n1.INTRODUCTION\nWediscuss ourworkintranscribing sampled recordings ofpiano\nmusic intoaMIDI- orpiano-roll-type representation. Therelevance\nofthisendea vortothemusic information retrie val(MIR) commu-\nnityisstraightforw ard: MIDI provides acartoon-lik erepresentation\nofmusical data thatisextremely compact when compared tothe\nsampled audio data, yetretains thenecessary information formany\ncontent based analyses andqueries. This workhasapplications out-\nsideofMIR such asscore-follo wing forcomputer -human interacti ve\nperformance, andispartofabroader research agenda ofours —the\nmusical “signal-to-score” problem.\nOurapproach isbased onhidden Mark ovmodeling since webelie ve\nthatHMMs [1],[2],and, moreo ver,theideas ofstatistical pattern\nrecognition andstatistical machine learning havegreat potential in\nmusic recognition problems. First ofall,agiven“state ofnature, ”\nsuch asaparticular voicing ofaCmajor triad, canberealized through\nawide range ofdata (say e.g. spectral) conﬁgurations, depending\nonmanyvariables such astheinstrument, theroom acoustics, the\nplacement ofthemicrophone, aswell asmanyplayer -speciﬁc vari-\nables. This variability ofpresentation argues foraprobabilistic data\nrepresentation inwhich onemodels the\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\t\u0001\b\n\f\u000b\r\u0005\b\u0001\b\u000e\u0002\u000fofthedata\ngiventhetruestate ofnature. Furthermore, automatically\u0010\u0012\u0011\u0014\u0013\n\u0007\u0015\u000f\u0016\u0001\u0017\u000f\u0019\u0018\nthisprobabilistic relationship isinherently more robustandﬂexible\nthan optimizing aparticular model by“hand-tweaking” themany\nparameters. While ourpreference forlearning models from data\nisshared bythemajority ofthemachine learning community ,we\nhavearrivedatthispreference notthrough “recei vedwisdom” but\nrather duetooursigniﬁcant experience hand-tuning various mod-\nels.Secondly ,musical data ishighly structured andanyrecognition\napproach iswell-advised toexploit rather than disre gard thisthis\nstructure. While musical structure canbepartly captured interms\nof“rules, ”anexamination ofdata ofreveals thattherules canbe\nmore accurately represented astendencies andareeasiest tocapture\ninaprobabilistic frame work. Forthisreason wepropose learning\nmusical structure bytraining aprobabilistic model from musical\nscore data, rather than imposing anyspeciﬁc assumptions \u0013\u001b\u001a\n\u0007\t\u0001\b\u000e\u0002\u0007\t\u0001.\nIntegraltotheHMM methodology isaprobabilistic representation\nofdata andthedesired interpretation, aswell asmuch need compu-\ntational machinery fortraining andrecognition.\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feeprovided\nthat copies arenotmade ordistrib uted forprofitorcommercial\nadvantage andthatcopies bear thisnotice andthefullcitation on\nthefirstpage. \u001c\n\u001d\n2002 IRCAM -Centre PompidouEarly workonautomatic music transcription didnotemphasize any\nclear methodological frame workandaddressed signiﬁcantly scaled-\ndowntranscription problems asin[3], [4], [5]. More recently\nseveral authors havedemonstrated progress inpolyphonic music\nrecognition using so-called “blackboard systems” which attempt to\nfuse bottom-up andtop-do wnrecognition schemes, [6],[7],[8].[9]\ndescribes asystem inwhich prior information isincorporated into\nagraphical model atmultiple levelsinthegraph’ shierarchy ,but\nrelies onthecomputationally intensi vesimulated annealing method.\nCertainly among themost successful transcription results arethose\nofKlapuri [10], [11]. Hidden Mark ovmodels (and approaches with\nverysimilar spirit) havefound applications inscore following in\n[12], [13], [14] and[15]. Additionally HMMs havebeen applied\ntomonophonic music recognition asin[16]. TheHMM approach\ntotranscription isformally quite similar tothese twoapplications\nexcept thatthestate space islargerbyseveralorders ofmagnitude.\n2.THEMODEL\nOur approach begins bysegmenting theacoustic signal into ase-\nquence offrames, each corresponding toabout ashort “snapshot”\nofsound. From each ofthese frames wecompute acollection of\nfeatures which reduce thedimension ofthedata signiﬁcantly ,while\nstill retaining thenecessary information forinterpreting thedata.\nWedenote these feature vectors as \u001e\u0016\u001f! #\"#\"\f\"# \u0014\u001e%$ .Theprecise features\nweusewillbediscussed later.\nOurgoal istoassign alabel toeach ofthese frames describing the\nframe’ scontent. Themost important aspect ofthelabel isthecurrent\ncollection ofsounding pitches, however,wewillintroduce several\n“ﬂavors”foreach collection inwhat follows.Ourapproach ismodel-\nbased. Weform agenerati veprobabilistic model, ahidden Mark ov\nmodel, whose output istheobserv edsequence offeatures vectors\u001e\r\u001f! #\"#\"#\"\f \u0014\u001e%$ andwhose hidden variables, thelabels, correspond to\ntheinterpretation weseek.\nAhidden Mark ovmodel iscomposed oftwoprocess wedenote by&(')&\u001f #\"!\"#\"\f \n&$and *\n'*\n\u001f \f\"#\"#\"! \u0014*\n$.The\n&process isthe\n“hidden” process or“label” process anddescribes probabilistically\nthewayasequence offrame labels canevolveandisassumed to\nbeaMark ovchain. Ofcourse, wedonotobserv ethe\n&process\ndirectly ,butrather observ eourfeature vector data. TheHMM model\nassumes thatthelikelihood ofagivenfeature vector depends only\nonthecorresponding label. These assumptions willbemade more\nprecise inthefollowing.\n2.1TheLabelProcess\nOurgoal istoassign alabel toeach frame ofdata where each label\ncomes from setofpossible values+.Themost important component\ninthelabel isthepitch conﬁguration or“chord. ”Ifwedenote the\npossible notes under consideration as,\n'.-\u0014/\u001f# \f\"!\"#\"# \n/1032then, in\nprinciple, anyofthe 4\n0\npossible subsets of ,, 5768,:9 ,correspond to\npossible chords. Clearly thespace ofpossible chords isenormous.\nInaddition thelabel contains information thatdistinguishes between\nthe“attack, ”“sustain, ”and“rest” portions ofachord. Inparticular ,\nifwetake;\n'<-atck-1 atck-2 sust-1\"#\"#\"sust-K rest-1\"#\"\f\"rest-K\n2.\nThelabel setisthen+\n'<-6>=? \u0014@?9BA\u0019=\u001bCD5768,:9\t E@FCG;\n2AutomaticTranscription ofPianoMusic\natck_1 atck_2 sust_1 sust_2\nrest_1 rest_2sust_K\nrest_K. . .\n. . .\n . . .\n . . .\n . . .chord 1\nchord 2\nchord 2rest rest\nP\n. . . . . . . . . rest restchord 1 chord 2 chord S\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u000e\u0013\u0014\r\u0016\u0015\u0018\u0017\u0019\u0007\u001b\u001a\u000e\u0011\u001d\u001c\u001f\u001e \u0011\"!#\t\u001d$&%'\u0011\u000e\u0007(\u0017*)+\u0001-,#\u0003\u0006$\u0004\t*.\u001b/#\u00110\u0007\u001b!21\u0015*\u0001\u0004!3!3$\u0004\t\u0006\r4\u00155\u00170\u0007\n\u001a\u000e\u0011\u0019\u001c6\u001e \u0011\"!#\t\u001d$7%'\u0011\u000e\u00078\u00079\t\u0019.\u001b\u0011\u000e\u0003\u0006,3\u0001\u0004:;\u0001\u0004\u0011\u0006,<\u00133\u00079\u0011\u000e=>$?\t0\u001e@1AB\u0011\u000e:;:;\u0011C\u001e@\rB\u0015\u0018\u00170\u0007\n\u001a\u000e\u0011\u001d\u001cD\u001e \u0011\"!#\t\u001d$E%F\u00110\u0007G:H\u0007\n\u00170\u0001-,3\u0001-,\b\u0003D\u0013#\u0007\n\u00110=>$\u0004\t\u0019\u001eD1Amore sophisticated model might allowfortheassociation ofa\nﬂavor(attack, sustain, etc.) foreach chord note; inourworkhere\nwehaveopted forthecomputationally simpler version presented.\nWedeﬁne arandom process —aMark ovchain —\n&\u001f  #\"#\"\f\"# \n&$\ntaking values inthelabel set+.The fundamental assumption of\naMark ovchain isthattheprobability oftheprocess occup ying a\ncertain state (label) inagiveframe depends only onthepreceding\nstate (label):, 6\n&JILK\u001f\n'NM0OFP &\nI\u001f\n'(M\nI\u001f\n9\n', 6\n&JILK\u001f\n'QM0ORP &SI '(M9T;UWV' /6\nM\nOP M9\nwhere\n/6\nMO\nP M9isthetransition probability matrix and\n&\nI\u001fisdeﬁned\nhere by\n&\nI\u001f\n'6\n&\u001f! #\"#\"!\"# \n&$ 9.Wewillchoose\n/6\nMO\nP M9toincorpo-\nraterelevantinformation about thewaythelabel process canevolve.\nForinstance, there isaminimum possible duration foranote. Ad-\nditionally ,certain transitions between chords aremore likelythan\nothers. Thetoppanel ofFigure 1showsagraph structure depicting\nthetransitions with nonzero probability forasingle chord. While\nthefollowing construction would notbecomputationally feasible,\nconceptually therecognition phase ofourapproach builds such a\nmodel foreverypossible chord andconnects theﬁnal states ofeach\nchord model totheinitial state ofeach chord model. Asilence\nmodel isconstructed toaccount fortherecorded space before and\nafter theperformance andareconnected asinthemiddle panel of\nFigure 1.\nDue totheoverwhelming sizeofthestate space, anenormous num-\nberofpossible data interpretations exist. Where verpossible, we\nhaveattempted tochoose transition probabilities thatwillguide our\nrecognition toward\u0013 \u001a\n\u0007\u0015\u00018\u000e\u0002\u0007\u0015\u0001more plausible hypotheses. Forin-\nstance, wecanchoose theprobability distrib utions associated with\nthetransitions inthetoppanel ofFigure 1toreﬂect aplausible or\nlearned probability distrib ution onthenumber offrames spent ina\nparticular note. Inaddition, wewilllearn amodel forchord tran-\nsition probabilities from actual score data which weimpose onour\nrecognition graph.\n2.2TheObservableProcess\nRather than observ ethelabel process directly ,\nM\u001f# #\"!\"#\"# \nM$,weob-\nserveourfeature vector data\u001e\n\u001f #\"!\"#\"# \u0014\u001e\n$,which areprobabilistically\nrelated tothelabels. Theassumption ofthehidden Mark ovmodel\nisthateach visited state,\n&SI,produces afeature vector ,*\nI,from\nadistrib ution thatischaracteristic ofthatstate. More precisely ,we\nassume, 6\b*\nI'\u001e\nP &$\u001f\n'(M$\u001f\n9\n', 6\b*\nI'\u001e\nP &\nI'QM9T;UWV' /6\b\u001e\nP M9\nthus, given\n&\nI,*\nIisconditionally independent ofallother frame\nlabels andallother feature vectors. While thefundamental assump-\ntions oftheHMM simplify oursituation considerably ,therepre-\nsentation andtraining oftheoutput distrib utions,\n/6\b\u001e\nP M9,isfurther\nsimpliﬁed bytwomore assumptions.\nFirst ofall,recall thatwecompute a X\u0002\u0011ZY\n\u0005>\u000e\u0002\u0007offeatures foreach\nframe ofdata \u001e\n'6\b\u001e\n\u001f #\"#\"#\"# \u001e0[ 9.Weassume thatthecomponents\nofthisvector areconditionally independent giventhestate sothat/6\b\u001e\nP M9\n']\\[^;_\u001f\n/6\b\u001e\n^P M9.Furthermore, thestates aretiedsothat\nmanydifferent states share thesame feature distrib utions. That is/6\b\u001e\n^P M9\n' /6\b\u001e\n^P `\n^6\nM9\u00149where thetying function\n`\n^6\nM9iscon-\nstructed byhand. Thus wehave/6\b\u001e\nP M9\n'\n[\na^;_\u001f\n/6\b\u001e\n^P `\n^6\nM9\u00149\nThenature ofthetying function\n`\n^6\nM9canbeclariﬁed bydescrib-\ningmore explicitly thefeatures thatwecompute. First ofall,weAutomaticTranscription ofPianoMusic\nneed tobeable todistinguish between thetimes when thepianist\nplays andwhen there issilence, sowecompute afeature \u001e\n\u001fthat\nmeasures thetotal energyinthesignal. Most thestates,+,ofour\nmodel correspond tovarious note conﬁgurations where weexpect\ntheenergycontent toberelati velyhigh. Howeverboth theﬁrstand\nlaststates ofthemodel, which describe thesilence before andafter\ntheperformance, aswell asthe“rest” states thatrepresent space\nthatcanoccur between notes, model situations inwhich weenergy\ncontent will tend tobelower.Welet\n`\u001f6\nM9\n'\u0001\u0000forthesilence\nandreststates, and\n`\u001f6\nM9\n'\u0003\u0002fortheremaining states. Thus we\nmust learn twoprobability distrib utions:\n/6\b\u001e\n^P `\u001f6\nM9\n'\u0004\u00009and/6\b\u001e\n^P `\u001f6\nM9\n'\u0005\u00029foreach ofthese twocases. Theparticular values\nof0and1arenotrelevant; theonly important issue istheparti-\ntionofthelabel setgenerated by\n`\u001f6\nM9:\n-9MC + A\n`\u001f6\nM9\n'\u0006\u0000%2,-+MC3+ A\n`\u001f6\nM9\n'\u0005\u0002 2.\nWealsoneed todistinguish between note “attacks” andsteady state\nbehavior ifwehaveanyhope ofdetecting rearticulations. Forthis\nreason wecompute afeature \u001e\b\u0007that measures thelocal “bursti-\nness” ofthesignal. Manydifferent measures arepossible including\ndifferenced energiesanddifferenced spectral energiessummed over\nvarious regions infrequenc yspace. Ourfeature\u001e\t\u0007computes several\nsuch measures ofburstiness ( \u001e\n\u0007isavector). Forthisfeature, states\ncanbepartitioned intothree groups: thestates atthebeginning of\neach note where weexpect thesignal tohavehigh burstiness, for\nwhich weset\n`\u0007\n6\nM9\n'\n\u0000;thestates corresponding tosteady state be-\nhaviorwhere weexpect theburstiness torelati velylow,forwhich we\nset\n`\u0007 6\nM9\n'\u000b\u0002;andthesilence states where theburstiness measure\nwillhaveathird type ofbehaviorforwhich weset\n`\u0007\u00026\nM9\n'4.\nTheremainder ofourfeatures areconcerned with themost funda-\nmental problem ofdistinguishing between themanypossible pitch\nconﬁgurations. Each ofthefeatures\u001e\r\f\u0002 #\"!\"#\"\f \u0014\u001e\u0019[ iscomputed from a\nsmall frequenc yinterv aloftheFourier transformed frame data. For\neach such windo wwecompute theempirical mean andempirical\nvariance when thespectral energyfunction, restricted tothewin-\ndow,isnormalized asaprobability distrib ution. The mean value\nwillestimate thelocation oftheharmonic, (when there isasingle\nharmonic inthewindo w),andthevariance canbeused todistin-\nguish (probabilistically) between when there isasingle harmonic\n(lowvariance) andwhen there isnot(high variance). Forevery\nconﬁguration ofnotes wecancompute ideally where harmonics\nwilllieinthespectrum. Wewilltiestates thatshould havesimilar\nbehaviorinafrequenc ywindo w.Forinstance, if\nMCD+corresponds\ntoaconﬁguration inwhich none ofthenotes contain energyinthe\nwindo windexedbyfeature\u000e,then\n`\n^6\nM9\n'\u000f\u0000.Similarly ,wehave\nchosen tocollect together thestates having severalharmonics inthe\nwindo wbyletting\n`\n^6\nM9\n'\u0005\u0002forthese states. Theremaining states\narepartitioned by\n`\n^6\nM9into groups having asingle harmonic at\napproximately thesame frequenc yinthewindo w.That is,thecol-\nlection ofstates\n-9MC + A\n`\n^6\nM9\n'\u0011\u0010\u00062allhaveasingle harmonic in\nthe \u000ethwindo watapproximately thesame location.\nNote thatwehaverepresented millions ofstate-conditional distri-\nbutions interms ofasmall number of“basis” distrib utions.\n3.TRAINING THEMODEL\nOneofthemost important virtues oftheHMM formulation isthatthe\nprobability distrib utions canbetrained inan\n\u000b\u0019\u000f\u0016\u0003\u0006\u000b\u001a \u0011\n\u0007X\n\u0001\u0004\u0003\u0011\n\u0000fashion.\nThis means thatwedonotneed tohand-label frames from allofthe\ndifferent probability distrib utions described intheprevious section.\nRather ,aniterati veprocedure, knownastheforward-backw ardor\nBaum-W elchalgorithm, allowsustoautomatically train from signal-\nscore pairs. Thus, thetraining assumes thatwehaveamusical score\nforeach sound data ﬁle, andtherefore weknowtheapproximate\nsequence ofpitch conﬁgurations realized inthedata, aswell asthe\napproximate durations ofthose conﬁgurations. However,wedonot\nhavethecorrespondence between states andframes thatwould allowasimple-minded training procedure fortheoutput distrib utions.\nWhen thescore isknown, wecanbuild amodel forthehidden\nprocess using thetemplate inFigure 1(bottom). The algorithm\nbegins from aneutral starting place —webeginwith uniformly\ndistrib uted output distrib utions —anditerates theprocess ofﬁnd-\ninga \u001a\n\u0007\u0014\u000e \n\u0013\n\n\f\u0001\u0010\n\u0001\u0004\u0003\u0006\u0005\b\u0001Ycorrespondence between model states anddata\nframes andthen retraining theprobability distrib utions using this\ncorrespondence.\nMore precisely ,theforward-backw ardalgorithm allowsustocom-\npute the\u001a\n\u000e \u0003\u0015\u0005\u0011\n\u0007\u0015\u00018\u000e\u0002\u0007distrib utions, 6\n&\nI' M3P*\n\u001f\n'\u001e\n\u001f\"#\"\f\"\u0014*\n$\n'\u001e $ 9.Ifthisdistrib ution concentrated ismass onasingle state,\nM,\nthen itwould bereasonable tousethedata frame\u001e\nIasanexample\nof\n/6\b\u001e\nP M9.Inthemore typical case inwhich theposterior distrib u-\ntionallowforseveralpossible states, weconsider \u001e\nIasafractional\nexample of\n/6\b\u001e\nP M9where thefraction istheposterior probability, 6\n&JI '\fM>P* \u001f\n'\u001e \u001f \"#\"#\"\u0006* $\n'\u001e%$ 9.Thefractional examples are\nthen used toretrain theoutput distrib utions bycomputing empirical\nprobabilities ofthevarious observ ables.\nOuroutput distrib utions onfeature vectors arerepresented through\ndecision trees asfollows. Foreach distrib ution\n/6\b\u001e\n^P `\n^6\nM9\u00149we\nform abinary treeinwhich each nonterminal node ofthetreecor-\nresponds toaquestion oftheform \u001e\n^\u0013\u0012 \u0014\u0011\u0015\u0016\u000f\u0017where \u001e\n^\u0013\u0012 \u0014isthe \u0018th\ncomponent offeature\u000e.Anobserv ation\u001e\n^canbeassociated with\naterminal node by“dropping” theobserv ation downthetree. That\nis,starting attheroot weevaluate theroot question andmoveto\ntheleftorright subtree astheanswer isyesorno. This process\niscontinued until wearriveatterminal node which wedenote by\u0019\n^6\b\u001e\n^9.Thus thedecision trees quantize thedatavectors, andtobe\nprecise, thequantized values\n-\n\u0019\n^6\b\u001e\n^9\n2aretheactual features we\nuse. Asthetraining procedure evolves,thetrees arereestimated at\neach iteration tobeprogressi velyproduce more informati veproba-\nbility distrib utions bymaximizing themutual information between\ntheterminal node\n\u0019\n^6\b\u001e\n^9andtheclass label\n`\n^6\nM9.Thus weseek\ntobuildtrees which would function well atpredicting theclass label`\n^6\nM9ofavector\u001e\n^,although wedonotusethem forthispurpose.\n4.RECOGNITION\nThetraditional HMM approach torecognition seeks themost likely\nlabeling offrames, giventhedata, through dynamic programming:\u001aM$\u001f\n'\n\u001b\u0013\u001c\u001e\u001d \u001f!\u001b\u0013\"#%$ &\n, 6\n&$\u001f\n'(M$\u001f\nP*\n$\u001f\n'\u001e\n$\u001f\n9\nwhere\n\u001aM$\u001f\n'6\n\u001aM\u001f\f #\"#\"#\"# \n\u001aM$ 9.This corresponds toﬁnding thebest\npath through thegraph ofFigure 1(middle) where therewardin\ngoing from state\nM\u0006I('\u001fto\nM\u0006Iinthe )thiteration isgivenby*I6\nM\nI+'\u001f \nM\nI9\n' /6\b\u001e\nIP M\nI9\n/6\nM\nIP M\nI('\u001f9\nAwell-kno wnrecursion, theViterbi algorithm, incrementally con-\nstructs theoptimal paths oflength)from theoptimal paths of\nlength )-,\n\u0002.The computational comple xity offull-ﬂedged dy-\nnamic programming growswith thesquare ofthestate-space which\niscompletely intractable inourcase: Under restricti veassumptions\nonthepossible collection ofpitches andthenumber ofnotes ina\nchord, thestate space isontheorder of\n\u0002.\u00000/.Evenwith standard\npruning techniques, theconventional conception ofDPiscompletely\nhopeless with astate space solarge.\nInstead, weusethedata model constructed inthetraining phase to\nproduce agreatly condensed version ofthestate graph ofFigure\n1(middle). Foreach frame, ),weperform agreedy search that\nseeks aplausible collection ofstates\nMC +forthatframe. This\nisaccomplished bysearching forstates\nMgiving largevalues to/6\b\u001e\nI\"P M9.The search isperformed byﬁnding themostly likely\n1-note hypotheses, then considering 2-note hypotheses formed by\nadding single notes tothebest 1-note hypotheses, andsoon.ThusAutomaticTranscription ofPianoMusic\neach frame,),willbeassociated with apossible collection ofstates\u0000I.Thestates are“blended” byletting \u0001\nI7'\u0003\u0002\nILK\u0005\u0004^;_\nI('\u0006\u0004\n\u0000^represent\nthecollection ofstates wewillentertain atthe)thframe. Then the\nstate graph isconstructed byrestricting thefullgraph ofFigure 1\n(middle) tothe\u0001\nIsets.\nTheprincipal disadv antage ofsuch anapproach isthatifthe“true”\nstate atframe )isnotcaptured in \u0001\nI,then itcannot berecovered\nduring recognition. However,theapproach iscomputationally fea-\nsible. Wearecurrently exploring anextension oftheaboveidea in\nwhich, foreach frame),weseek hypotheses\nMC +maximizing/6\nM3P\u001e\nI9where/6\nM>P\u001e\nI9\n'\n/6\b\u001e\nICP M9\n/6\nM9/6\b\u001e\nI9\nTothisendwelearn adistrib ution onstates\n/6\nM9,mostly depend-\ningonthechord conﬁguration, from actual score data. Thus the\noriginal identiﬁcation ofpossible hypotheses isguided towardmore\nplausible conﬁgurations. Aparticularly valuable contrib ution ofthis\nmodiﬁcation isthatthe“prior” distrib ution\n/6\nM9helps todistinguish\nbetween chord conﬁgurations wedeem “homon yms” —these are\nchords thatareindistinguishable toourdata model\n/6\b\u001e\nP M9.Exam-\nplesofchord homon yms arechords differing bytheinclusion ofan\noctaveor12th aboveanyofthechord notes.\n5.EXPERIMENTS\nWepresent here preliminary experiments onseveral movements\nfrom Mozart piano sonatas. Theresults presented here arederived\nfrom aperformance ofthe3rdmovement ofSonata 18,K.570. Both\ntheoriginal data andaMIDI ﬁleofourrecognition results canbe\nheard heard at\u0007\t\b\n\b\f\u000b\u000e\r \u000f\u0010\u000f\t\u0011\u0013\u0012\t\u0011\u0015\u0014\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u0012\u001f\b \u0007\u000e\u001a\"!#\u001c\u001e\u0012\u001f$%$&\u001a \u0016&'(!\u0019\u000f\u0010) $\f\u001c*)\"\u0018,+\u0010- .\nWerestricted ourattention tonotes falling inthetruerange ofthe\nmovement, from ctwooctavesbelowmiddle ctoftwoandahalf\noctavesabovemiddle c.Also consistent with theactual movement,\nweconsidered only chords containing four orlessnotes.\nWetrained ourHMM inthemanner described using data alsotaken\nfrom various Mozart piano sonata movements recorde dunder similar\nconditions.\nWhile theMIDI ﬁleontheweb page islikelytheeasiest description\nofourlevelofsuccess tounderstand, itonly allowsforsubjecti ve\ncomparisons. Forthisreason wehavealso performed anobjec-\ntivemeasure ofperformance wedescribe now.Such ameasure is\nessential indeveloping arecognition system since itallowsoneto\nobjecti velyevaluate theconsequences ofasystem modiﬁcation.\nThe“WordError Recognition Rate” isacommon measure ofsystem\nperformance used inspeech recognition. The essential idea isto\ncompute theminimum “edit distance” —theminimum number\nofinsertions, deletions andsubstitutions necessary totransform the\nrecognized wordsequence intothetruewordsequence. Recognition\nerror rates areoften reported as\nError Rate\n' \u0002 \u0000 \u0000/. Insertions 0Deletions 0Substitutions\nTotalWords inTruthSentence\nWewillusethesame convention.\nIncomputing theminimum editdistance wetakethesequenceofrec-\nognized chords ordered bynote onset time with each chord spelled\ninarbitrary order asourrecognized data. Thus, forthepurposes\nofcomputing theerror rate, therecognized data iscollapsed into\nastring ofnotes. Conceptually ,weconvertthescore also into a\nsequence ofpitches, howeverour“matching” algorithm considers\nall )21permutations ofeach )-note chord inthescore. Thus, if\nthescore contained asingle triad chord, anyrecognition oftwoof\nthetriad notes, (asachord orinanysequence), would produce a\nsingle deletion error .This computation canbeperformed with a\nsmall variation ontheusual dynamic programming computation ofminimum editdistance. Thedata aboveyielded a“note error rate”\nof39% with 184substitutions, 241deletions, and108insertions out\nof1360 notes.\nIncomputing therecognized sequence ofpitches, iftwoadjacent\nrecognized chords haveapitch incommon, itisassumed thatthe\nnote isnotrearticulated. This assumption iscorrect most ofthe\ntime andourcurrent model does nothavetheability todetect any\npossible combination ofrearticulations. Thedominance ofdeletions\ninourrecognition results ispartly duetodeletions caused bythis\nsimple-minded assumption.\nAsecond signiﬁcant contrib utor toboth thenumber ofdeletions is\nourinability todistinguish between chord homon yms. Inourinitial\ngeneration ofchord hypotheses weconsider only thehomon ym\nwith theleast number ofnotes; thus wedonotallowanote to\nbeadded toachord hypothesis ifitdoes notgenerate anynew\nharmonics. Asdiscussed above,thealgorithm cannot recoverfrom\nerrors committed atthisstage, duetotheneed tokeepthesearch\nspace manageable.\n6.DISCUSSION\nWhile ourrecognition results leaveroom forimpro vement, itshould\nbenoted thatourcurrent system wasdevised asarelati velysim-\nplemodiﬁcation ofanearlier HMM system used for3\n\u000e\u0002\u000f \u000e\u001a54\n\u000e \u000f\u0016\u0001Y\nrecognition ofa 6\n\u000f \u000e\u00197 \u000fscore. Results ofthisquality might already\nbeuseful inanumber ofMusic Information Retrie valapplications\ntolerant oferrorful representations.\nWeexpect, however,thatsome simple additions may yield substan-\ntialgains inperformance. Forinstance, thecurrent system works\nwith nearly noknowledge oftheplausibility ofvarious sequences\nofchords. Wearecurrently working ondeveloping aprobabilistic\nmodel, trained from realmusic data, thatmodels thelikelihood of\nchord sequences. Such models provide dramatic impro vement in\nother HMM domains. Additionally ,ourcurrent system makesal-\nmost noefforttomodel theacoustic characteristics ofthehighly\ninformati venote onsets. Wehope thatamore sophisticated “attack”\nmodel would help inrecognizing themanyrepeated notes which our\nsystem currently misses.\n7.ACKNOWLEDGMENTS\nThis workissupported byNSF grants IIS-0113496 andIIS-9987 898.\n8.REFERENCES\n[1]Rabiner L.(1989), “ATutorial onHidden Mark ovModels and\nSelected Applications inSpeech Recognition, ”8\n\u0007\u0014\u000eY\u0015\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f\u0019\u0018\u0002\u0003\u000e%9\u001b\u00054\u0016\u0011;:%<=<=<?> 77,257–286, 1989.\n[2]Bahl L.,Jelinek F.,Mercer P.(1983) ,“AMaximum Likelihood\nApproach toContinuous Speech Recognition, ” :%<=<@<BA\n\u0007\u0013\n\u000f \u0003 C\u0013LY\n\u0005\b\u00018\u000e \u000f\u0016\u0003 \u000e\u0002\u000f8 \u0013\n\u0005\b\u0005\u0011\n\u0007\u0015\u000fED \u000f\u0013?\u0010 F\n\u0003\u0006\u0001\u0004\u0003\u0013\n\u000f \u0000HG\u0013LYI4\n\u0001\u0004\u000f\u0011J:\n\u000f\u0016\u0005\u0011\f\u0010\u0004\u0010\n\u0001 \u0018\u0011\n\u000fY\u0015\u0011\nPAMI–(52), 179–90, 1983.\n[3]Piszczalski M.,Galler B.(1977), “Automatic Music Transcrip-\ntion,”K\n\u000e3 \u001a\n\u000b\r\u0005\u0011\n\u0007*G \u000b\u0019\u0003\u0006\u0001YML\n\u000e\u0002\u000b\u0019\u0007\t\u000f\u0013?\u0010,vol.1,no.3,1977, 24–31.\n[4]Moorer ,J.(1977), “On theTranscription ofMusical Sound by\nComputer ,” K\n\u000e3 \u001a\n\u000b\u0019\u0005\u0011\n\u0007JG \u000b\u0019\u0003\u0006\u0001YNL\n\u000e \u000b\r\u0007\u0015\u000f\u0013\u0002\u0010vol.1,no.4,1977,\n32–38.\n[5]Chafe C.,Mont-Re ynaud B.,Rush L.,(1982), “Towardan\nIntelligent Editor ofDigital Audio: Recognition ofMusical\nConstructs, ” K\n\u000e3 \u001a\n\u000b\u0019\u0005\u0011\n\u0007OG \u000b\u0019\u0003\u0015\u0001YJL\n\u000e \u000b\r\u0007\u0015\u000f\u0013\u0002\u0010vol.6,no.1,1982,\n30–41.\n[6]Bello J.P.,Monti G.,Sandler M.(2000), “Techniques forAu-\ntomatic Music Transcription, ” 8\n\u0007 \u000eY\t\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f%\u0018\u0002\u0003 \u000e%9 \u00054\u0016\u0011O:\n\u000f\u0016\u0005\u0011\n\u0007\u0015\u000f\u0013\nC\u0005\b\u00018\u000e\u0002\u000f\u0013?\u0010QP5F\u00193 \u001a\n\u000e\u0002\u0003\u0006\u0001\u0004\u000b3\n\u000e\u0002\u000fRG \u000b\u0019\u0003\u0006\u0001Y*:\n\u000f&9#\u000e\u0002\u000737\u0013\n\u0005\b\u0001\b\u000e\u0002\u000fNS\u0011\n\u0005\b\u0007\u0015\u0001\u0011 X \u0013?\u0010,Ply-\nmouth, Massachusetts, 2000.AutomaticTranscription ofPianoMusic\n[7]Martin K.(1996), “Automatic Transcription ofSimple Poly-\nphonic Music: RobustFront End Processing, ”\nG:\u0019A\nG\u0011\n\u0000\u0002\u0001\u0013\u0000\u0013\n\n>\u001eA1\u0011ZYI4\n\u000f\u0016\u0001Y\u0015\u0013\u0002\u0010\nS\u0011 \u001a\n\u000e\u0002\u0007\t\u0005\u0002\u0001\u0004\u0003\u0006\u0005\u0007\u0005,December ,1996.\n[8]Ellis D.(1995), “Mid-Le velRepresentation forComputational\nAuditory Scene Analysis, ” 8\n\u0007\u0014\u000eY\u0015\u0011\u0006\u0011\n\u0000 \u0001\u0004\u000f\u0019\u0018\u0002\u0003 \u000e%9G\u00054\u0016\u0011 K\n\u000e3 \u001a\n\u000b\u0019\u0005\u0013\nC\u0005\b\u00018\u000e\u0002\u000f\u0013?\u0010\nD \u000b \u0000\u0002\u0001\u0004\u0005>\u000e\u0002\u0007F P\u0006Y\u0015\u0011\n\u000f\u0011\nD \u000f\u0013?\u0010 F\n\u0003\u0006\u0001\u0004\u0003\t\b \u000e\u0002\u00076\n\u00034\n\u000e\u001a,1995 Interna-\ntional Joint Conference onArtiﬁcial Intelligence, Montreal\nCanada, August 1995.\n[9]Kashino K.,Hagita N.,(1996), “AMusic Scene Analysis Sys-\ntem with theMRF-Based Information Integration Scheme, ”:%<@<=< 8\n\u0007\u0014\u000eY\u0015\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f\u0019\u0018\u0002\u0003 \u000e%9D\u00054\u0016\u0011R:\n\u000f\u0016\u0005\u0011\n\u0007\u0015\u000f\u0013\n\u0005\b\u00018\u000e \u000f\u0013?\u0010OK\n\u000e \u000f\t9\u0011\n\u0007\u0011\n\u000fY\u0015\u0011\n\u000e\u0002\u000f8 \u0013\n\u0005\b\u0005\u0011\n\u0007\t\u000f S\u0011ZY\n\u000e\u0006\u0018\u0002\u000f\u0016\u0001\u0004\u0005\b\u0001\b\u000e\u0002\u000f,725–729.\n[10] Klapuri A.(2001), “Multipitch Estimation andSound Sepa-\nration bytheSpectral Smoothness Principle, ”8\n\u0007\u0014\u000eY\u000b\n\u001e:%<@<=<:\n\u000f\u0016\u0005\u0011\n\u0007\u0015\u000f\u0013\n\u0005\b\u0001\b\u000e\u0002\u000f\u0013?\u0010 K\n\u000e\u0002\u000f&9\u0011\n\u0007\u0011\n\u000fY\t\u0011\n\u000e\u0002\u000f DY\n\u000e\u0002\u000b\u0019\u0003\u0015\u0005\b\u0001Y\n\u0003> P\u0002\u001a \u0011\u0015\u0011ZYI4G\u0013\n\u000f \u0000P\n\u0001\u0012\u0018\u0010C\u000f\u0013?\u0010\u00068\n\u0007 \u000eY\u0015\u0011\n\u0003\u0015\u0003\u0006\u0001\u0004\u000f%\u0018,2001.\n[11] Klapuri A.,Virtanen, Eronen, Seppanen (2001), “Automatic\nTranscription ofMusical Recordings, ” 8\n\u0007 \u000eY\u000b\n K\n\u000e\u0002\u000f \u0003\u0006\u0001\u0004\u0003\u0006\u0005\u0011\n\u000f\u0016\u0005\u0013\n\u000f \u0000S\u0011\f\u0010\n\u0001\u0013\n\n\u0010\u0012\u0011\nDY\n\u000e\u0002\u000b\u0019\u0003\u0006\u0005\b\u0001Y K\n\u000b\u0011\n\u0003\f\bD\u000e\u0002\u00076\n\u00034\n\u000e\u001a,Aalbor g,Denmark,\n2001.[12] Raphael C.(1999), “Automatic Segmentation ofAcoustic Mu-\nsical Signals Using Hidden Mark ovModels, ” :%<=<@< A\n\u0007\u0013\n\u000f \u0003 C\u0013LY\n\u0005\b\u00018\u000e \u000f\u0016\u0003 \u000e\u0002\u000f8 \u0013\n\u0005\b\u0005\u0011\n\u0007\u0015\u000f D \u000f\u0013?\u0010 F\n\u0003\u0006\u0001\u0004\u0003\u0013\n\u000f \u0000 G\u0013LYI4\n\u0001\u0017\u000f\u0011 :\n\u000f\u0016\u0005\u0011\f\u0010\u0004\u0010\n\u0001\u0012\u0018\u0011\n\u000fY\u0015\u0011,\nvol.21,no.4,pp.360–370.\n[13] Orio N.,Dechelle F.,(2001), “Score Following Using Spectral\nAnalysis andHidden Mark ovModels, ” 8\n\u0007\u0014\u000eY\u0015\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f\u0019\u0018\u0002\u0003 \u000e%9 \u00054\u0016\u0011:\u0017K\nGK,151–154, 2001.\n[14] Grubb L.,Dannenber gR.,(1998), “Enhanced Vocal Perfor -\nmance Tracking Using Multiple Information Sources, ” 8\n\u0007 \u000e\u0010CY\u0015\u0011\u0006\u0011\n\u0000 \u0001\u0004\u000f\u0019\u0018\u0002\u0003 \u000e%9\u001b\u00054\u0016\u0011*:\u0017K\nGK,37-44, 1998.\n[15] Cano P.,Loscos A.,Bonada J.,(1999), “Score-Performance\nMatching Using HMMs, ”8\n\u0007 \u000eY\t\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f%\u0018\u0002\u0003D\u000e\u00159 \u00054\u0016\u0011J:\u0017K\nGK,441-\n444, 1999.\n[16] Dure yA.,Clements M.(2001), “Melody Spotting Using Hid-\nden Mark ovModels, ”8\n\u0007 \u000eY\t\u0011\u0006\u0011\n\u0000\u0002\u0001\u0004\u000f%\u0018\u0002\u0003 \u000e%9 \u00054 \u0011 :\n\u000f\u0016\u0005\u0011\n\u0007\u0015\u000f\u0013\n\u0005\b\u0001\b\u000e\u0002\u000f\u0013\u0002\u0010P(F\u00173 \u001a\n\u000e \u0003\u0015\u0001\u0017\u000b3\n\u000e\u0002\u000f G \u000b\u0019\u0003\u0006\u0001Y :\n\u000f\t9#\u000e \u00073 \u0013\n\u0005\b\u00018\u000e\u0002\u000f S\u0011\n\u0005\b\u0007\u0015\u0001\u0011 X \u0013?\u0010,Blooming-\nton,Indiana, 2001."
    },
    {
        "title": "Using Psycho-Acoustic Models and Self-Organizing Maps to Create a Hierarchical Structuring of Music by Musical Styles.",
        "author": [
            "Andreas Rauber",
            "Elias Pampalk",
            "Dieter Merkl"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417143",
        "url": "https://doi.org/10.5281/zenodo.1417143",
        "ee": "https://zenodo.org/records/1417143/files/RauberPM02.pdf",
        "abstract": "With the advent of large musical archives the need to provide an organization of these archives becomes eminent. While artist-based organizations or title indexes may help in locating a specific piece of music, a more intuitive, genre-based organization is required to allow users to browse an archive and explore its contents. Yet, currently these organizations following musical styles have to be designed manually. In this paper we propose an approach to automatically create a hierarchical organizationof music archives following their perceived sound similarity. More specifically, characteristics of frequency spectra are extracted and transformed according to psycho-acoustic models. Subsequently, the Growing Hierarchical Self-Organizing Map, a popular unsupervised neural network, is used to create a hierarchical organization, offering both an interface for interactive exploration as well as retrieval of music according to perceived sound similarity.",
        "zenodo_id": 1417143,
        "dblp_key": "conf/ismir/RauberPM02",
        "keywords": [
            "organization",
            "musical archives",
            "intuitive",
            "genre-based",
            "organization",
            "musical styles",
            "manually",
            "psycho-acoustic models",
            "Growing Hierarchical Self-Organizing Map",
            "unsupervised neural network"
        ],
        "content": "UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\nUsingPsycho-Acoustic ModelsandSelf-Organizing Maps\ntoCreateaHierarchicalStructuring of\nMusicbySoundSimilarity\nAndreasRauber\n\u0007\nDept.ofSoftwareTechnology\nViennaUniv.ofTechnology\nA-1040Vienna,Austria\nandi@ifs.tuwien.ac.atEliasPampalk\nAustrianResearch Institutefor\nArtiﬁcialIntelligence\nA-1010Vienna,Austria\nelias@ai.univie .ac.atDieterMerkl\nDept.ofSoftwareTechnology\nViennaUniv.ofTechnology\nA-1040Vienna,Austria\ndieter@ifs .tuwien.ac.at\nABSTRA CT\nWiththeadventoflargemusicalarchivestheneedtoprovidean\norganization ofthesearchivesbecomeseminent. Whileartist-based\norganizations ortitleindexesmayhelpinlocatingaspeciﬁcpiece\nofmusic,amoreintuitive,genre-based organization isrequiredto\nallowuserstobrowseanarchiveandexploreitscontents. Yet,\ncurrently theseorganizations followingmusicalstyleshavetobe\ndesigned manually.\nInthispaperweproposeanapproach toautomatically createa\nhierarchical organization ofmusicarchivesfollowingtheirperceived\nsoundsimilarity .Morespeciﬁcally ,characteristics offrequency\nspectraareextractedandtransformed according topsycho-acoustic\nmodels. Subsequently ,theGrowingHierarchical Self-Organizing\nMap,apopularunsupervised neuralnetwork,isusedtocreatea\nhierarchical organization, offeringbothaninterfaceforinteractive\nexploration aswellasretrievalofmusicaccording toperceived\nsoundsimilarity .\n1.INTRODUCTION\nWiththeavailability ofhigh-quality audioﬁleformatsatsufﬁcient\ncompression rates,weﬁndmusicincreasingly beingdistributed\nelectronically vialargemusicarchives,offeringmusicfromthe\npublicdomain,sellingtitles,orstreaming themonapay-per-play\nbasis,orsimplyintheformofon-lineretailersforconventional\ndistributionchannels. Acorerequirement forthesearchivesisthe\npossibility fortheusertolocateatitleheorsheislookingfor,orto\nﬁndoutwhichtypesofmusicareavailableingeneral.\nThus,thosearchivescommonly offerseveralwaystoﬁndadesired\npieceofmusic.Astraightforw ardapproach istousetextbased\nqueriestosearchfortheartist,thetitleorsomephraseinthelyrics.\nWhilethisapproach allowsthelocalization ofadesiredpieceofmu-\nsic,itrequirestheusertoknowandactivelyinputinformation about\nthetitleheorsheislookingfor.Analternativeapproach, allow-\ninguserstoexplorethemusicarchive,searching formusicalstyles,\nratherthanforaspeciﬁctitleorgroup,isthususuallyprovidedinthe\nformofgenrehierarchies suchas\b\n\t\f\u000b\u000e\r\u000f\r\u0011\u0010\u0013\u0012\u0011\u000b\u0014\t,\u0015\u0016\u000b\u0018\u0017\u000f\u0017,\u0019\u001b\u001a\u001c\u0012\u000f\u001d.Hence,a\ncustomer lookingforanoperarecording mightlookintothe \b\n\t\f\u000b\u001e\r\u0011\r\u000f\u0010\u001f\u0012\nsection,andwillthereﬁnd-depending onthefurtherorganization of\nthemusicarchive-avarietyofinterpretations, beingsimilarinstyle,\nandthuspossiblysuitinghisorherlikings.However,suchorgani-\nzationsrelyonmanualcategorizations andusuallyconsistofseveral\nhundredcategorieswhichinvolvehighmaintenance costs,inpar-\nticularfordynamicmusiccollections, wheremultiplecontributors\u0007\u0014 \"!$#\u001f%'&\u000f(\u0014%*)\u000e+ ,\u0002-.&$#\u0013/0-.!$,'1\u000e&$2\u000e34-5)\u000e+ 6 3.%7)\u000e34!$89%7)\u000e&$#\"-.!$,'!:2\u001b;=<4>.?A@B<53C,\u00133C!:#*D7)E\n3C6 6 &C-\u0006!:%F?\u001f;'?*G\u0016>=&$2\u000e,*+ H$6 + &JIF!:KC+ &$2\u000e!$6 301\u000e3C6 6 3\n<4+ DC3C#\u0013DL)\u000e3NM\u001f>4IO<FP*GQ '+ ,\u0013!\u001cGR?S%7!:6 T$U\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage. V\nW\n2002IRCAM-CentrePompidouhavetoﬁletheircontributionsaccordingly .Theinherentdifﬁcul-\ntiesofsuchtaxonomies havebeenanalyzed, forexample,in[22].\nAnotherapproach takenbyon-linemusicstoresistoanalyzethe\nbehaviorofcustomers togivethoseshowingsimilarinterestsrec-\nommendations onmusicwhichtheymightappreciate. Forexample,\nasimpleapproach istogiveacustomer lookingforpiecessimilar\nto XZY\n[Q\\^]\t \u0010A\r$_recommendations onmusicwhichisusuallybought\nbypeoplewhoalsopurchased XZY\n[Q\\\u0004]\t \u0010A\r$_.However,extensiveand\ndetailedcustomer proﬁlesarerarelyavailable.\nThe `5aNbc_\u0018\u0015\u000ed ,i.e.the `FaNbfeL_:g\"h\"\u000b\u001eg=\u0012\u000f_Cij\u0015\n[\u001dR_\u0018k\u0011\u001a\u001clsystem,outlined\nin[26],facilitates exploration ofmusicarchiveswithoutrelyingon\nfurtherinformation suchascustomer proﬁlesorpredeﬁned cate-\ngories.Itdoesnotrequiretheavailability ofdetailed,high-quality\nmeta-data onthevariouspiecesofmusic,ormusicalscores.Rather,\nwerelyonthesoundinformation, presentintheformofanyacous-\nticalwaveformat,asitisavailablee.g.fromCDtracksorMP3ﬁles.\nBasedonthesoundsignalweextractlow-levelfeaturesbasedonfre-\nquencyspectradynamics, andprocessthemusingpsycho-acoustic\nmodelsofourauditorysystem.Theresulting representation allows\nustocalculate toacertaindegreetheperceivedsimilarity between\ntwopiecesofmusic.Weusethisformofdatarepresentation as\ninputtothem\n\\\u001a\u001en\n\u0010AgQoqp\u0004\u0010*_\n\\\u000b\n\\\u0012\u000fh\u0016\u0010\u0013\u0012\u0011\u000b\u0014\tN`=_$\t r$e\u000fa\n\\oR\u000b\u001egs\u0010t\u0017$\u0010AgQoubv\u000b:w (m0pxe`5aNb)[6],anextensiontothepopularself-organizing map[13].\nThisneuralnetworkprovidesclusteranalysisbymapping similar\ndataitemsclosetoeachotheronamapdisplay.Speciﬁcally ,them0p\u0004`5aNb iscapableofdetecting hierarchical relationships inthe\ndata,andthusproduces ahierarchy ofmapsrepresenting various\nstylesofmusic,intowhichthepiecesofmusicareorganized.\nTheremainder ofthispaperisorganizedasfollows.Section2brieﬂy\nreviewstherelatedwork.Thefeatureextractionprocessispresented\nindetailinSection3,followedbyadescription oftheprinciples and\ntrainingprocedure ofthe`._$\t r:e\u000fa\n\\oR\u000b\u001eg\u0002\u0010\f\u0017\u0018\u0010ygQozbv\u000b\u000fw ,andthem\n\\\u001a\u000en0\u0010ygQop{\u0010*_\n\\\u000b\n\\\u0012\u000fhQ\u0010\u001f\u0012\u000f\u000b\u001e\ts`._$\t r:e\u000fa\n\\oR\u000b\u001eg\u0002\u0010\f\u0017\u0018\u0010ygQo|bv\u000b:w inSection4.Wethendescribe\nexperimental results,usingbothareducedcollection of77pieces\nofmusic,aswellasalargerarchiveconsisting of359piecesin\nSection5.Finally,inSection6someconclusions aredrawn.\n2.RELATEDWORK\nAvastamountofresearchhasbeenconducted intheareaofcontent-\nbasedmusicandaudioretrieval.Forexample,methodshavebeen\ndevelopedtosearchforpiecesofmusicwithaparticular melody.\nThequeriescanbeformulated byhumming andareusuallytrans-\nformedintoasymbolic melodyrepresentation, whichismatched\nagainstadatabase ofscoresusuallygiveninMIDIformat.Re-\nsearchinthisdirection isreportedin,e.g.[1,2,10,16,28].Other\nthanmelodicinformation itisalsopossibletoextractandsearchfor\nstyleinformation usingtheMIDIformat.Forexample,in[4]solo\nimprovisedtrumpetperformances areclassiﬁed intooneofthefour\nstyles: \t }\n\\\u0010\u0013\u0012\u0011\u000b\u0014\t, r\n\\\u000b\u001egs~\u001f\u0010\u0013\u0012, \r\u0011}\u000eg=\u0012\u0011\u001a:w\"\u000b\u001e~7_Ci ,or w\"\u001a\u001e\u0010yg\u0002~\u001f\u0010\u001f\tA\t \u0010y\r\u000f~\u001f\u0010\u001f\u0012.\nTheMIDIformatoffersawealthofpossibilities, however,only\nasmallfractionofallelectronically availablepiecesofmusicareUsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\navailableasMIDI.Amorereadilyavailableformatistherawaudio\nsignaltowhichallotheraudioformatscanbedecoded. Oneofthe\nﬁrstaudioretrievalapproaches dealingwithmusicwaspresented\nin[35],whereattributessuchasthepitch,loudness, brightness and\nbandwidth ofspeechandindividualmusicalnoteswereanalyzed.\nSeveraloverviewsofsystemsbasedontherawaudiodatahavebeen\npresented, e.g.[9,18].However,mostofthesesystemsdonottreat\ncontent-based musicretrievalindetail,butmainlyfocusonspeech\norpartly-speech audiodata,withoneofthefewexceptions being\npresented in[17],usinghummed queriesagainstanMP3archive\nformelody-based retrieval.\nFurthermore, onlyfewapproaches intheareaofcontent-based music\nanalysishaveutilizedtheframeworkofpsychoacoustics. Psychoa-\ncousticsdealswiththerelationship ofphysicalsoundsandthehuman\nbrain’sinterpretation ofthem,cf.[37].Oneoftheﬁrstexceptions\nwas[8],wherepsychoacoustic modelsareusedtodescribethesimi-\nlarityofinstrumental sounds.Theapproach wasdemonstrated using\nacollection ofabout100instruments, whichwereorganizedusing\na`._$\t r$e\u0011a\n\\oR\u000b\u000eg\u0002\u0010t\u0017$\u0010Ag\u0016ojbv\u000b:w inasimilarwayaspresented inthispa-\nper.Foreachinstrument a300milliseconds soundwasanalyzedand\nsteadystatesoundswithadurationof6milliseconds wereextracted.\nThesesteadystatesoundscanberegardedasthesmallestpossible\nbuildingblocksofmusic.Amodelofthehumanperceptual behav-\niorofmusicusingpsychoacoustic ﬁndingswaspresented in[30]\ntogetherwithmethodstocompute thesimilarity oftwopiecesof\nmusic.Amorepracticalapproach tothetopicwaspresented in[33]\nwheremusicgivenasrawaudioisclassiﬁed intogenresbasedon\nmusicalsurfaceandrhythmfeatures. Thefeaturesaresimilarto\ntherhythmpatternsweextract,themaindifferencebeingthatwe\nanalyzethemseparately in20frequencybands.\nOurworkisbasedonﬁrstexperiments reportedin[26].Inparticular\nwehaveredesigned thefeatureextraction processusingpsychoa-\ncousticmodels.Additionally ,byusingahierarchical extensionof\ntheneuralnetworkfordataclustering weareabletodetectthe\nhierarchical structurewithinourarchive.\n3.FEATUREEXTRACTION\nThearchitecture ofthe `Fa0bc_\u0018\u0015\u000ed systemmaybedividedinto3\nstagesasdepictedinFigure1.Digitized musicingoodsoundqual-\nity(44kHz,stereo)withadurationofoneminuteisrepresented\nbyapproximately 10MBofdatainitsrawformatdescribing the\nphysicalproperties oftheacoustical waveswehear.Inaprepro-\ncessingstage,theaudiosignalistransformed, down-sampled and\nsplitintoindividualsegments(stepsP1toP3).Wethenextractfea-\ntureswhicharerobusttowardsnon-percepti vevariationsandonthe\notherhandresemble characteristics whicharecriticaltoourhear-\ningsensation, i.e.rhythmpatternsinvariousfrequencybands.The\nfeatureextractionstagecanbedividedintotwosubsections, consist-\ningoftheextraction ofthespeciﬁcloudnesssensation expressedin`'\u001a\u001eg._(stepsS1toS6),aswellastheconversionintotime-invariant\nfrequency-speciﬁc rhythmpatterns(stepR1toR3).Finally,thedata\nmaybeoptionally converted,beforebeingorganizedintoclustersin\nstepsA1toA3usingthe mNp{`Fa0b .Thefeatureextraction stepsare\nfurtherdetailedinthefollowingsubsections, withtheclustering pro-\ncedurebeingdescribed inSection4,withthevisualization metaphor\nbeingonlytoucheduponbrieﬂyduetospaceconsiderations.\n3.1Preprocessing\n( )Thepiecesofmusicmaybegiveninanyaudioﬁleformat,\nsuchase.g.MP3ﬁles.Weﬁrstdecodethesetotheraw\n[\t \r$_\b0\u001a\u001ciR_bv\u001a\u001ci\n[\t\f\u000b\u000e~\u001f\u0010\u0013\u001a\u001eg(PCM)audioformat.\n( )Therawaudioformatofmusicingoodqualityrequireshuge\namountsofstorage.Ashumanscaneasilyidentifythegenreofa\npieceofmusicevenifitssoundqualityisratherpoorwecansafely\nreducethequalityoftheaudiosignal.Thus,stereosoundquality\nisﬁrstreducedtomonoandthesignalisthendown-sampled fromPreprocessingP1: Audio -> PCM\nP2: Stereo -> Mono, 44kHz->11kHz\nP3: music -> segments\nSpecificS1: Power Spectrum\nS2: Critical Bands\nS3: Spectral Masking\nS4:Decibel - dB-SPL\nS5: Phon: Equal LoudnessLoudness\nSensation\n(Sone)\nS6: Sone: Specific Loudness Sens.\nRhythmR1: Modulation Amplitude\nR2: Fluctuation Strength\nR3: Modified Fluctuation StrengthPatterns\nper\nFrequency\nBandF\ne\na\nt\nu\nr\ne\nE\nx\nt\nr\na\nc\nt\ni\no\nn\nAnalysisA1: Median vector (opt.)\nA2: Dimension Reduction (opt.)\nA3: GHSOM Clustering\nVisualization: Islands of Music and Weather Chartsy=F\u001c\u0002f45\u0018\ns=\"\u001cFyQq\n\u001c\n\u001c4\u0014\u001c\n\u001f5F\"\u0011\u0018\u0018\n'\u0002\n59q\"\n\n\ny.0 R¡\u001f\u0018\n\"\n\n¡F\n¢\nF£¤5\n\n¡Ay¥s\ny.\n44kHzto11kHz,leavingadistorted, butstilleasilyrecognizable\nsoundsignalcomparable tophonelinequality.\n( ¦)Wesubsequently segmenteachpieceinto6-second sequences.\nThedurationof6seconds(§R¨L©samples) waschosenheuristically\nbecauseitislongenoughforhumanstogetanimpression ofthe\nstyleofapieceofmusicwhilebeingshortenoughtooptimize\nthecomputations. However,analyseswithvarioussettingsforthe\nsegmentation haveshownnosigniﬁcant differenceswithrespectto\nsegmentlength.Afterremovingtheﬁrstandthelast2segments\nofeachpieceofmusictoeliminate lead-inandfade-outeffects,\nweretainonlyeverythirdoftheremaining segmentsforfurther\nanalysis. Again,theinformation lostbythistypeofreduction has\nshowninsigniﬁcant invariousexperimental settings.\nWethusendupwithseveralsegmentsof6secondsofmusicevery\n18secondsat11kHzforeachpieceofmusic.Thepreprocessing\nresultsinadatareduction byafactorofover24withoutlosing\nrelevantinformation, i.e.ahumanlistenerisstillabletoidentifythe\ngenreorstyleofapieceofmusicgiventhefew6-second sequences\ninlowerquality.\n3.2SpeciﬁcLoudness Sensation -Sone\nLoudness belongstothecategoryofintensitysensations. Theloud-\nnessofasoundismeasured bycomparing ittoareference sound.\nThe1kHztoneisaverypopularreference toneinpsychoacoustics,UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\nandtheloudnessofthe1kHztoneat40dBisdeﬁnedtobeªZ`'\u001a\u001eg._.\nAsoundperceivedtobetwiceasloudisdeﬁnedtobe2 `=\u001a\u001eg=_and\nsoon.Intheﬁrststageofthefeatureextraction process,thisspe-\nciﬁcloudnesssensation (Sone)percritical-band (Bark)inshorttime\nintervalsiscalculated in6stepsstartingwiththePCMdata.\n( \n)Firstthepowerspectrum oftheaudiosignaliscalculated. To\ndothis,therawaudiodataisﬁrstdecomposed intoitsfrequencies\nusinga X4\u000b\u001e\r\u0011~NX5\u001a\n[Q\\\u0010*_\n\\u«s\\\u000b\u001egs\rSr\u0018\u001a\n\\\u000f¬\u000b\u001e~\u001f\u0010\u0013\u001a\u000eg®­\u0013X X\n«'¯.Weuseawindow\nsizeof256samples, whichcorresponds toabout23msat11kHz,\nandaHanningwindowwith50%overlap.WethusobtainaFourier\ntransform of11/2kHz,i.e.5.5kHzsignals.\n(F)Theinnerearseparates thefrequencies andconcentrates them\natcertainlocations alongthebasilarmembrane. Theinnerear\ncanthusberegardedasacomplexsystemofaseriesofband-\npassﬁlterswithanasymmetrical shapeoffrequencyresponse. The\ncenterfrequencies oftheseband-pass ﬁltersarecloselyrelatedtothe\ncritical-band rates,wherefrequencies arebundledinto24critical-\nbandsaccording tothe d|\u000b\n\\\u001dscale[37].Wherethesebandsshould\nbecentered, orhowwidetheyshouldbe,hasbeenanalyzed through\nseveralpsychoacoustic experiments. Sinceoursignalislimitedto\n5.5kHzweuseonlytheﬁrst20criticalbands,summing upthe\nvaluesofthepowerspectrum withintheupperandlowerfrequency\nlimitsofeachband,obtaining apowerspectrum ofthe20critical\nbandsforthesegments.\n( F¦)SpectralMaskingistheocclusion ofaquietsoundbyalouder\nsoundwhenbothsoundsarepresentsimultaneously andhavesimilar\nfrequencies. Spectralmaskingeffectsarecalculated basedon[31],\nwithaspreading functiondeﬁningtheinﬂuence ofthe°-thcritical\nbandonthe \u0010-thbeingusedtoobtainaspreading matrix.Using\nthismatrixthepowerspectrum isspreadacrossthecriticalbands\nobtained inthepreviousstep,wherethemasking inﬂuence ofa\ncriticalbandishigheronbandsaboveitthanonthosebelowit.\n(4±)Theintensityunitofphysicalaudiosignalsissoundpressure\nandismeasured in \u001b\u000b\u001e\r:\u0012\u0011\u000b\u0014\t(Pa).ThevaluesofthePCMdata\ncorrespond tothesoundpressure. Beforecalculating`=\u001a\u000eg._values\nitisnecessary totransform thedataintodecibel.Thedecibelvalue\nofasoundiscalculated astheratiobetweenitspressureandthe\npressureofthehearingthreshold, alsoknownasdB-SPL, where\nSPListheabbreviationforsoundpressurelevel.\n(F²)Therelationship betweenthesoundpressurelevelindecibel\nandourhearingsensation measured in `'\u001a\u001eg._isnotlinear.The\nperceivedloudnessdependsonthefrequencyofthetone.Fromthe\ndB-SPLvalueswethuscalculatetheequalloudnesslevelswiththeir\nunitPhon.The Nhs\u001a\u000eglevelsaredeﬁnedthroughtheloudnessindB-\nSPLofatonewith1kHzfrequency.Alevelof400hs\u001a\u001egresembles\ntheloudness levelofa40dB-SPL toneat1kHz.Apuretoneat\nanyfrequencywith400hs\u001a\u001egisperceivedasloudasapuretone\nwith40dBat1kHz.Wearemostsensitivetofrequencies around\n2kHzto5kHz.Thehearingthreshold rapidlyrisesaroundthelower\nandupperfrequencylimits,whicharerespectivelyabout20Hzand\n16kHz.Although thevaluesfortheequalloudness contourmatrix\nareobtainedfromexperiments withpuretones,theymaybeapplied\ntocalculate thespeciﬁcloudness ofthecriticalbandratespectrum,\nresultinginloudnesslevelrepresentations forthefrequencyranges.\n( F³)Finally,astheperceivedloudnesssensation differsfordifferent\nloudnesslevels,thespeciﬁcloudnesssensation in`=\u001a\u001eg=_iscalculated\nbasedon[3].Theloudnessofthe1kHztoneat40dB-SPL isdeﬁned\ntobe1Sone.Atoneperceivedtwiceasloudisdeﬁnedtobe2 `=\u001a\u001eg=_\nandsoon.Forvaluesupto400hs\u001a\u001egthesensation risesslowly,\nincreasing atafasterrateafterwards.\nFigure2illustrates thedataaftereachofthefeatureextraction steps\nusingtheﬁrst6-second sequences extractedfromd{_\u0011_:~yhs\u001a\u000e´\u001e_:g\u0002µNXZY\n[Q\\]\t \u0010A\r$_andfrom ¶^\u001a\n\\g\u0002µ·X\n\\_\u0011\u000b\u001c\u001df\u001a\u001eg¸\u000bº¹F_C\u000b\u001e\r»h .Thesequence of XZY\n[Q\\−0.0500.05PCM Audio SignalAmplitude\n204060\nPower Spectrum [dB]Frequency [kHz]024\n204060\nCritical−Band Rate Spectrum [dB]Critical−band [bark]5101520\n204060\nSpread Critical−Band Rate Spectrum [dB]Critical−band [bark]5101520\n204060\nSpecific Loudness Level [phon]Critical−band [bark]5101520\n12345\nSpecific Loudness Sensation [sone]\nTime [s]Critical−band [bark]\n0 2 45101520−101PCM Audio SignalAmplitude\n20406080\nPower Spectrum [dB]Frequency [kHz]024\n406080\nCritical−Band Rate Spectrum [dB]Critical−band [bark]5101520\n406080\nSpread Critical−Band Rate Spectrum [dB]Critical−band [bark]5101520\n406080\nSpecific Loudness Level [phon]Critical−band [bark]5101520\n5 10152025\nSpecific Loudness Sensation [sone]\nTime [s]Critical−band [bark]\n0 2 45101520Beethoven, Für Elise Korn, Freak on a Leash y=F\u001c4Z4\nOu\nR:5³4\n\u001c.9¼\n'Q½4¾\n¥¢¿^ÀÁ\nF£Fy\nS.\n¡\n\u001c\ns\u0014SÂ\n\u0006¡y.5£ 5\u001cz\nsZR9\nA\n¡\u0011ÃF\n5£]\t \u0010A\r$_containsthemainthemestartingshortlybeforethe2ndsecond.\nThespeciﬁcloudness sensation depictseachpianokeyplayed.On\ntheotherhand, X\n\\_\u0011\u000b\u001c\u001d¤\u001a\u000egÄ\u000bj¹F_C\u000b\u001e\r»h ,whichisclassiﬁed as p¢_C\u000b\u001e´\u001c}bv_:~*\u000b\u0014\t Å\"Æ^_C\u000b\u001e~yhubc_:~*\u000b\u0014\t ,isquiteaggressive.Melodicelements donot\nplayamajorroleandthespeciﬁcloudness sensation isarather\ncomplexpatternspreadoverthewholefrequencyrange,whereas\nonlythelowercriticalbandsareactiveinXZY\n[Q\\\u001b]\t \u0010A\r$_.Noticefurther,\nthatthevaluesofthepatternsof X\n\\_C\u000b9\u001dv\u001a\u001eg¸\u000bq¹F_C\u000b\u001e\r»h areupto18\ntimeshighercompared tothoseofXZY\n[Q\\{]\t \u0010A\r$_.\n3.3RhythmPatterns\nAftertheﬁrstpreprocessing stageapieceofmusicisrepresented\nbyseveral6-second sequences. Eachofthesesequences contains\ninformation onhowloudthepieceisataspeciﬁcpointintimeina\nspeciﬁcfrequencyband.Yet,thecurrentdatarepresentation isnot\ntime-invariant.Itmaythusnotbeusedtocompare twopiecesof\nmusicpoint-wise, asalreadyasmalltime-shift ofafewmilliseconds\nwillusuallyresultincompletely differentfeaturevectors.Inthe\nsecondstageofthefeatureextraction process,wecalculate atime-\ninvariantrepresentation foreachpieceofmusicin3furthersteps,\nnamelythefrequency-wiserhythmpattern.Theserhythmpatterns\ncontaininformation onhowstrongandfastbeatsareplayedwithin\ntherespectivefrequencybands.\n( Çj)Theloudness ofacritical-band usuallyrisesandfallsseveral\ntimesresulting inamoreorlessperiodical pattern,alsoknownas\ntherhythm. Theloudness valuesofacritical-band overacertain\ntimeperiodcanberegardedasasignalthathasbeensampledat\ndiscretepointsintime.Theperiodical patternsofthissignalcan\nthenbeassumed tooriginate fromamixtureofsinuids. These\nsinuidsmodulate theamplitude oftheloudness, andcanbecalcu-\nlatedbyaFouriertransform. Themodulation frequencies, whichUsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\n12345\nSpecific Loudness SensationCritical−band [bark]1020\n0 2 40.513.6Hz +− 1.5Hz\nTime [s]Amplitude\n5 10152025Critical−band [bark]Modulation Amplitude\n1020\n0.51  1.52  Critical−band [bark]Fluctuation Strength\n1020\n0.10.20.30.40.5\nModulation Frequency [Hz]Critical−band [bark]Modified Fluctuation Strength\n24681010205 10152025\nSpecific Loudness SensationCritical−band [bark]1020\n0 2 40.516.9Hz +− 2.7Hz\nTime [s]Amplitude\n1020304050Critical−band [bark]Modulation Amplitude\n1020\n5 101520Critical−band [bark]Fluctuation Strength\n1020\n2468\nModulation Frequency [Hz]Critical−band [bark]Modified Fluctuation Strength\n2468101020Beethoven, Für Elise Korn, Freak on a Leash y.59¦4\u00064\nOqÇ\u0006\u0014\u0011Çº¦4\n9=È¡y.5£ 5\u001cj\ns\u0018\u0002\ny.\nv4£OtÂ \"£BÉ0F\n\u0002\ny=\u0018\n\u001c\u000249¼\ncanbeanalyzed usingthe6-second sequences andtimequantaof\n12ms,areintherangefrom0to43Hzwithanaccuracyof0.17Hz.\nNoticethatamodulation frequencyof43Hzcorresponds toalmost\n2600bpm. Thus,theamplitude modulation oftheloudness sensa-\ntionpercritical-band foreach6-second sequence iscalculated using\naFFTofthe6-second sequence ofeachcriticalband.\n( Çº)Theamplitude modulation oftheloudnesshasdifferenteffects\nonoursensation depending onthefrequency.Thesensation ofÊ[\u0012$~\n[\u000b\u001e~\u001f\u0010\u0013\u001a\u001eg®\r\u0011~\n\\_:g\u0016o\u001e~yhismostintenseataamodulation frequency\nofaround4Hzandgradually decreases upto15Hz.At15Hzthe\nsensation of\n\\\u001a\n[o9h\u0016g._:\r\u0011\rstartstoincrease, reachesitsmaximum at\nabout70Hz,andstartstodecreases atabout150Hz.Above150Hz\nthesensation ofhearing~yh\n\\_\u0011_\u0004\r\u0018_7w\"\u000b\n\\\u000b\u001e~7_$\t }º\u000b\n[i\u001e\u0010\u001fk\u0018\tt_|~*\u001a\u001eg._:\r increases.\nItistheﬂuctuation strength,i.e.rhythmpatternsupto10Hz,which\ncorresponds to600beatsperminute(bpm),thatweareinterested\nin.Foreachofthe20frequencybandsweobtain60valuesfor\nmodulation frequencies between0and10Hz.Thisresultsin1200\nvaluesrepresenting theﬂuctuation strength.\n( Çº¦)Todistinguish certainrhythmpatternsbetterandtoreduce\nirrelevantinformation, gradientandGaussian ﬁltersareapplied.\nInparticular ,weusegradientﬁlterstoemphasize distinctivebeats,\nwhicharecharacterized througharelativelyhighﬂuctuation strength\nataspeciﬁcmodulation frequencycompared tothevaluesimmedi-\natelybelowandabovethisspeciﬁcfrequency.Wefurtherapplya\nGaussian ﬁltertoincreasethesimilarity betweentworhythmpattern\ncharacteristics whichdifferonlyslightlyinthesenseofeitherbeing\ninsimilarfrequencybandsorhavingsimilarmodulation frequen-\nciesbyspreading theaccording values.Wethusobtainmodiﬁed\nﬂuctuation strengthvaluesthatcanbeusedasfeaturevectorsfor\nsubsequent clusteranalysis.\nThesecondpartofthefeatureextraction processissummarized in\nFigure3.Looking atthemodulation amplitude of XZY\n[Q\\º]\t \u0010A\r$_it\nseemsasthoughthereisnobeat.Intheﬂuctuation strengthsubplot\nthemodulation frequencies around4Hzareemphasized. Yet,there\narenoclearverticallines,astherearenoperiodicbeats.Ontheotherhand,notethestrongbeatofaround7HzinallfrequencybandsofX\n\\_C\u000b9\u001d\u001a\u001egq\u000bx¹F_C\u000b\u001e\r»h .Foranin-depthdiscussion ofthecharacteristics\nofthefeatureextraction process,pleasereferto[23,24].\n4.HIERARCHICAL DATACLUSTERING\nUsingtherhythmpatternsweapplythe `=_$\t r$e\u0011a\n\\oR\u000b\u001egs\u0010t\u0017$\u0010AgQo¸bv\u000b\u000fw\n(`5aNb)[13],aswellasitsextension, them\n\\\u001a\u001en\n\u0010AgQoqp\u0004\u0010\u0013_\n\\\u000b\n\\\u0012\u000fhQ\u0010\u0013\u0012\u0011\u000b\u0014\t`=_$\t r$e\u0011a\n\\oR\u000b\u001egs\u0010t\u0017$\u0010AgQovbv\u000b:w ( mNp\u0004`5aNb )[6]algorithm toorganizethe\npiecesofmusicona2-dimensional mapdisplayinsuchawaythat\nsimilarpiecesaregroupedclosetogether.Inthefollowingsections\nwewillbrieﬂyreviewtheprinciples ofthe`Fa0bandthemNp{`Fa0b ,\nfollowedbyadescription ofthelaststepsofthe `5aNbc_\u0018\u0015\u000ed system,\ni.e.theclusteranalysisstepsA1toA3inFigure1.\n4.1Self-Organizing Maps\nThe `._$\t r$e\u0011a\n\\oR\u000b\u000eg\u0002\u0010t\u0017$\u0010Ag\u0016oËbv\u000b:w ( `FaNb),asproposed in[12]andde-\nscribedthoroughly in[13],isoneofthemostdistinguished un-\nsupervised artiﬁcialneuralnetworkmodels. Itbasically provides\nclusteranalysisbyproducing amapping ofhigh-dimensional input\ndataontoausually2-dimensional outputspacewhilepreserving the\ntopological relationships betweentheinputdataitemsasfaithfully\naspossible. Inotherwords,the`5aNbproduces aprojection ofthe\ndataspaceontoatwo-dimensional mapspaceinsuchaway,that\nsimilardataitemsarelocatedclosetoeachotheronthemap.\nMoreformally,the `FaNbconsistsofasetofunits Ì,whicharear-\nrangedaccording tosometopology,wherethemostcommonchoice\nisatwo-dimensional grid.EachoftheunitsÌisassignedamodel\nvector ÍqÎofthesamedimension astheinputdata, ÍqÎ|Ï¸Ð\u001bÑ .In\ntheinitialsetupofthemodelpriortotraining,themodelvectors\narefrequently initialized withrandomvalues.However,moreso-\nphisticated strategiessuchas,forexample,Principle Component\nAnalysis, maybeapplied. Duringeachlearningstep Ò,aninput\npattern ÓOÔAÒ\u0011Õisrandomly selectedfromthesetofinputvectorsand\npresented tothemap.Next,theunitshowingthemostsimilarmodel\nvectorwithrespecttothepresented inputsignalisselectedasthe\nwinnerÖ,whereacommonchoiceforsimilarity computation isthe\nEuclidean distance, cf.Expression 1.Ö\u0014ÔAÒ\u0011ÕØ×'ÙtÙ ÓOÔAÒCÕFÚcÍuÛ\u001eÔAÒCÕ\u0018ÙtÙ\u001eÜÄÝÞtßÎ¸à\nÙtÙ ÓOÔAÒCÕ ÚvÍjÎCÔAÒCÕ\u0018ÙtÙ á Ô»â\u001cÕ\nAdaptation takesplaceateachlearningiterationandisperformed\nasagradualreduction ofthedifferencebetweentherespectivecom-\nponentsoftheinputvectorandthemodelvector.Theamountof\nadaptation isguidedbyamonotonically decreasing learning-rateã,ensuring largeadaptation stepsatthebeginningofthetraining\nprocess,followedbyaﬁne-tuning-phase towardstheend.\nApartfromthewinner,unitsinatime-varyingandgradually de-\ncreasingneighborhood aroundthewinnerareadaptedaswell.This\nenablesaspatialarrangement oftheinputpatternssuchthatalike\ninputsaremappedontoregionsclosetoeachotherinthegridof\noutputunits.Thus,thetrainingprocessoftheself-organizingmap\nresultsinatopological orderingoftheinputpatterns. According\nto[27]theself-organizingmapcanbeviewedasaneuralnetwork\nmodelperforming aspatiallysmoothversionof ä-meanscluster-\ning.Theneighborhood ofunitsaroundthewinnermaybedescribed\nimplicitly bymeansofaneighborhood-k ernel å\u0002ÛLÎtakingintoac-\ncountthedistance–intermsoftheoutputspace–betweenunitÌ\nunderconsideration andunit Ö,thewinnerofthecurrentlearning\niteration. AGaussian maybeusedtodeﬁnetheneighborhood-\nkernel,ensuringstrongeradaptionofunitsclosetothewinner.Itis\ncommon practicethatinthebeginningofthelearningprocessthe\nneighborhood-k ernelisselectedlargeenoughtocoverawidearea\noftheoutputspace.Thespatialwidthoftheneighborhood-k ernel\nisreducedgradually duringthelearningprocesssuchthattowards\ntheendofthelearningprocessjustthewinneritselfisadapted.UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\ncx(t)\nm(t)cm(t+1)c\næn\nInput Space Output Spacey.F\u001c±F^\nÀèç\n\n\u001fF\u001f4\nv4£F\"¡Ø=\"\n=\n£'F\nS.\nIncombining theseprinciples ofself-organizing maptraining,we\nmaywritethelearningruleasgiveninExpression (2),with\nãrep-\nresenting thetime-varyinglearning-rate,å\nÛLÎrepresenting thetime-\nvaryingneighborhood-k ernel, Órepresenting thecurrently presented\ninputpattern,andÍqÎdenotingthemodelvectorassignedtounitÌ.Í\nÎÔAÒ5éÄâ9Õ\nÜÄÍ\nÎÔAÒ\u0011Õ4é\nãÔAÒ\u0011ÕOê\u001cå\nÛLÎÔAÒCÕ êRë ÓOÔAÒ\u0011ÕOÚfÍ\nÎÔAÒ\u0011Õ7ì Ô*§\u001eÕ\nAsimplegraphical representation ofaself-organizingmap’sarchi-\ntectureanditslearningprocessisprovidedinFigure4.Inthis\nﬁguretheoutputspaceconsistsofasquareof36units,depicted\nascircles,formingagridofíºî\u0006íunits.OneinputvectorÓOÔAÒCÕis\nrandomly chosenandmappedontothegridofoutputunits.The\nwinnerÖshowingthehighestactivationisdetermined. Consider the\nwinnerbeingtheunitdepictedastheblackunitlabeledintheﬁg-\nure.Themodelvectorofthewinner,ÍqÛ\u000eÔAÒ\u0011Õ,isnowmovedtowards\nthecurrentinputvector.Thismovementissymbolized intheinput\nspaceinFigure4.Asaconsequence oftheadaptation, unit Öwill\nproduceanevenhigheractivationwithrespecttotheinputpatternÓatthenextlearningiteration, Ò.éïâ,becausetheunit’smodelvec-\ntor,ÍqÛ9ÔAÒFéðâ\u001cÕ ,isnownearertotheinputpatternÓintermsofthe\ninputspace.Apartfromthewinner,adaptation isperformed with\nneighboring units,too.Unitsthataresubjecttoadaptation arede-\npictedasshadedunitsintheﬁgure.Theshadingofthevariousunits\ncorresponds totheamountofadaptation, andthus,tothespatial\nwidthoftheneighborhood-k ernel.Generally ,unitsinclosevicinity\nofthewinnerareadaptedmorestrongly,andconsequently ,theyare\ndepictedwithadarkershadeintheﬁgure.\nBeingadecidedly stableandﬂexiblemodel,the `5aNbhasbeenem-\nployedinawiderangeofapplications, rangingfromﬁnancialdata\nanalysis,viamedicaldataanalysis,totimeseriesprediction, indus-\ntrialcontrol,andmanymore[5,13,32].Itbasically offersitself\ntotheorganization andinteractiveexploration ofhigh-dimensional\ndataspaces.Oneofitsmostprominent application areasistheorga-\nnizationoflargetextarchives[15,19,29],which,duetonumerous\ncomputational optimizations andshortcuts thatarepossibleinthis\nNNmodel,scaleuptomillionsofdocuments [11,14].\nHowever,duetoitstopological characteristics, the `FaNbnotonly\nservesasthebasisforinteractiveexploration, butmayalsobeusedas\nanindexstructure tohigh-dimensional databases, facilitating scal-\nableproximity searches. Reportsonacombination of `5aNbË\rand\nR*-trees asanindextoimagedatabases havebeenreported, for\nexample,in[20,21],whereasanindextreebasedonthe `FaNb\nisreportedin[36].Thus,the`5aNbcombines andoffersitselfin\naconvenientwaybothforinteractiveexploration, aswellasfor\ntheindexingandretrieval,ofinformation represented intheform\nofhigh-dimensional featurespaces,whereexactmatchesareei-\ntherimpossible duetothefuzzynatureofdatarepresentation or\ntherespectivetypeofquery,oratleastcomputationally prohibitive,\nmakingthemparticularly suitableforimageormusicdatabases.layer 0\nlayer 1\nlayer 2\nlayer 3JS.F\u001c²4jñ\u0004ò\u0006ó·ôõö\n9¼\n\n\"\nF\u001c\n4.2TheGHSOM\nThekeyideaofthem\n\\\u001a\u000en0\u0010ygQoxp{\u0010*_\n\\\u000b\n\\\u0012\u000fhQ\u0010\u0013\u0012\u0011\u000b\u0014\t\u0016`=_$\t r$e\u000fa\n\\oR\u000b\u001egs\u0010t\u0017$\u0010AgQobv\u000b\u000fw [6]\nistouseahierarchical structureofmultiplelayers,whereeachlayer\nconsistsofanumberofindependent `5aNbË\r.One `Fa0bisusedat\ntheﬁrstlayerofthehierarchy,representing therespectivedatain\nmoredetail.Foreveryunitinthismapa `FaNbmightbeaddedto\nthenextlayerofthehierarchy.Thisprinciple isrepeatedwiththe\nthirdandanyfurtherlayersofthe mNp{`FaNb .\nSinceoneoftheshortcomings of`FaNbusageisitsﬁxednetwork\narchitecture weratheruseanincrementally growingversionofthe`5aNb.Thisrelievesusfromtheburdenofpredeﬁning thenetwork’s\nsizewhichisratherdetermined duringtheunsupervised training\nprocess.Westartwithalayer0,whichconsistsofonlyonesingle\nunit.Theweightvectorofthisunitisinitialized astheaverageof\nallinputdata.Thetrainingprocessbasically startswithasmallmap\nof,say,§îj§unitsinlayer1,whichisself-organizedaccording to\nthestandard `Fa0btrainingalgorithm.\nThistrainingprocessisrepeatedforaﬁxednumber ÷oftraining\niterations. Everafter÷trainingiterations theunitwiththelargest\ndeviationbetweenitsweightvectorandtheinputvectorsrepresented\nbythisveryunitisselectedastheerrorunit.Inbetweentheerrorunit\nanditsmostdissimilar neighbor intermsoftheinputspaceeithera\nnewroworanewcolumnofunitsisinserted. Theweightvectors\nofthesenewunitsareinitialized astheaverageoftheirneighbors.\nAnobviouscriteriontoguidethetrainingprocessisthe ø\n[\u000b\u001eg\u0002~\u001f\u0010\f\u00179\u000b\u001ee~\u001f\u0010\u001f\u001a\u001egï_\n\\:\\\u001a\n\\¢ùÎ,calculated asthesumofthedistances betweenthe\nweightvectorofaunit Ìandtheinputvectorsmappedontothisunit.\nItisusedtoevaluatethemapping qualityofa`5aNbbasedonthe¬_C\u000b\u001egúø\n[\u000b\u001egs~\u001f\u0010t\u0017\u001c\u000b\u001e~\u001f\u0010\u0013\u001a\u001eg¤_\n\\:\\\u001a\n\\( b®û\n])ofallunitsinthemap.Amap\ngrowsuntilitsb¤û\n]fallsbelowacertainfractionü¨ofthe\nùÎof\ntheunit Ìinthepreceding layerofthehierarchy.Thus,themapnow\nrepresents thedataofthehigherlayerunit Ìinmoredetail.\nAsoutlinedabovetheinitialarchitecture ofthemNp{`Fa0b consists\nofone `FaNb.Thisarchitecture isexpandedbyanotherlayerincase\nofdissimilar inputdatabeingmappedonaparticular unit.These\nunitsareidentiﬁed byaratherhighquantization error\nùÎwhichis\naboveathresholdü9ý.Thisthreshold basically indicates thedesired\ngranularity levelofdatarepresentation asafractionoftheinitial\nquantization erroratlayer0.Insuchacase,anewmapwillbe\naddedtothehierarchy andtheinputdatamappedontherespective\nhigherlayerunitareself-organizedinthisnewmap,whichagain\ngrowsuntilitsb®û\n]isreducedtoafractionü¨oftherespective\nhigherlayerunit’squantization error\nùÎ.Notethatthisdoesnot\nnecessarily leadtoabalanced hierarchy.Thedepthofthehierarchy\nwillratherreﬂectthediversityininputdatadistributionwhichshould\nbeexpectedinreal-worlddatacollections. Depending onthedesired\nfraction ü¨of b®û\n]reduction wemayendupwitheitheraverydeep\nhierarchy withsmallmaps,aﬂatstructure withlargemaps,or–in\ntheextremecase–onlyonelargemap.Thegrowthofthehierarchy\nisterminated whennofurtherunitsareavailableforexpansion.UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\n0  0.51\n0  0.92\n0  1.13\n0  0.54\n0  1.85\n0  0.96\n0  0.77\n0  1.88\n0  0.99\n0  0.7MedianÔ\u0013þRÕØÿ\u0001\u0000\u0002\u0000\n\u0003\u0005\u0004\u0007\u0006\t\b\u0000\u0018ß\u000b\n\u0007\f\u000e\r\n\u000f\u0007\u0010\u0012\u0011\u0014\u0013Þ\n\u0005\u0000\n0.39.11\n0.34.62\n0.36.93\n0.34.44\n0.34.55\n0.39  6\n0.34.87\n0.34.28\n0.34.79\n0.59.110\n0.24.411\n0.44.2MedianÔ\u0016\u0015'Õ\u0018\u0017\n\u0006\u0010ß\u0019\n\u001a\f\n\u0010\u0000\u001cþ\u001c\u001b\n\u0006ß\u0006þ\u001e\u001d\u0019\u0000\u001cþ\n\u0005\u0004y.59³4 ç¼\n ¼=59¼\nF\u0002\u0018\ns\u000e\n\n \u001f\"!\t!$#&%('*)\u0007!\u001c+-,/.1024357698&:\n!\n5£<;\n'3\n+=,>.3\n!@?\u0007AB'*+C?BDE!@?:\n%\nF£9¼\nsyZv\"£O\n\nAgraphical representation ofa mNp{`FaNb isgiveninFigure5.The\nmapinlayer1consistsofF\u001bî¢§unitsandprovidesaroughorganiza-\ntionofthemainclustersintheinputdata.Thesixindependent maps\ninthesecondlayerofferamoredetailedviewonthedata.Twounits\nfromoneofthesecondlayermapshavefurtherbeenexpandedinto\nthird-layer mapstoprovidesufﬁcientlygranularinputdatarepre-\nsentation. Byusingaproperinitialization ofthemapsaddedateach\nlayerinthehierarchy basedontheparentunit’sneighbors, aglobal\norientation ofthenewlyaddedmapscanbereached[7].Thus,sim-\nilardatawillbefoundonadjoining bordersofneighboring mapsin\nthehierarchy.\n4.3Clusteranalysisofmusicdata\nThefeaturevectorsextractedaccording totheprocessdescribed in\nSection3areusedasinputtothe mNp\u0004`5aNb .However,somefurther\nintermediary processing stepsmaybeappliedinordertoobtain\nfeaturevectorsforpiecesofmusic,ratherthanmusicsegments,as\nwellasto,optionally ,compress thedimensionality ofthefeature\nspaceasfollows.\n( G\u0006)Basically,eachsegmentofmusicmaybetreatedasanin-\ndependent pieceofmusic,thusallowingmultipleassignment ofa\ngivenpieceofmusictomultipleclustersofvaryingstyleifapieceof\nmusiccontainspassages thatmaybeattributedtodifferentgenres.\nAlso,atwo-levelclustering procedure maybeappliedtoﬁrstgroup\nthesegmentsaccording totheiroverallsimilarity .Inasecondstep,\nthedistributionofsegmentsacrossclustersmaybeusedasakind\nof H0gQo\u0016_\n\\w\n\\\u0010Ag\u0002~todescribethecharacteristics ofthewholepiece\nofmusic,usingtheresulting distributionvectorsasaninputtothe\nsecond-le velclustering procedure [26].\nOntheotherhand,ourresearchhasshown,thatsimplyusingthe\nmedianofallsegmentvectorsbelonging toagivenpieceofmusic,\nresultsinastablerepresentation ofthecharacteristics ofthispieceofmusic.WehaveevaluatedseveralalternativesusingGaussian mix-\nturemodels,fuzzyc-means, andk-meanspursuing theassumption\nthatapieceofmusiccontainssigniﬁcantly differentrhythmpatterns.\nHowever,themedian,despitebeingbyfarthesimplesttechnique,\nyieldedcomparable resultstothemorecomplexmethods. Other\nsimplealternativessuchasthethemeanprovedtobetoovulnerable\nwithrespecttooutliers.\nTherhythmpatternsofall6-second sequences extractedfrom XZY\n[Q\\]\t \u0010A\r$_andfrom X\n\\_\u0011\u000b\u001c\u001d®\u001a\u001eg \u000bf¹F_C\u000b\u001e\r»h aswellastheirmediansare\ndepictedinFigure6.Theverticalaxisrepresents thecritical-bands\nfrom d|\u000b\n\\\u001d1-20,thehorizontal axisthemodulation frequencies\nfrom0-10Hz,whered|\u000b\n\\\u001d1and0Hzislocatedinthelowerleft\ncorner.Generally ,thepatternsofonepieceofmusichavecommon\nproperties. WhileXºY\n[\"\\¢]\t \u0010A\r$_ischaracterized byaratherhorizon-\ntalshapewithlowvalues, X\n\\_C\u000b9\u001dË\u001a\u001egï\u000bº¹F_C\u000b\u001e\r»h hasacharacteristic\nverticallinearound7Hz.Tocapturethesecommon characteristics\nwithinapieceofmusicthemedianisasuitableapproach. The\nmedianof XZY\n[Q\\|]\t \u0010A\r$_indicates thattherearecommonbutweakac-\ntivitiesintherangeof3-10d|\u000b\n\\\u001dwithamodulation frequencyofup\nto5Hz.Thesinglesequences of XZY\n[Q\\\u001b]\t \u0010A\r$_havemanymoredetails,\nforexample,theﬁrstsequence hasaminorpeakaround5dJ\u000b\n\\\u001dand\n5Hzmodulation frequency.However,themaincharacteristics, e.g.\ntheverticallineat7Hzfor X\n\\_\u0011\u000b\u001c\u001dz\u001a\u000eg\u0006\u000bx¹5_\u0011\u000b\u000e\rCh ,aswellasthegeneric\nactivityinthefrequencybandsarepreserved.\n( Gu)Furthermore, the1200-dimensional featurespacemaybecom-\npressedusingPrinciple Component Analysis (PCA).Ourexperi-\nmentshaveshownthatareduction downto80dimensions maybe\nperformed withoutmuchlossinvariance.Yet,fortheexperiments\npresented inthispaperweusetheuncompressed featurespace.\n( Gu¦)Followingtheseoptionalsteps,a mNp{`FaNb maybetrainedto\nobtainahierarchical mapinterfacetothemusicarchive.Apartfrom\nobtaining hierarchical representations, the mNp{`FaNb mayalsobe\nappliedtoobtainﬂatmapssimilartoconventional`Fa0bË\r,orgrow\nlineartreestructures.\n(I\n\n\n¡Ay¥s\ny.)Theresultingmapsofferthemselvesasinterfaces\ntoexploreamusicarchive.Yetadvancedclustervisualization tech-\nniquesbasedonthe `FaNb,suchasthe JFeAbv\u000b\u001e~\n\\\u0010tl[34],maybeused\ntoassistinclusteridentiﬁcation. Aspeciﬁcally appealing visual-\nizationbasedon \r\n¬\u001a\u001c\u001a\u001e~yh\u0002_CiËiR\u000b\u001e~*\u000buhQ\u0010y\r\u000f~*\u001a\u0011o\n\\\u000b\n¬\rº­L`QÆ{p\n¯[25]aretheK\r\u000f\t\f\u000b\u001eg'i\u001e\r\u001a»rxb\n[\r\u0011\u0010\u0013\u0012,whichusethemetaphor ofgeographical maps,\nwhereislandsresemble stylesofmusic,toprovideanintuitivein-\nterfacetomusicarchives.Furthermore, attributeaggregatesare\nusedtocreate L\u0006_C\u000b\u001e~yh\u0002_\n\\\u0012\u000fhs\u000b\n\\~\u001f\rthathelptheusertounderstand the\nsoundcharacteristics ofthevariousareasonthemap.Foradetailed\ndiscussion andevaluationofthesevisualizations, see[24].\n5.EXPERIMENTS\nInthefollowingsectionswepresentsomeexperimental resultsofour\nsystembasedonamusicarchivemadeupofMP3-compressed ﬁles\nofpopularpiecesofmusicfromavarietyofgenres.Speciﬁcally ,\nwepresentinmoredetailtheorganization ofasmallsubsetof\ntheentirearchive,consisting of77piecesofmusic,withatotal\nplayingtimeofabout5hours,usingthem0p\u0004`5aNb .Thissubset,\nduetoitslimitedsizeoffersitselffordetaileddiscussion. We\nfurthermore presentresultsusingalargercollection of359pieces\nofmusic,withatotalplayinglengthofabout23hours.Inboth\ncases,eachpieceisrepresented by1200featureswhichdescribe\nthedynamics oftheloudnessinfrequencybands.Theexperiments,\nincluding audiosamples, areavailableforinteractiveexploration\natthe`Fa0bc_\u0018\u0015\u000ed projecthomepage atM$N$NPO\u0014QSRPR\u001cT$T$T\u0014USV\u001cW*XYUZN\\[$T\u001aVP]\u001c^-U_$`U\n_NaR*b\n_^dc*V\\RdXPe\tf\u001a]Pg\u001ch .\n5.1AGHSOM of77piecesofmusic\nFigure7depictsamNp\u0004`5aNb trainedonthemusicdata.Ontheﬁrst\nlevelthetrainingprocesshasresultedinthecreationofa FjîiFUsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\ncocojambo\nlimp−n2gether\nmacarena\nrockdj\ndancingqueen\nfirsttime\nforeveryoung\nfrozenbfmc−uprocking\nthemangotreebfmc−instereo\nbfmc−rocking\nbfmc−skylimit\nbongobong\nsl−summertime\nrhcp−californication\nrhcp−world\nsl−whatigotbfmc−freestyler\nsexbomb\ncaliforniadream\nrisingsun\nunbreakmyheart\nbigworld\naddict\nga−liemissathing\nangelsfriend\nyesterday−b\nnewyork\nsml−adia\namericanpie\nlovedwomantorn\nga−doedel\nga−iwantit\nga−japan\nnma−bigbluelimp−nobody\npr−broken\nga−nospeechlimp−pollution\nkorn−freak\npr−deadcell\npr−revenge\neternalflame\nfeeling\nrevolutiondrummerboy\nfatherandson\nironic\nmemory\nrainbow\nthreetimesaladyconga\nmindfiels\nlovsisintheaireifel65−blue\nfromnewyorktola\ngowest\nmanicmonday\nradio\nsupertrouper\nfuture\nlovemetender\ntherose\nbrandenbeethoven\nfuguedminor\nvm−bach\nvm−brahms\nair\navemaria\nelise\nkidscene\nmondy.F\u001ckjmlu¾\u0006\nÀ\n\nv\nyq\u0014=¡y¡y\"\ny.\nmap,organizing thecollection into9majorstylesofmusic.The\nbottomrightrepresents mainlyclassicalmusic,whiletheupperleft\nmainlyrepresents amixtureofHipHop,Electro,andHousebyd|\u001a\n¬r\n[g\"\u001dzb¤\b \r­Ck\u001fr\n¬\u0012\n¯.Theupper-right,center-right,andupper-\ncenterrepresent mainlydiscomusicsuchas \u0019\u001a\u001c\u0012\u000f\u001duÆ¢\u0015 by \u0019\u001aRk\u001ck$\u0010\u0013_Lº\u0010\u001f\tA\t \u0010\u0013\u000b\n¬\rx­\n\\\u001a\u001c\u0012\u000f\u001d\u0014iL°\n¯,dJ\t\n[_by\n]\u0010 n|_$\tpoPqq­C_:\u0010 n|_$\troPq\u0014e7k\u0018\t\n[_\n¯,orX\n\\\u001a\u0018\u0017\u000e_:g\nby bf\u000bRiR\u001a\u001egsg=\u000b\u0006­ r\n\\\u001a\u001c\u00179_:g\n¯.Pleasenote,thattheorganization doesnot\nfollowclean“conceptual” genrestyles,splittingbydeﬁnition, e.g.p{\u0010 wspx\u001a:wand p\u001a\n[\r$_,butratherreﬂectstheoverallsoundsimilarity .\nSevenofthese9ﬁrst-levelcategoriesarefurtherreﬁnedonthesec-\nondlevel.Forexample,thebottomrightunitrepresenting classical\nmusicisdividedinto4furthersub-categories.Ofthese4cate-\ngoriesthelower-rightrepresents slowandpeacefulmusic,mainly\npianopiecessuchas XZY\n[Q\\Z]\t \u0010A\r$_Ë­C_$\t \u0010A\r$_\n¯and bv\u001a\u001eg'i\u001e\r:\u0012\u000fh\u0002_:\u0010Ags\r$\u001a\u000eg=\u000b\u001e~7_­\n¬\u001a\u000eg=i\n¯byd{_\u0011_:~yhs\u001a\u001e´\u001e_:g ,orX\n\\_\n¬i\u0016_º¹Y \u000b\u001eg'i\u0016_\n\\q[g=i\u0006bc_:g\u0002\r:\u0012\u000fh\u0002_:g by`'\u0012\u000fh\n[Q¬\u000b\u001egsg ­\u0013\u001d\u000e\u0010\u0013i\u000e\r$\u0012\u000f_:g=_\n¯.Theupper-rightrepresents, forexample,\npiecesby ss\u000b\u001eg=_:\r\u000f\r:\u000bËbf\u000b\u0016_ (vm),which,inthiscase,aremoredy-\nnamicinterpretations ofclassicalpiecesplayedontheviolin.Inthe\nupper-leftorchestral musicislocatedsuchastheastheendcredits\noftheﬁlmdJ\u000bR\u0012\u000f\u001d~*\u001a¢~yhs_\u001bX\n[~\n[Q\\_\nKtKtK­ r\n[~\n[Q\\_\n¯andtheslowlovesong«h\u0002_\u0019\u001b\u001a\u001e\r$_by d{_:~\u001f~7_JbË\u0010\u001fis\tt_\n\\­7~yh\u0002_\n\\\u001a\u001e\r$_\n¯,exhibiting amoreintensive\nsoundsensation, whereasthelowerrightcornerunitrepresents thed\n\\\u000b\u001eg=iR_:g=k\n[Q\\ov\b0\u001a\u001eg'\u0012\u000f_\n\\~\u001f\rby d|\u000bR\u0012\u000fhf­»k\n\\\u000b\u001eg'i\u0016_:g\n¯.\nGenerally speaking, weﬁndthesofter,morepeacefulsongsonthis\nsecondlevelmaplocatedinthelowerhalfofthemap,whereasthe\nmoredynamic, intensivesongsarelocatedintheupperhalf.This\ncorresponds tothegeneralorganization ofthemapintheﬁrstlayer,\nwheretheunitrepresenting Classicmusicislocatedinthelowerright\ncorner,havingmoreaggressivemusicasitsupperandleftneighbors.\nThisallowsus,evenonlower-levelmaps,tomoveacrossmap\nboundaries toﬁndsimilarmusicontheneighboring mapfollowingthesamegeneraltrendsoforganization, thusalleviatingthecommon\nproblemofclusterseparation inhierarchical organizations.\nSomeinteresting insightsintothemusiccollection whichthe m0pxe`5aNbrevealsare,forexample,thatthesongX\n\\_\u0011_:\r\u0011~\u001f}\u001e\tt_\n\\bydJ\u001a\n¬er\n[gQ\u001djb®\b \r (center-left)isquitedifferentthentheothersongsby\nthesamegroup.X\n\\_\u0011_:\r\u000f~\u001f}\u001e\tt_\n\\wasthegroupsbiggesthitsofarand,\nunliketheirothersongs,hasbeenappreciate byabroaderaudience.\nGenerally ,thepiecesofonegrouphavesimilarsoundcharacteristics\nandthusarelocatedwithinthesamecategories.Thisapplies,for\nexample,tothesongsof m\n[\u000b\u001eg'\u001avu·ws_:\r\u0004­\u001foR\u000b\n¯and \u001b\u000b:w\"\u000b\u0019\u001b\u001a\u001c\u000bR\u0012\u000fhj­Sw\n\\7¯,\nwhicharelocatedinthecenterofthe9ﬁrst-levelcategoriestogether\nwithotheraggressiverocksongs.However,anotherexception is¹O\u0010A´\u001c\u0010Ag\u0016o\u0006\u0010Ag¸\u000bu¹O\u0010*_ bym\n[\u000b\u001eg'\u001a uØws_:\ru­\u001foR\u000b\u0014e*\t \u0010*_\n¯,locatedinthelower-\nleft.Listening tothispiecereveals,thatitismuchslowerthanthe\notherpiecesofthegroup,andthatthissongmatchesverywellto,\nforexample, uiRi\u001e\u0010\u0013\u0012$~by ¶xw \r\bFhs\u001a\u001e\u0010\u001f\u0012:_.\n5.2AGHSOM of359piecesofmusic\nInthissectionwepresentresultsfromusingthe `5aNbc_\u0018\u0015\u000ed system\ntostructurealargercollection of359piecesofmusic.Duetospace\nconstraints wecannotdisplayordiscussthefullhierarchy indetail.\nWewillthuspickafewexamplestoshowthecharacteristics ofthe\nresulting hierarchy,invitingthereadertoexploreandevaluatethe\ncomplete hierarchy viatheprojecthomepage.\nTheresultingmNp{`Fa0b hasgrowntoasizeof§ºîzyunitsonthe\ntoplayermap.All8top-layer unitswereexpandedontoasecond\nlayerinthehierarchy,fromwhich25unitsoutof64unitstotal\nonthislayerwerefurtherexpanded intoathirdlayer.Noneof\nthebranches requiredexpansion intoafourthlayerattherequired\nlevel-of-detail setting.Anintegratedviewofthetwotop-layers of\nthemapisdepictedinFigure8.WewillnowtakeacloserlookatUsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\nJS.F\u001c1{mlu¾Ë\nÀ\n\n\u001c¼\nË¡\n\u001c'\"uv\nAj\u0014=¡y¡y\"\ny.}|¦\u0002²(~c\ny\"\u0014\u001c\nsomebranchesofthismap,andcomparethemtotherespectiveareas\ninthe mNp{`FaNb ofthesmallerdatacollection depictedinFigure7\nGenerally ,weﬁndpiecesofsoftclassicalmusicintheupperright\ncorner,withthemusicbecoming gradually moredynamicandag-\ngressiveaswemovetowardsthebottomleftcornerofthemap.\nDuetothecharacteristics ofthetrainingprocessofthe mNp{`FaNb\nwecanﬁndthesamegeneraltendencyattherespectivelower-layer\nmaps.Theoverallorientation ofthemaphierarchy isrotatedwhen\ncompared tothesmaller mNp{`FaNb ,wheretheclassicaltitleswere\nlocatedinthebottomrightcorner,withthemoreaggressivetitles\nplacedontheupperleftareaofthemap.Thisrotationisdueto\ntheunsupervised natureofthemNp\u0004`5aNb trainingprocess. Itcan,\nhowever,beavoidedbyusingspeciﬁcinitialization techniques ifa\nspeciﬁcorientation ofthemapwererequired.\nTheunitintheupperrightcornerofthetop-layer map,representing\nthesoftestclassicalpiecesofmusic,isexpandedontoa FZîj§map\ninthesecondlayer(expandedtotheupperrightinFigure8).Here\nweagainﬁndthesoftest,mostpeacefulpiecesintheupperright\ncorner,namelypartofthesound-track ofthemovie\u0015\n[Q\\\u000b\u000e\r\u000f\r\u0011\u0010\u0013\u0012{\u001b\u000b\n\\\u001d,\nnextto ¹F_C\u000b\u001e´\u001c\u0010AgQou\u001b\u001a\n\\~by \u0015\u0016\u000b\n¬_:\r¢px\u001a\n\\g._\n\\,\n«h\u0002_^bc_\n\\:\\}u_C\u000b\u001e\r:\u000b\u001eg\u0002~\u001f\r\nby `'\u0012\u000fh\n[Q¬\u000b\u001egsg,and \b0\u000b\u000eg=\u001a\u001egby \u001b\u000bR\u0012\u000fh\u0002_$\t\fk\u000f_$\t .Belowthisunitweﬁnd\nfurthersofttitles,yetsomewhatmoredynamic. Webasically ﬁnd\nalltitlesthatweremappedtogetherinthebottomrightcornerunit\nofthemNp\u0004`5aNb ofthesmallercollection depictedinFigure7on\nthisunit,i.e. uJ\u0010\n\\µ-uJ´\u001e_bv\u000b\n\\\u0010\u0013\u000b\u001eµNXZY\n[Q\\x]\t \u0010A\r$_:µ\nX\n\\_\n¬i\u0016_¢¹Y \u000b\u001eg'i\u0016_\n\\[g=ibc_:gs\r:\u0012\u000fh\u0002_:g¸­\u0013\u001d\u000e\u0010\u0013i\u001e\r:\u0012\u000f_:g._\n¯andthebv\u001a\u001eg'i\u001e\r:\u0012\u000fh\u0002_:\u0010Ag\u0002\r:\u001a\u001eg'\u000b\u001e~7_ .Furthermore,\nafewadditional titlesofthelargercollection havebeenmapped\nontothisunit,themostfamousofwhichprobably are Æx\u0010*_¢\u001d\u001e\tt_:\u0010Ag=_\u000bR\u0012\u000fhQ~\n¬^[\r\u0011\u0010S\u001dby bv\u001a\u001c\u0017\u001c\u000b\n\\~,the X\n[g._\n\\\u000b\u001e\t0bv\u000b\n\\\u0012\u000fhby \bFhs\u001a:w'\u0010ygortheuiR\u000b\u001co\u001e\u0010\u0013\u001afromtheClarinetConcertbyemphMozart.\nLetusnowtakealookatthetitlesthatweremappedontothe\nneighboring unitsinthepreviouslypresented smallerdatacollection.\nThed\n\\\u000b\u001eg'i\u0016_:g=k\n[Q\\o\u001e\u0010A\r:\u0012\u000fh\u0002_x¶^\u001a\u001eg\u0016\u00179_\n\\~7_,locatedontheneighboring unit\ntotherightintheﬁrstexample,canbefoundinthelowerleftcornerofthismap,togetherwith,forexample,u|\t \r:\u001az\r\u0013w\n\\\u000bR\u0012\u000fhzO\u000b\n\\\u000b\u001e~yh\n[\r\u0011~\n\\\u000b\nby \u0019\u001b\u0010\u0013\u0012\u000fhs\u000b\n\\i^`\u0002~\n\\\u000b\n[.Mappedontotheupperneighboring unitinthe\nsmallermNp{`FaNb wehadtitlesliketheX0\u0010\n\\\r\u0011~=bv\u001a\u001e´\u001e_\n¬_:gs~O\u001aLrN~yh\u0002_\u0012q\u001e~yh`s}\n¬wshs\u001a\u001egs}by d{_\u0011_:~yhs\u001a\u001e´\u001e_:g ,orthe\n«\u001a\u001c\u0012\u0011\u0012\u000f\u000b\u000e~*\u000bu\u000b\u001eg'i¢X\n[o\u0016_x\u0010yguÆ bË\u0010yg=\u001a\n\\\nbyd|\u000b\u0014\u0012\u000fh.Weﬁndthesetwotitlesintheupperleftcornerofthe\n2-layermapofthis mNp\u0004`5aNb ,togetherwithtwoofthethreetitles\nmappedontothediagonally neighboring unitintheﬁrst mNp{`Fa0b ,\ni.e.¹4\u001a\u001e´\u001e_\n¬_\n«_:g'i\u0016_\n\\by\n]\t ´\u001c\u0010A\r{\n\\_:\r\u000f\tt_:},and\n«h\u0002_\u0019\u001a\u000e\r\u0018_ byd{_:~\u001f~7_b\u0006\u0010\u0013is\tt_\n\\,whichareagainsoft,mellow,butabitmoredynamic. The\nthirdtitlemappedontothisunitinthesmallerm0p\u0004`5aNb ,i.e.the\nsoundtrackofthemovie d|\u000b\u0014\u0012\u000f\u001dj~*\u001aj~yh\u0002_^X\n[~\n[Q\\_\nKtKtKisnotmapped\nintothisbranchofthismNp{`Fa0b anymore.Whenwelistentothis\ntitleweﬁndittohavemainlystrongorchestral parts,whichhavea\ndifferent,moreintensesoundthanthesoftpiecesmappedontothis\nbranch,whichismorespeciﬁcwithrespecttoverysoftclassicaltitles\nasmoreofthemareavailableinthelargerdatacollection. Instead,\nwecanﬁndthistitleontheupperrightcornerintheneighboring\nbranchtotheleft,originating fromtheupperleftcornerunitofthe\ntop-layer map.Thereitismappedtogetherwith\n«h\u0002_d|_\u0011\u000b\n[~\u001f}q\u000b\u001eg'i~yhs_d{_C\u000b\u001e\r\u0011~ andotherorchestral pieces,suchas u|\tA\tt_\u0013o\n\\\u001aubv\u001a\u0014\t ~*\u001abyd\n\\\u000b9h\n¬\r.WethusﬁndthisbranchofthemNp\u0004`5aNb tobemoreor\nlessidenticaltotheoverallorganization ofthesmaller mNp{`FaNb in\nsofarasthetitlespresentinbothcollections aremappedinsimilar\nrelativepositions toeachother.\nDuetothetopology preservationprovidedbythe mNp{`FaNb wecan\nmovefromthesoftclassicalclustermaptothelefttoﬁndsomewhat\nmoredynamic classical piecesofmusicontheneighboring map\n(expandedtotheleftinFigure8).Thus,atypicaldisadvantageof\nhierarchical clustering andstructuring ofdatasets,namelythefact\nthataclusterthatmightbeconsidered conceptually verysimilaris\nsubdividedintotwodistinctbranches, isalleviatedinthemNp{`Fa0b\nconcept,becausethesedatapointsaretypicallylocatedintheclose\nneighborhood. Wethusﬁnd,ontherightborderoftheneighboring\nmap,themorepeacefultitlesofthisbranch,yetmoredynamicthan\ntheclassicalpiecesontheneighboring rightbranchdiscussed above.UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\nRatherthancontinuing todiscusstheindividualunitsweshallnow\ntakealookatthetitlesofaspeciﬁcartistanditsdistributionin\nthishierarchy.Intotal,thereare7titlesbys\u0002\u000b\u000eg._:\r\u0011\r:\u000b¸bv\u000b\u0016_ in\nthiscollection, allviolininterpretations, yetofdistinctly different\nstyle.Hermost“conventional” classical interpretations, suchas\nBrahm’s `'\u0012\u000fh\u0002_\n\\\u0017\u001c\u001av\u0010yg \bðbË\u0010Ag=\u001a\n\\­7´\n¬eLk\n\\\u000b9h\n¬\r\n¯orBach’s \u001b\u000b\n\\~\u001f\u0010y~*\u000bv\u0010Ag\n]r\u0018\u001a\n\\`'\u001a\u0014\t\f\u001as5\u0010\u0013\u001a\u001e\t \u0010AgB­7´\n¬e7k\u000f\u000b\u0014\u0012\u000fh\n¯arelocatedintheclassic-\nclusterintheupperrightcornerbranchontwoneighboring units\nontheleftsideofthesecond-layer map.Thesearedeﬁnitely the\nmost“classical” ofherinterpretations inthegivencollection, yet\nexhibiting strongdynamics. Further3piecesofVanessaMae(\n«hs_`._C\u000b\u001e\r:\u001a\u001egs\r byVivaldi,\u0019_Ci<s5\u0010\u0013\u001a\u001e\t \u0010Aginitssymphonic version,and«_Cø\n[\u0010\u001f\t\f\u000bZbf\u001a\u001c\u0012\u000f\u001d\u000e\u0010AgQoRk$\u0010\n\\i)arefoundintheneighboring branchtothe\nleft,theformertwomappedtogetherwithLj_:\r\u000f~7_\n\\gÆ\n\\_\u0011\u000b\n¬by\n_:nbv\u001a\u001ciR_$\tau\n\\\u000f¬}.Allofthesetitlesareverydynamicviolinpieceswith\nstrongorchestral partsandpercussion.\nWhenwelookfortheremaining 2titlesbyss\u000b\u001eg._:\r\u0011\r:\u000bzbv\u000bR_ ,weﬁnd\nthemontheunitexpanded belowthetoprightcornerunit,thus\nalsoneighboring theclassicalcluster.Onthetop-leftcornerunit\nofthissub-mapweﬁnd \b\n\t\f\u000b\u000e\r\u000f\r\u0011\u0010\u0013\u0012\u0011\u000b\u0014\tNm\u000b\u001e\r ,whichstartsinaclassical,\nsymphonic version,andgradually hasmoreintensivepercussion\nbeingadded,exhibiting aquiteintensebeat.Alsoonthismap,on\ntheone-but-nextunittotheright,weﬁndanotherinterpretation ofthe«\u001a\u001c\u0012\u0011\u0012\u0011\u000b\u001e~*\u000bº\u000b\u001eg=i¢X\n[o\u0016_\u0004\u0010AguÆ bË\u0010Ag'\u001a\n\\byBach,thistimeintheclassical\ninterpretation of ss\u000b\u001eg=_:\r\u000f\r:\u000bºbv\u000bR_ ,alsowithaveryintensebeat.The\nmore“conventional” organinterpretation ofthistitle,aswehave\nseen,islocatedintheclassicclusterdiscussed before.Although\nbotharethesametitles,theinterpretations areverydifferentintheir\nsoundcharacteristic, with ss\u000b\u001eg._:\r\u0011\r:\u000bZbv\u000b\u0016_ ’sinterpretation deﬁnitely\nbeingmorepop-likethanthetypicalclassicalinterpretation ofthis\ntitle.Thus,twoidentical titles,yetplayedindifferentstyles,end\nupintheirrespectivestylisticbranches ofthe `Fa0bc_\u0018\u0015\u000ed system.\nWefurthermore ﬁnd,thatthesystemdoesnotorganizealltitles\nbyasingleartistintothesamebranch,butactuallyassignsthem\naccording totheirsoundcharacteristics, whichmakesitparticularly\nsuitableforlocalizing piecesaccording tooneslikingsindependent\nofthetypicalassignment ofanartisttoanycategory,ortothe\nconventionalassignment oftitlestospeciﬁcgenres.\nInspiteofthesedesiredcharacteristics, however,severalweaknesses\nremain,especially whentitles,thatmaybeverysimilarintermsof\ntheirbeatcharacteristics inthevariousfrequencybands,aremapped\ntogether,yetderivefromverydifferentgenresandareimmediately\nassociated withthosegenres.Thisrefers,forexample,totitleswhere\nthelanguage isaspeciﬁccharacteristic, suchasseveralGerman-\nlanguage songsinourcollection. Furthermore, insomecaseslike\nthepreviously-mentionedL\u0006_:\r\u0011~7_\n\\guÆ\n\\_C\u000b\n¬by\n_:núbv\u001a\u001ciR_$\t*u\n\\:¬},\nwhichismappedtogetherwithtitlesby ss\u000b\u001eg=_:\r\u000f\r:\u000bqbv\u000b\u0016_ ,therhyth-\nmicproperties mightbesimilar,yettheperceivedsoundisstill\ndistinctivelydifferentbecauseofthestrongvocalparts.Evenifthe\nacousticbackground sharessomesimilarities overlongdistances\nofthetitle,therhythmic vocalpartsareperceivedmuchstronger.\nThispointstowardsthenecessity toincorporate additional features\ntobettercapturesoundcharacteristics. Furthermore, insomecases\nliketheseitmightbeadvisable tousethetwo-stageclustering ap-\nproachoutlinedin[26],asforsometitlesthevarianceofsound\ncharacteristics ofsegmentsisratherlarge.Whentakingalookat\nthemapping oftherespectivesegmentsofL\u0006_:\r\u0011~7_\n\\gfÆ\n\\_C\u000b\n¬inan-\notherexperiment weﬁnd3segmentsofittobelocatedinamore\nclassicalsub-branch, whereastheothersegmentsarelocatedinthe\nmoredynamic, aggressivebranches ofthehierarchy.\nFurtherunitsdepictedinmoredetailinFigure8arethebottomright\nunitrepresenting themoreaggressive,dynamictitles.Weleaveitto\nthereadertoanalyzethissub-map andcompare thetitleswiththe\nonesmappedontotheupperleftcornermapinFigure7.6.CONCLUSIONS\nWehavepresented the `FaNbËe»_:gQhs\u000b\u001eg=\u0012\u000f_CiB\u0015\n[\u001dR_\u0018k\u0011\u001a\u001clB­L`5aNbc_\u0018\u0015\u000ed\n¯,a\nsystemforcontent-based organization andvisualization ofmusic\narchives.Givenpiecesofmusicinrawaudioformatahierarchical\norganization iscreatedwheremusicofsimilarsoundcharacteristics\nismappedtogether.Oursystemthusenablesausertobrowse\nthroughthearchive,searching formusicrepresenting aparticular\nstyle,withoutrelyingonmanualgenreclassiﬁcation.\nRhythmpatternsinvariousfrequencybandsareextractedandused\nasadescriptor ofperceivedsoundsimilarity ,incorporating psychoa-\ncousticmodelsduringthefeatureextraction stage.The mNp{`Fa0b\nautomatically identiﬁes theinherentstructureofthemusiccollection\nandoffersanintuitiveinterfaceforgenrebrowsing.Furthermore,\nbymapping apieceofmusicrepresenting a“query”ontothemap\nstructure, theuserispointedtoalocationwithinthemaphierarchy,\nwhereheorshewillﬁndsimilarpiecesofmusic.Weevaluatedour\napproach usingacollection ofabout23hoursofmusicandobtained\nencouraging results.Futureworkwillmainlydealwithimproving\nthefeatureextraction process. Whilethepresented featuresoffer\nasimplebutpowerfulwayofdescribing themusic,additional in-\nformation isrequiredtobettercapturesoundcharacteristics thatgo\nbeyondfrequency-speciﬁc beatpatterns,focusing e.g.onthetim-\nbreandinstrumentation. Furthermore, moreabstractfeaturesare\nnecessary toexplaintheorganization principles totheuser.\nWhilethecurrentevaluationallowsforanintuitiveanalysisofthe\nsystem’sperformance, amoreformalevaluationisdesired.Wethus\nplantoperformauserstudyallowingustoevaluatebothusers’\nexpectations towardssuchasystemaswellastoobtainfeedback on\ntheperceivedqualityofthecurrentapproach.\n7.ACKNOWLEDGMENTS\nPartofthisresearchhasbeencarriedoutintheprojectY99-INF,\nsponsored bytheAustrianFederalMinistryofEducation, Science\nandCulture(BMBWK) intheformofaSTARTResearch Prize.The\nBMBWK alsoprovidesﬁnancialsupporttotheAustrianResearch\nInstituteforArtiﬁcial Intelligence. TheauthorswishtothankSimon\nDixon,MarkusFrühwirth, andWernerGöbelforvaluablediscus-\nsionsandcontributions.\n8.REFERENCES\n[1]D.Bainbridge, C.Nevill-Manning, H.Witten,L.Smith,and\nR.McNab.Towardsadigitallibraryofpopularmusic.In\n\\\u001a\u001c\u0012\u001a»rË~yh\u0002_Bu¢\bFbÁ\b0\u001a\u000eg\u001crc\u001a\u000egðÆ\u0004\u0010to\u001e\u0010A~*\u000b\u0014\t\u001b¹ \u0010\u0013k\n\\\u000b\n\\\u0010\u0013_:\r¤­u¢\bFbjÆ|¹vw P\n¯,\npages161–169, Berkeley,CA,August11-141999.ACM.\n[2]W.Birmingham, R.Dannenber g,G.Wakeﬁeld,M.Bartsch,\nD.Bykowski,D.Mazzoni, C.Meek,M.Mellody,andW.Rand.\nMUSART:Musicretrievalviaauralqueries.In\n\\\u001a\u001c\u0012 \u001a»r~yh\u0002_uJg\u0002g\n[\u000b\u0014\t\u0002`\u0002}\n¬w\"\u001a\u000e\r\u000f\u0010\n[\"¬\u001a\u001egºb\n[\r\u0011\u0010\u0013\u0012\nKg\u0018r\u0018\u001a\n\\\u000f¬\u000b\u001e~\u001f\u0010\u001f\u001a\u001egZ\u0019_:~\n\\\u0010*_:´\u000e\u000b\u0014\t­\nK`\"b\nK\u0019}*$\u0014ª\n¯,Bloomington, ID,October15-172001.\n[3]R.Bladon.Modeling thejudgement ofvowelqualitydif-\nferences. \u0015\u0016\u001a\n[Q\\g=\u000b\u0014\t\u001b\u001aLrº~yh\u0002_u\u0012\u0011\u001a\n[\r\u000f~\u001f\u0010\u001f\u0012\u000f\u000b\u001e\t`'\u001a\u001c\u0012$\u0010*_:~\u001f}c\u001a»ru\n¬_\n\\\u0010\u001f\u0012\u000f\u000b,\n69:1414–1422, 1981.\n[4]J.DanielsandE.Rissland. Findinglegallyrelevantpassages\nincaseopinions.\nKg\u0002~\u001b\b0\u001a\u001eg\u0018r\u0012u\n\\~\u001f\u0010 HØ\u0012$\u0010\u0013\u000b\u001e\t\nKg\u0002~7_$\tA\t \u0010to\u0016_:g'\u0012\u000f_\u0004\u000b\u001eg=i¹4\u000b\u001en ,\npages39–46,1997.\n[5]G.DeBoeck andT.Kohonen,editors. s5\u0010y\r\n[\u000b\u0014\t\n]l:w'\t\f\u001a\n\\\u000b\u001e~\u001f\u0010\u001f\u001a\u001eg\u0002\r\u0010AgqX0\u0010Ag'\u000b\u001eg=\u0012\u000f_ .SpringerVerlag,Berlin,Germany,1998.\n[6]M.Dittenbach, D.Merkl,andA.Rauber.Thegrowinghierar-\nchicalself-organizingmap.In \n\\\u001a\u001c\u0012\u001a»r~yhs_\nKg\u0002~\tw \tF\u0015\u0016\u001a\u001e\u0010Ags~\u0004\b0\u001a\u001eg\u001cr\u001a\u001eg\n_\n[Q\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\r¢­\nK\u0015\u0002\b\n\u000e*$P\n¯,pages15–19,Como,\nItaly,July24-272000.IEEEComputer Society.UsingPsycho-Acoustic Modelsand\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005tocreateaHierarchicalStructuring ofMusic\n[7]M.Dittenbach, A.Rauber,andD.Merkl.Recentadvances\nwiththegrowinghierarchical self-organizing map.In \n\\\u001a\u001c\u0012\u001a»r~yh\u0002_Lq\u001a\n\\\u001d\u000e\r»hs\u001a:wB\u001a\u001eg¤`._$\t r$e\u0011a\n\\oR\u000b\u001eg\u0002\u0010t\u0017$\u0010Ag\u0016ojbv\u000b\u000fw'\r ,Advancesin\nSelf-OrganizingMaps,pages140–145, Lincoln,England, June\n13-152001.Springer.\n[8]B.FeitenandS.Günzel.Automatic indexingofasound\ndatabase usingself-organizing neuralnets. \b0\u001a\n¬w\n[~7_\n\\b\n[e\r\u0011\u0010\u0013\u0012\u0015\u0016\u001a\n[Q\\g=\u000b\u0014\t,18(3):53–65, 1994.\n[9]J.Foote.Anoverviewofaudioinformation retrieval.b\n[\t ~\u001f\u0010\u001fe¬_Ci\u001e\u0010\u0013\u000bz`\u0002}\u000e\r\u0011~7_\n¬\r,7(1):2–10, 1999.\n[10]A.Ghias,J.Logan,D.Chamberlin, andS.B.C.Queryby\nhumming: Musicalinformation retrievalinanaudiodatabase.\nIn \n\\\u001a\u001c\u0012{\u001a»rØ~yh\u0002_\u0018u¢\bFb\nKg\u0002~\tw \t=\b0\u001a\u001eg\u0018r\u001b\u001a\u001egb\n[\t ~\u001f\u0010\n¬_Ci\u001e\u0010\u0013\u000b,pages231–\n236,SanFrancisco, CA,November5-91995.ACM.\n[11]S.Kaski.FastwinnersearchforSOM-based monitoring and\nretrievalofhigh-dimensional data.In\n\\\u001a\u001c\u0012^\u001aLr{~yh\u0002_\nKgs~\tw \tO\b0\u001a\u001eg\u001cr\u001a\u001egu\n\\~\u001f\u0010 HØ\u0012$\u0010\u001f\u000b\u0014\t\n_\n[Q\\\u000b\u001e\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\rË­\nK\bYu\nP\n¯,pages940–\n945.IEE,September ,7.-10.1999.\n[12]T.Kohonen.Self-organizedformation oftopologically correct\nfeaturemaps. d\u0010\u0013\u001a\u0014\t\f\u001a\u0011o\u001e\u0010\u0013\u0012\u0011\u000b\u0014\t\n\b }\u0014k:_\n\\g._:~\u001f\u0010\u001f\u0012$\r,43:59–69, 1982.\n[13]T.Kohonen. `._$\t r$e7\u001a\n\\oR\u000b\u000eg\u0002\u0010t\u0017$\u0010Ag\u0016o\n¬\u000b:w'\r.Springer-Verlag,Berlin,\n1995.\n[14]T.Kohonen.Self-organization ofverylargedocument collec-\ntions:Stateoftheart.In \n\\\u001a\u001c\u0012{\u001a»rØ~yhs_\nKg\u0002~\tw \t.\b0\u001a\u000eg\u001cr\u001a\u001eg\u001eu\n\\~\u001f\u0010 HØ\u0012$\u0010\u0013\u000b\u001e\t_\n[Q\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\r,pages65–74,Skövde,Sweden,1998.\n[15]T.Kohonen,S.Kaski,K.Lagus,J.Salojärvi, J.Honkela,\nV.Paatero,andA.Saarela.Self-organization ofamassivedoc-\numentcollection.\nK]0]N] «s\\\u000b\u001eg\u0002\r:\u000bR\u0012$~\u001f\u0010\u0013\u001a\u000eg\u0002\r\u001a\u001eg\n_\n[Q\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\r,\n11(3):574–585, May2000.\n[16]N.Kosugi,Y.Nishihara, T.Sakata,M.Yamamuro, and\nK.Kushima.Apracticalquery-by-humming systemforalarge\nmusicdatabase. In\n\\\u001a\u001c\u0012\u001a»r\u0004~yh\u0002_u¢\bFb\nKg\u0002~\tw \t0\b0\u001a\u001eg\u0018r\u001a\u001egqb\n[\t ~\u001f\u0010\u001fe¬_Ci\u001e\u0010\u0013\u000b,pages333–342, MarinadelRay,CA,2000.ACM.\n[17]C.LiuandP.Tsai.Content-based retrievalofmp3music\nobjects.In\n\\\u001a\u001c\u0012ú\u001aLrË~yh\u0002_\nKg\u0002~\tw \t¢\b0\u001a\u001eg\u001crf\u001a\u001eg\nKg\u001cr\u0018\u001a\n\\:¬\u000b\u001e~\u001f\u0010\u0013\u001a\u001eg \u000b\u001eg=i¶g=\u001a\u000en\tt_\u0011i\u0018o\u0016_xbv\u000b\u001eg'\u000b\u001co\u0016_\n¬_:gs~|­:\b\nK¶\u0004ba$\u0014ª\n¯,pages506–511,\nAtlanta,Georgia,2001.ACM.\n[18]M.LiuandC.Wan.Astudyofcontent-based classiﬁcation and\nretrievalofaudiodatabase. In \n\\\u001a\u001c\u0012J\u001a»r0~yh\u0002_\nKgs~\tw \t\u001eÆ¢\u000b\u001e~*\u000b\u0014k\u000f\u000b\u000e\r\u0018_\n]g'eo\u001e\u0010Ag._\u0011_\n\\\u0010AgQoË\u000b\u001eg'iuØwRw=\t \u0010\u001f\u0012\u000f\u000b\u000e~\u001f\u0010\u0013\u001a\u001eg\u0002\r^`\u0002}\n¬w\"\u001a\u001e\r\u0011\u0010\n[Q¬­\nKÆ\n]u{`*$\u001eª\n¯,\nGrenoble, France,2001.IEEE.\n[19]D.MerklandA.Rauber.Document classiﬁcation withunsu-\npervisedneuralnetworks.InF.CrestaniandG.Pasi,editors,`=\u001a»r\u000f~N\b0\u001a\n¬w\n[~\u001f\u0010AgQox\u0010Ag\nKg\u0018r\u0018\u001a\n\\\u000f¬\u000b\u001e~\u001f\u0010\u0013\u001a\u000eg\u0019J_:~\n\\\u0010*_:´\u000e\u000b\u0014\t,pages102–121.\nPhysicaVerlag,2000.\n[20]K.Oh,Y.Feng,K.Kaneko,A.Makinouchi, andS.Bae.SOM-\nbasedR*-treeforsimilarity retrieval.In\n\\\u001a\u001c\u0012c\u001a»rj~yh\u0002_\nKg\u0002~\tw \t\b0\u001a\u001eg\u001crº\u001a\u001eg¤Æ¢\u000b\u001e~*\u000b\u0014k\u000f\u000b\u000e\r\u0018_u`s}\u001e\r\u0011~7_\n¬\r{r$\u001a\n\\uJi\u001e´\u000e\u000b\u001eg'\u0012\u000f_\u0011izu·wRw'\t \u0010\u0013\u0012\u0011\u000b\u001e~\u001f\u0010\u0013\u001a\u001egs\r,\npages182–189, Hong-Kong,China,April18-212001.IEEE.\n[21]K.Oh,K.Kaneko,andA.Makinouchi. Imageclassiﬁca-\ntionandretrievalbasedonwavelet-som. In\nKgs~\tw \t=`\u0002}\n¬w\"\u001a\u001e\r\u0011\u0010\n[Q¬\u001a\u001egcÆ¢\u000b\u000e~*\u000bRk\u0011\u000b\u001e\r$_u·w\u0014w=\t \u0010\u0013\u0012\u0011\u000b\u001e~\u001f\u0010\u0013\u001a\u001egs\r\u0010Ag\n\u001a\u001eg\u0002e\n«\"\\\u000bRi\u001e\u0010y~\u001f\u0010\u0013\u001a\u001eg=\u000b\u001e\t\n]g\u0002´\u001c\u0010\n\\\u001a\u001eg'e¬_:gs~\u001f\rx­\u0013Æ\u0012u\n«4]w $\n¯,pages164–167, Kyoto,Japan,Novem-\nber28-301999.IEEE.\n[22]F.PachetandD.Cazaly.Ataxonomy ofmusicalgenres.In\n\\\u001a\u001c\u0012\u0004\u001a»r\u001b~yh\u0002_\nKgs~\tw \t4\b0\u001a\u000eg\u001crJ\u001a\u001egË\b0\u001a\u001eg\u0002~7_:gs~\u0013e\u001fd|\u000b\u000e\r\u0018_Ci¢b\n[\t ~\u001f\u0010\n¬_Ci\u001e\u0010\u0013\u000b\nKg'er\u0018\u001a\n\\\u000f¬\u000b\u001e~\u001f\u0010\u001f\u001a\u001eg uJ\u0012\u0011\u0012\u000f_:\r\u000f\r­\u0013\u0019\nKu¢aC*$$\n¯,Paris,France,2000.[23]E.Pampalk\nK\r\u000f\t\f\u000b\u001eg=i\u001e\rv\u001a»rqb\n[\r\u0011\u0010S\u001d$uJg=\u000b\u0014\t }\u000e\r\u0011\u0010A\r\u0011µ\u0006a\n\\oR\u000b\u001egs\u0010t\u0017\u001c\u000b\u001e~\u001f\u0010\u0013\u001a\u001egsµ\u000b\u001eg'is5\u0010y\r\n[\u000b\u0014\t \u0010\f\u00179\u000b\u000e~\u001f\u0010\u0013\u001a\u001eg¤\u001aLrxb\n[\r\u0011\u0010\u0013\u0012vu\n\\\u0012\u000fhQ\u0010A´\u001e_:\r.Master’sthesis,Vi-\nennaUniversityofTechnology ,2001.\n[24]E.Pampalk,A.Rauber,andD.Merkl.Content-based organi-\nzationandvisualization ofmusicarchives.In \n\\\u001a\u001c\u0012Z\u001a»rvu¢\bFbb\n[\t ~\u001f\u0010\n¬_\u0011i\u000e\u0010\u0013\u000b\u000e*$\t ,Juan-les-Pins, France,December 1-62002.\nACM.\n[25]E.Pampalk,A.Rauber,andD.Merkl.Usingsmoothed data\nhistograms forclustervisualization inself-organizing maps.\nIn\n\\\u001a\u001c\u0012u\u001aLr^~yh\u0002_\nKg\u0002~\tw \t\u001b\b0\u001a\u001eg\u0018r\u001a\u001eg\n_\n[\"\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\rº­\nK\bYu\n\u000e*$\t\n¯,Madrid,Spain,August27-302002.Springer.\n[26]A.RauberandM.Frühwirth. Automatically analyzing and\norganizing musicarchives.In\n\\\u001a\u001c\u0012\u0006\u001aLrZ~yh\u0002_\n]·[Q\\\u001a\u000fws_\u0011\u000b\u000eg \b0\u001a\u001eg\u001cr\u001a\u001egB\u0019_:\r$_\u0011\u000b\n\\\u0012\u000fh \u000b\u000eg=i1uJi\u001e´\u000e\u000b\u000eg=\u0012\u000f_Ci\n«_C\u0012\u000fhQg'\u001a\u0014\t\f\u001a\u0011o\u001e}ºr\u0018\u001a\n\\Æx\u0010to\u001e\u0010y~*\u000b\u0014\tØ¹ \u0010\u001fek\n\\\u000b\n\\\u0010*_:\rx­\n]\bFÆ|¹i*P\u0014ª\n¯,Darmstadt, Germany,Sept.4-82001.\nSpringer.\n[27]B.Ripley.\u001b\u000b\u001e~\u001f~7_\n\\g \u0019_\u0011\u0012\u0011\u001a\u0011o\u001egs\u0010A~\u001f\u0010\u0013\u001a\u001eg \u000b\u001eg'i\n_\n[\"\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\r.\nCambridge UniversityPress,Cambridge, UK,1996.\n[28]J.Rolland, G.Raskinis, andJ.Ganascia. Musicalcontent-\nbasedretrieval:AnoverviewoftheMelodisco vapproach and\nsystem.In \n\\\u001a\u001c\u0012j\u001a»rz~yh\u0002_\"u¢\bFb\nKgs~\tw \t|\b0\u001a\u000eg\u001crº\u001a\u001eg®b\n[\t ~\u001f\u0010\n¬_Ci\u001e\u0010\u0013\u000b,\npages81–84,Orlando, FL,1999.ACM.\n[29]D.RoussinovandH.Chen.Information navigationontheweb\nbyclustering andsummarizing queryresults.\nKg\u001cr$\u001a\n\\:¬\u000b\u001e~\u001f\u0010\u0013\u001a\u001eg\n\\\u001a\u001c\u0012\u000f_:\r\u0011\r\u000f\u0010ygQoj\u000b\u001eg'ibv\u000b\u000eg=\u000b\u001co\u0016_\n¬_:g\u0002~,37:789–816,2001.\n[30]E.Scheirer. b\n[\r\u0011\u0010\u0013\u0012\u0018eA¹ \u0010y\r\u000f~7_:gs\u0010AgQo^`s}\u001e\r\u0011~7_\n¬\r.PhDthesis,MITMe-\ndiaLaboratory ,2000.\n[31]M.Schröder,B.Atal,andJ.Hall.Optimizing digitalspeech\ncodersbyexploiting masking properties ofthehumanear.\u0015\u0016\u001a\n[Q\\g=\u000b\u0014\tØ\u001a»r~yh\u0002_u\u0012\u0011\u001a\n[\r\u000f~\u001f\u0010\u001f\u0012\u000f\u000b\u001e\tN`=\u001a\u001c\u0012$\u0010\u0013_:~\u001f}Ë\u001a»rmu\n¬_\n\\\u0010\u0013\u0012\u0011\u000b,66:1647–\n1652,1979.\n[32]O.Simula,P.Vasara,J.Vesanto,andR.Helminen. Theself-\norganizing mapinindustryanalysis. InL.JainandV.Ve-\nmuri,editors,\nKg=i\n[\r\u0011~\n\\\u0010\u001f\u000b\u0014\tauØwRw=\t \u0010\u0013\u0012\u0011\u000b\u001e~\u001f\u0010\u001f\u001a\u001eg\u0002\r\u001aLr\n_\n[Q\\\u000b\u0014\t\n_:~\u001fnN\u001a\n\\\u001d\u000e\r,\nWashington, DC.,1999.CRCPress.\n[33]G.Tzanetakis, G.Essl,andP.Cook.Automatic musicalgenre\nclassiﬁcation ofaudiosignals.In \n\\\u001a\u001c\u0012\nKgs~\tw \t5`s}\n¬w\"\u001a\u001e\r\u0011\u0010\n[Q¬\u001a\u001egb\n[\r\u0011\u0010\u0013\u0012\nKg\u001cr\u0018\u001a\n\\:¬\u000b\u001e~\u001f\u0010\u0013\u001a\u001egv\u0019_:~\n\\\u0010*_:´\u000e\u000b\u0014\tØ­\nK`\"b\nK\u0019\n¯,Bloomington, In-\ndiana,October15-172001.\n[34]A.UltschandH.Siemon.Kohonen’sself-organizing feature\nmapsforexploratory dataanalysis.In\n\\\u001a\u001c\u0012\u001a»r|~yh\u0002_\nKgs~\tw \t\n_\n[e\\\u000b\u001e\t\n_:~\u001fnN\u001a\n\\\u001dú\b0\u001a\u001eg\u001crº­\nKt\bvw d\n¯,pages305–308, Dordrecht,\nNetherlands, 1990.Kluwer.\n[35]E.Wold,T.Blum,D.Keislar,andJ.Wheaton. Content-based\nclassiﬁcation searchandretrievalofaudio.\nK]0]N]b\n[\t ~\u001f\u0010\n¬_$ei\u001e\u0010\u0013\u000b,3(3):27–36, Fall1996.\n[36]H.ZhangandD.Zhong.Aschemeforvisualfeaturebased\nimageindexing.In\n\\\u001a\u001c\u0012¢\u001aLrJ~yh\u0002_\nK`\u000b\n«Å\u0002`\"\nK]\b0\u001a\u001eg\u001cr{\u001a\u001egq`s~*\u001a\n\\e\u000b\u001co\u0016_x\u000b\u001eg'i\u0019J_:~\n\\\u0010*_:´\u000e\u000b\u0014\t9r$\u001a\n\\\nK¬\u000b\u001co\u0016_\u0004\u000b\u001eg'i1s5\u0010\u001fi\u0016_C\u001aÆ¢\u000b\u001e~*\u000b\u0014k\u000f\u000b\u000e\r\u0018_:\r ,pages\n36–46,SanJose,CA,February 4-101995.\n[37]E.ZwickerandH.Fastl. ·\r\u0011}\u0014\u0012\u000fhs\u001a\u001c\u000bR\u0012\u0011\u001a\n[\r\u0011~\u001f\u0010\u0013\u0012$\r\u0011µNX5\u000bR\u0012$~\u001f\rx\u000b\u001eg=ibv\u001a\u001ci\u0014e_$\t \r,volume22of`._\n\\\u0010*_:\rJ\u001a»r\nKg\u001cr\u0018\u001a\n\\\u000f¬\u000b\u000e~\u001f\u0010\u0013\u001a\u001egZ`=\u0012$\u0010*_:g'\u0012\u000f_:\r .Springer,\nBerlin,2.edition,1999."
    },
    {
        "title": "About this Business of Metadata.",
        "author": [
            "Eric Schreier"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1414742",
        "url": "https://doi.org/10.5281/zenodo.1414742",
        "ee": "https://zenodo.org/records/1414742/files/Schreier02.pdf",
        "abstract": "A brief discussion presents some of the opportunities and chal- lenges involved with creating metadata-centric businesses that bring Music Information Retrieval technologies to the market- place.  In particular, two related difficulties -- that of the difficulty of proving incremental value for new metadata systems, and that of the relative influidity of the marketplace for MIR -- are high- lighted.  Potential directions for resolving these issues are also discussed.",
        "zenodo_id": 1414742,
        "dblp_key": "conf/ismir/Schreier02",
        "keywords": [
            "metadata-centric businesses",
            "Music Information Retrieval technologies",
            "incremental value",
            "marketplace for MIR",
            "proof of incremental value",
            "relative influence",
            "potential directions",
            "challenges involved",
            "opportunities",
            "proving incremental value"
        ],
        "content": "About this Business of Metadata \nAbout this Business of Metadata\nEric D. Scheirer\nBose Corporation\nThe Mountain\nFramingham, MA 01701 USA\n+1 508 766 7157\nEric_Scheirer@bose.com\nABSTRACT  \nA brief discussion presents some of the opportunities and chal-\nlenges involved with creating metadata-centric businesses that bring Music Information Retrieval technologies to the market-place.  In particular, two related difficulties -- that of the difficulty of proving incremental value for new metadata systems, and that of the relative influidity of the marketplace for MIR -- are high-lighted.  Potential directions for resolving these issues are also \ndiscussed. \n \n1. INTRODUCTION \nAs research innovation in musical metadata systems pro ceeds, it is \nnatural to examine the mechanisms by which new results will enter the marketplace.  Researchers in the field must (at least sometimes) be interested in improving the experience of day-to-day users of music systems -- online, in stores, or embedded in consumer electronics.  And some researchers might hope to be-come entrepreneurs, turning their academic interest into new busi-ness opportunities, and possibly even new companies.   \nFor all these reasons, this brie f paper will explore the business \nimplications of music information retrieval systems, and metadata creation and management in partic ular.  Three main sections will \noutline a sketch of the business opportunity, challenge, and poten-tial solutions involved with brin ging MIR systems to the market-\nplace. \n2. OPPORTUNITY AND VALUE \nThe business appeal -- as separate from the academic appeal or scientific motivation -- that must underlie any profit-seeking ven-ture involves value .  That is, in order for a seller to sell something \nto a customer, the customer must perceive that he derives more benefit with the product than without it, and must be willing to pay the seller in order to receive the benefit.  The am ount that the \ncustomer is willing to pay in an open market (considering all fac-tors, such as competition, the alte rnatives to purchasing, etc.) is \nthe definition of the business value of the product. \nThe question of asking “is there business appeal for metadata and \nmetadata systems?” is therefore the same as asking “is a potential customer willing to pay for the perceived benefit of having (and being able to use) that metadata?” \nOwnership of a large collection of metadata itself would not seem \na source of benefit for a typical end-user customer (a consumer).  The home user listening to a stereo system, or the online user searching a database of online records looking for a new CD to purchase, is not interested in metadata per se  but only in the meta-\ndata as a means to an end -- managing the listening experience or finding what she’s looking for more easily.  (Philosophically, one might say that to the extent that  the metadata information becomes \nitself the subject of interest, it ceases to be ‘meta’ and simply be-comes data). \nTherefore, the end-user is not interested in paying for the metadata \nitself, but might be interested in paying for the simplified or im-proved experience that metadata provides.  This implies, in turn, \nthat the proper customer market for metadata and metadata sys-tems is not the end consumer, but the intermediaries -- such as record shops and consumer electronics manufacturer -- that pro-vide these experiences.  Alternatively, the metadata producer could purchase or develop such an experience as a “carrier” for his metadata system and then sell the experience rather than the metadata to the customer. \nWhat is the perceived benefit that metadata can bring in a user \nexperience?  Fortunately -- for those that would be in the metadata business, this benefit is already large and continues to grow.  In \nthe modern world, consumers perceive that they are besieged with content offerings -- dozens or hundreds of terrestrial and satellite music channels, thousands of records available at physical stores, tens of thousands of online radio stations on the Net, and millions of tracks available for download through file-sharing services.   \nThe primary benefit of metadata-based experiences must be in \ntheir offer to help consumers sort through the clamor of alterna-tives that compete for attention -- to choose the book, the movie, the piece of music that is most appropriate for me at this particular \nmoment .  In what has come to be called an “Attention Economy” \n[1], any solution that manages, shapes, or (especially) delivers attention is a source of great potential value. \n3. CHALLENGES \nGiven this set of wide-ranging opportunities, is the marketpl ace \nattractive for solutions providers?   Unfortunately, the answer at \nthe present time is no.  There are two major reasons for this.  This first is that the benefits of incrementally superior  metadata deliv-\nery are, as yet, unproven.  The second is that even with a proven benefit, the market for metadata systems is at present not fluid .  \nWe will consider each of these in turn. \n3.1 Perceived Incremental Benefits \nRecall from the first section that  the source of business value is \nthe perceived incremental benefit of customer access to a g ood or \nservice.  Incremental  in this case refers to the difference between \nhaving access to, for example, a new metadata format, and not having access to it. \nAn important consideration is that in many cases, the alternative to \na prospectively valuable metadata system is not no metadata, but rather is the use of free or commodity metadata.  For example, a provider of track-level musical information is required to demon-strate benefits not just beyond having no metadata in the system, but beyond using free track-level information such as that pro-vided formerly by CDDB and now by FreeDB.org.   \nIt may seem obvious to a researcher that a special, proprietary, \nadvanced metadata format (perhaps based on machine listening technology) can outperform low-quality, commodity Track/Title/Artist listings.  But th e burden of proof  still falls on \nthe seller of the purportedly more-valuable system to prove – quantitatively, whenever possible -- that the incremental  benefit to \nthe customer is worth the selling price. About this Business of Metadata \n3.2 Lack of Fluidity \nEven if the incremental benefit of a particular technology solution \ncan be quantitatively demonstrated, a second obstacle still main-tains.  This is a lack of fluidity  in the marketplace – what an \neconomist might term an inefficiency in the cost of switching. \nConsider the electronic program guide (EPG) that helps a user \nmanage television programming on a digital cable or direct-broadcast satellite system (see Fig. 1).  The EPG presents the show-level metadata that helps the user decide what show to watch, and thereby (at least in the system provider’s judgment) provides an incremental benefit compared to no EPG that custom-ers are willing to pay for in the fo rm of increased service charges. \n \nFigure 1.  An electronic program guide (EPG) helps a TV \nviewer to decide what program to watch via attractive presen-\ntation of program-level metadata. \nNow suppose that an enterprising researcher develops a new \nmetadata-based EPG that is clearly superior – in the sense of pro-viding additional incremental benefit – to the built-in EPG.  It is still not possible to immediately capitalize on this benefit by sell-ing the EPG directly to a television customer, because today’s television and satellite set-top-boxes are not built to handle inter-changeable EPGs.  The EPG implementation is, for technical and/or business reasons, tightly coupled to the set-top box itself. \nInstead, the sales opportunity for the researcher is only to a ca-\nble/satellite operator – either to th e present EPG provider, with the \nargument that the incremental benefit to customers over the cur-rent EPG will justify higher monthly charges, or to a competing provider, with the argument that it will encourage customers to switch services.   \nIn either case, there is a cost that now must be balanced against \nthe benefit proposition provided by the incremental technology advantage. This cost is the cost of switching  to the new EPG sys-\ntem – in the first case, the cost to  the current provider required to \nswitch over the infrastructure of a cable system, and in the second, the cost to each consumer of switching providers in order to ac-quire access to the improved EPG system. \nThus, even in circumstances in which the new technology clearly \nprovides an incremental benefit to customers individually or in aggregate, the increased amount that customers are willing to pay for the new system compared to the old one may still not be enough to justify the switching costs.  \n4. DIRECTIONS TOWARD SOLUTION \nRight now, it is not possible to id entify solutions to these very \ndifficult channels with full confidence.  However, some initial thoughts may provide directions for fruitful work that could help to clear out the logjam in the marketplace. \nFirst, creators of musical metadata and MIR systems that use it \nhave the advantage that there is a great deal of music-related activ-ity on the Internet, predominantly enabled through software appli-cations.  The development and deployment of software implemen-\ntations of MIR must be considered a more efficient method than incorporating new techniques in, for example, consumer electron-ics products such as MP3 players.  \nIn particular, in the area of so-called peer-to-peer (P2P) file ex-\nchange, there is a rapid and fertile marketplace for new software platforms.  Only in the last two years, the leading application has \nshifted twice, from Napster to Morpheus, and today to BearShare and other Gnutella clients.  It is quite likely that a new P2P client program with advanced MIR capability would be an attractive product for one of the small companies that operates in the sp ace.  \nOn the other hand, the problematic legal status of these applica-tions (and/or the file-swapping behavior they enable) in various jurisdictions around the world might give pause to researchers with ties to conservative funding bases. \nSecond, there is some role for standards organizations to play in \nfostering an active and interoperable marketplace for metadata.  MPEG-7 [2] is clearly meant to enable such a marketplace.  How-ever, it seems at this early stage that MPEG-7 will be most useful in the case when the application is fixed, and the underlying meta-data is improving.  From the examples presented at ISMIR [3] and other conferences, it surely is the case that the applications them-selves are improving, not only the quality of the metadata.  Per-haps MPEG-21, with its broader view of systems, platforms, and owners, will be able to make progress on this front. \nStill, it is not always in existing business owners’ interest to sup-\nport an active, standardized marketplace.  Returning to the EPG example, there has been some technical activity in the USA to create a so-called “Open Cable” standard.  This would allow third-party manufacturers to build set-top boxes with advanced func-tionalities, guarantee their interoperation with existing cable ser-vices, and sell them directly to consumers.  But – as supporting this would effectively enable new competitors to cable operators’ locked-in cable-box rental business where none presently exist – it is perhaps unsurprising that the Open Cable initiative is languish-\ning for lack of support. \nThird, not all of the avenues for public dissemination of MIR ac-\ntivity are through strictly competitive businesses.  To give two examples, libraries and the European Union are very interested in projects like CUIDADO [4].  It may be that the public benefits – as assessed by public officials – may be sufficient to warrant in-vestment and deployment of some level of metadata infrastructure as a sort of national (or multinational) capital expenditure.  How-ever, this avenue also seems unlikely to promise rapid transitions and improvement in metadata systems as research developments warrant, given the typical speed of deployment of governmental systems. \n5. CONCLUSION \nThe marketplace for metadata systems is faced with a dilemma that seems unusual (although it is actually rather common in trans-fer-of-innovation scenarios).  This is that, although incremental advances in MIR technology will likely offer  incremental benefits \nto consumers that wish to use digital music systems, it is difficult to find cost justification for deploying  these systems.   \nWhat are the implications for individual researchers and research \nteams?  First, expectations for business interest (including generat-ing funding through licensing activities) must be kept limited for the time being.  In fact, organiza tions that seek to build business \npartnerships should expect to work on these deployment and value-building issues at least as much as on the core MIR research itself. \nSecond, the repeated call [5, Ch. 7] for public databases and About this Business of Metadata \nevaluation scenarios must be taken more seriously.  As long as \nMIR systems are evaluated in isolation, not really in a direct com-parison with each other, there is no way for a potential customer to understand the incremental benefits of one approach versus another. \nFinally, the variety of deployment scenarios (cell phones, Web \nsites, set-top boxes, consumer electronics, MP3 players, etc) must be rigorously evaluated separately.  While some commentators [6] are fond of speaking of the “world of digital music”, in reality, each individual technology area presents its own requirements and challenges.  Research that examined the coupling between under-lying MIR technology, user interf ace, and salable g oods (for ex-\nample, cell phone services with reasonable pricing and supply-chain models) would be welcome in this regard.\n 6. REFERENCES \n[1] Davenport, T., and Beck, S.  The Attention Economy: Under-standing the New Currency of Business.  Cambridge, MA: Harvard Business School Press, 2001. \n[2] Quackenbush, S., Schuyler, S., and Lindsay, A.T. Overview of MPEG-7 audio.  IEEE Tran sactions on Circuits and Sys-\ntems for Video Technology 11, in press. \n[3] ISMIR 2002 Web site <URL:http://ismir2002.ircam.fr/> \n[4] CUIDADO Web site <URL:http://www.cuidado.mu> \n[5] Scheirer, E. Music-Listening Sy stems.  Ph.D. Dissertation, \nMIT Media Laboratory, 2000. \n[6] Scheirer, E. “The Real Digital Music Revolution”. <URL:http://www.mp3.com/news/ 336.html>."
    },
    {
        "title": "Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System.",
        "author": [
            "Jungmin Song",
            "So-Young Bae",
            "Kyoungro Yoon"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1418309",
        "url": "https://doi.org/10.5281/zenodo.1418309",
        "ee": "https://zenodo.org/records/1418309/files/SongBY02.pdf",
        "abstract": "Recently a great attention is paid to content-based multimedia retrieval that enables users to find and locate audio-visual materials according to the intrinsic characteristics of the target. Query-by-humming (QBH) is also an application that makes retrieval based on major characteristics of music, that is, \"melody\". There have been some researches on QBH system, most of which are to retrieve music from symbolic music data by humming query. However, when the usability of technology is taken into consideration, retrieval of music in the form of polyphonic raw audio would be more useful and needed in the applications such as internet music search or music juke box, where the music data is stored not in symbolic form but in raw digital audio signal because such music data is more natural format for consumption. Our focus is on the realization of query-by-humming technology for an easy-to-use application, which entails full automation of all the processes of the system, including melody information extraction from polyphonic raw audio. In our system, melody feature of music database and humming is not represented by distinct note information but by the probability of note occurrence. Similarity is then measured between the melody features of humming and music data using DP matching method. This paper presents developed algorithms and experimental results for key steps of QBH system including the melody feature extraction method from polyphonic audio and humming, their representation for matching, and matching method between represented melody information from polyphonic audio and humming.",
        "zenodo_id": 1418309,
        "dblp_key": "conf/ismir/SongBY02",
        "keywords": [
            "content-based multimedia retrieval",
            "query-by-humming (QBH)",
            "music retrieval",
            "polyphonic raw audio",
            "melody feature extraction",
            "DP matching method",
            "melody information extraction",
            "humming representation",
            "system automation",
            "experimental results"
        ],
        "content": "Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n Mid-Level Music Melody Representation of Polyphonic \nAudio for Query-by-Humming System \nJungmin Song \nLG Electronics \n16 Woomyeon-Dong, Seocho-Gu \nSeoul, Korea \n+82-2-526-4111 \njmsong73@lge.com So Young Bae \nLG Electronics \n16 Woomyeon-Dong, Seocho-Gu \nSeoul, Korea \n+82-2-526-4111 \nsybae@lge.com Kyoungro Yoon \nLG Electronics \n16 Woomyeon-Dong, Seocho-Gu \nSeoul, Korea \n+82-2-526-4133 \nyoonk@lge.com \nABSTRACT  \nRecently a great attention is paid to content-based multimedia \nretrieval that enables users to find and locate audio-visual materials according to the intrinsic characteristics of the target. Query-by-humming (QBH) is also an application that makes retrieval based on major characteristics of music, that is, \"melody\". There have been some researches on QBH system, most of which are to retrieve music from symbolic music data by humming query. However, when the usability of technology is taken into consideration, retrieval of music in the form of polyphonic raw audio would be more useful and needed in the applications such as internet music search or music juke box, where the music data is stored not in symbolic form but in raw digital audio signal because such music data is more natural format for consumption. Our focus is on the realization of query-by-humming technology for an easy-to-use application, which entails full automation of all the processes of the system, including melody information extraction from polyphonic raw audio. In our system, melody feature of music database and humming is not represented by distinct note information but by the probability of note occurrence. Similarity is then measured between the melody features of humming and music data using DP matching method. This paper presents developed algorithms and experimental results for key steps of QBH system including the melody feature extraction method from polyphonic audio and humming, their representation for matching, and matching method between represented melody information from polyphonic audio and humming.\n \n1. INTRODUCTION \nMusic information retrieval is a research area of developing \ntechnologies, which helps retrieving music data to accomplish the \nprocess of production, acquisition, transmission, and consumption of music in easy and comfortable way. Query-by-humming (QBH) is one of the music retrieval methods, which finds music that contains similar or same melody to the humming query. Humming a tune is very natural and familiar habit of human and humming as a query for retrieval would be also very natural and convenient way for finding music of interest among a group of music. Recognizing this fact, several technologies have been developed to accomplish this function, and they reported quite promising results in terms of their retrieval accuracy and speed. But most of those technologies are targeted to match humming queries to symbolic music [1][2]. The assumption made by these works is that pitch information or melody information has been extracted from music or target music is in symbolic format (i.e. MIDI files). Approximate matching method is the major choice of these systems, allowing errors or partial variations of the humming query. Allowing errors or partial variations in QBH system is very \nimportant as most of the humming by users have errors intrinsically [3][4]. \nHowever, the natural music format for most of the music \nconsumers is not a symbolic music, but polyphonic audio that can be obtained from CD’s or decoded MPEG audio files. If we want to include a song available through CD or MPEG compressed files in the symbol-based QBH system, the song should be represented in symbols used in the selected QBH system. There are two possibilities in converting raw polyphonic audio into symbol representation. One is the manual conversion, which requires tremendous and tedious human work of extracting melody information. The other is using automatic conversion engine, but it is well known that melody extraction from polyphonic raw audio is a hard-to-solve problem. Though some researchers are trying to solve this problem using various techniques [5][6][7], it seems that they cannot guarantee the accuracy of the melody \nextracted from the generic music signal with large number of sound mixture, and so does the performance of the QBH system utilizing the approximate symbol matching method based on the extracted melody information. \nThe QBH system proposed in this paper starts from an effort to \navoid the hard-to-solve problem of extracting exact pitch information or melody information from polyphonic audio in signal level, which is also known as automatic transcription problem. In the proposed system, melody feature information is extracted at mid-level, which is conceptually lower than the symbolic representation of melody, but higher than the raw audio signal. To be specific, the melody information is represented by the sequence of vectors whose element describes the estimated strength of note in audio signal, expressing probability of each note being a melody element. Main focuses of this approach are how to extract and represent melody information with the level of accuracy to enable the matching of humming query and the music data, from polyphonic audio, and how to measure the similarity between melody representations. \nThe melody information extraction and representation method and \nmatching method for matching music and humming query in the proposed mid-level representation are presented in this paper. Experimental results and some discussion on this approach are also presented in the last part of the paper. \n2. SYSTEM OVERVIEW \nQuery by humming system is composed of three main modules, which are music melody feature extraction module, humming melody feature extraction module and similarity measuring module as shown in Figure 1. The music melody feature extraction module extracts melody information from music and stores extracted melody information in the melody feature database. Music melody feature is represented as a sequence of vectors, each element of which indicates the probability or the strength of note occurrence of audio frame or audio segment. The \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM – Centre Pompidou  Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n humming melody feature extraction module extracts melody \nfeature from humming acquired via stored humming files or microphone. The extracted melody feature is also represented by a sequence of vectors. The music melody extraction module and the humming melody extraction module also contain the process of note segmentation. The note segmentation process groups contiguous audio frames which seem to have a same note. Finally, the similarity measuring module compares music melody feature in the feature database with the humming melody feature and outputs the matching result in the order of their similarity. When measuring the similarity between representations of music melody and humming melody, disparities of overall length and local variances between the music melody in the database and the humming melody are considered. \nAdditional modules, which are not included in this paper, such as \nmusic filtering module that filters out those music with low possibility of being the right answer based on the features other than melody, and melody part detection module which separates and locates melody parts, can be added for completeness of the system and for the better performance in accuracy and speed. \n3. MELODY FEATURE EXTRACTION \nFor matching polyphonic audio and humming based on their melody information, we propose to describe such information in a mid-level, in which melody information of music and humming are represented as a set of possible notes of audio frame or audio segment, not as a definite musical note of audio frame or audio segment. Those note candidates of a frame or a segment are selected to be the most audible notes from the sound sources among the notes that are simultaneously generated from multiple sound sources. Audio frames that are believed to have same notes are concatenated to be an audio segment and the note candidates are updated based on the concatenated frames. This process can be divided into five parts as shown in Figure 2.  \n3.1 Harmonic Enhancement \nOrdinary music is composed of several sounds from various sound sources, such as vocal, piano, guitar, percussion, and so on, and the source separation and musical note recognition from music melody feature \nextraction module humming melody feature \nextraction module music data humming data\nmelody feature \ndatabase \nsimilarity measuring \nmodule \nretrieval result \nFigure 1. Overview of QBH system \nFigure 2. Melody feature extraction process audio frame\nnote segment sequence construction note segmentation note strength calculation harmonic sum harmonic enhancement \nmelody feature \nFigure 3.  Spectrogram of music clip \nFigure 4. Enhanced harmonic of music clip Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n polyphonic raw music are not easy tasks to achieve. But when \nmusic retrieval by humming query is the target application, the problem of precise source separation and automatic transcription from polyphonic audio can be avoided, b ecause only the parts and \nsounds of the music, which are easy for human to recognize and memorize, can be hummed. In other words, only those melodies composed of consecutive sounds that are most audible and predominant than other sounds can be candidates for human to memorize and hum. \nHarmonic enhancement process, which is the first step of the \nmelody feature extraction process, extracts those predominant pitches. Musical sounds are constructed by harmonics, which are composed of partials, their positions in frequency and amplitudes, to be specific. Most audible pitch is one that exhibits their clear harmonic structure and has larger amplitude. When the sounds are mixed together, such harmonic structure and amplitude can be impaired by other harmonics. For example, when some harmonics from two sound sources are adj acent in frequency axis, partials of \none harmonic can be masked by partials of other harmonics. Consequently, the sound that contains masked partials is less audible than the sound that contains masking partials. In other words, due to the frequency masking effect, a signal with large amplitude tends to perceptually mask other nearby signal in frequency domain. So the sounds that include harmonics of large amplitude have more probability to be the predominant sound. However, even though the amplitudes of harmonics are large, if their surrounding signals also have large amplitudes, those harmonics are less possible to contribute to the predominant sound. For example, if the first harmonic (fundamental frequency) of a sound has large amplitude, but it is superimposed in the band where the noise-like percussion sound is spanning, the predominance of the sound cannot rely on the large amplitude of the first harmonic. Harmonic enhancement process extracts such harmonics that have outstanding peaks compared to the surroundings and the enhanced harmonics can be represented as \nthe equation (1). \n()∑\n−=<≤+− =W\nWit tEP\nt Nk ikEkEA k E 0,) ( )( )( - (1) \nwhere () 0 ,≥∀= xxxA and () 0 ,0<∀= x xA  \nIn equation (1), N is the FFT index range, EtEP(k) represents the \ndegree of the predominance of the harmonic in the frequency index k, considering the spectral amplitudes E\nt(k) of surrounding \nsignals within the frequency range W in time t. EtEP(k) is large \nwhen the spectral amplitude of the harmonic in the frequency index k is a peak in the spectrum and it becomes much larger \nwhen the spectral amplitude of given index k is large compared to \nits surroundings and when there are no adj acent peaks within the \ngiven window size W. Figure 3 and 4 shows spectrogram of a \nmusic clip and enhanced harmonics  respectively. From these two figures, one can easily find that small peaks and wide band signals are eliminated and large and prominent peaks are emphasized. Figure 5 and 6 illustrates this phenomenon more clearly by showing spectrum and enhanced harmonics at a specific time. \n3.2 Harmonic Sum \nThe most important ingredient in recognizing musical sound is the harmonicity of the sound. The singing voice of human and the playing sounds of musical instruments show periodic peaks in the frequency axis according to the characteristics of each s ound \nsource. It is said that the recognition of the sound is the process of perceiving how much those partials have harmonic characteristics [8]. Pitch extraction using harmonic sum based on above properties has been reported as successful and is adopted as the second step of our melody extraction process. In our algorithm, Figure 5. Spectrogram of music clip at a specific time \n0246810\n0 100 200 300 400 500\nFFT indexamplitude\nFigure 6. Enhanced harmonic of music clip at a specific time 0102030405060\n0 100 200 300 400 500\nFFT indexamplitudeMid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n the pitch information is extracted using the harmonic sum of the \nenhanced harmonics as shown in the equation (2). \n\n∑\n==pN\nmEP\nt t mp EpNpF/\n1)(/1)(  - (2) \nIn equation (2), x is an integer which is not bigger than x and \nFt(p) is the average strength of harmonics having the fundamental \nfrequency p in the audio frame at time t. Above equation shows \nthat Ft(p) is decided by averaging the degree of predominance \nvalue obtained by harmonic enhancement process in equi-distanced frequency indexes. If there is predominant harmonics of fundamental frequency \np, the value Ft(p) becomes large and \nrepresents the possibility that the sound of fundamental frequency \np would occurs is large. \n3.3 Note Strength Calculation \nWhen a human sings musical notes according to the musical score, it happens that fundamental frequency of the note sung varies slightly, but he/she feels like singing the same note and others cannot distinguish such slight variations. It is also true for playing any musical instrument. Even though there are variations in fundamental frequency and a person can feel them, it is not difficult for people to feel that those variations are within a range of a note and to regard those as same note (like vibrato). In addition to such a psycho-acoustical reason, dimension of the frequency index generated by Fourier transform need to be minimized to speed up the matching by reducing the feature dimension.  \nBased on above phenomenon and fact, we quantize frequencies \ninto the frequency bands according to the frequencies of musical notes represented in 12-note scale (12 notes per octave). Strength of the fundamental frequency in frequency index acquired by the equation (2) is converted to one in musical note index as shown in equation (3) in the third step. \n1 0,)(\n)( −≤≤−=∫MmL UdppF\nmNS\nm mU\nLt\ntm\nm -(3) \nIn equation (3), m is a musical note index and Um and Lm are \nfrequencies of the upper and lower limit for a musical note indexed as \nm, and M is the total number of musical notes in the \nmusical scale. In our approach 512 frequency index is converted to 108 note index (12 notes per octave, 9 octaves). According to the relationship of the note index to frequency [9], frequencies are converted to note index ranging from C\n0 to B 8. For example, C 0 corresponds to 16.352Hz and B 8 to 7902.1Hz. Frequencies of the \nupper limit Um and the lower limit Lm for a note index can be \ncalculated by following equation. \n( )\n() β ββ β\n×=×=×=×=\n−−−\n+\n111\n1\nm m mm m m\nN N LN N U\n - (4) \nα×=+ m m N N1 \n242log\n122log\n10 , 10 = = β α  \nIn equation (4), Nm is the frequency of note index m. αand \nβare the frequency ratio of neighboring note index and the \nfrequency ratio of a note and its boundary, respectively. \n3.4 Note Segmentation  \nConsecutive audio frames that show the similar pitch \ncharacteristics are grouped into an audio segment in the fourth step. It is known that there is a transient period to reach a specific note when human voices or instrumental sounds occur. The transient period is called onset period, and the sound energy in the period is very low. As we are considering and investigating the prominent sound, we also make use of the measure of prominent sound, that is, the enhanced harmonic data of section 3.1, in the process of note segmentation based on the characteristics of onset period. The segment boundaries are selected as the points that show the local minimum energy of the enhanced harmonics as defined in the equation (5).  \n∑−\n==< =\n1\n02)(1)(} ))( min()),((min|{\nN\nktt\nk EPNtFETH tFE tFE t SB\n - (5) \nFE(t)  is the frame energy of enhanced harmonics at time t and TH \nis the threshold value used to avoid selecting too many local minima. Additionally, when the frame energy of the enhanced harmonics shows very small value over a specific duration, that group of frames is classified as silent segment. A segment between two silent segments is merged with the silent segments constructing a single silent segment, if it is shorter than a certain threshold. Figure 7 shows fluctuation of frame energy of enhanced harmonics and segmentation results. Figure shows six segments (A, B, C, D, E, F) produced by the note segmentation process and segment “E” is classified as a silent segment. 1800230028003300380043004800\n120 140 160 180 200 220 240 260 280 300\nframe numberframe energyA B C D E F \nFigure 7. Frame energy of enhanced harmonics Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n 3.5 Construction of Note Segment Sequence \nEach audio segment is composed of several consecutive audio \nframes and pitch information of the segment is constructed by averaging each com ponent of the note strength vector as shown in \nequation (6). \n1 0,)(1)( −≤≤ =∑\n=Mm mNSCmSe\nsl\nltt l - (6) \nIn equation (6), Sl is the note strength vector of a segment, C is the \nnumber of audio frames included in the segment, and ls and le are \nthe start audio frame and the end audio frame of the note segment respectively, which are obtained by equation (5). \nFinally, note candidates are selected by picking several peaks that \nhave large note strength. In our experiments, 7 peaks from the music segment, and 3 peaks from the humming segment are selected. Figure 8 shows the example of the note strength for a segment. Selected peaks are the candidate pitch of a segment and peak values represent the possibility of each peak. The pitch information with their possibility value for consecutive segments constructs the sequence of vectors and it is used as the representation of melody information. \n3.6 Storing Melody Feature in Feature \nDatabase \nExtracted melody feature of music is stored in feature database for \nretrieval process. Melody feature is represented by a sequence of vectors containing note information. Not all the elements of the vector have strength value, because only the note candidates are selected in the process of note segment sequence construction. Only those elements that have note strength information of the note strength vector are saved in database. Hence, a sequence of segment information including pairs of note index and its strength and segment ID describing each segment and a header that \ncontains additional information used in the process of retrieval (number of segments, number of note candidates per segment, music title, etc) are stored for each music clip in the music database, as depicted in figure 9. \nIn figure 9, Seg\nP stands for segment ID of pth segment, and NI x, \nand Str x means the note index and its strength of xth note \ncandidate of the given segment, respectively. L and P in the figure means the number of note candidates in a segment and the number of segments in the music clip respectively.  \n4. MATCHING \nHumming query can have disparities with music in their length. Also, erroneous notes can be inserted and some notes can be even omitted. Furthermore, erroneous note and segment information can also be extracted during melody feature extraction process. \nTo overcome such erroneous environment, DP matching method \nis used to match two patterns of different length while permitting partial variations and errors [10]. Because there also exist overall biases in pitch between music and humming, DP array is generated using note strength vector while shifting the vector index. \n4.1 Dissimilarity Calculation using DP \nMatching \nLet sequences of music segments and humming segments, which \nare constructed by the vector containing the information of note, 00.511.522.5\n41 46 51 56 61 66 71 76 81 86\nnote indexnote strength\nFigure 8. Note strength of a segment \nheader info.\nSeg 1 (NI 1, Str 1) (NI 2, Str 2) (NI L, Str L)\nSeg 2 (NI 1, Str 1) (NI 2, Str 2) (NI L, Str L)\nSeg P (NI 1, Str 1) (NI 2, Str 2) (NI L, Str L)\nFigure 9. Data format for storing feature data Figure 10. Matching path on DP array 0 N Q-1 NR-1\nc1c2c3c4cH\nchMid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n be R and Q, \n] ,,,,,,[] ,,,,,,[\n1 2 1 01 210\n−−\n==\nNQ jNR i\nq q qqq Qr r rrr R\nKKKK\n \nwhere ri is the note strength vector of ith segment of music and qj \nis the note strength vector of jth segment of humming. NR and NQ \nare the number of segments in the music and humming respectively. \nTo match two sequences \nR and Q, a matrix D of size NR × NQ is \nconstructed. An element of the matrix, dps(ri, q i) as given in \nequation (7), represents the dissimilarity between ri and qj when \nthe overall pitch shift is ps. \n[]\n1 , 0,\n) ( )() ( )(\n),(\n1\n021\n021\n02\n−≤−≤\n−−−\n=\n∑∑∑\n−\n=−\n=−\n=Mpsmm\npsmq mrpsmq mr\nqrd\nM\nmjM\nmiM\nmj i\nj i ps - (7) \nThe matching path Cps in case of pitch shift ps is defined as a set \nof consecutive vector elements dps(ri, q i) which decides the \nmatching between R and Q (figure 10). The hth element of \nmatching path Cps is defined as cps,h=(i, j)  and the matching path \ncan be represented as the following equation assuming that the length of the matching path is \nH. \n1 ) , max(, ,, , ,, , 3, 2, 1,\n++<≤=\nNQ NR H NQNRc c c c c CHps hps ps ps ps ps KK - (8) \nAfter generating DP arrays for various pitch shift values, matching \npath can be selected for the matching cost to be minimized for each pitch shift values. \n()\n\n\n=∑=\npsH\nh hps ps\nps psHcd\nMIN C DPps\n1 ,\nmin,) (  - (9) \nFinal dissimilarity value between R and Q is calculated by \nselecting matching cost of a pitch shift that has minimal matching cost among several pitch shift values as shown in Equation (9). In \nequation (9), ) (min,ps psC DP  is the selected matching cost and \npsH  is the length of matching path with pitch shift ps. \n4.2 Windowing \nTo reduce the matching time by avoiding invalid matching path \nthat is too far from the ideal diagonal matching path, we used windowing method on DP arrays. It is depicted in figure 10 as dotted lines. When the width between upper and lower dotted line is narrow, the matching time is reduced, but the degree of allowed variations between music and humming is lowered, hence the matching becomes even strict. \n4.3 Additions to the Conventional DP \nMatching \nWe added the measure that reflects the amount of how far the \nmatching path is from the diagonal path, and it give more emphasis on matching paths generated along with the diagonal line in DP array. This is calculated by the following equation. \nNQ NRHQRfps\nps +=),(  \nwhere Hps is the length of the matching path. This measure is applied to the matching cost DP ps as a \nnormalization factor for calculating dissimilarity value between R \nand Q in the form of addition or multiplication. \n5. EXPERIMENTATION \n5.1 Experiment Configuration \nExperiment was executed at the music database that is composed \nof 92 melody clips. Each melody clip contains melody part of music and is about 15~20 seconds long. These clips are selected from Korean and Western popular songs. 176 humming samples from 7 males and 3 females are gathered using microphone. \nAll the data is stored in PCM format with mono, 16kHz sampling \nrate and 8 bits/sample resolution. The experiments are performed on a system with Windows 2000 O/S and Pentium IV 1.5GHz CPU. \n5.2 Experimental Results \nExperiment was done according to the various c onditions, such as \nthe size of neighbors in partial enhancing process (W=4, W=8) and usage of \nf measure which measures how far the matching path \nis from the diagonal path (NNF: no use, NNF1: multiplication, NNF2: addition) as the following table. \nTable 1. Configuration of experiment \n NNF NF1 NF2 \nW=8 M01 M02 M03 \nW=4 M04 M05 M06 \nTable 2 shows the retrieval performance. Top n means the rate of \nqueries that retrieves correct music within top n rank. For example, \nwhen the experiment configuration is M03 (W=8, use of f measure \nwith addition to DP distance value), we can say that the number of cases of retrieving correct music as top 1 is more than 4 times from 10 trial and the number of cases of retrieving within top 10 is over 7 times. In general, result shows that when the size of neighboring frequency index to be considered (\nW) in the process \nof harmonic enhancing is 4, the retrieval accuracy is better than the case of \nW=8, and the usage of measure that measures how far \nthe matching path is from the diagonal path (i.e., f measure) is \nhelpful. \nTable 2. Retrieval accuracy according to various experimental \nconfigurations defined in table 1. \n Top 1 Top 3 Top 5 Top 10 \nM01 11.93 23.30 34.66 50.56 \nM02 43.75 52.84 57.95 67.05 \nM03 43.18 56.82 64.77 76.14 \nM04 30.11 49.43 60.23 68.75 \nM05 40.91 60.80 67.05 76.13 \nM06 37.50 58.52 65.91 75.57 \nWhen windowing on DP arrays is used, the matching time was \ngreatly reduced and still preserves the retrieval rate. The size of window was decided proportional to the length of music clip and humming. Following is the retrieval rate and the matching time according to the various wi ndow sizes. It shows that the matching \ntime is reduced while the retrieval rate is not noticeably impaired. \nTable 3. Experimental result of various window size on \nmatching path and corresponding matching time \nM3 Matching \ntime (sec) Top 1 Top 3 Top 5 Top 10 \nW0 14.15 43.18 56.82 64.77 76.14 \nW1 10.64 43.18 56.82 64.77 76.14 \nW2 6.95 43.18 56.82 64.77 76.14 \nW3 4.42 43.75 56.82 65.91 75.00 \nW4 2.89 42.61 54.55 61.93 69.31 Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System \n W0, W1, W2, W3 and W4 are specification of size of window \nsuch as no windowing, (NR+NQ)/2, (NR+NQ)/4, (NR+NQ)/8, and (NR+NQ)/16, respectively.  \n6. CONCLUSION \nThis paper presented the method of matching humming query with polyphonic music data. Focus on this paper is to extract melody information from polyphonic music and to match the melody information in the mid-level representation. For this purpose, the melody feature extraction module contains the process of harmonic enhancement that emphasizes the most audible sound from sound mixture, the harmonic sum that calculates pitch candidates and their possibility of occurrence, the note strength calculation that converts amplitudes in frequency index to those in musical note index, the note segmentation that groups adj acent \nframes and extracts note duration according to the frame energy of the enhanced harmonics, and the note segment sequence construction that extracts note information of the segment and constructs melody feature. Melody feature extracted from music clip have couples of note candidates and feature from humming may have erroneous notes that are slightly different from right answer. To overcome this fact, DP matching method is adopted. Experimental results show the possibility of matching humming with music in the mid-level representation within proper matching time. We can avoid exact transcription problem by representing melody information as a sequence of vectors that contain the information of pitch and its possibility of occurrence. The method proposed in this paper is only performed on a clip-based database and DP matching of a humming with a complete clip is performed. To overcome the given constraint in a large database of complete songs, additional processes may be adopted. First, beat/tempo analysis with database indexing such as R*-tree would make it possible to filter out many songs from the candidates so as to reduce matching time. Second, detection of important melody parts, such as beginning melody part and repetitive part of a song, and approximate subsequence matching of the detected melody parts can be applied to matching of longer songs. Further researches will be conducted to perform matches on a database that contains features extracted from complete songs based on the technique proposed in this paper. \n 7. REFERENCES \n[1] H.Y. Tseng, \"Content-based Retrieval for Music \nCollections,\" Proc. of 4th ACM conference on Digital Libraries, pp.176~182, California, 1999.  \n[2] Y. Kim, W. Chai, R. Garcia, B. Vercoe, \"Analysis of a Contour-Based Representation for Melody,\" Proc. International Symposium on Music Information Retrieval, Oct. 2000. \n[3] C.-C. Liu, J.-L. Hsu, A.L.P. Chen, \"An Approximate String Matching Algorithm for Content-based Music Data Retrieval,\" IEEE Int. Conf. on Multimedia Computing and Systems, vol.1, pp.451~456, 1999. \n[4] R.J. McNab, L. Smith, I.H. Witten, C.L. Henderson, \"Tune Retrieval in the Multimedia Library,\" Multimedia Tools and Applications, vol.10, pp.113~132, 2000. \n[5] A. Klapuri, “Multipitch Estimation and Sound Separation by the Spectral Smoothness Principle,” IEEE International Conference on Acoustics, Speech and Signal Processing, vol.5, pp.3381~3384, 2001. \n[6] T. Miwa, Y. Tadokoro and T. Saito, “Musical Pitch Estimation and Discrimination of Musical Instruments Using Comb Filters for Transcription,” 42nd Midwest Symposium on Circuits and Systems, vol.1, pp.105~108, 2000. \n[7] M. Goto, “A Predominant-F0 Estimation Method for CD Recordings: MAP Estimation Using EM Algorithm for Adaptive Tone Models,” IEEE International Conference on Acoustics, Speech and Signal Processing, vol.5, pp.3365~3368, 2001. \n[8] A. Klapuri, \"Pitch Estimation Using Multiple Independent Time-Frequency Windows,\" IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp.115~118, 1999. \n[9] Thomas D. Rossing, The Science of Sound , Addison-Wesley \nPublishing Company, 1990. \n[10] L.Rabiner, B. –H. Juang, Fundamentals of Sp eech \nRecognition,  Prentice-Hall, 1993."
    },
    {
        "title": "Mobile Melody Recognition System with Voice-Only User Interface.",
        "author": [
            "Timo Sorsa",
            "Katriina Halonen"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415866",
        "url": "https://doi.org/10.5281/zenodo.1415866",
        "ee": "https://zenodo.org/records/1415866/files/SorsaH02.pdf",
        "abstract": "A melody recognition system with a voice-only user interface is presented in this paper. By integrating speech recognition and melody recognition technology we have built an end-to-end melody retrieval system that allows a users to do voice controlled melodic queries and melody generation using a dial-in service with a mobile phone.",
        "zenodo_id": 1415866,
        "dblp_key": "conf/ismir/SorsaH02",
        "keywords": [
            "Mobile melody recognition",
            "Voice-only user interface",
            "Query-by-humming",
            "Melody transcription",
            "Speech recognition",
            "Database engine",
            "Melody generation",
            "VoiceXML",
            "User evaluation",
            "Retrieval accuracy"
        ],
        "content": "Mobile Melody Recognition System with Voice-Only User Interface\nMobile Melody Recognition System with Voice-Only User\nInterface\nTimo Sorsa\nNokia Research Center\nP.O.Box 407, FIN-00180 Helsinki, Nokia Group,\nFinland\ntimo.sorsa@nokia.comKatriina Halonen\nNokia Research Center\nP.O.Box 100, FIN-33720 Tampere, Nokia Group,\nFinland\nkatriina.halonen@nokia.com\nABSTRACT\nA melody recognition system with a voice-only user interface is\npresented in this paper. By integrating speech recognition andmelody recognition technology we have built an end-to-endmelody retrieval system that allows a users to do voice controlledmelodic queries and melody generation using a dial-in servicewith a mobile phone.\n1. INTRODUCTION\nAs a result of new, widely accepted music storage and transfer\nformats, the amount of musical information accessible by users\nhas risen rapidly during the past years. Content-based, automatedtools for data management and retrieval have become an attractivegoal from the research as well as from the commercial point ofview. Indications of this increasing interest are, for example,standardization efforts such as MPEG-7 [1].\nOne approach for content-based data management in music\ndomain is query-by-humming type of solutions. A number ofquery-by-humming applications have been presented during therecent years (see for example [2]-[5]). All of the pr oposed\napplications are web or PC based implementations.\nRecently, voice markup languages have appeared that enable\ndevelopers to build web-based speech applications [6]. Userscontrol the dialogue with speech commands, which areautomatically recognized at the system end, and speech synthesisis used to provide the user with instructions and prompts. Speechtelephony technology is used to develop applications into dial-inend-to-end services, accessible by a mobile or fixed phone.\nIn this paper we consider one possible use of query-by-humming\ntechnology in commercial mobile applications.\nThe paper is organized as follows. First we present an overview of\nthe implemented melody recognition service. Then, in Section 3we report some user evaluation results. We end with Section 4discussing some conclusions and considerations.\n2. SYSTEM OVERVIEW\nThe implemented query-by-humming system is intended for\nmobile melody retrieval and generation based on user givenacoustic inputs.\nThe system is controlled with voice commands given by the user.\nA voice user interface (UI) is well suited for this type ofapplications since the melody input is most often given by voiceas well. The voice UI also enables the usage of the service insituations where a keypad is hard to use.2.1 System architecture\nA general overview of the mobile melody retrieval system is\npresented in Figure 1. The main functional blocks are the voice-only UI, melody transcription module and the database engine.\nVoice-\nonly \nUIMelody \nTranscriptionDatabase \nEngine\nMIDI\ndatabaseau irp\nReportingDatabase \nimage\nFigure 1. General overview of the melody recognition system\nwith a voice-only user interface.\n2.1.1 Melody Transcription\nThe melody transcription engine used in this system is the same as\nreported in [7]. Some parameters, such as analysis thresholds,have been tuned in order to get better average performance in themobile context taking the GSM and channel coding into account.\nConstant amplitude thresholds are used for detecting note\nboundaries. An autocorrelation pitch tracker [8] with centerclipping [9] is used for estimating the pitch within each 20 msanalysis frame. On a note level the pitch is estimated as themedian of the pitch values of the frames within a note.\n2.1.2 Database Engine\nThe database engine used in the system is based on an engine\nmade by Lemström and Perttu [10]. A fast bit-parallel dynamicprogramming algorithm by Myers is used for approximate stringmatching [11].\n2.1.3 Voice-Only User Interface\nThe voice UI was implemented using a standard voice markup\nlanguage VoiceXML[6]. Speech recognition is used for userinput, and sp eech synthesis for system output. On calling the\nservice, the user hears short instructions for selecting betweenmelody search and generation modes. In both cases, the user isprompted to produce a melody sample by humming, whistling orplaying. The sample is played back to the user, and the user canrecord a new sample or send the sample for transcription.\nIn melody search, five best matches are retrieved from the\ndatabase and played back to the user on request. The user canbrowse the results by going forward or backward in the list.\nIn melody generation, the transcription output is transformed into\nAU format and played back. The user can then save or reject thegenerated melody.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.\n© 2002 IRCAM – Centre PompidouMobile Melody Recognition System with Voice-Only User Interface\n3. SYSTEM PERFORMANCE\nThe implemented system was evaluated with user tests. Nine users\ntested the system by calling the dial-in service. The user feedbackwas collected with a questionnaire and discussions.\nThe evaluation tests indicated positive user feedback and\nacceptance for the concept. The users felt the user interface andthe back-end audio analysis as well as the database engine to bewell integrated into an entertaining and easy-to-use service. Theconcept proved to be well suited for mobile use and the usersconsidered the service to be a good additional service for mobilemusic distribution and generation.\n3.1 Transcription and Retrieval Accuracy\nThe melody transcription and retrieval perform essentially in a\nsimilar manner as with the PC application reported in [7],although the mobile context introduces some additional errors inthe transcription, mostly in the detection of note boundaries.\nThe users rated the melody transcription accuracy a bit below 3 on\nthe scale 1-5 (5 = the best). Even though this rating is not tied toany reference it indicates that the transcription should be furthertuned and alternative ways for improvement should be considered.\nThe small decrease in transcription accuracy affects the retrieval\nsomewhat. However, the retrieval performance is still roughly atthe same level with the PC implementation. That is, the recallpercentage is at the level of 65-85% like reported in [7],depending on the chosen inputs.\n3.2 User Interface\nVoice commands are a quick and fairly natural way to interact.\nThe difficulties of voice interfaces come from the serial nature ofspeech. Only one instruction can be offered at a time, compared toa graphical user interface (GUI) with several active areas andbuttons.\nMost users considered the instructions given by the UI clear and\ninformative but also thought there were too many of them. Duringthe database search, clear indication of the system status washoped.\n4. CONCLUSIONS AND FUTURE WORK\nThe system reported in this paper is an end-to-end system that\noffers an easy-to-use music retrieval and generation applicationfor mobile phone users. The evaluation tests indicate the voice-only UI to be a good choice for this kind of applications. The usertests also indicate the concept to be well accepted by the users.\nThe presented system allows the user to generate or search for a\nmelody. These two tasks set different requirements for thetranscription. For search, it is often beneficial to clean thetranscription from small inaccuracies present in the i nput signals.\nThis, however, often makes the transcription sound clumsy incomparison to the original melody. It would be beneficial to tunethe transcription process differently for music retrieval andgeneration tasks.\nOne major advantage of the presented system over PC-based\nsolutions is the well-controlled signal path. The properties of themobile terminal and network are well known and the recordingand transcription parameters can be tuned in the design phase.\nEven with well-controlled signal path two major weaknesses\nremain in query-by-humming applications: the optimal parametersettings differ from user to user and in a general case the input isrelatively noisy or even erroneous.\nThe retrieval accuracy could be significantly improved if the\ntranscription parameters were tuned separately for each user.Some sort of learning scheme could be used in which the\napplication would learn to adjust parameters by the user’s input.\nSpeech \nRecognitionSpoken and \nhummed input Database size reduction\n(based on spoken key words)\nDatabase\nMelody\nTranscriptionDatabase \nEngine\nQuery resultsMelody portion extracted \nfrom the spoken input\nFigure 2. A block diagram of a speech enhanced melody\nrecognition system.\nA way for increasing the retrieval accuracy of a noisy signal is to\nenhance the retrieval process with speech (Figure 2). Spoken keywords (e.g., music style or artist name) can be used to limit thedatabase search to relevant parts of the database.\nSome of the technologies described in this paper are subject to\npending patents.\n5. REFERENCES\n[1] MPEG Requirements Group, \"Mpeg-7 Overview\", Doc.\nISO/MPEG N4674, MPEG Jeju Meeting, March 2002,edited by Martinez, J. M.\n[2] McNab, R.J., Smith, L.A., Bainbridge, D., and Witten, I.H.,\n\"The New Zealand digital library MELody inDEX\", D-Lib\nMagazine, May 1997.\n[3] Ghias, A., Logan, J., Chamberlin, D., and Smith, B.C.,\n\"Query by humming – musical information retrieval in anaudio database\", ACM Multimedia '95, San Francisco, USA,1995.\n[4] http://name-this-tune.com\n[5] Chai, Wei, \"Melody Retrieval on the Web\", M.Sc Thesis,\nMassachusetts Institute of Technology, August 2001.\n[6] http://www.w3.org/Voice/\n[7] Sorsa, T., \"Development and Evaluation of a Melody\nRecognition System\", M.Sc Thesis, Helsinki University ofTechnology, October 2000.\n[8] Rabiner, L.R., Cheng, M.J., Rosenberg, A.E. and McGonegal\nC.A., \"A comparative performance study of several pitchdetection algorithms\", IEEE Transactions on Acoustics,Speech, and Signal Processing, vol. 24, no. 5, pp. 399-418,\nJuly 1976.\n[9] Sondhi, M.M., \"New methods of pitch extraction\", IEEE\nTrans. Audio Electroacoust. Special issue on SpeechCommunication and Processing – Part II, AU-16, pp.262-266, June 1968.\n[10] Lemström, K. and Perttu, S., \"SEMEX - An efficient music\nretrieval prototype\", Proceedings of Music IR 2000,\nPlymouth, MA, USA, Oct. 2000.\n[11] Myers, G., \"A fast bit-vector algorithm for approximate string\nmatching based on dynamic programming\", Proceedings ofthe 9th Annual Symposium on Combinatorial PatternMatching, Piscataway, USA, 1998."
    },
    {
        "title": "Pitch Histograms in Audio and Symbolic Music Information Retrieval.",
        "author": [
            "George Tzanetakis",
            "Andrey Ermolinskiy",
            "Perry R. Cook"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416146",
        "url": "https://doi.org/10.5281/zenodo.1416146",
        "ee": "https://zenodo.org/records/1416146/files/TzanetakisEC02.pdf",
        "abstract": "In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques. In this paper, Pitch Histograms are defined and proposed as a way to represent the pitch content of music signals both in symbolic and audio form. This representation is evaluated in the context of automatic musical genre classification. A multiple-pitch detection algorithm for polyphonic signals is used to calculate Pitch Histograms for audio signals. In order to evaluate the extent and significance of errors resulting from the automatic multiple-pitch detection, automatic musical genre classification results from symbolic and audio data are compared. The comparison indicates that Pitch Histograms provide valuable information for musical genre classification. The results obtained for both symbolic and audio cases indicate that although pitch errors degrade classification performance for the audio case, Pitch Histograms can be effectively used for classification in both cases.",
        "zenodo_id": 1416146,
        "dblp_key": "conf/ismir/TzanetakisEC02",
        "keywords": [
            "Pitch",
            "Timing",
            "Symbolic",
            "Music",
            "Information",
            "Retrieval",
            "MIR",
            "Audio",
            "Classification",
            "Pitch Histograms"
        ],
        "content": "Pitch Histograms in Audio and Symbolic Music Information Retrieval \nPitch Histograms in Audio and Symbolic  \nMusic Information Retrieval \nGeorge Tzanetakis \nComputer Science Department \n35 Olden Street \nPrinceton NJ 08544 \n+1 609-258-5030 \ngtzan@cs.princeton.edu Andrey Ermolinskyi \nComputer Science Department \n35 Olden Street  \nPrinceton NJ 08544  \n+1 609-258-5030 \nandreye@princeton.eduPerry Cook \nComputer Science Department \n35 Olden Street \nPrinceton NJ 08544  \n+1 609-258-5030 \nprc@cs.princeton.edu3\nABSTRACT  \nIn order to represent musical content, pitch and timing \ninform ation is utilized in the m ajority  of existing work in \nSymbolic Music Inform ation Retrieval (MIR). Sy mbolic \nrepres entations  such as MIDI allow the eas y calculation of s uch \ninformation and its manipulation. In  contrast, most of the existing \nwork in Audio MIR uses timbral and beat information, which can \nbe calculated using autom atic computer audition techniques. \nIn this paper, Pitch Histograms are defined and proposed as a way  \nto represent the pitch content of music signals both in sy mbolic \nand audio form. This representation is evaluated in the context of \nautom atic musical genre classification. A m ultiple-pitch detection \nalgorithm for poly phonic signals is used to calculate Pitch \nHistograms for audio signals. In or der to evaluate the extent and \nsignificance of errors resulting from  the autom atic multiple-pitch \ndetection, autom atic m usical genre classification results from  \nsymbolic and audio data are com pared. The com parison indicates  \nthat Pitch Histograms provide va luable inform ation for m usical \ngenre clas sification. The res ults obtained for both sy mbolic and \naudio cases indicate that although pitch errors degrade \nclassification perform ance for the audio case, Pitch Histogram s \ncan be effectively  used for clas sification in both cas es. \n1. INTRODUCTION \nTraditionally , music inform ation retrieval (MIR) has been \nseparated in sy mbolic  MIR whe re struc tured signa ls suc h as MIDI \nfiles are used, and audio MIR where arbitrary  unstructured audio \nsignals are used. For sy mbolic MIR, m elodic inform ation is \ntypically  utilized while for audio MIR typically  timbral and \nrhythmic inform ation is used. In this paper, the m ain focus is the \nrepresentation of global pitch cont ent statistical information about \nmusical signals both in sy mbolic and audio form. More \nspecifically , Pitch Histograms are defined and proposed as a way  \nto represent pitch content inform ation and are evaluated in the \ncontext of autom atic m usical genre clas sification.  \nGiven the rapidly  increasing importance of digital music \ndistribution, as well as  the fact that large web-bas ed m usic \ncollections are continuing to grow in size exponentially , it is \nobvious that the ability  to effec tively  navigate within these \ncollections is a desirable quality . Hierarchies of m usical genres \nare used to structure on-line m usic stores, radio s tations  as well as  \nprivate collections  of com puter us ers.  \nUp to now, genre classification for digitally  stored music has been \nperform ed m anually  and therefore autom atic classification \nmechanism s would constitute a valuable addition to existing music inform ation retrieval systems. One could, for ins tance, \nenvis ion an Internet m usic search engine that s earches  for a s et of \nspecific musical features  (genre being one of them ), as specified \nby the user, within a space of feature-annotated audio files. \nMusical content features that are good for genre classification can \nbe used in other ty pe of analy sis such as sim ilarity  retrieval or \nsummarization. Therefore genre cl assification provides a way  to \nevaluate autom atically  extracted features  that describe musical \ncontent. Although the division of mu sic into genres is somewhat \nsubjective and arbitrary , there exis t perceptual criteria related to \nthe timbral, rhy thmic and pitch content of m usic that can be us ed \nto characterize a particular musical genre. In this  paper, we focus  \non pitch content information and propose Pitch Histograms as way  \nto represent such inform ation. \nSymbolic repres entations  of m usic such as  MIDI files are \nessentially  similar to m usical scores and ty pically  describe the \nstart, duration, volume, and inst rument ty pe of every  note of a \nmusical piece. Therefore, in the cas e of s ymbolic repres entation \nthe extraction of statistical inform ation related to the distribution \nof pitches, nam ely the Pitch Histogram , is trivial. On the other \nhand, extracting pitch information from audio signals is not easy . \nExtracting a sy mbolic representation from an arbitrary  audio \nsignal, called “poly phonic transcrip tion”, is still an open research \nproblem solved only for simple and sy nthetic “toy ” examples. \nAlthough the complete pitch information of an audio signal can \nnot be extracted reliably , autom atic multiple pitch detection \nalgorithm s can still provide enough accurate inform ation to \ncalculate overall statistical inform ation about the distribution of \npitches in the form  of a Pitch Histogram . In this paper, Pitch \nHistogram s are evaluated in the context of musical genre \nclassification. The effect of pitc h detection errors for the audio \ncase is inves tigated by  com paring genre clas sification res ults for \nMIDI and audio-from-MIDI signals. For the remainder of the \npaper it is important to define the following term s: sy mbolic, \naudio-from-MIDI and audio. Sy mbolic refers to MIDI files, \naudio-from-MIDI refers to a udio signals generated using a \nsynthesizer play ing a MIDI file a nd audio refers to general audio \nsignals such as mp3 files found on the web. \nThis work can be viewed as a bridge connecting audio and \nsymbolic MIR through the use of pitch information for retrieval \nand genre classification. Another valuable idea described in this \npaper is the use of MIDI data as the ground truth for evaluating \naudio analy sis algorithms applied to audio-from-MIDI data.  \nThe remainder of this paper is structured as follows: A review of \nrelated work is provided in Sec tion 2. Section 3 introduces Pitch \nHistograms and describes their cal culation for sy mbolic and audio \ndata. The evaluation of Pitch Histogr ams features  in the context of \nmusical genre clas sification is  described in Section 4. Section 5 \ndescribes the implementation of th e system and Section 6 contains \nconclusions and directions for future work. Permission to m ake digital or  hard copies of all or  part of this \nwork for per sonal or  classr oom use is gr anted without fee \nprovided that copies ar e not made or distr ibuted for  profit or  \ncommercial advantage and that copies bear this notice and the full \ncitation on the fir st page.   \n© 2002 I RCAM  – Centr e Pom pidou Pitch Histograms in Audio and Symbolic Music Information Retrieval \n2. RELATED WORK  \nMusic Information Retrieval (MIR) refers to the process of \nindexing and searching music collections. MIR systems can be \nclassified according to various aspects such as the type of queries \nallowable, the similarity algorithm, and the representation used to \nstore the collection.  Most of the work in MIR has traditionally \nconcentrated on symbolic representations such as MIDI files. This is due to several factors such as the relative ease of extracting \nstructured information from symbolic representations as well as \ntheir modest performance requirements, at least compared to MIR \nperformed on audio signals. More recently a variety of MIR \ntechniques for audio signals have been proposed. This development is spurred by increases in hardware performance and \ndevelopment of new Signal Processing and Machine Learning \nalgorithms. \nSymbolic MIR has its roots in dic tionaries of musical themes such \nas Barlow [1]. Because of its symbolic nature, it is often \ninfluenced by ideas from the field of text information retrieval [2]. \nSome examples of modeling symbo lic music information as text \nfor retrieval purposes are describe d in [3, 4]. In most cases the \nquery to the system consists of a melody or a melodic contour. \nThese queries can either be entered manually or transcribed from \na monophonic audio recording of the user humming or singing the desired melody. The second approach is called Query-by-\nhumming and some early examples are [5, 6]. A variety of \ndifferent methods for calculating me lodic similarity are described \nin [7]. In addition to melodic information, other types of information extracted from symbolic signals can also be utilized for music retrieval. As an example the production of figured bass \nand its use for tonality recognition is described in [8] and the \nrecognition of Jazz chord sequences is treated in [9]. \nUnlike symbolic MIR which typically focuses on pitch information, audio MIR has traditionally used features that \ndescribe the timbral characteristics of musical textures as well as  \nbeat information. Representative examples of techniques for \nretrieving music based on audio si gnals include: performances of \nthe same orchestral piece based on its long-term energy profile \n[10], discrimination of music and speech [11, 12], classification, \nsegmentation and similarity retr ieval of musical audio signals \n[13], and automatic beat detection algorithms [14, 15]. \nAlthough accurate multiple pitch detection on arbitrary audio \nsignals (polyphonic transcription)  is an unsolved problem, it is \npossible to extract statistical information regarding the overall pitch content of musical signals.  Pitch Histograms are such a \nrepresentation of pitch content that  has been used together with \ntimbral and rhythmic features for automatic musical genre \nclassification in [16]. Pitch Hist ograms are further explored and \ntheir performance is compared both for symbolic and audio signals in this paper. The goal of  the paper is not to demonstrate \nthat features based on Pitch Hist ograms are better or more useful \nin any sense compared to other existing features but rather to \nshow their value as additional alternative source of musical \ncontent information. As alread y mentioned, symbolic MIR and \naudio MIR traditionally have used  different algorithms and types \nof information. This work can be viewed as an attempt to bridge \nthese two distinct approaches. \n3. PITCH HISTOGRAMS \nPitch Histograms are global statisti cal representations of the pitch \ncontent of a musical piece. Features calculated from them can be \nused for genre classification, similarity retrieval as well as any \ntype of analysis where some representation of the musical content \nis required. In the following s ubsections, Pitch Histograms are \ndefined and used to extract features for genre classification. 3.1 Pitch Histogram Definition \nA Pitch Histogram is, basically, an array of 128 integer values \n(bins) indexed by MIDI note num bers and showing the frequency \nof occurrence of each note in a musical piece. Intuitively, Pitch Histograms should capture at leas t some amount of information \nregarding harmonic features of different musical genres and pieces. One expects, for instance, that genres with more complex tonal structure (such as Classical music or Jazz) will exhibit a \nhigher degree of tonal change and therefore have fewer \npronounced peaks in their histograms than genres such as Rock, \nHip-Hop or Electronica music that typically contain simple chord \nprogressions.  \nTwo versions of the hist ogram are considered: an unfolded  (as \ndefined above) and a folded  version. In the folded version, all \nnotes are transposed into a single octave (array of size 12) and mapped to a circle of fifths, so that adjacent histogram bins are \nspaced a fifth apart, rather than a semitone. More specifically if \nwe denote n the MIDI note number (C4 is 60) then the following \nconversion can be used to get the folded  version index c: c  = (n \nmod 12). For mapping to the circle of fifths the following \nconversion can be used c’ = (7 x c) mod 12.  \nThis is done in order to make the histogram better suited for expressing tonal music relations and it was found empirically that \nthe extracted features result in better classification accuracy. As an example a peak in C major will have strong peaks at C and G \n(tonic and dominant) and will be more closely related to a piece in \nG major (G and D peaks) than a piece in C# major. It can therefore be said that the folded version of the histogram contains \ninformation regarding the pitch content of the music (or a crude approximation of harmonic information), whereas the unfolded \nversion is useful for determining the pitch range of the piece. \n3.2 Pitch Histogram Features \nIn order to perform automatic musical genre classification, after \nthe Pitch Histogram has been computed, it is transformed into a \nfour-dimensional feature vector .  This feature vector is used as a \ncharacterization of the pitch content of a particular musical piece. For classification, a supervised learning approach is followed, \nwhere collections of such feature vectors  are used to train and \nevaluate automatic musical genre classifiers.  \nThe following four features based on the Pitch Histogram are \nproposed for classifying musical genres: \n• PITCH-Fold : Bin number of the maximum peak of the \nfolded histogram. This typica lly corresponds to the most \ncommon pitch class of the musical piece (in tonal music usually the dominant or the tonic).  \n• AMPL-Fold : Amplitude of the maximum peak of the \nfolded histogram. This corre sponds to the frequency of \noccurrence of the main pitch class of the song. This peak is typically higher for pieces that do not contain \nmany harmonic changes. \n• PITCH-Unfold :  Period of the maximum peak of the \nunfolded histogram. This corresponds to the octave range of the musical pitch of the song. For example, a \nflute piece will have a higher value of this feature than a \nbass piece even if they are in the same tonal key.  \nDIST-Fold : Interval (in bins) between the two highest peaks of \nthe folded histogram. For pieces with simple harmonic structure, this feature will have value 1 or –1 corresponding to a music \ninterval of a fifth or a fourth. \nThese features were chosen based on experimentation and \nsubsequent evaluation in the task of musical genre classification. Pitch Histograms in Audio and Symbolic Music Information Retrieval \nAs an exam ple J azz m usic tends  to have more chord changes  and \ntherefore has lower values of AMPL-Fold on average. Rather \nthan try ing to find thresholds empirically , a dis ciplined m achine \nlearning approach was used were these inform al obs ervations  as \nwell as other non-obvious patterns in the data are learned and \nevaluated for clas sification. The choice of the particular feature \nset is an im portant one, as it is desirable to filter out the irrelevant \nstatistical properties of the histogram  while retaining inform ation \nidentify ing the pitch content. Although this choice is not \nnecessarily  optim al, it will em pirically  be shown to be effective \nfor m usical genre clas sification. \n3.3 Pitch Histogram Calculation \nFor MIDI files, the histogram  is constructed using a simple linear \ntraversal over all MIDI events in the file. For each encountered \nNote-On  event (excluding the ones play ed on the MIDI drum \nchannel), the algorithm increments the corresponding note’s \nfrequency  counter. The value in each histogram  bin is norm alized \nin the last stage of the calculation by  dividing it by  the total \nnumber of notes. Example unfol ded Pitch Histograms belonging \nto two genres  (Jazz and Iris h Folk m usic) are s hown in F igure 1. \nBy visual inspection of this figur e, it can be s een that the P itch \nHistograms corresponding to Irish  Folk music have few and \nsparse peaks indicating a smaller amount of harmonic change than \nexhibited by Jazz m usic. This  type of inform ation is  what the \nproposed features attempt to capture and use for automatic \nmusical genre clas sification.  \nFor calculating Pitch Histogram s from  audio data, the multiple \npitch detection algorithm proposed in [17]  is used. The following \nsubsection provides a description of this algorithm. \n3.4 Multiple Pitch Detection Algorithm  \nThe m ultiple pitch detection used for Pitch Histogram  calculation \nis based on the two channel pitch analy sis model described in \n[17]. A block diagram of this model is shown in Figure 2. The \nsignal is separated into two cha nnels, below and above 1kHz. The \nchannel separation is done with filters that have 12 dB/octave \nattenuation at the stop band. The lowpass block also includes a \nhighpass rolloff with 12dB/octave below 70 Hz. The high-channel \nis half-wave rectified and lowpass filtered with a sim ilar filter \n(including the highpass characteristic at 70Hz) to that used for \nseparating the low channel. \nThe periodicity  detection is based on “generalized \nautocorrelation” i.e. the com putati on consists of a discrete Fourier \ntransform (DFT), magnitude compression of the spectral \nrepresentation, and an inverse tr ansform (IDFT). The signal x2 of \nFigure 2 is obtained as follows: \n \nX2    = IDFT(|DFT(x low)|k) + IDFT(|DFT(x high)|k)  \n = IDFT(|DFT(x low)|k + |DFT(x high)|k)  \n \nwhere xlow and xhigh are the low and the high channel signals \nbefore the periodicity  detection bl ocks in Figure 2. The parameter \nk determines the frequency -domain compression (for normal \nautocorrelation k=2). The Fast Fourier Transform (FFT) and its \ninvers e (IFFT) are used to s peed the com putation of the \ntransform s. \nThe peaks of the summary  autocorrelation function (SACF) \n(signal x2 of Figure 2) are relatively  good indicators of potential \npitch periods in the signal analy zed. In order to filter out integer \nmultiple of the fundam ental period, a peak pruning technique is used. The original SACF curve is  first clipped to positive values \nand then tim e-scaled by  a factor of two and s ubtracted from  the \noriginal clipped SACF function, a nd again the result is clipped to \nhave positive values only . That way , repetitive peaks with double \nthe tim e lag of the bas ic peak ar e removed. The resulting function \nis called the enhanced sum mary autocorrelation (ESACF) and its \nprom inent peaks  are accum ulated in the P itch His togram  \ncalculation. More details about the calculation steps of this \nmultiple pitch detection m odel, as well as its evaluation and \njustification can be found in [17] . \n \n \n \n \nFigure 1. Pitch Histograms of tw o jaz z pieces (left) and tw o \nIrish folk songs (right). \n \n Input \n HighPass \n1kHz LowPa ss \n1kHz  \n \n \n \n Half-wave Rectifier \nLowPa ss \n \n \n \n \n \n \n \n \n \nFigu re 2. Mu ltiple Pitch  Detection  Periodicity  \ndetection \nSACF \nEnhance\nrx1 \nPeriodicity  \ndetection \nx2 Pitch Histograms in Audio and Symbolic Music Information Retrieval \n4. GENRE CLASSIFICATION  USING \nPITCH HISTOGRAMS  4.3 MIDI Representation \nThe classification res ults for the M IDI repres entation are s hown in  \nFigure 3, plotted against the probability  of random  classification \n(gues sing). It can be s een that the res ults are s ignificantly  better \nthan random, which indicates th at the proposed pitch content \nfeature set does contain a non-negligible amount of genre-specific \ninformation. The full 5-genre classifier performs with 50% \naccuracy , which is  more than tw ice better than chance (20%). One way of evaluating musical content features is through \nautom atic musical genre clas sification. In this  section, the \nproposed Pitch Histogram features are computed from MIDI and \naudio-from-MIDI representations, evaluated and the results for \neach cas e are com pared. \n4.1 Overview  of Pattern Classification \n In order to evaluate the performan ce of the proposed feature set, a \nsupervised learning approach was used. Statistical pattern \nrecognition (SPR) classifiers were  trained and evaluated using a \nmusical data s et collected from  various  sources . The basic idea \nbehind SPR is to estim ate the probability  density  function ( pdf) of \nthe feature vectors  for each clas s. In supervis ed learning, a labeled \ntraining set is used to estim ate this pdf and this estim ation is used \nto classify  unknown data. In the de scribed experim ents, each class \ncorresponds to a particular musical genre and the k-nearest-\nneighbor (KNN) classifier is used.  \nIn the KNN classifier, an unknown feature vector is classified \naccording to the m ajority  of its  neares t labeled feature vectors  \nfrom the training set. The main purpose of the described \nexperim ents is com paring the clas sification perform ance of Pitch \nHistogram features in audio and sy mbolic form  rather than \nobtaining the best classification performance.  The KNN classifier \nis a good choice for this purpose b ecause its perform ance is not as \nsensitive to the form  of the underly ing class pdf as that of the \nother classifiers. Moreover, it can als o be shown that the error rate \nof the KNN classifier will be at most twice the error rate of the \nbest possible (Bay es) clas sifier as  the s ize of the training s et goes  \nto infinity . A proof of this statem ent, as well as a detailed \ndescription of the KNN classifier  and pattern classification in \ngeneral can be found in [18] . Figu re 3.  MI DI classification  accu racy for tw o gen res (top ) \nand five genres (bottom) \nTable 1. MI DI genre con fusion matrix (p ercen tage valu es) \n Electr. Class.  Jazz Irish Rock  \nElectr. 32 2 3 1 21 \nClass.  8 33 24 9 15 \nJazz 9 42 55 2 21 \nIrish 12 19 8 83 12 \nRock 39 4 10 5 31 4.2 Details \nThe five genres used in our experiments are the following ones: \nElectronica, Classical, Jazz, Iris h Folk and Rock. W hile by  no \nmeans exhaus tive or even fully  repres entative of all exis ting \nmusical classes, this list of genr es is diverse enough to provide a \ngood indication of the amount of genre-specific information \nembedded into the proposed feature vectors.  The choice of genres \nwas mainly dictated by  the eas e of obtaining exam ples for each \nparticular genre from  the web. A set of 100 m usical pieces in \nMIDI form at is used to repres ent and train classifiers for each \ngenre. An additional 5*100 audio pi eces were generated using the \nTimidity software audio sy nthesizer to  convert the MIDI files. \nMoreover, 5*100 general audio pi eces (not corresponding to the \nMIDI files but belonging to the same genres) were also used for \ncomparison and evaluation. Each file is  repres ented as  a single \nfeature vector and 150 seconds of  the file are used in the \nhistogram  calculation in all thes e cas es.  \nThe classification res ults are als o summarized in Table 1 in the \nform of a so-called confusion matrix . Its columns correspond to \nthe actual genre and the rows to the genre predicted by  the \nclassifier. For exam ple, the cell of row 5, colum n 3 contains  value \n10, meaning that 10% of jazz (row 5) was  incorrectly  classified as  \nrock m usic (colum n 3). The percentages  of correct classifications  \nlie on the main diagonal of the confusion matrix. It can be seen \nthat 39% of rock was  incorrectly  classified as Electronica and the \nconfusion between Electronica and other genres is a source of \nseveral other significant miscalculati ons. All of this indicates that \nthe harm onic content analy sis is not well s uited for Electronica \nmusic becaus e of its extrem ely broad nature. S ome of its  melodic \ncomponents can be m istaken for rock, jazz or even classical \nmusic, whereas Electronica’s main distinguishing feature, namely  \nthe extrem ely repetitive structure of its percussive and melodic \nelem ents is not reflected in any way  on the P itch His togram . It is  \nclear from  inspecting the Table that certain genres  are m uch better \nclassified based on their pitc h content something which is \nexpected. However even in the cas es of confus ion, the results are \nsignificantly  better than random and therefore would provide \nuseful inform ation es pecially  if com bined with other features . For classification,  the KNN(3) classi fier is used.  For evaluation,  a \n10-fold cross-validation paradigm is followed. In this paradigm , \nthe training set is randomly  divided into k disjoint sets of equal \nsize n/k, where n is the total num ber of labeled exam ples. The \nclassifier is  trained i times, each tim e with a different set held out \nas a validation set in order to ensure that the evaluation results are \nnot affected by  the particular choice of training and testing sets. \nThe estimated perform ance is the m ean and s tandard deviation of \nthe i iterations of the cross-validation. In the described \nexperiments, 100 iterations are used. \n Pitch Histograms in Audio and Symbolic Music Information Retrieval \nIn addition to these results, som e repres entative pair-wis e genre \nclassification accuracy  results are s hown in F igure 4. A 2-genre \nclassifier succeeds  in correctly  identify ing the genre with 80% \naccuracy  on average (1.6 tim es bette r than chance). The classifier \ncorrectly  distinguishes between Irish Folk m usic and Jazz with \n94% accuracy , which is  the bes t clas sification res ult. The wors t \npair is Rock and Electronica, as  can be expected, s ince both of \nthese genres often employ sim ple and repetitive tonal \ncombinations. \n \n \nFigu re 4. Pair-w ise evaluation in MIDI \nIt will be shown below that othe r feature-evaluating techniques, \nsuch as the analy sis of rhy thmic features  or the exam ination of \ntimbral texture can provide additional information for musical \ngenre classification and be more  effective in distinguishing \nElectronica from  other m usical genres . This  is expected becaus e \nElectronica is  more characterized by  its rhy thmic and timbral \ncharacters  rather than its  pitch content. \nAn attem pt was m ade to investigate the dy namic properties of the \nproposed classification technique by study ing the dependence of \nthe algorithm ’s accuracy  on the tim e-dom ain length of the \nsupplied input data. Instead of letting the algorithm  process MIDI \nfiles in their entirety , the histogram -constructing routine was \nmodified to only  process the first n-second chunk of the file, \nwhere n is a variable quantity . The average classification accuracy  \nacross one hundred files is plotted as a function of n in Figure 5.  \nThe observed dependence of classification accuracy  to the input \ndata length is characterized by  two pivotal points on the graph. \nThe first point occurs at around 0.9 seconds, which is when the \naccuracy  improves to approxim ately 35% from  the random  20%. \nHence, approximately  one second of musical data is needed by  \nour clas sifier to s tart identify ing genre-related harmonic properties \nof the data. The second point occurs at approximately  80 seconds \ninto the MIDI file, which is  when the accuracy  curve s tarts \nflattening off. The function reaches its absolute peak at around \n240 seconds (4 minutes). \n4.4 Audio generated from MIDI \nrepresentation \nThe genre classification results for the audio-from-MIDI \nrepresentation are shown in Figur e 5. Although the results are not \nas good as the ones obtained from  MIDI data, they are still \nsignificantly  better than random cl assification. More details are \nprovided in Table 2 in the form of a confusion matrix . From Table \n2, it can be seen that Electronica is  much harder to clas sify correctly  in this case, probably  due to nois e in the feature vectors  \ncaused by pitch errors of the m ultiple-pitch detection algorithm . A \ncomparison of these results with the ones obtained using the MIDI \nrepresentation and general audio is provided in the next \nsubsection. We have no reason to believe that the outcome of the \ncomparison was  in any  way  influenced by  the s pecifics  of the \nMIDI-to-Audio conversion procedure. \n \n \nFigure 5. Average classification accuracy  as a function of the \nlength of input M IDI data (in se conds) \n \n \nFigu re 6. Clas sification  accu racy comp arison of ran dom an d \nAudio-from-MIDI \n \n \nTable  2. Audio-fr om-M IDI ge nre confusion matr ix  \n Electr. Class.  Jazz Irish Rock  \nElectr. 9 8 10 3 19 \nClass.  26 25 20 6 25 \nJazz 30 39 51 6 25 \nIrish 19 20 9 83 10 \nRock 16 8 10 2 21 \n Pitch Histograms in Audio and Symbolic Music Information Retrieval \n4.5 Comparison \nOne of the objectives  of the described experi ments wa s to \nestimate the amount of classifi cation error introduced by  the \nmulti-pitch detection algorithm  used for the construction of Pitch \nHistograms from audio signals. Knowing that MIDI pitch \ninform ation (and therefore pitch content feature vectors  extracted \nfrom  MIDI) is fully  accurate by  definition, it is possible to \nestimate this amount by  comparing the MIDI classification results \nwith those obtained from the audio-from-MIDI representation. A \nlarge discrepancy  would indicate that the errors introduced by \nmultiple-pitch detection algorithm  significantly  affect the \nextracted feature vectors . \n \n \n \nFigu re 7. Clas sification  accu racy comp arison \nTable 3. Comparison of classification  results \n Multi-pitch \nFeatu res Full Featu re \nSet RND \nMIDI 50 ±7% N/A 20% \nAudio-from-\nMIDI 43 ±7% 75 ±6% 20% \nAudio 40 ±6% 70 ±6% 20% \n \nThe results of the comparison are s hown in F igure 6. The s ame \ndata is also provided in Table 3. It can be observed that there is a \ndecrease in performance between the MIDI and audio-from-MIDI \nrepres entations . However, des pite the errors , the features  \ncomputed from  audio-from -MIDI still provide significant \ninformation for genre classificati on. A further s maller decreas e in \nclassification accuracy  is observed between the audio-from -MIDI \nand audio representations. This is  probably  due to the fact that \ncleaner m ultiple pitch detection results can be obtained from  the \naudio-from -MIDI exam ples because of the artificial nature of the \nsynthesized signals. It is important  to note that the general audio \nsignals only  correspond at the genre level, while the MIDI and \naudio-from -MIDI correspond at the specific piece level. \nIn addition to information regard ing pitch or harmonic content, \nother ty pes of information, such as timbral texture and rhythmic \nstructure can be utilized to char acterize m usical genres. The full \nfeature set results shown in Figure 6 and Table 3 refer to the \nfeature set described and used for genre classification in [16] . In \naddition to the described pitch content features, this feature set contains timbral texture features  (Short-Time Fourier Transform \n(STFT) based, Mel-Frequency  Ceps tral Coefficients  (MFCC)), as \nwell as features about the rhy thmic structure derived from Beat \nHistograms calculated using the Di screte W avelet Transform .  \nIt is interesting to com pare th is result with the perform ance of \nhumans in classify ing musical genre, which has  been inves tigated \nin [19] . It was  determ ined that hum ans are able to correctly  \ndistinguish between ten genres with 53% accuracy  after listening \nto only  250 m illiseconds audio sam ples. Listening to three \nseconds of m usic y ielded 70% accuracy  (against 10% chance). \nTen genres were used for this study . Although direct comparison \nof these results with the describe d results is not possible due to \ndifferent number of genres, it is clear that the autom atic \nperform ance is  not far away  from  the hum an perform ance. Thes e \nresults also indicate the fuzzy  nature of musical genre boundaries. \n \n \nFigu re 8. Th ree-d imensional time-p itch surface \n5. IMPLEMENTATION \nThe s oftware us ed for the audio P itch His togram  calculation, as  \nwell as  for the clas sification and evaluation, is  available as  a part \nof MARSYAS [20] , a free software framework for rapid \ndevelopment and evaluation of computer audition applications. \nThe s oftware for the M IDI P itch His togram  calculation is \navailable as separate C++ code and will be integrated into \nMARSYAS in the future.  The fra mework follows a client-server \narchitecture. The server contains  all the pattern recognition, s ignal \nprocessing and numerical computa tions and runs on any  platform \nthat provides C++ compilation facilities. A client graphical user \ninterface written in Java controls the server. MARS YAS  is \navailable under the Gnu Public License (GPL) at: \nhttp://www.cs.princeton. edu/~gtzan/marsyas.html  Pitch Histograms in Audio and Symbolic Music Information Retrieval \nAnother conclusion is that, desp ite being a highly  subjective and \nill-defined procedure, m usical genre classification can be \nperform ed autom atically  by  determ inistic means with \nperform ance com parable to hum an genre clas sification and pitch \ncontent information has a significan t part in this process both for \nsymbolic and audio musical signals.  In order to experimentally  inve stigate the res ults and perform ance \nof the P itch His togram s, a s et of vis ualization interfaces  for \ndisplay ing the time evolution of pitch content information was \ndeveloped.  \nThese tools provide three distinct modes of visualization:  \n1) Standard Pitch Histogram plots (Figure 1) A multiple-pitch detection algorithm  was used to estim ate musical \npitches from  audio signals, while the direct availability  of pitch \ninformation in MIDI format made the construction of MIDI Pitch \nHistogram s an easier process. Although the multiple-pitch \ndetection algorithm is not perf ect and subsequently  causes \nclassification accuracy  degradation for the audio case, it still \nprovides significant information for musical genre clas sification.  2) Three-dim ensional pitch-tim e surfaces  (Figure 7). \n3) Projection of the pitch-tim e surfaces  onto a two-dim ensional \nbitmap, with height repres ented as  the gray scale color value \n(Figure 8). \nThese visualization tools are written in C++ and use OpenGL for \nthe 3D graphics rendering. \nIt is our belief that the methodology  of using MIDI data and \naudio-from-MIDI data to compare and evaluate audio analy sis \nalgorithm s applied in this paper can  also be applied to other types \nof audio analy sis algorithm s, such as similarity  retrieval, \nclassification, s ummarization,  instrument tracking, and \npolyphonic transcription. Another im portant conclusion, is that an \naudio analy sis technique does not ha ve to give perfect results in \norder to be useful especially  when machine learning methods are \nused to collect statistical inform ation.  \n An interesting direction for furthe r res earch is  a m ore extens ive \nexploration of the statistical pr operties of Pitch Histograms and \nthe expans ion of the pitch content feature s et. For exam ple, we are \nplanning to investigate a real- time running version of the Pitch \nHistogram, in which time-domain va riations of the pitch content \nare taken into account (see Figures  7, 8, 9). A running Pitch \nHistogram contains information a bout the temporal evolution of \npitch content that can potentially  can be utilized for better \nclassification perform ance. Another interesting idea is the use of \nthe running Pitch Histogram to conduct more detailed harmonic \nanaly sis such as figured bass extraction, tonality  recognition, and \nchord detection. The vis ualization interfaces  des cribed in this \npaper will be used for exploring the extraction of m ore detailed \npitch content inform ation from  music signals in sy mbolic and \naudio form.  \n \nFigure 9. Examples of gray scale pitch-time surfaces: Jaz z \n(top) and Irish Folk music (bottom), X  axis = time, Y \naxis=pitc h. Although mainly  designed for genr e classification it is possible \nthat features derived from P itch Histograms might also be \napplicable to the problem of content-based audio identification or \naudio fingerprinting (for an example of such a system see [21]). \nWe are planning to explore this possibility  in the future.  The upper part of Figure 8 shows an ascending chromatic scale of \nequal-length non-overlapping notes. A snapshot of the time-pitch \nsurface of an actual m usic piece is  shown in the lower part of \nFigure 8. By  visual inspection of Figure 9, various ty pes of \ninteres ting inform ation can be obs erved. S ome exam ples are: the \nhigher pitch range of the particular Irish piece (lower part) \ncompared to the jazz piece, as  well as  its different periodic \nstructure and melodic movement . Thes e obs ervations  seem to \ngeneralize to the particular genres and potentially  be used for the \nextraction of more powerful pitch content features.. Alternative feature sets, as we ll as different m ultiple pitch \ndetection algorithms also need to be explored and evaluated in the \ncontext of this  work. P itch content features  also enable the \nspecification of new ty pes of queries  and constraints, such as key \nor am ount of harm onic change that go bey ond the traditional \nquery -by-humming (for symbolic) and query -by-example (for \naudio) paradigms for music inform ation retrieval. Finally , we are \nplanning to use the proposed feature set as a part of a query -based \nretrieval mechanism for audio music signals. 6. CONCLUSIONS AND FUTURE WORK \nIn this paper, the notion of Pitch Histograms was introduced and \nits applicability  in the context of m usical genre classification was \nevaluated. A feature set for representing the harmonic content of \nmusic was derived from Pitch Histograms and proposed as a basis \nfor genre clas sification. S tatistical pattern recognition classifiers \nwere trained to recognize this feat ure set and an attempt was made \nto evaluate their perform ance on a s ample collection of m usical \nsignals both in sy mbolic and audio form. It was established that \nthe proposed classification technique produces results that are \nsignificantly  better than random, wh ich allowed us to conclude \nthat Pitch Histograms do carry  a certain amount of genre-\nidentify ing inform ation and therefore they  are a us eful tool in the \ncontext of autom atic m usical genre clas sification.  \n7. REFERENCES \n[1] Barlow, H., DeRoure, D. A Dictionary  of Musical Themes. \nNew York, Crown, 1948. \n[2] Baeza-Yates , R.. Ribeiro-Neto, B., Modern Inform ation \nRetrieval, Addison-Wesley , 1999.  \n[3] Downie, J. S. Evaluating a Simple Approach to Music \nInform ation Retrieval: Conceivi ng Melodic N-grams as Text, \nPh.D thesis, University  of Western Ontario, 1999.  \n[4] Pickens, J. A Comparis on of Language Modeling and \nProbabilistic Text Inform ation Retrieval Approaches to Pitch Histograms in Audio and Symbolic Music Information Retrieval \nMonophonic Music Retrieval. In Proc. Int. Symposium on \nMusic Information Retrieval (ISMIR), Plymouth, MA, 2000. \n[5] Kageyama, T., Mochizuki, K., Takashima, Y. Melody \nRetrieval with Humming. In Proc. Int. Computer Music \nConference (ICMC), 1993. \n[6] Ghias, A., Logan, J., Chamberlin, D., and Smith, B.C., Query \nby humming: Musical information retrieval in an audio database. In Proc. of ACM Multimedia, 231-236, 1995. \n[7] Hewlett, W.B., and Selfridge -Field, Eleanor (Eds), Melodic \nSimilarity: Concepts, Procedures and Applications. Computing in Musicology, 11.  \n[8] Barthelemy, J., and Bonardi,  A. Figured Bass and Tonality \nRecognition. In Proc. Int. Symposium on Music Information Retrieval (ISMIR), Bloomington, Indiana, 2001. \n[9] Pachet, F. Computer Analysis of Jazz Chord Sequences: Is \nSolar a Blues. Readings in Music and Artificial Intelligence, Miranda, E. Ed, Harwood Acad emic Publishers, 2000.   \n[10] Foote, J. ARTHUR: Retrieving Orchestral Music by Long-\nTerm Structure. In Proc. Int. Symposium on Music Information Retrieval (ISMIR), Plymouth, MA, 2000. \n[11] Logan, B. Mel Frequency Ce pstral Coefficients for Music \nModeling. In Proc. Int. Sy mposium on Music Information \nRetrieval (ISMIR), Plymouth, MA, 2000 \n[12] Scheirer, E., and Slaney, M. Construction and Evaluation of \na Robust Multifeature Speech/Music Discriminator. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing \n(ICASSP), Munich, Germany, 1997. \n[13] Tzanetakis, G., and Cook, P. Audio Information Retrieval \n(AIR) Tools. In Proc. Int. Symposium on Music Information \nRetrieval (ISMIR), Plymouth, MA, 2000. [14] Scheirer, E. Tempo and Beat Analysis of Acoustic Musical \nSignals. Journal of the Acous tical Society of America, \n103(1):588,601, Jan. 1998. \n[15] Laroche, J. Estimating Tem po, Swing and Beat Locations in \nAudio Recordings. In Proc. IEEE Int. Workshop on \nApplications of Signal Pro cessing to Audio and Acoustics \n(WASPAA), 135-139, Mohonk, NY, 2001. \n[16] Tzanetakis, G., and Cook, P., Musical Genre Classification \nof Audio Signals (to appear) IEEE Transactions on Speech and Audio Processing, July 2002.  \n[17] Tolonen, T., and Karjalainen, M. A Computationally \nEfficient Multipitch Analysis Model IEEE Trans. On Speech and Audio Processing, 8(6):708-716, Nov. 2000. \n[18] Duda, R., Hart, P., and Stork,  D., Pattern Classification. John \nWiley & Sons, New York, 2000. \n[19] Perrot, D., and Gjerdige n, R. Scanning the dial: An \nexploration of factors in the identification of musical style. In Proc. of the 1999 Society for Music Perception and \nCognition pp.88, (abstract) \n[20] Tzanetakis, G., Cook, P. Marsyas: A framework for audio \nanalysis. Organised Sound , vol. 4(3), 2000. \n[21]  Allamanche, E. et al., Content-based identification of audio \nmaterial using MPEG-7 Low Leve l Description. In Proc. Int. \nSymposium on Music Information Retrieval (ISMIR), Bloomington, 2001."
    },
    {
        "title": "A Review of Factors Affecting Music Recommender Success.",
        "author": [
            "Alexandra L. Uitdenbogerd",
            "Ron G. van Schyndel"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417783",
        "url": "https://doi.org/10.5281/zenodo.1417783",
        "ee": "https://zenodo.org/records/1417783/files/UitdenbogerdS02.pdf",
        "abstract": "6\u0014798;:+=\b?@=BAC:D:EAC?GFH=\b=\bI\u0017JK79FKLNMO?@:K=BP+Q I\u0017R$7K?@MN8\bA LTS;A ?US>= VW:9QBXTY =\bZ[=\\LN=1:9A ?GFH=\b=\\I\r?US>7EP^MO=BP+F`_\u0007S>:K=&F97KMNLNP^=\\?GQ a3R\u001e79?@MN8b=\b8cY Q RGRG=\bIEP^=\\?\bdTefRGJKLOMg8cMOShACIEP&=\\i^J9LOMN8\\M]S\u00158\\Q[LOLNA FHQCMOZj=\u001ek9L]S>=\\MOI9lm:EAC? FH=\b=\\I179?@=BP\u0014anQ =B8\\Q RGRG=\bIEP^=\\?\bVKMNIrA P9P^MOS>MOQ I&S>QqS>:9=sA 7^Y S>Q RmACS>MN8t8\\LNA ?@?@M]kE8\bACS>MOQ[I\u0014Q a\u0019R\u001e79?@MN8hMOI`S>Qs?USf_^LO=t8\bAuS>=\bl[QCMN=\\?hF9A ?@=BPmQ[I =\\ipS@=BP%AC7EPKMOQ&an=BACS>7^=\b?\bd1vw:KMN?sJEACJH=\\Zj=\\_^?t=\b?@=\bAC:\u0007MNI`S>Q R\u001e79?@MN8\bA LjS;AC?US>=[V[=\\Z^MO=\\XW?xR\u001e79?@MN8x=B8\\Q[RGRG=\\IEPK=c=\b?@=\bAC:yV`ACIEPzQ[7^S@Y LOMOI9=\b?{JKQ RGMN?@MOIKlzPKM]=B8cS>MOQ[IK?\bd|efIqJ9ACMN8\\79LNAuIK=BPqS>:EACS}PK=cY RGQ l ?@Q[IEACLOMOS~_\u0012aA[8;S>Q ?m:EABZj=&FH=\\=\bI-?@:KQBXWI-S>Q\u0005FH= aA[8cS>QC?MOIK979=\bI98\\MOI9l2R$79?@MN8bJK=\\an=\\=\bIE8c=[d9QC:K=&RmA MOI aA[8cS>QC?&AC=&S>=\\RGJHQKVwS>Q IEA LOM]Sf_`VP^MO?US>MNI98cS>MOZj=\bIK=\b?@?mQ a3:`_pS>:9RACIEP JKMOS;8>:\u0014:K=\bMOl[:`SBd",
        "zenodo_id": 1417783,
        "dblp_key": "conf/ismir/UitdenbogerdS02",
        "keywords": [
            "6",
            "798",
            "I\u0017",
            "JK79FKLNMO",
            "TS;A",
            "US>",
            "Q",
            "Q",
            "R",
            "GQ"
        ],
        "content": "AReviewofFactorsAffectingMusicRecommender Success\u0000 \u0001\u0003\u0002\u0005\u0004\u0007\u0006\b\u0002\n\t \u000b\r\f\u000f\u000e\u0007\u0010\u0012\u0011\u0014\u0013\u0015\u000b\u0017\u0016\u0019\u0018\u001a\u0000\u001c\u001b\u001d\u0002\u0005\u0011\u0014\u0013\u001e\u0006 \u001f\u0012! \" #\u0012\u0018$\u0006\b\u0011 \u0001\u0003\u0002%\u0011&\u000b(')'*\u0002+\u001f+,\u0017\u0002-\u0016/.0#+\u00111\u0011&\u00022\u00183\u0018\nAlexandr aUitdenbogerd and Ron vanSch yndel\nDepar tment ofComputer Science ,RMIT Univ ersity\nGPO Box2476V ,Melbour ne3001, Austr alia\n+613 9925 22464alu,ron vs 5@cs .rmit.edu.au\nABSTRACT6\u0014798;:+<>=\b?@=BAC<;8>:D:EAC?GFH=\b=\bI\u0017JK79FKLNMO?@:K=BP+Q I\u0017R$7K?@MN8\bA LTS;A ?US>= VW:9QBXTY=\bZ[=\\<BVTLOM]S@S>LN=1:9A ?GFH=\b=\\I\r?US>7EP^MO=BP+F`_\u0007S>:K=&F97KMNLNP^=\\<>?GQ a3R\u001e79?@MN8b<>=\b8cYQ RGRG=\bIEP^=\\<>?\bdTefRGJKLOMg8cMOShACIEP&=\\i^J9LOMN8\\M]S\u00158\\Q[LOLNA FHQC<;ACS>MOZj=\u001ek9L]S>=\\<>MOI9lm:EAC?FH=\b=\\I179?@=BP\u0014anQ <hRmA opMOI9lq<>=B8\\Q RGRG=\bIEP^=\\<>?\bVKMNIrA P9P^MOS>MOQ I&S>QqS>:9=sA 7^YS>Q RmACS>MN8t8\\LNA ?@?@M]kE8\bACS>MOQ[I\u0014Q a\u0019R\u001e79?@MN8hMOI`S>Qs?USf_^LO=t8\bAuS>=\bl[QC<>MN=\\?hF9A ?@=BPmQ[I=\\ipS@<;A 8cS>=BP%AC7EPKMOQ&an=BACS>7^<>=\b?\bd1vw:KMN?sJEACJH=\\<\u001e?@7K<>Zj=\\_^?t<>=\b?@=\bAC<;8>:\u0007MNI`S>QR\u001e79?@MN8\bA LjS;AC?US>=[V[<>=\\Z^MO=\\XW?xR\u001e79?@MN8x<>=B8\\Q[RGRG=\\IEPK=c<y<>=\b?@=\bAC<;8>:yV`ACIEPzQ[7^S@YLOMOI9=\b?{JK<>Q RGMN?@MOIKlzPKM]<>=B8cS>MOQ[IK?\bd|efIqJ9AC<@S>MN8\\79LNAu<BV`Xw=WLO=BAC<>IK=BPqS>:EACS}PK=cYRGQ l <;A JK:9MN8&ACIEP-JH=c<>?@Q[IEACLOMOS~_\u0012aA[8;S>Q <>?m:EABZj=&FH=\\=\bI-?@:KQBXWI-S>Q\u0005FH=aA[8cS>QC<>?MOIK979=\bI98\\MOI9l2R$79?@MN8bJK<>=\\an=\\<>=\bIE8c=[d9QC<RGQ`QpPV}S>:K=&RmA MOIaA[8cS>QC<>?&AC<>=&S>=\\RGJHQKVwS>Q IEA LOM]Sf_`VP^MO?US>MNI98cS>MOZj=\bIK=\b?@?mQ a3<>:`_pS>:9RACIEPJKMOS;8>:\u0014:K=\bMOl[:`SBd\n1.INTRODUCTIONe~I\nS>:K=qJEAC?UStSfX}=\bLOZj=$_`=BAu<>?\bVyS>:K=\\<>=q:9A ?3FH=\b=\bI\nMOI`S>=\\<>=\b?USzMOI2S>:K=qPK=cYZj=\\LNQ J9RG=\bI`S$Q atS>=B8>:9IKMNp7K=\b?$S>:9ACSGJ^<>QCZpMNPK=\u0014JH=\\<>?@Q[I9A LOMO?@=BP\u00178\\Q IpS>=\\IpSS>Q%7K?@=\\<>?\bd(vw:9=&Sf_^JH=Q asA JKJ9LOMN8\bACS>MOQ I9?b:EABZj=&MOIE8cLN79PK=BP+kEL]S>=\\<>MOI9lQCa{IK=\\XW?RG=\b?@?>A l =\b?\bVJK<>=\b?@=\\IpS>MOIKlqLOMN?US>?hQ a|?US>Q <>MO=\b?hQ <\u0015AC<@S~XwQ <>obS>:EAuSA&7K?@=\\<sRmAB_2FH=GMOI`S>=\\<>=\b?US>=BP\u0005MOIVA I9P\u0005?@Q1Q[Idr6\u0014Q[?US\u001eQ aTS>:K=\b?@=bA J^YJKLNMN8\bAuS>MOQ[I9?h:EABZj=3ACJ9JKLNMO=BP\u0014A\u001eS>=B8>:9I9MN`79=\u0015o^IKQBXWI&A ?bU8cQ[LOLNA FHQ <;AuS>MOZj=k9LOS>=c<>MNIKl[^d\u0012vw:9MO?qMOIpZjQ[LOZj=\\?q8\\Q LNLO=B8;S>MNIKl\u0005QCS>:9=\\<$79?@=\\<>?\bQ J9MOI9MOQ[IK?qQ a:KQuX\rl[Q`QpPmQ <}79?@=\\an79LA IGM]S>=\bRMO?\bVKA IEP$S>:K=\bIq<;A IKopMNIKlzMOS>=\\RG?{F9A ?@=BPQ I\u0014S>:9MO?WMOIKaQC<>RmACS>MOQ[IaQ <J^<>=\b?@=\bI`S;ACS>MOQ[IS>Q$S>:9=t7K?@=\\<BdD:KMOLN=M]SqRmA\b_%FH=AC<>l 79=BP%S>:EAuS$S>:K=\\<>=:9A ?qFH=\\=\bI\u0007?@Q[RG=b?@7E8\b8c=\b?@?XWM]S>:\u0012S>:9MO?$S>=B8>:9IKMg`7K=[VS>:9=\\<>=bMO?qR$798;:%<>Q`Q RaQC<GMORGJK<>QuZj=\bRG=\\IpSBdxAu<;A LOLO=\bL`S>QwS>:9={PK=\bZj=\\LNQ J9RG=\bI`SQCa98cQ[LOLNA FHQ <;AuS>MOZj=}kEL]S>=\\<>MOIKl:9A ?FH=\b=\bI8\\Q I`S>=\bI`S@Y~F9A ?@=BP+kEL]S>=\\<>MOI9l^dvw:9MO?bMN?A IACJ9J^<>QjA[8>:\rS>:9ACSbS@<>MN=\\?bS>Q=\\ipS@<;A 8cSq7K?@=\\a7KLwMOIKaQC<>RmACS>MOQ[I\u0007ag<>Q[RS>:9=mM]S>=\bRG?\u001eQCaWS>:9=b8\\Q[LOLO=B8cS>MOQ IS>:9ACSbAC<>=\u0014l[Q`QpP\rMOIEP^MN8\bACS>QC<>?bQCatS>:9=\\MO<m7K?@=\\a7KLOI9=\b?@?qanQ <A27K?@=\\<BdDe~SMO?t8cLNQ ?@=\bL]_1<>=\bLNACS>=BP&S>QqS>:K=zk9=\bLNP&Q a{MOIKaQC<>RmACS>MOQ[I&<>=cS@<>MN=\\Z[ACLVHXW:KMN8;:ACMNRG?WS>QmPK=\\Zj=\bLOQ[J1FH=\\S@S>=c<S>=\b8;:KI9MN`79=\b?WS>QGLOQp8\bACS>=\u001ePKQp8\\7KRG=\bI`S>?wS>:EAuS?>AuS>MN?Uag_1A$7K?@=\\<B ?WMNI^aQC<>RmACS>MOQ[I1IK=\b=BPd7^<@<>=\bI`S>LO_`V{RGQ[?USqR$79?@MN8G<>=B8\\Q RGRG=\bIEP^=\\<$?@=c<>Z^MN8\\=\\?mAC<>=&F9A ?@=BP\u0012Q[I=BP^M]S>Q <>MNA LTPKACS;ApVx<>=B8\\Q[RGRG=\\IEP9AuS>MOQ[I9?zl LN=\bA I9=\bP\u0005an<>Q[RS>:9=Ge~I`S>=\\<>I9=\\S7K?@=\\<x8\\Q RGR$7KI9M]Sf_`V\\A I9P\u0015FK<>QBXW?@MOI9lJEAuS@S>=\\<>I9?\bdy\u0015QBXw=\\Zj=\\<BVuM]SMO?<>=B8\\Q[lCYIKMN?@=\bP-S>:EAuS\u00148c7K<@<>=\bI`SA JKJK<>QjA 8;:K=\b?m:EABZj=1MORGJHQ <@S;ACI`SmLNMORGM]S;ACS>MOQ I9?\bVMOIE8cLN79PKMOI9lzMOIEA PK=B`79ACS>=W<;ABXDP9ACS;AGMOIGS>:9=h8\bA ?@=Q ay=BP^MOS>QC<>MNA LMOIKaQC<@YRmAuS>MNQ I;VLgA 8;o\nQ aTp79A LOM]Sf_28\\Q IpS@<>Q LhMOI\nS>:9=m8\bAC?@=GQ aT7K?@=\\<sJK<>=\\an=\\<@Y=\bI98\\=\b?c;VACIEP\u0014LNA 8;oQ a79?@=c<\u0015J^<>=\\a=\\<>=\\IE8\\=\b?aQC<\u0015IK=\\X<>=\b8\\Q <;P^MNIKl[?\bdWt=cY<>MOZpMOI9lGa=BAuS>7K<>=\b?han<>Q[RS>:9=zR\u001e79?@MN83MOS>?@=\\LOa~V<;ACS>:9=c<hS>:9A I&<>=\bL]_^MOI9lmQ[I8\\7K?US>Q[RG=\\<\u001eFH=\b:9AuZpMOQ[7^<qMO?$J9AC<@S>MN8\\7KLgAu<>L]_\u0007MORGJHQ <@S;A I`SsaQC<qMOI`S@<>QpPK7E8;YMOI9l\u001eI9=\\XR\u001e79?@MN8 dD<>=B8\\Q RGRG=\bIEP^=\\<w?U_^?US>=\bRXwQ[7KLNPbIK=\bZj=\\<T?@7Kl[l =\b?USIK=\\X\u0012AC<@S>MO?US>?xF9A ?@=BP3Q[IKL]_zQ[I\u001e8\\79?US>Q RG=\\<\u0019FH=\b:9ABZ^MOQ 7K<BVCM]a9IKQ\u00158\\7K?US>Q[RG=\\<=\bZ[=\\<zMOI9M]S>MNA LOL]_\u0005?@=\bLO=B8;S>=BP2S>:9=\u001eI9=\\XAu<@S>MN?USBdGhI9QCS>:9=\\<tLNMORGM]S;ACS>MOQ I%MO?S>:K=\u001eXTA\b_1S>:K=\u001e<>=B8\\Q RGRG=\bIEPKACS>MOQ[IK?tAu<>=$JK<>=\\?@=\bI`S>=BPhRGQ[?US3?U_^?US>=\bRG?7K?@=sIKQqRGQ <>=tS>:9A I1Aq?@MORGJKLN=3LOMO?UShQ ax<>=B8\\QC<;PKMOI9l ?\bdW97^<@S>:9=\\<BV^S>:9=\\<>=\nPermissiontomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroomuseisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.\n8\n \n2002IRCAM-CentrePompidou\n:9A ?3FH=\b=\\I2LOM]S@S>LO=G=\\¡Q <@StS>Q79?@=qopIKQuXWLO=BP^l[=$ag<>Q[R¢R\u001e79?@MN8\u001eJK?U_K8;:KQ[L]YQ l _G<>=\b?@=\bAC<;8>:S>QzMOIKaQC<>R£S>:9=h8>:9Q[MN8\\=hQ aya=BAuS>7K<>=\b?}S>Qs=\\ipS@<;A[8;S}an<>Q RAC7EP^MNQ^VEQC<S>Q\u001ekEL]S>=\\<WR$7K?@MN8\u0015?@=\bLO=B8cS>MOQ[IK?\bde~IS>:KMN?TJ9A JH=\\<BV^Xw=h<>=\bJ9:^<;A ?@=tS>:9=h<>=\b?@=BAC<;8>:1`79=\\?US>MNQ IXWM]S>:&As?@JH=\\Y8cMOkE8\u0015aQp8\\7K?WQ[I\u0014R\u001e79?@MN8<>=\b8\\Q[RGRG=\bI9PK=\\<}?U_^?US>=\bRG?\bVKA IEP8\\Q[IK?@MNPK=\\<WS>:9=RmACMOI2aA 8cS>Q <>?sS>:EAuS3XwQ[7KLNP%AC¡=B8;SsS>:K=q?@7E8\\8\\=\b?@?sQ aTR\u001e79?@MN8\u001e<>=B8\\Q RqYRG=\\IEPK=c<S>=\b8;:KI9Q[LOQ l _`d\u0005=x<>=\bZpMN=cX2X}Q <>o\u0015ag<>Q[RAwZ[Au<>MN=cSf_tQCa^?@Q[7K<;8c=\b?\bVMOI98\\LO7EPKMOIKls<>=\b?@=\bAC<;8>:mag<>Q[RS>:9=WkE=\bLNPK?{Q ayJ9?U_K8>:9Q[LOQ l _mACIEPGRmAC<>oj=cS@YMOIKlS>:9ACS\u0019<>=\bLNAuS>=\b?\u0019S>QhR\u001e79?@MN8\bACL[S;A ?US>=[dxanS>=c<A[PKP^<>=\b?@?@MOI9lWS>:9=\\?@=}RmA MOIaA[8cS>QC<>?A IEPPKMO?>8\\7K?@?@MNIKl\u001eS>:K=t8\\Q IE8\\LO79?@MOQ I9?T<>=BA[8>:9=\bPban<>Q R£S>:9=hZ[Au<@YMOQ 79?tF^<;A IE8>:9=\\?tQ a|<>=\b?@=BAu<;8;:r<>=\bLNACS>=BP1S>QGS>:9MO?tJ^<>Q[F9LO=\bR&V9Xw=qP^MO?US>MNLOLA2?@=\\SbQ a3l 79MNPK=\bLOMOI9=\\?AC?GXw=\bLOLhA ?b`79=\b?US>MOQ[IK?GS>:EACSG<>=\\RmA MOI-S>Q2FH=<>=\\?@Q[LOZj=BPd|\u0005=t<>=\bLNACS>=tS>:KMN?TS>Q$S>:K=3A MORG?WQ a\u0019Q[7K<W<>=\b?@=\bAC<;8>:&JK<>Qu¤U=B8;SBd\n2.THEPROBLEM^<>Q[R¥S>:9=W79?@=c<B ?|JHQ[MOI`S{Q aZpMN=cX3VjS>:9=TJK7K<>JHQ[?@=WQCaAtR\u001e79?@MN8}<>=B8\\Q RqYRG=\\IEPK=c<T?U_^?US>=\bR¦MO?WS>Q\u001e<>=B8\\Q[RGRG=\\IEPGR$7K?@Mg8S>:EACSTS>:K=t79?@=c<XWMOLOLFH=MOI`S>=\\<>=\b?US>=\bPqMNIdxefIGQC<;PK=\\<|aQC<{S>:K=W79?@=\\<|S>QtXTACIpS|S>Q379?@=TS>:K=T?U_K?US>=\\RM]SR$7K?USTFH=3?@MORGJ9LO=\u0015S>Qq7K?@=[V^XWMOS>:\u0014A$RGMOI9MOR$7KR£QCaxMOI9J97^ST<>=B`79M]<>=BPag<>Q[RS>:9=W79?@=c<BdhL]S>=\\<>I9ACS>MOZj=\bL]_`VjS>:K=\\<>=TR$7K?US{FH=A38\\LO=BAu<TACIEPqQ[FpZpM]YQ 79?TMOIE8c=\bI`S>MNZ[=hS>Q\u001eS>:K=\u00157K?@=\\<TS>:EAuSTRGQ <>=h=\\¡Q <@STMOIJK<>QuZpMgP^MOI9lsMNIKJ97^SXWMOLOLLO=BA PbS>QqFH=cS@S>=\\<w<>=B8\\Q RGRG=\bIEPKACS>MOQ[IK?\bd|vw:9=t7K?@=\\<WRmA\b_GXTACIpSTS>Q<>=cS@<>MN=\\Zj=3R$7K?@Mg8hFEA ?@=\bP\u0014Q[IJK<>=\\an=\\<>=\bIE8c=\b?\bVE?US~_KLO=3QC<hRGQ`QpPd\n3.PSYCHOLOGICAL FACTORSANDMU-\nSICALTASTE\u0017:9=\bI\u0014PK=\b?@MOl[IKMOI9lGA$?U_^?US>=\bRS>:EACST<>=\b8\\Q[RGRG=\bI9PK?wR\u001e79?@MN8 V^M]SMN?W7K?@=\\Yan79LS>Q%LO=BAC<>I\rag<>Q[R§=\\i^MO?US>MOI9l\u0005<>=\b?@=\bAC<;8>:DMNI`S>Q\u0005aA[8cS>QC<>?bS>:9ACSAC¡=\b8cSR\u001e79?@MN8\bACL^S;AC?US>=[defIsS>:9MO??@=B8cS>MOQ[IqX}=T?@7K<>Zj=\\_z<>=\b?@=\bAC<;8>:qS>:EACS|:9A ?Qp8cY8c7K<@<>=BP\u0014MOIbS>:9=tLNA ?USh=\bMOl[:`S~_b_j=BAC<>?WQ[IR$7K?@Mg8\\A LJK<>=\\an=\\<>=\bIE8c=\b?\bd}6\u0014Q[?USQCahS>:K=q<>=\b?@79L]S>?$8\\Q[RG=Gan<>Q R¨X}Q <>o%F`_2?@Qp8\\MNA LTJK?U_98>:9Q LOQ[l[MO?US>?\bV{F97^S?@Q RG=t8\\Q RG=\b?}ag<>Q[R£S>:K=\u0015RGQ <>=hA JKJ9LOMO=BPbk9=\bLNPGQ axP^=\bRGQ[lC<;A JK:9MN8\\?}anQ <RmAu<>oj=\\S>MOI9l^d©Q[RG=wQCaES>:9=w<>=\\?@=BAC<;8>:G8\\M]S>=BPsJ979FKLOMN?@:K=BP\u001eFH=caQ <>=3ª\b«j¬ ­t?@:KQBXw=BPq8c79L]YS>7^<;A LtFKMNA ?@=\b?\bdefIA P9P^MOS>MOQ I\u0017S>:9=\\<>=1XTAC?1A%?US@<>Q[IKl\u0012F9MNA ?\u0014A ljACMOI9?USJHQ J97KLgAu<R$79?@MN8 V`anQ <=\\iKA RGJKLO=[V^Q[I9=tA 7KS>:KQ <P^=\\kEIK=BPbM]SAC?b~R$7K?@Mg8S>:9ACSzMO?t<;A IKoj=BPrF`_r8c<>M]S>MN8\\?sA ?tS;A\bXP^<@_`VFEACIEACLV\u0019A I9P\nMOI9?@MOJKMgP^\u0005® «C¯d°=\b?@=BAu<;8;:K=\\<>?WFH=\bLOMO=\bZj=BPmS>:EACSWQ I9=3MORGJHQ <@S;ACI`STJ97^<>JHQ[?@=tQCaxR$7K?@Mg8\\A L=\bPK7E8\\ACS>MOQ[I%XwA ?\u001eS>Q%±n²\u0015³E´Uµ ¶ ·A1?US>7EP^=\bI`SB ?sR\u001e79?@MN8\bACL{S;A ?US>= d1\u0015QBXTY=\\Zj=\\<BVS>:9=$=ciKJH=c<>MNRG=\\IpS>?tA IEP1S>:9=\\MO<\u0015<>=\b?@79L]S>?zA JKJH=BAC<3S>QFH=q?@Q[7KIEPACIEP+8\bA I\u0012FH=79?@=BP\u0012A ?qA\n?US;Au<@S>MNIKl\u0005JHQ MNI`S\u001eaQ <q=\\i^JH=\\<>MORG=\bI`S>?$RGQ <>=S;Au<>l[=\\S>=\bP\u0014S>Q$R$7K?@MN8h<>=B8\\Q RGRG=\bIEP^=\\<T?U_^?US>=\bRG?\bd\n3.1Personality,Demographics andMusicPref-\nerencee¸S:EAC?xFH=\b=\bI$?@:KQBXWI$S>:EAuS8\\=\\<@S;ACMNIqA ?@JH=B8cS>?QCaHJH=\\<>?@Q[I9A LOM]Sf_$AC<>=T8cQ <@Y<>=\\LgAuS>=BP\u001eXWM]S>:qR\u001e79?@MN8}JK<>=ca=\\<>=\bI98\\=[dxe~I\u001e<>=\b?@=\bAC<;8>:qJ97KF9LOMO?@:9=BPsMNIbªB« ¹[«^Vº{7K<@SG7K?@=BP%S>:9=\u0014MOI`S@<>QuZj=\\<@S$Zj=\\<>?@79?\u001e=\\ipS@<;ABZj=\\<@SGACIEP\u0007?US;A FKLO=\u0014Zj=\\<>?@7K?7KI9?US;ACF9LO=TJH=\\<>?@Q[I9A LOM]Sf_\u001eS>=\b?US>?|PK=\bZpMO?@=BP$F`_\u001e»_^?@=\bI98;osACIEP$8cQ[IE8cLN79PK=BPS>:9ACSz?US;ACF9LO=q=\\ipS@<;ABZj=\\<@S>?tJ^<>=\\a=c<\u001e?@Q LOMgP\nJK<>=BP^MN8cS;A FKLN=$R$79?@MN8 Vy?US;A FKLO=MOI`S@<>QuZj=\\<@S>?|S>:9=RGQ <>=h8\\Q[l I9M]S>MOZj=\u0015QCa\u00198\\LNAC?@?@Mg8\\A LyA IEPqF9AC<>Qp`79=h?USf_^LO=\b?\bV7KI9?US;ACF9LO=}=\\ipS@<;ABZj=\\<@S>?S>:9=<>Q[RmACIpS>MN8?USf_^LO=\b?\u0019=\\i^J^<>=\b?@?@MOI9lQCZ[=\\<@S\u0019=\bRGQCYS>MOQ I9?\bV^A I9P$79IK?US;A FKLN=WMOI`S@<>QuZj=\\<@S>?S>:9=TRGQ <>=TRs_^?US>Mg8\\A LEACIEP$MORGJ^<>=\b?UY?@MOQ I9MO?US>MN8G<>Q RmA I`S>MN8\u001eXwQC<>op?bP^MO?>8\\79?@?@=BP2MNI-®]ªB«C¯wA I9P-® ¼ ­u¯;d6\u0014QC<>=AReviewofFactorsAffectingMusicRecommender Success<>=B8c=\bI`S>LO_`VyM]Ss:EA ?3FH=\\=\bI\u0005?@:KQuXWI2S>:EACS3S>:K=$LO=\bZj=\\L{QCaWA l l <>=\b?@?@MOZj=\bIK=\b?@?8\\QC<@<>=\bLNACS>=\b?mXWM]S>:\rR\u001e79?@MN8\bA LW?US~_KLO=[V}XWMOS>:-RGQ <>=1ACl[lC<>=\b?@?@MOZj=rJH=\bQ[JKLN=FH=\bMOIKl\u0007RGQ <>=&LOMOoj=\bL]_+S>Q\u0007=\bI ¤fQu_-:9=\bAuZ`_+RG=\\S;ACLhQC<:9AC<;P-<>Qp8>o+R$7KY?@MN8G®]ªB«C¯d©S>7EPKMO=\b?Q a}PKM]¡=\\<>=\\IpSh8\\7KL]S>7K<;A Ll <>Q[7KJ9?\bV?@:KQuX PKM]¡=\\<>=\bI`S\u0015PKMO?US@<>MOF97^YS>MOQ[IK?QCasR$79?@MN8\bACLhJ^<>=\\a=c<>=\bIE8\\=\\?\bd9Q <=\\iKA RGJKLO=[V\u0002\u0001jACJEA IK=\b?@=2A[PKQCYLO=\b?>8\\=\\IpS>?W:9AuZ[=tAz:9MOl[:K=\\<WLOMNo[=\bLOMN:KQ`QpP\u0014Q a=\bI ¤fQB_KMOIKlq8\\LNA ?@?@MN8\bACLQC<x¤@A\u0004\u0003\u0005\u0003R\u001e79?@MN8{S>:EACI\u001eS>:K=\bM]<|hRG=\\<>MN8\bACIq8\\Q 79I`S>=\\<>JEAu<@S>?W® ¹ ¹C¯d|vw:9=}?US>7EP^_\u001eACLN?@Q8\\Q IE8\\LO79PK=BP&S>:EAuShS>:K=\\<>=zXTA ?hl[=\bIK=\\<;A LOL]_\nAmPKMO?@LOMOoj=\u001eQCa}:K=BABZ`_\u0014RG=\\S;ACLP^79=\u0015S>Q$S>:K=3?@Qp8\\MNA L8\\Q[IKI9Q S;AuS>MOQ[I9?hA ?@?@Qp8cMgAuS>=BP\u0014XWM]S>:\u0014S>:9=3?US~_^LN= d©S>7EPKMO=\b?bQ a\u001eACl[=2A IEP\rPK=\bRGQ l <;A JK:9MN8\\?b:EABZj=r?@:9QBXWI\u0017S>:9ACSJH=\bQ[JKLN=J^<>=\\a=\\<sR\u001e79?@MN8sS>:9ACS3S>:9=c_1Xw=\\<>=G=\\i^JHQ ?@=BPrS>Q&ACSsA\u00148;<>MOS>MN8\bACL}JH=\\<>MOQpPQCaxS>:9=\bM]<WLOM]a=s8\\79LORGMOIEAuS>MOI9lGAuSS>:K=3A l =3Q a|ACFHQ[7KS\u0015¼C¹^d ¬\u0014®]ª\u0007\u0006 ¯d\bIK==BAu<>L]_\u0007?US>7EPp_%=\\iKA RGMOIK=BP2S>=\bRGJHQ1J^<>=\\a=\\<>=\\IE8\\=\b?\bd%vw:9=\b?@=bXw=\\<>=?@:KQuXWIsaQC<}8c=\\<@S;A MOI$Qp8\\8\\79J9ACS>MOQ[I9A L^l <>Q[7KJ9?\u0019S>QhRmACS;8>:zS>:K=}S>=\bRGJHQWQ aS>:K=\u0015Qp8\b8\\7KJEAuS>MNQ I2P^MO?>8\\79?@?@=BPmMOI\u00149AC<>I9?UX}Q <@S>:r® \tC¯\n:EACJKS>=\\<\u0015ªC;d|9QC<=\\iKACRGJ9LO=[VWPp<>=\b?@?@RmA oj=c<>?bJ^<>=\\a=c<@<>=BP\u0017A\u0005RGQpP^=\\<;ACS>=\bL]_-?@LOQuX\u0003S>=\bRGJHQ^VXW:K=\\<>=BA ?WSf_^JKMN?US>?TJ^<>=\\a=c<@<>=BP&A\u001eaA ?USS>=\bRGJHQKd ?US>79P^_Fp_\n©8>:p79=\b?@?@LO=\\<\u001e® ¼ «u¯x?@:9QBXw=\bP\u0014S>:EACSh?@Qp8\\MOQ Y¸=B8cQ[I9Q RGMN8 VA l =ACIEP?@=ci&P^MO¡=c<>=\bIE8\\=\\?WAC<>=z8\\QC<@<>=\bLNACS>=BP\u0014XWM]S>:&R\u001e79?@MN8hJ^<>=\\a=\\<>=\\IE8\\=zP^MOagYan=\\<>=\bIE8c=\b?\bdxefI3J9AC<@S>MN8\\7KLgAu<BV[:KMO??US>7EPp_tQCaKR\u001e79?@MN8xJK<>=\\an=\\<>=\bI98\\={anQp8\\79?@MOI9lQ I\u0007JH=\bQ[JKLN=GMOI%e~IEP^MgACIEA\u0014?@:KQBXw=BP\u0005S>:9ACSs79JKJH=\\<s8\\LNA ?@?sXwQ[RG=\\I\u0005Xw=\\<>=RGQC<>=\u0014LOMOoj=\bL]_\u0012S>Q2=\bIC¤UQB_-8\\LNA ?@?@MN8\bACL\u0015R$7K?@MN8mXW:K=\\<>=BA ?GX}Q <>opMOI9l%8cLgAC?@?RG=\bI2Xw=\\<>=mRGQ <>=GLOMOoj=\bL]_2S>Q1=\bI ¤fQu_2:9MOLOLOF9MOLOL]_\u0007R$7K?@Mg8Cd\u0014h=mXwA ?s79I^YACF9LO=$S>Q<;ACI9o&S>:K=\u001eaA 8cS>Q <>?sMOI2QC<;PK=\\<zQCawMORGJHQ <@S;ACIE8\\=$A ?sPKM]¡=\\<>=\\IpSaA[8cS>QC<>?Xw=c<>=\nRGQC<>=1MOIKE7K=\bI`S>MNA LWaQ <&PKM]¡=\\<>=\bI`SmJKMN=\b8\\=\b?bQ asR$7K?@Mg8CdI9Q S>:K=\\<sQ[FK?@=\\<>Z[AuS>MOQ[I%S>:EAuS\u001eXwA ?sRmA[P^=qXTA ?sS>:EAuSsS>:K=mRGQ <>=qaAuYRGMOLOMNAC<sAJKMO=B8\\=[VyS>:9=$RGQC<>=$LOMOoj=\bL]_\nM]S3XTA ?tS>QFH=q=\bIC¤UQB_`=BPd\u001eD:KMOLN=RGQ ?USmQ atS>:K=\b?@=&Q[FK?@=\\<>Z ACS>MOQ[IK?GJK<>Q[F9A FKLO_+?US>MOLOL\u0015ACJ9JKLO_\u0007S>QpPKA\b_`VwS>:9=R\u001e79?@MN8\bA Ll[=\\IK<>=\b?WRmA\b_PKM]¡=\\<Bdvw:K=\\<>=t:EABZj=FH=\b=\bIb?US>7EP^MO=\b?}S>:EAuST8\\Q LOLN=\b8cS>=BP\u0014PKACS;AzQ[IbJEAu<@S>Mg8cMNJ9A I`S>?\bAuS@S>MOS>79PK=TS>QzR$7K?@Mg8CVj<;A IKl[MOI9l3an<>Q Rl =\bI9=c<;A LPKMO?@MOI`S>=\\<>=\b?US|S>Qz?@JH=\bIEPpYMOI9lLNAC<>l =wJHQ <@S>MOQ[IK?\u0019Q a^S>:9={P9A\b_3LOMO?US>=\bIKMNIKlS>Q^V[QC<xJ9LNA\b_^MOI9lWR$7K?@Mg8W® \tC¯dI%MOIEPKMOZpMNPK79A L ?qACS@S>M]S>79PK=GS>Q1R$7K?@Mg8qMO?sLOMNo[=\bL]_\u0005S>QrFH=q<>=\bLNACS>=BP%S>QS>:K=\bM]<TR$7K?@Mg8\\A LEJ^<>=\\a=\\<>=\\IE8\\=[Vp:KQBXw=\bZj=c<BV`Xw=hanQ[7KIEPqa=\\X\u0017?US>7EPKMO=\b?|S>:EAuS=\\i^JKLNQC<>=BP\rS>:9MO?m<>=\bLNACS>MOQ[IK?@:9MOJyd\n©8;:p7K=\b?@?@LO=\\<B ??US>79P^_® ¼C«C¯3?@:KQBXw=BPS>:9ACS|XwQ[RG=\\IqXw=\\<>=WRGQ <>=WMOI`S>=\\<>=\b?US>=BPqMOIGR\u001e79?@MN8}S>:EACIGRG=\bIyV[A ?|Xw=\bLOLAC?q:EABZpMOI9lr?@Q[RG=\\XW:9ACSqPKM]¡=\\<>=\bI`S$S;AC?US>=[d%hQBXw=\bZj=c<BV{M]S\u001eXwQ[7KLNP\u0012FH=P^M\u000b\nG8\\79L]S3S>Q&?@=\bJ9AC<;AuS>=qS>:9=$Z AC<>MOQ 79?3aA 8cS>Q <>?sQ a}l[=\bI9PK=\\<BV\u0019ACS@S>M]S>7EP^=[V8\\7KL]S>7K<;A LMO?@?@79=\b?\bVEA I9P\u0014JH=\\<>?@Q[I9A LOM]Sf_ban<>Q R0S>:9MO?W?US>7EP^_`d\n3.2PerceivedMusicQualityEAu<>I9?UX}Q <@S>::EAC??@:KQuXWI\u0017S>:9ACSmS>:9=c<>=\nMO?\u0014A\u00078\\Q[IK?@MN?US>=\\IE8c_\rMNID:KQBXJH=\bQ J9LO=$<;ACI9or8\\LNA ?@?@MN8\bACL}R\u001e79?@MN8sMOIrS>=\\<>RG?tQCaT`7EACLNM]S~_\u0012® \tu¯dq97^<@S>:9=\\<?US>79PKMO=\b?t:9AuZ[=\u001e?@:KQuXWIrS>:EAuS\u0015S>:K=\u001e?>ACRG=$ACJ9J9LOMO=\b?tS>QmJHQ[JK79LNAC<tR$79?@MN8P^MN?>8c79?@?@=BP\u0014MOI%®]ªB¼u¯;dD:K=\bI\u0007M]Sq8cQ[RG=\b?3S>QrPK=\b8\\MNPKMOI9l1Q I%S>:9=m=\bI ¤fQB_KRG=\\IpSzQCahA&J9MO=B8\\=mQ aR\u001e79?@MN8 VjS>:K=\\<>=AC<>=WaA[8;S>Q <>?}S>:9ACS{:EABZj=WLOMOS@S>LO=WS>QsPKQtXWM]S>:GS>:9=W?@Q[7KIEPQCa}S>:9=\u001eJ9MO=B8\\=\u001eM]S>?@=\bL]afd$?US>79P^_&ag<>Q[R Au<>Q[79I9P&S>:9=\u001eS>MORG=sQ a{%Q <>LNP%AC<Te@exS>:EAuSTAC?@?@Qp8\\MNACS>=BPS>:K=hLNACFH=\bLO?q~<>Q RmA I`S>MN8\\KV|\r\f\u0015A\u0004\u0003\bMO^V9QC<WA J^YJKLNMO=BPmI9QsLNA FH=\bLES>QzS>:9=R$7K?@MN8 V^AC¡=B8;S>=BPqS>:9=\u0015LOMO?US>=\bIK=\\<>?\b^=\bI ¤fQu_^RG=\bI`SQCaES>:9={R$79?@MN8 d\n©MORGMOLNAC<>L]_`VCXW:9=\bI\u001eA<>=\bZ[=\\<>=BPs8\\Q[RGJHQ ?@=\\<B ?xIEA RG=XTAC?AC?@?@Qp8\\MNACS>=BP\u0007XWM]S>:\u0012A&J9MO=B8\\=mQ aR\u001e79?@MN8qM]SsXwA ?\u001eRGQ <>=G=\bIC¤UQB_`=BP\u0005S>:9A IXW:K=\bI&A$LO=\b?@?WopI9QBXWI8\\Q[RGJHQ ?@=\\<B ?WIEA RG=XTAC?7K?@=BP2P^MO?>8\\79?@?@=BPbF`_EAu<>I9?UX}Q <@S>:%® \tC¯;d\n3.3PerceptionofStyleandMoodvw:K=\\<>=wAu<>={RmACIp_tPKM]¡=\\<>=\bI`Sy?USf_^LO=\b?\u0019QCa9R\u001e79?@MN8 dvw:9={hLOLg6\u001479?@MN8\u000f\u000e\u001579MNP^=:9A ?\u0015¬ ¹^ªzl =\bIK<>=\\?\bVERmA\u0004\u0003\bQ I\u0011\u0010pª\b«^VACIEP\u00146&{¹pd 8cQ[R\u0012\u0006j¹[­&® ¼ ¬u¯d}D:K=\bIM]Ss8\\Q RG=\b?tS>Q&RGQ`QpPVy:9QBX}=\bZj=\\<BVS>:9=G8\\ACS>=\bl Q <>MO=\b?\u001eAC<>=qIKQ Ss`79M]S>=q?@QIp79RG=c<>Q[79?\bd\u0019\u0015=\bZpI9=c<TPK=\\Z^MO?@=BP\u0014\u0013s8\\LO79?US>=\\<>?W8\\Q I9?@MO?US>MOI9lqQCa\u0015\t\u0016\u0010sRGQ`QpP^?MOI1S>Q S;ACLVXW:KMg8>:1Xw=\\<>=sS>:9=\bI1RGQpP^M]kE=BP&F`_19AC<>I9?UX}Q <@S>:rFEAC?@=BP1Q[I\n=ciKJH=c<>MNRG=\\IpS;ACL`=\bZpMNPK=\bI98\\=wMOI`S>Qsª\b­\u00158\\LO79?US>=c<>?|QCaHAWS>Q S;A LpQCa¬[¼WRGQ`QpPK?\bVXWM]S>:2Q[I9=$RGQ`QpP2Qp8\b8\\7^<@<>MOI9l\u0014MOI\nSfXwQ\u00148\\LO79?US>=\\<>?m® \tC¯dq\u0019=\\<;8\\=\bJ^S>MNQ I\u0005QCaRGQ`QpP\u0007MO?$aA M]<>L]_\r8cQ[I9?@MO?US>=\bI`SqXWM]S>:KMNI\u0012?@7KF8\\79L]S>7K<>=\\?\bV}FK7KSG8\\A I\u0012Z[Au<@_anQ <tPKM]¡=\\<>=\bI`S\u00158\\79L]S>7K<;ACLlC<>Q[79JK?\bdWefI9PKMOZpMgP^7EACLN?RmA\b_PKM]¡=\\<hMNI&S>:K=\bM]<JH=c<;8\\=\bJKS>MOQ I1PK7K=\u0015S>QqJH=\\<>?@Q IEA Ly=\\i^JH=\\<>MO=\bIE8c=3:9QBXw=\bZ[=\\<Bdvw:K=2an=BACS>7^<>=\b?\u0014S>:EAuS&?@=\b=\\RG=BP\u0017S>Q+FH=\\?US1PKMO?US>MOI9l 79MO?@: RGQ`QpPDanQ <rA?@=cS1Q a\u001eJ9MO=B8\\=\\?\u0014Xw=\\<>=\nS>=\bRGJHQKVWS>Q IEA LOM]Sf_anQ <1=ci9ACRGJ9LO=rRmAu¤fQ <1QC<RGMOIKQ <boj=\\_E;VPKMO?US>MOIE8;S>MNZ[=\bI9=\b?@?bQ a3<>:`_pS>:9R&VwA I9P\rJ9M]S;8>: :9MOl[:DQC<LOQBX\u0015;dr9QC<s=\\iKA RGJ9LO=[V?@Q[LO=\bRGI%R\u001e79?@MN8$S>=\bI9PK?tS>Q1FH=G?@LOQBX3VxXWMOS>:\u0012AP^=\\kEIKM]S>=t<>:p_pS>:KR&VpA I9P\u0014ACShA$LOQBXJ9M]S;8;:2® \tC¯d©Sf_^LO=[VH:KQuX}=\bZj=\\<BV:9A ?hFH=\b=\bIr8\\LNA ?@?@M]k9=BP2A 8\b8\\QC<;PKMOI9lmS>QbAmZ AC<>MO=\\S~_&QCaJ9AC<;ACRG=\\S>=\\<>?\bdxA 8;:K=\\S&ACIEP\nA\u0017\u0003BA L]_\u0017anQ[79I9P-S>:EAuS&8cLgAC?@?@M]kH8\bAuS>MNQ I9?X}=\\<>=\nFEA ?@=\bP\rQ[IDl =\bI9=BACLOQ[l[MN8\bACLV3:9MO?US>Q <>MN8\bACLV\u0015l =\bQ[lC<;A J9:KMN8\bA LV\u0015an79I98cYS>MOQ IEA LVKMOI9?US@<>79RG=\\IpS;ACLV9AC?wXw=\bLOLyA ?TQCS>:9=\\<T=\\I^7KRG=\\<;ACS>=\bPGS~_^JH=\b?3® ¼[¬u¯d\n3.4OtherHumanFactorse¸SW:EAC?TFH=\b=\bIb?@:9QBXWIS>:9ACST7K?@=\\<>?AC<>=\u0015XWMOLOLOMNIKl$S>Q$J^<>QuZ^MNP^=\u0015RGQ <>=\u0015MNI^YanQ <>RmAuS>MNQ ImMNIq<>=cS>7K<>IqaQC<wFH=\\S@S>=c<w`7EACLOMOS~_\u001e<>=\b8\\Q[RGRG=\bI9P9AuS>MNQ I9?® ¹^ª\\¯VS>:p7K?xA?@:KQ <@S|`79=\\?US>MNQ I9I9A M]<>=}S>:EAuSxJK<>QuZpMNPK=\b?oj=\\_3JH=\\<>?@Q IEACLNM]S~_\u001eMOI9PKM]Y8\\ACS>Q <>?xRmA\b_3FH=|FH=\bIK=\\kH8\\MNACL^MOIzkEIK=\\YS>79I9MOIKlw<>=B8\\Q[RGRG=\\IEP9AuS>MOQ[I9?\bdyefI^YP^=\b=BPV[S>:9=c<>=wMO?{AuS|LN=\bA ?US{Q[I9=w<>=\b8\\Q[RGRG=\bI9PK=\\<yXw=\bF^Y¸?@MOS>=wXW:K=\\<>=wS>:9=c<>=MO?hA$S>:K<>MOZpMOI9lG8\\Q RGR$7KI9M]Sf_GQCa79?@=\\<>?TS>:9ACSW<>=\bl[7KLNAC<>L]_1A P9P<;AuS>MNIKl[?\bVP^MO?>8\\79?@?zMO?@?@79=\b?3<>=\bLNAuS>=BP\nS>QbS>:9=$?@M]S>=GMOI2A\u0014PK=\bPKMN8\bACS>=\bP2I9=cXW?@l <>Q[7KJyVACIEP\nJ9LNA\b_1XWMOS>:rS>:9=$P9AuS;A\u0014ABZ[ACMOLgACF9LO=[VyaQ <z=ci9ACRGJ9LO=[VyPK=cS>=\\<>RGMOI9MOI9lXW:KQqS>:9=\bM]<I9=\\MNl :pFHQ[7K<>?hAC<>=zF`_b79?@MOI9lqAq79I9MN`79=hkEI9l =\\<>JK<>MOI`Ss®]ªu¬u¯d©Q[RG=q7K?@=\\<>?sQ awS>:KMN?z?@M]S>=m:EABZj=qQuZj=c<$AJH=\\<>MOQpP\u0005Q aT_`=\bAC<>?z=\bI`S>=\\<>=BPRGQC<>=tS>:EACI\u0014Q[IK=tS>:9Q 79?>A I9Pb<;AuS>MNIKl[?WQ a?US>QC<>MO=\b?\bd\n3.5MeasurementofRelevantFactors6\u0014Q ?USm?US>7EP^MN=\\?$S>:EAuSGA ?@?@=\\?@?@=BP-JH=\\<>?@Q IEACLNM]S~_\u0012Q <GJ^<>=\\a=\\<>=\\IE8\\=\u0014<>=\\LNMO=BPQ I\u0014`7K=\b?US>MOQ[I9I9A M]<>=\b?\bd{6&ACIp_GX}=\\<>=\u0015QCayS>:K=hR\u001e79L]S>MOJ9LO=h8;:KQ[MN8\\=\u0015Z AC<>MO=\\S~_`d\bS>:K=\\<\u0015S>=\\?US>?tAC<>=z<>=\bZj=c<>?@=BPVHanQ <t=ci9ACRGJ9LO=[VHM]S\u0015:EA ?hFH=\b=\bIrJK<>Q JHQ[?@=BPS>Q\u001579?@=wR\u001e79?@MN8\bA LpJ^<>=\\a=\\<>=\\IE8\\=wS>Q\u0015PK=\\S>=\\<>RGMOIK=}JH=\\<>?@Q IEA LOM]Sf_\u001eJK<>QCkELO=t® \tC¯d\u0019=c<>?@Q[IEACLOMOS~_+A I9P%Q S>:9=c<$JK?U_98>:9Q LOQ[l _\u0005S>=\b?US>?q8\bACI\u0012=BA ?@MOL]_\u0007FH=bA 7^S>Q YRmAuS>=BP%ACIEP\u0005P^=\bLOMOZj=\\<>=BP\u0005Q IKY¸LOMOI9=[VACLOLNQBXWMOI9lr=\\ipS@<;A\u0014=\\Z^MNP^=\bIE8\\=qS>Q\u0014FH=8cQ[RGJ9MOLO=BPmaQ <7K?@=tMOI<>=B8\\Q RGRG=\bIEP^=\\<>?\bV`aQC<WS>:9Q[?@=37K?@=\\<>?wXW:9Q\u001eXWMN?@:S>Q\u001ekEI9=cY¸S>7KI9=<>=B8\\Q[RGRG=\\IEP9AuS>MOQ[I9?\bd\n3.6Discussion\u0005=W8\bA I$?@=\b=Tan<>Q[R¥S>:KMN??@7^<>Zj=\\_sQ aHaA[8cS>QC<>?wAC¡=\b8cS>MOI9l3R$7K?@MN8}JK<>=\\an=\\<@Y=\\IE8\\=[VHS>:9ACSS>:9=3aQ LOLNQBXWMOI9l\u0014PK=\\S;ACMOLN?tA FHQ[7^StACIrMNI9PKMOZpMNPK7EACLRmA\b_FH=7K?@=\\a7KLyanQ <JK<>QpP^7E8\\MOI9l$R$79?@MN8h<>=\b8\\Q[RGRG=\bI9P9AuS>MNQ I9?\b\u0018\nA l =\u0018\nQ <>MOl MNI\u0018\nQp8\b8\\7KJEAuS>MNQ I\u0018\n?@Qp8\\MOQ Y¸=B8cQ[I9Q RGMN8zF9A[8>opl <>Q[7KIEP\u0018\nJH=\\<>?@Q IEA LOM]Sf_baA 8cS>Q <>?\b\u0019\n?US;A FKMOLNM]S~_\u001b\u001auMOI9?US;A FKMOLNM]S~_%I9=\\7K<>Q S>MN8\\MO?@R\u0019\nMOI`S@<>QCZ[=\\<>?@MOQ[I\u001c\u001au=\\ipS@<;ABZj=\\<>?@MOQ[I\u0019\nA l l <>=\b?@?@MOZj=\u000f\u001aCJEAC?@?@MOZj=\u0018\nl[=\\IEPK=c<\u0018\nR$7K?@MN8\bA L=BPK798\bACS>MOQ I\u0018\nACS@S>M]S>79PK=tS>QBXTAu<;P\u0014R$7K?@Mg8\u0018\naACRGMOLNMNAu<>MOS~_\u0014XWM]S>:\u0014S>:K=tR$7K?@Mg8hQ <?USf_^LO=AReviewofFactorsAffectingMusicRecommender Successe~I\u0014S>:9=zACF9?@=\bI98\\=3Q aLNAC<>l =s8\\Q[LOLO=B8cS>MOQ[IK?\u0015QCa|P9ACS;ApV9S>:K=\b?@=taA 8cS>Q <>?\u00158\bA IFH=37K?@=BPbS>QqMORGJ^<>QCZ[=\u0015<>=B8\\Q RGRG=\bIEP^=\\<TJK<>=\b8\\MO?@MNQ Iyd=\\<@S;ACMOI\u0012A ?@JH=B8cS>?zS>:EAuSBVS>Q&Q 7K<sopI9QBXWLO=BPKl =[V:EABZj=$I9Q SsFH=\b=\\I\u0007A[PpYPp<>=\b?@?@=BP-MOI+J9?U_K8>:9Q[LOQ l[MN8\bA LQ <mQCS>:9=\\<q?US>7EPKMO=\b?mAu<>=\u0014S>:9=JK<>=\\an=\\<@<>=BPLO=\bZj=\\LQCat8cQ[RGJ9LO=\\i^M]Sf_%Q atR\u001e79?@MN8 d+vw:K=\u0014l[=\\I9=\\<;ACLh8cQ[I9?@=\\I9?@79?\bV}?@79J^YJHQC<@S>=BPGFp_\u001e?@Q[RG=W?US>79PKMO=\b?\bVpMO?{S>:9ACS{JH=\bQ[JKLN=WXWMOLOLE<>=\bRmA MOIGMOI`S>=\\<>=\b?US>=BPMOImR$7K?@Mg8WQ ayl <>=\bACS>=\\<W8cQ[RGJ9LO=\\i^M]Sf_\u001eaQC<wLOQ[IKl[=\\<Bd{vw:p79?\bVp?@MORGJ9LO=JHQ[J^Y7KLgAu<3R$7K?@MN8sRmA\b_1FH=$A J9J^<>=B8\\MNACS>=\bPrXWM]S>:ra=\\XLOMO?US>=\bI9MOIKl[?\bVyF97KShS>:9=LOMO?US>=\bI9=c<\u0015S>M]<>=\b?Q aM]S\u0015?@Q`Q I9=\\<Bd}%=z8\\Q 79LNP&=\\ipS@<;A JHQ LgAuS>=sag<>Q[R\u0003A I1MOI^YP^MNZpMNPK79A L ?tJK<>=ca=\\<>=\bI98\\=\u001eanQ <z8cLgAC?@?@MN8\bA L|Q <3JHQ J97KLgAu<3R$7K?@MN83XW:K=\\S>:9=c<S>:K=\\_+JK<>=\\an=\\<&8cQ[RGJ9LO=\\i+QC<?@MORGJKLN=1R\u001e79?@MN8 VTFK7KSGS>:K=\\<>=1AC<>=rRmACIp_LO=\bZj=\\LN?Q ax8\\Q[RGJKLN=ciKM]S~_GS>QqFH=taQ 79IEPmXWM]S>:9MOI&=BA 8;:&QCaxS>:9=\b?@= dI9Q S>:K=\\<\u0015aA[8cS>QC<3S>:EAuS3PKQ`=\\?@Iy SzA JKJH=BAC<\u0015S>Q:9AuZ[=\u001eFH=\b=\\I\n?US>79PKMO=BPrMO?S>:K=m<>=\\LgAuS>MOZj=MORGJHQC<@S;A IE8c=GQ aWS>:9=mL]_p<>Mg8m8\\Q I`S>=\bI`SsS>Q1S>:K=GR$79?@MN8\bACL8\\Q I`S>=\bI`SBd$vw:^<>Q[7Kl[:2A I9=\b8\bPKQCS;A L=\bZpMNPK=\bI98\\=[V?@Q[RG=\u001eJH=\bQ[JKLN=sXWMOLNL{PK=cY8\\MNP^=zXW:K=\\S>:9=c<hS>:K=\\_\u0014=\\I ¤UQB_\u0014Aq?@Q[IKlbLNAu<>l[=\bL]_&Q I&S>:9=3L]_p<>MN8s8cQ[I`S>=\bI`SBVXW:KMNLO=\u001eQ S>:9=c<>?hXwQ Iy St=\\Zj=\bI&IKQ S>MN8\\=zS>:9=3S>=\\ipSBVH:EABZpMOI9lmACLNL\u0019S>:K=\bM]<tAuS@YS>=\bI`S>MOQ[IaQp8\\7K?@=BP\u0014Q[IS>:9=tR\u001e79?@MN8 d\n4.TECHNIQUESAPPLICABLE TOREC-\nOMMENDERSI9?UXw=c<>MNIKl\np7K=\\<>MO=\b?s?@7E8>:\u0012A ?\bVsfe3LOMOoj=mS>:9=m?@Q[IKl\u0001\u0000 e; Z[=b?@=\\=\bI%S>:EAuSaA[8\\=mFH=\\anQ <>=[\u0003\u0002XW:EAuS\u001e=\\LN?@=GMO?3S>:9=c<>=qS>:EACSzMO?z?@MORGMOLgAu<\u0005\u0004\bKVzfeXTACIpS\u001eAJKMN=\b8\\=$QCawR$7K?@MN8zanQ <zR\u001e_kELOR&VES>:9ACS3=\\i^J^<>=\b?@?@=\b?3P^=\\kHACIE8\\=\b^VfeLNMOoj=S>:KMN?opMOIEP\u0014QCa{R\u001e79?@MN8 V^XW:EAuS\u0015=\bLO?@=\u001ePKQq_`Q 7\u0014S>:9MOIKoe}X}Q[7KLgP&LOMOoj=\u0006\u0004\b+MO?S>:K=zA MOR¦Q aP^=\b?@MOl[IK=\\<>?hQCa\u0019<>=B8\\Q RGRG=\bIEP^=\\<T?U_^?US>=\bRG?\bdvw:K=rkK<>?USp7K=\\<@_\u0012XwQ 79LNP\u0017JK<>Q FEA FKL]_\rIK=\b=BP-S>Q\u0007FH=1=\bI`S>=\\<>=\bP\u0017A ?A IAC7EPKMOQ+an<;A l RG=\bI`SrQ a\u001eS>:9=\u0005QC<>MOl[MOIEACL$J9MO=B8c=[VsA I9P8\\Q RGJEAu<>=BPXWM]S>:A1PKACS;A F9A ?@=maQC<q?USf_^LOMO?US>MN8bACIEP%RGQ`QpP\u0005?@MORGMOLNAC<>M]S>MO=\b?\bd2vw:9=m?@=B8\\Q IEPX}Q[79LNP1<>=B`79M]<>=\u001eAbRGQ`QpP1MOIEP^=\\iVA IEP&S>:K=sS>:KMO<;P&X}Q[79LNPrJK<>Q FEA FKL]_<>=\bL]_Q[I\u0014S>:K=tLOMOoj=\b?A I9P\u0014PKMO?@LOMOoj=\b?Q axQCS>:9=\\<7K?@=\\<>?\bde~IrS>:KMO?t?@=B8cS>MOQ[I\nXw=s<>=\bZpMN=cX S>:9=sS>=B8;:KI9MN`79=\b?hQ a}8\\Q[I`S>=\bI`SBVHan=BACS>7^<>=ACIEPs8\\Q[LOLNA FHQC<;ACS>MOZj=wkEL]S>=\\<>MOIKlS>:9ACSxAu<>=}<>=\bLNACS>=\bPzS>QhA IK?UXw=\\<>MOI9lS>:9=\b?@=S~_KJH=\\?TQ aR$7K?@MN8\bA Ly`79=\\<@_`d\n4.1CollaborativeFilteringQ LNLNACFHQ <;ACS>MOZj=\u0007k9LOS>=c<>MNIKl\u00178\\Q I9?@MO?US>?1Q aqRmA opMOI9l\u001279?@=%QCa$an=\b=BPKF9A[8>oag<>Q[R¨79?@=\\<>?zS>Q1MORGJK<>QuZj=$S>:K=mp79A LOM]Sf_2Q aRmACS>=c<>MgACL}J^<>=\b?@=\bI`S>=BP2S>Q7K?@=\\<>?\bd{vw:9=\u0015an=\b=BPKF9A[8>omljACS>:K=\\<>=BP&8\\A I\u0014FH=\u0015=\\i^J9LOMN8\\M]SBV9MOIS>:K=hanQ <>R¦Q a7K?@=\\<\u001e<;AuS>MOI9l[?\u001eA I9P%A IKI9QCS;ACS>MOQ[IK?\bVQ <sMORGJ9LOMN8\\M]SBVx?@7E8>:\u0007A ?zS>:9=qS>MORG=7K?@=\\<>?W?@JH=\bIEP\u0014=ci9ACRGMOI9MOI9lsS>:9=38\\Q I`S>=\bI`Sz® ¬B¯d»|AC<>L]_$X}Q <>osMOI\nQ[LOLNA FHQ <;AuS>MOZj=\u0015\u0019MOL]S>=\\<>MOI9l3JK79FKLNMO?@:K=BP$F`_ \u000e\u0015Q[LNPKFH=\\<>l=\\SACLd®]ª[ªc¯\u0019P^=\b?>8c<>MOFH=BPmS>:9=tvA JH=\b?US@<@_b?U_^?US>=\bR£XW:KMg8>:k9L]S>=\\<>=BPmRmA MOLACIEP&I9=cXW?haQC<\u00157K?@=\\<>?\u0015F9A ?@=BP&Q I&a=\b=\bPKFEA 8;oban<>Q R QCS>:9=\\<h79?@=\\<>?\bVEF97KS<>=B`7KMO<>=\bPrS>:K=$79?@=c<tS>Q=ciKJKLOMg8cMOS>L]_\n?@JH=B8\\M]ag_\nk9L]S>=\\<>?\bdst=\bZj=\bLOQ JH=\\<>?tQ aS>:K= \u000eh<>Q 79J9LO=\bIK?\u0019JK<>Qu¤U=\b8cS=\\ipS>=\bIEP^=BP\u0015S>:9=S>=B8>:9IKMg`7K={F`_3AC7KS>Q RmACS>MOI9lS>:K=kEL]S>=\\<qFEA ?@=\bP+Q I-?@MORGMOLNAC<>M]Sf_\u0012FH=\\S~Xw=\b=\\I\rAr79?@=\\<B ?q<;AuS>MNIKl[?GACIEPS>:KQ[?@=3Q aQCS>:9=\\<W7K?@=\\<>?s® ¼ \u0006 ¯dvw:KMN?xMNP^=BAh:9A ?x?@MOIE8\\={FH=\b=\bI$A J9JKLOMN=\bPzS>QhAhZ AC<>MO=\\S~_zQ a9S~_^JH=\b?\u0019Q a9MOIKaQC<@YRmAuS>MNQ IyV^MOIE8\\LO79PKMOI9l\u001eAC<@S@YXwQC<>o1® ¼pª\\¯VK?US>Q <>MO=\b?z®]ªu¬B¯ACIEPmRGQuZ^MO=\b?h®]ªB¹C¯d°=B8\\=\bI`S>L]_\n:9=\bI\u0014A I9P\n:9=\bI2® ¼u¯y:EABZj=tMORGJ9LO=\bRG=\\IpS>=\bPA$R\u001e79?@MN8<>=\b8cYQ RGRG=\bIEP^=\\<T?U_^?US>=\bR0S>:EAuSW79?@=\b?A\u001eanQ <>RQ ax8\\Q[LOLNA FHQC<;ACS>MOZj=zkEL]S>=\\<>MOI9lAC?\u0019Xw=\bLOLpA ?\u0019an=BACS>7^<>=\b?\u0019=\\ipS@<;A 8cS>=BP3an<>Q[R 6\u0014efteyP9AuS;AwS>Q\u00158\\LO7K?US>=\\<xJ9MO=B8c=\b?A 8\b8\\Q <;P^MOI9lqS>Qq?US~_KLO=3MOI\u0014QC<;PK=\\<WS>Q$<>=\b8\\Q[RGRG=\bI9PbR\u001e79?@MN8 d?h8cQ[LOLO=B8cS>MOQ[IK?hQCa79?@=\\<>?A I9Pb<;AuS>MNIKl[?Wl[=cShLNAu<>l[=\\<S>:9=\\<>=\u0015RmAB_ACLN?@QFH=3MO?@?@79=\\?W<>=\\LgAuS>MOI9lqS>Q$= \nm8\\MO=\bIE8;_&AC?WX}=\bLOL\u0019AC?:KQBX=c¡=B8cS>MOZj=3Aq8cQ[L]YLNA FHQC<;ACS>MOZj=&k9LOS>=c<>MNIKl%ACJ9JK<>Q[A[8>:\rMO?\bdDvw:K=S>=B8>:9IKMNp7K=[ ?G=\\¡=B8cS>MOZj=cYIK=\b?@?RmA\b_-A LO?@Q+PKM]¡=\\<manQ <\u0014PKM]¡=\\<>=\bI`SmSf_^JH=\b?mQ azM]S>=\bR&de~SmMO?bI9QCS8\\LO=BAu<wXW:9=cS>:9=\\<}A\u0015S>=B8>:9I9MN`79=wS>:9ACS|XwQC<>o^?|Xw=\bLOL9anQ <TIK=\\XW?|Q <w?US>QC<>MO=\b?XWMOLOLyFH=hS>:9=\u0015FH=\\?USWA JKJK<>QjA 8;:baQC<WR$79?@MN8 de¸SwXWMOLOLFH=\u0015MNI`S>=\\<>=\\?US>MNIKl\u001eS>Q=\\i^JKLNQC<>=tS>:9MO?`79=\b?US>MOQ[Id\n\bIK=wJK<>Q F9LO=\bR(XWM]S>:G8cQ[LOLNA FHQ <;AuS>MOZj=TkEL]S>=\\<>MOI9ltMO?\u0019S>:9ACSI9=cX-=\\IpS@<;ACI`S>?MOI\u0012A1R$7K?@MN8GP9ACS;ACFEAC?@=\u0014P^Q1I9Q S\u001el[=cSqA 8\b8\\=\b?@?@=BP\u0007F`_2=\\i^MN?US>MOIKl\n7K?@=\\<>?\bVXW:KMN8;:\u0007RG=BACI9?zS>:EAuS\u001eS>:K=\\_2XWMOLOLwIKQ S\u001eFH=b?@=\\LN=\b8cS>=BP\u0005aQC<$<>=B8cQ[RGRG=\bI^YPKACS>MOQ[Id3D:9MOLO=\u001eS>:KMO?tJK<>Q F9LO=\bR¢8\\A I\nFH=$A P9P^<>=\\?@?@=BP\n?@Q RG=\\XW:EAuS3F`_:9ABZ^MOIKl\u0014A\u0012fIK=\\XR$7K?@Mg8cb?@=\b8cS>MOQ[I%ABZ A MOLNA FKLN=\u001eS>Q7K?@=\\<>?\bVMOS\u0015XWMNLOLI9QCSMOI^E79=\\IE8\\=\u0015S>:K=379?@=\\<TXW:KQGF`_KJ9A ?@?@=\b?wS>:KMO??@=\b8cS>MOQ[Iyd\n4.2Content-Based Filtering6\u0014798;:1QCaxS>:9=3<>=\b?@=BAu<;8;:1MOI\u0014S>:K=3kE=\bLNP\u0014QCa}AC7EP^MNQmACIEACLO_^?@MO?\u0015:9A ?FH=\b=\bIF^<>Q[7Kl[:`S{S>QzFH=BAu<wMOImA I9A L]_^?@MOI9lsR$79?@MN8 d\u0019\u0017l[=\bIK=\\<;A LACIEPqMOI`S@<>QpPK798cYS>QC<@_1S@<>=BACS>RG=\\IpS\u0015Q a|S>:9=skE=\bLNP1XTA ?hXT<>MOS@S>=\\I\u0005F`_&°QjA PK?q® ¼\u0016\u0010B¯dqvw:K=RGQ ?USw?@MOl[IKM]kH8\bACIpSwJK<>Q[FKLO=\bR£MOImR$7K?@Mg8A I9A L]_^?@MO?TMN?{S>Qs?@=\\JEAC<;AuS>=\u0015S>:9=R\u001e79?@MN8\bACLy?@=\bRmACI`S>Mg8c?wan<>Q[R0S>:K=\u0015<;A\bX ?@MNl IEACLVKXW:9MN8>:\u0014RmA\b_m:EABZj=t:9A[PRmACI`_m?@JH=B8\\MNA L\u0019=c¡=B8cS>?z?@7E8>:&AC?=\b8;:KQ[MOI9l`TACJ9JKLNMO=BPbS>QqM]SBde~I\ra=\bACS>7K<>=cY~F9A ?@=BP\rk9L]S>=\\<>MOI9lKVhA\u0005Ip7KR$FH=\\<mQCa3a=BAuS>7K<>=\b?AC<>=\nPK=cS>=\\<@YRGMOIK=BPbag<>Q[R¦S>:9=t?>ACRGJ9LO=BP\u0014R\u001e79?@MN8 d|vw:K=\b?@=ta=BAuS>7K<>=\b?MOIpZjQ[LOZj=3?@798;:?@MOl IEA L]Y¸JK<>Qp8c=\b?@?@MOI9lrRG=\\S>:9QpP^?zA ?z?@JH=B8cS@<;ACL}ACIEP28\\=\bJ9?US@<;ACL}ACIEACLO_^?@MO?ACIEPGS>:9=79?@=hQ a\u0019MOIK?US@<>79RG=\bI`S{JK<>Q k9LO=\b?\bd|vw:9=\b?@=\u0015JEAC<;ACRG=\\S>=\\<>?wQC<wa=\bACYS>7^<>=W8\\Q[R\u001eF9MOIEAuS>MOQ[I9?xRGMOl :pSFH=T?@=\\LN=\b8cS>=BP$QC<}P^=\\S>=\\<>RGMOI9=\bP\u001eAC7KS>Q[RmAuS@YMN8\bACLOLO_AC?TAz<>=\\?@79L]SwQCa\u0019A3S@<;A MOI9MOIKls<>7KIyV`XW:9=c<>=S>:K=h7K?@=\\<B ?w<>=\b?@JHQ[IK?@=S>QqP9ACS;AsXWMOS>:\u0014o^IKQBXWI&ACS@S@<>MOFK7KS>=\b?WMO?W79?@=BPS>Q$RGQpPK=\bLJK<>=ca=\\<>=\bI98\\=\b?\bdQ I`S>=\bI`S@Y~F9A ?@=BP$k9L]S>=\\<>MOI9lKV`At?@JH=B8\\MNA LH8\\A ?@=WQ aa=\bACS>7K<>=cY~F9A ?@=BP$k9L]S>=\\<@YMOIKlKVMO?XW:K=\\<>=zXw=sPK=\\k9I9=zAqS>=B8>:9IKMg`7K=tS>:EACS<>=\bLOMO=\b?\u0015Q I&S>:9=sPK=cS>=\\<@YRGMOI9ACS>MOQ[I+Q a\b\u0007\b·c²\n\t\f\u000b\u000e\r±\u0010\u000fGACS@S@<>MOF97^S>=\b?qQ aS>:9=\u0014P9AuS;A^d+efI\u0007S>:9MO?G8\\A ?@=[VS>:K=ta=BAuS>7K<>=z=\bLO=\bRG=\bI`S>?WAu<>=s?@=\\RmA I`S>MN8t8\\Q[RGJHQ I9=\bI`S>?TQCaxS>:9=3R$7K?@MN8 dvw:KMO?hMO?TS>:9=tRGQC<>=38\\Q RGRGQ[I&ACJ9J^<>QjA[8>:S>Q$R$79?@MN8tACIEACLO_^?@MO?\bde~I\rS>:9MO?m8\\Q I`S>=\\ipSBVTS>:9=\u0014X}Q <;P\r?@=\bRmACI`S>Mg818\\A I\rFH=rACJ9JKLNMO=BP-MNI-SfXwQXwA\b_K?\b=\bM]S>:9=c<GA ?qA\nJ^<>Q[l <>=\\?@?@MNQ I\rQCa\u0015LOQBXTY¸LN=\\Zj=\bLTR$7K?@MN8\bA LT?U_^R$FHQ LO??@798;:\u0017A ?mI9QCS>=\b?bA IEP\u0012<>:`_^S>:KR =\bLO=\bRG=\\IpS>?\u0011\u0002QC<\u0014AuSA\u0005:9MOl[:K=\\<BVWRGQ <>=l =\bI9=c<;A L3LO=\bZj=\bLXW:9=\\<>=rS>:9=&an=BACS>7^<>=\b?\u00147K?@=BPDAC<>=rS>:9Q ?@=\nACS@S@<>MOF97^S>=\b?S>:9ACSmAu<>=\u0014<>=BA PKMOL]_\u0007<>=B8\\Q[l I9MO?>A FKLO=rA ?qS>:9=A[Pu¤U=B8;S>MNZ[=\b?qS>:EAuSGJH=\bQ[JKLO=RGMOl :pSt79?@=\u001eS>QPK=\\?>8c<>MOFH=qA I9P28cLgAC?@?@M]an_\nS>:EACStJ9MO=B8\\=\u001eQ <z8\\Q RGJHQ[IK=\bI`SQCawR\u001e79?@MN8zY}S>:9=\u0012\u0000 P^=\\kHACIE8\\=[QCa{S>:K=q`79=\\?US>MNQ I\nACStS>:K=\u001e?US;Au<@SzQ a|S>:9MO??@=\b8cS>MOQ[IydIrMgP^=BA L\u0019P9ACS;AuY~P^<>MOZj=\bI1kEL]S>=\\<hXwQ 79LNP1FH=\u001e=\bI`S>M]<>=\bL]_r8\\Q I`S>=\bI`S@Y~F9A ?@=BPdvw:9ACS\u0014MN?\bV\u0015Q[IK=\nXW:K=\\<>=\nS>:K=\nan=BACS>7^<>=\b?\u00147K?@=BPDF`_\r:p79RmACI9?mS>Q+8\\LNA ?UY?@M]ag_¥A I9PMOIEP^=\\iR$7K?@MN82XwQ 79LNP(FH=%=\bRGJKLOQu_j=BP(P^MO<>=\b8cS>L]_F`_S>:9=8cQ[RGJ97^S>=\\<@Y¸FEAC?@=BPq8\\LNA ?@?@M]kE8\bACS>MOQ[Id{vw:9=PKM \nm8\\7KL]Sf_$XWM]S>:m8\\Q I9?US@<>798cS@YMOIKl+?@798;:A\u0007?U_^?US>=\bR§LOMO=\b?&MOI\u0017S>:9=1<>=B`79M]<>=\bRG=\\IpSmS>:9ACSbS>:9=rR$7K?@Mg8FH=3J9AC<>?>ACF9LO=zFp_8\\Q RGJ97^S>=\\<z® ¹[­C¯}JEAu<@ShQCaxS>:9=3RGQCS>MNZ ACS>MOQ IS>Qq7K?@=8cQ[LOLNA FHQ <;AuS>MOZj=sk9L]S>=\\<>MOI9l$XwA ?TS>Qq:EABZj=hS>:9=t7K?@=\\<PKQ$S>:K=38\\LNA ?@?@M]kE8\bACYS>MOQ I;dwe~I&JK<;A 8cS>MN8\\=[VHRGQ ?USMNRGJKLO=\bRG=\bI`S;ACS>MOQ[IK?T79?@=sAG8\\Q R$F9MOI9ACS>MOQ[IQCaxa=BAuS>7K<>=zACIEP&8\\Q I`S>=\bI`S@Y~F9A ?@=BPGkEL]S>=\\<>MOIKl&® ¼\u0016\u0010u¯dvw:K=tR$7K?@MN8hMOIKanQ <>RmACS>MOQ I<>=\\S@<>MO=\bZ A LHkE=\\LgPbQ ay<>=\b?@=BAC<;8>:&:9A ?}_^MO=\bLNPK=BP?U_^?US>=\bRG?\u0019S>:9ACS|ACLNLOQBX+79?@=\\<>?xS>QkEIEPsAhJKMO=B8\\=TQ a9R$7K?@Mg8{Fp_z:p7KRGRGMNIKlAzan<;ACl[RG=\bI`STQCayS>:K=hS>79IK=\u001e®]ª Vyª\b­^Vª \u0010`VE¼C¹C¯|?@Q[RG=hQ ayS>:9=\b?@=h?U_K?US>=\\RG?Au<>=\u0015P^=\bJ9LOQB_`=BPqQ IqS>:9=TXw=\\F\u0019F97KSXW:KMNLO=RGQ[?US{Q aS>:9MO?<>=\b?@=BAu<;8;:m:EAC?anQp8\\79?@=\bP$Q[I\u001eRG=\bLOQpP^_zRmACS;8>:9MOIKlKVC?@Q[RG={Q S>:9=c<R$79?@MN8a=BAuS>7K<>=\b?:EABZj=FH=\\=\bIz79?@=BPtS>QWRmACS;8>:tJ9MO=B8c=\b?\bV[?@798;:zAC?<>:`_pS>:9R¥® ¹C¯V[8;:KQ <;PK?{® \u0006 ¯VjACIEP?US@<>798cS>7K<>=® \u0010pV\u0004\u0013C¯dKQ <\u0019=ci9A 8cSyMgP^=\bI`S>M]kH8\bAuS>MNQ IzQ a^<>=B8\\Q <;P^MOI9l[?\bVCS>:9=7K?@=QCa}=ci^S@<;A 8cS>=BPrA 79PKMOQGa=BAuS>7K<>=\b?hS>Q8;<>=BACS>=\u001eAb7KI9MN`79=3kEIKl[=\\<>J^<>MOIpShMO?ACLO?@QGFH=\bMOI9lq=\\i^JKLNQC<>=BPd\bS>:K=\\<3R\u001e79?@MN8cY<>=\bLNACS>=BP&S>=B8>:9IKQ[LOQ[l MO=\b?z:EABZj=\u001eA LO?@Q&FH=\b=\bI\nP^=\bZj=\bLOQ[JH=\bPV?@798;:+AC?$S>=B8>:9I9MN`79=\\?\u001eaQC<mA 7^S>Q[RmAuS>Mg8GS@<;A IK?>8c<>MOJKS>MOQ[I+Q a<>=B8\\QC<;PK=BPR\u001e79?@MN8m®Oª\u0007\u0013^V\u0019¼\u0004\u0013u¯}ACIEP1?@MOl[I9A LJK<>Qp8\\=\\?@?@MNIKl\u0014aQC<3MNPK=\bI`S>M]kH8\\ACS>MOQ[I\nQ a}MNI^Y?US@<>7KRG=\bI`S;ACS>MOQ[I®]ª\u000f\u0013^V$¼ ¼u¯d¨¨an=\\XS>=B8;:KI9MN`79=\b?\u0014:EABZj=%FH=\b=\\I A J^YJKLOMN=\bP\u0012S>Qr<>=B8\\Q RGRG=\bIEP^=\\<s?U_K?US>=\\RG?$MOI+S>:K=banQ <>RQ ahl[=\bI^<>=&8\\LNA ?@?@M]YkE8\bACS>MOQ I%® ¹j¼pVH¹\u0004\u0006 ¯de~I%JEAu<@S>MN8\\79LNAC<BVx\u0005=\bLO?@:%=\\S\u001eA LdT® ¹\u0017\u0006 ¯TP^=\b?>8c<>MOFH=BP\u0007A\u0014?U_^?US>=\bR anQ <\u001eJH=\\<@YanQ <>RGMOI9ls?@MORGMOLgAu<>M]Sf_b`79=\\<>MO=\b?{an<>Q[R0AsR\u001e79?@MN8hPKACS;ACFEA ?@=\u0015FEA ?@=\bPmQ I9L]_Q I&8>:EAu<;A[8cS>=\\<>MO?US>MN8\\?Q ayS>:9=tR\u001e79?@MN8hM]S>?@=\bL]afd{vw:K=t?@M\u000b\u0003\b=3Q ayS>:9=\u0015an=BACS>7^<>=AReviewofFactorsAffectingMusicRecommender Success?@J9A[8\\=mRmA o[=\b?zMOSsMORGJK<;A 8cS>MN8\bA L|aQC<$7K?@=\\<>?sS>Q1J^<>QuZ^MNP^=$S>:9=qan=BACS>7^<>=J9AC<;A RG=cS>=\\<>?PKM]<>=B8cS>L]_`VK?@Qq79?@=\\<>?hAC<>=t<>=\bp7KM]<>=BPbS>QqJ^<>QuZ^MNP^=zA$J9MO=B8\\=QCa3R$7K?@Mg8bS>Q%`79=\\<@_+ACljACMNIK?USBd vw:9MO?mJK<>QuZpMNPK=\b?mACI\rMOI9M]S>MNA L?@=\\SmQ aan=BACS>7^<>=\nJ9AC<;ACRG=\\S>=\\<>?bA I9P-S>:9=1R\u001e79?@MN8\bA LJ9MO=B8c=\nRmA\b_-8\\Q RG=\u0014an<>Q RS>:K=zP9ACS;ACFEAC?@=3MOS>?@=\\LOa~dTvw:K=\bM]<\u0015ACLOl[Q <>M]S>:KR =\b?@?@=\\IpS>MNACLNL]_&PK=\\<>MOZj=\b?3ªB¼\u0017\u0006 \u0013an=BACS>7^<>=1PKMORG=\bIK?@MNQ I9?qanQ <m=BA 8;:-?@Q[I9l^V}XWM]S>:-an=BACS>7^<>=&JEAu<;A RG=\\S>=c<>?MOIE8cLN79PKMOI9l\u0005A\nS>MORG=\\Y¸Z AC<@_^MOI9l1an<>=B`7K=\bIE8c_\u0012:KMO?US>Q[l <;ACR&VwS>:9=LO=\bZj=\bLWQ aJH=\\<;8c=\bMOZj=BP\u0007fIKQ[MO?@=\bsJK<>=\b?@=\bI`S}MNIGS>:K=R\u001e79?@MN8 V`Z[Q[LO79RG=8>:EA IKl[=\b?\bV^ACIEPS>=\bRGJHQ\u0014A I9P\n<>:`_pS>:9R&dGvw:K=\b?@=Ga=BAuS>7K<>=\b?sAu<>=P^=\\<>MOZj=BPran<>Q[Rª\b­CY>ªu¬?@=B8cQ[IEPr?>A RGJKLO=\b?3ACS\u0015Z[Au<>MOQ[79?\u0015?US;A l[=\\?3MNI1S>:K=$R$7K?@MN8 d3\u0001\u0000[Y~IK=BAC<>=\\?USIK=\bMOl[:pFHQ[7^<TRmACS;8>:9MOI9l\u001eA JKJK<>Q[A[8>:bXwA ?T79?@=\bPmS>Qsl[=\bIK=\\<;ACS>=hS>:9=\u0015FH=\\?USRmAuS;8;:K=\b?S>QbAq7K?@=\\<B ?3A 79PKMOQm`79=\\<@_`dvw:K=\u001eACJ9J^<>QjA[8>:1XTAC?\u0015RGQpPK=c<@YAuS>=\bL]_G?@7E8\b8c=\b?@?Ua7KLMOIb8\\LNA ?@?@M]ag_KMOIKl$R$7K?@MN8WMOI`S>QzQ[IK=hQCa?@=\bZ[=\bIGl[=\bI^<>=\b?\bVXWM]S>:18\\LNAC?@?@Mg8\\A L\u0019R\u001e79?@MN8\u0015FH=\bMOI9l\u001eS>:9=3=BAC?@MN=\\?UShS>QqMNP^=\bI`S>MOag_`d6\u0014QC<>=<>=B8c=\bI`S|X}Q <>o\u001eMNI$l[=\bI^<>=h8cLgAC?@?@M]kH8\bAuS>MNQ Im:EA ?|RmA[P^=w79?@=WQCaR$7KY?@MN8\bACL{?@7^<@aA[8c=$a=BAuS>7K<>=\b?\bV?@7E8>:\u0005AC?tS>:9=\u001eI^7KR$FH=\\<hQ a{S>MORG=\b?tJH=\\<3?@=\b8cYQ IEP%S>:9=A RGJKLNM]S>79PK=bQ a\u0015A I+A 79PKMOQ\nXwABZj=\\aQC<>R8>:EACI9l[=\\?qFH=\\SfX}=\b=\bIJHQ ?@MOS>MOZj=\nACIEP\rIK=\bljAuS>MNZ[=\u0007 \u0003\b=\\<>Q\u00128c<>Q ?@?@MNIKl[?c;V\u0015A\u0005RG=BAC?@7K<>=1Q az?@JH=\b8cYS@<;ACL}?@:EACJH=r<>Q[LOLOQ ¡w;V|A ?zXw=\bLOLTAC?s<>:`_pS>:9R a=BAuS>7K<>=\b?z<>=\bZj=BACLO=BP%F`_XwAuZ[=\bLO=\\SzS@<;A I9?UanQ <>RG?m® ¹j¼B¯¸d\u0014vw:9MO?zXTAC?\u001e`7KMOS>=$?@7E8\b8\\=\\?@?Ua79L}MNI2?@=\bJEAuY<;AuS>MNIKlt?@=\bZj=c<;A LpR$7K?@Mg8|l[=\bI^<>=\b?\bdljA MOIV`8\\LNA ?@?@MN8\bACL9R\u001e79?@MN8|XwA ?=BA ?@MOL]_MNPK=\\IpS>M]k9=BPV\u0019A LOQ[IKl&XWMOS>:\u0005:9MOJ9:KQ[Jydme~I`S>=\\<>=\b?US>MOI9l LO_`VS>:9=GR\u001e79?@MN8$?@7^<@YaA[8\\=ra=BAuS>7K<>=\b?bXw=\\<>=r?@LOMOl[:`S>L]_\u0017FH=\\S@S>=c<\u0014MOIEP^MN8\bACS>QC<>?QCa\u001el =\bIK<>=1S>:9A I<>:`_pS>:9R&VpFK7KS8\\Q R$F9MOIKMNIKl$FHQCS>:a=\bACS>7K<>=3?@=cS>?X}Q <>oj=BPbFH=\b?USBde¸SzMN?sACLN?@Q\u0014JHQ[?@?@MOF9LO=qS>Q\u00147K?@=qS>:9=\u001eJ9?U_K8;:KQ Y~A[8cQ[79?US>MN8qRGQpP^=\bLO?3Q aw:p7^YRmACIJH=\\<;8c=\bJKS>MOQ[IPK=\\Zj=\bLOQ[JH=BPmaQC<W?@Q[79I9P8cQ[RGJK<>=\\?@?@MNQ IS>Q\u001eA I9A L]_^?@=S>:K=}R$7K?@MN8{8\\Q I`S>=\bI`SBd\n©MOI98\\={S>:K=\b?@=}RGQpP^=\bLO?\u0019?@=\bJEAu<;ACS>=}S>:K={JH=c<;8\\=\bMOZj=BP8\\Q I`S>=\bI`Stan<>Q R\u001dS>:9=z<>=\b?USBVS>:9=\\_r8\\A I\nFH=$79?@=\bP1S>Qb<>=BPK798\\=zS>:9=qPKACS;A?@M\u000b\u0003\b=zaQC<\u0015RmAuS;8;:KMNIKlKd}vw:9MO?WS>=B8;:KI9MN`79=38cQ[79LNP1ACLNLOQBX R\u001e79?@MN8tRmACS;8>:KYMOI9l$79?@MOI9lm6&|¹\u001eanQ <>RmACSWk9LN=\\?WS>Q\u001eFH=3JH=\\<@aQC<>RG=BPbXWM]S>:KQ[7KSP^=B8\\Q RqYJ^<>=\b?@?@MOQ[Iyd \u0001jA\b_pACIpSb®]ª\u000f\tu¯w:9A ?sA\u0014l[=\\I9=\\<;ACLwPKMO?>8\\7K?@?@MOQ[I\u0012A FHQ 7KSs?@MOl[I9A L8\\Q RGJK<>=\b?@?@MOQ I\rFEAC?@=BP\u0012Q[I\rRGQpP^=\bLO?qQ a\u0015:p79RmA I\u0007JH=c<;8\\=\bJKS>MOQ IyV}XW:KMN8;:RmACoj=\b?$M]S$JHQ[?@?@MOF9LO=\u0014S>Q\nPKMO?>8\\=c<>I+J9ACS@S>=\\<>IK?$F9A ?@=BP\u0012Q[I\u0007S>:9=m³K·c´ \u000f;·~³\u0003\u0002\r\u0005\u0004 \t\u0007\u0006T<;ACS>:K=\\<sS>:EA I%S>:K=G³\t\b\u000b\n \u0007;±\u0010\u000f \t\f\u0006J^<>Q[JH=\\<@S>MO=\b?\u001eQ a?@Q[7KIEPd\u0005xACMOIpS>=c<® ¼\u0004\tu¯Q ¡=\\<>?WAzRGQ <>==\\i^:EAC79?US>MOZj=WS@<>=BACS>RG=\bI`S{Q aJH=\\<;8\\=\bJ^S>7EA LE8\\QpPKMOI9lQCaWPKMOl[M]S;ACLwA 79PKMOQKdGº}Q S>:\u0005S>:K=\b?@=qJ9A JH=\\<>?sA I9PrS>:9ACSzQCaT°hQ[A[P^?b® ¼\u0004\u0010u¯J^<>QCZpMNPK=zA\u001eLgAu<>l[=3LOMO?US\u0015QCa\u0019<>=\\an=\\<>=\bIE8c=\b?\bd\n5.RECOMMEND ATIONSFORRECOM-\nMENDERSK<>Q R Ab<>=\\?@=BAC<;8>:%JH=\\<>?@JH=B8;S>MNZ[=$S>:9=\\<>=$AC<>=qRmACI`_\np7K=\b?US>MOQ[IK?\u0015S>:EAuS8\bACIDFH=r=\\i^J9LOQ <>=BP-MNIDQC<;PK=\\<bS>Q\u0007FH=\\S@S>=\\<b7KIEPK=c<>?US;A IEP+S>=B8;:KI9MN`79=\b?S>:9ACSbXWMNLOLtMORGJ^<>QCZ[=&R$7K?@Mg8\u0014<>=\b8\\Q[RGRG=\bI9PK=\\<>?\bd(9QC<\u00148cQ[LOLNA FHQ <;AuS>MOZj=k9LOS>=c<>MNIKlmA JKJK<>QjA 8;:K=\b?hX}=zI9=\b=BPbS>QmPK=\\S>=\\<>RGMOIK=tXW:9MN8;:1QCaxS>:9=s8\\7^<@Y<>=\bI`S>L]_%S@<>MO=BP\u0012RG=\\S>:9QpP^?\u001eQCat8cQ[R$FKMOI9MOI9lr79?@=c<q<;ACS>MOI9l ?qMO?qQ[J^S>MORmA Ld©:9Q[7KLNP1P9ACS;AqRGMOIKMNIKlqS>=B8>:9I9MN`79=\\?FH=z7K?@=BPVHQC<\r\u0000[Y¸I9=BAu<>=\b?UShI9=\bMOl[:^YFHQ 7K<BV Q <?@Q[RG=cS>:9MOI9l=\bLO?@=\u0006\u0004\u0011\u000e\u0015MOZj=\bIsopI9QBXWLO=BPKl =TA FHQ 7KS\u0019XW:KMg8>:\u001eS>=\b8;:^YIKMg`7K=\b?TXwQ <>oHVK:9QBX8\bA IS>:9=\\_mFH=3MORGJ9LO=\bRG=\\IpS>=\bPm= \nm8\\MO=\bI`S>L]_\u000e\u0004\u0019=\\<>?@Q IEACL\u0015MOIKanQ <>RmACS>MOQ I\ran<>Q R\u001c7K?@=\\<>?\bVW?@7E8>:\u0017AC?mJH=\\<>?@Q[I9A LOM]Sf_`VA l =[VQC<>MNl MOI\u0017A I9P+Qp8\\8\\79J9ACS>MOQ[I\r8\bA I+FH=\u00148cQ[R$FKMOI9=BP%XWM]S>:\r8cQ[LOLNA FHQ <;AuS>MOZj=k9LOS>=c<>MNIKl\nS>QrMORGJK<>QuZj=mIK=BAC<>=\b?USqI9=\bMOl[:pFHQ 7K<q=\\?US>MNRmAuS>MOQ[Iyd\u00079Q <$=\\ipYACRGJ9LO=[VTAC?GM]Sm:EA ?GFH=\\=\bI-?@:KQBXWI-S>:EAuSmR$7K?@Mg8mS>:EAuSGQ[IK=&LOMN?US>=\\I9=BPS>Q2ACSqA\n?@JH=B8cMOkE8\u0014ACl[=\u0014MO?qRGQC<>=mLNMOoj=\\LO_\u0007S>QrFH=b=\bI ¤fQB_`=BPV{ACIEP%S>:EAuSR\u001e79?@MN8\bA LKJK<>=\\an=\\<>=\bI98\\=hMO?|?@Qp8\\MNA LOL]_b8\\Q[I9PKM]S>MOQ[IK=BPVjS>:K=TR$7K?@Mg8wQCaQ S>:9=c<JH=\bQ J9LO=tQCaS>:9=?>A RG=\u0015A l =tRmA\b_q:EABZj=hRGQ <>=QuZj=\\<>LNA JmS>:EACI7K?@=\\<>?wQ aP^MN?@J9AC<;AuS>=\u001eACl[=\b?\bd9QC<\u0015RGQ`QpPV9S>:K=ta=BAuS>7K<>=\b?WS>:EAuStAu<>=sRGQ ?USMNRGJHQC<@S;A I`ShAC<>=3S>=\bRGJHQ^VS>Q IEA LOM]Sf_`V9PKMO?US>MOIE8;S>MNZ[=\bI9=\b?@?WQCax<>:p_pS>:KR0A I9P\u0014J9M]S;8>:\u0014:K=\bMOl[:`SBd{v=\bRGJHQ8\bACI\u0017FH=&=\\ipS@<;A 8cS>=BP+ag<>Q[R§AC7EPKMOQ279?@MOI9l%=\\?US;A F9LOMO?@:9=\bP-S>=B8>:9I9MN`79=\\?\bdvQ IEACLNM]S~_%AC?\u001eA?@JH=B8\\M]kH8$an=BACS>7^<>=b:9A ?@I SsP^M]<>=B8cS>L]_\u0005FH=\\=\bI\u00057K?@=BPV\u0019A L]YS>:KQ[79l :qS>:9=WS>Q[I9A LE:KMN?US>Q l <;ACR0A I9P$S>Q[I9A L9S@<;ACI9?@M]S>MOQ[IGa=BAuS>7K<>=\b?}=\\ipYS@<;A 8cS>=BP2F`_1\u0005=\bLO?@:2=\\SsACLd{® ¹\u0017\u0006 ¯{RmAB_rFH=sXw=\bLOL{8\\Q <@<>=\\LgAuS>=BP2S>QbS>:9MO?\bdtMO?US>MOIE8;S>MNZ[=\bI9=\b?@?Q az<>:`_pS>:9R§8\bACI FH=\nP^=\\S>=\\<>RGMOI9=BP-Z^MNA\u0005S>:K=28c=\\<@Y\nS;ACMOIpS~_2A ?@?@Qp8\\MNACS>=\bP\u0005XWMOS>:\nS>=\bRGJHQb=\\ipS@<;A[8cS>MOQ[IVS>:9ACSzMO?\bV\u0019:KQuX8cLN=\bAC<S>:K=rFH=BACS>?\u0014AC<>=[de¸S&P^Q`=\b?@Iy S1ACJ9JH=BAu<S>Q\u0012:9AuZ[=1FH=\b=\bID79?@=\bPDA ?&Aan=BACS>7^<>=_`=cSBd\u00126\u0014=\bLOQpP^_\u0012JKM]S;8;:\u0012:9=\\MNl :`SqMO?qLOMOoj=\bL]_%S>Q\nFH=\u0014ArPKM \nm8c79L]San=BACS>7^<>=$S>Qm=\\ipS@<;A[8cS\u0015an<>Q[R JHQ[L]_^J9:KQ[I9MN8zR$7K?@Mg8sPK7K=sS>QGS>:K=$8\\Q IKan7KY?@MOQ I+XWM]S>:\u0012:EAu<>RGQ[I9MN8\\?$A ?@?@Qp8\\MNAuS>=BP+XWM]S>:+MNIK?US@<>79RG=\bI`S;ACL{S>MOR$F^<>=\b?\bVFK7KSGF`_\u00077K?@MOI9l\nJH=BA o\u0007Z A LO79=\\?qM]SGRmA\b_\u0007FH=\u0014JHQ ?@?@MNFKLO=\u0014S>Q\u0005P^=\\S>=\\<>RGMOI9=S>:KMO?\bdh8\b8\\7^<;ACS>=ql =\bIK<>=$8\\LNA ?@?@M]kH8\bAuS>MOQ[I\u0007ZpMNA\u0014A 79PKMOQbMN?3RGQC<>=$J^<>Q[F9LO=\bRmAuS>MN8S>:9A IbRGQ`QpPm8\\LNA ?@?@M]kH8\bAuS>MOQ[IrA ?{S>:9=\\<>=\u0015AC<>=\u0015?@Q\u001eRmA I`_ql =\bIK<>=\\?TMNImSf_^J^YMN8\bACLw8\\LNAC?@?@MOkE8\bACS>MOQ I+:KMO=\\<;AC<;8>:9MO=\b?\bdºwAC?@=BP\u0005Q[I\u0005=\\i^MN?US>MOIKl18\bACS>=\\l[Q <>MO=\b?\bVM]S?@=\b=\bRG?TS>:9ACSTS>:9=3RmACMOIa=\bACS>7K<>=\\?S>:9ACSTXwQ 79LNP&ACMgPMNI&PK=\\S>=\\<>RGMOI^YMOIKl\u001el =\bIK<>=Q <T?US~_^LN=MOIE8\\LO79PK=W<>:p_pS>:KRGMg8wJ9ACS@S>=\\<>IVjS>=\bRGJHQ^VjS>Q[I9A LOM]Sf_`VIKQ[MO?@=1ACIEP\u0012MNIK?US@<>79RG=\bI`S;AuS>MNQ Iyd\u0012%=\\LN?@:+=\\SmACLd-anQ[79I9P\u0012S>:9ACSqS>:K=\bM]<IKQ[MO?@=$an=BACS>7^<>=$XTAC?sZ[=\\<@_r8\\LOQ[?@=\\LO_2AC?@?@Qp8\\MNACS>=BP2XWM]S>:2MNIK?US@<>79RG=\bI`S;AuYS>MOQ I&Q a|A$JKMO=B8\\=3Q a\u0019R$79?@MN8 d\b7^<$<>=\b?@=\bAC<;8>:-J^<>=\b?@=\bI`S>L]_%MOIpZjQ[LOZj=\b?$8\\Q[R\u001eF9MOI9MOIKl&S>:9=A FHQuZj=mS>=B8;:^YIKMNp7K=\b?WA I9PbS>:KQ[?@=tQCa\u0019%=\bLO?@:=\\SA Ldy® ¹\u0017\u0006 ¯yS>QqA LOLOQBX<>=\b8\\Q[RGRG=\bI9PK=\\<>?S>QqA LO?@Qm8\\LNA ?@?@M]an_1A IEPm<>=B8\\Q RGRG=\bIEPmI9=\\X R\u001e79?@MN8 d\n6.SUMMARYvw:^<>Q[7Kl[:zQ[7^<?@7^<>Zj=\\_tQCa^R$79?@MN8xJ9?U_K8>:9Q LNQ l _hXw=|:EABZj=MO?@Q[LNACS>=BP3aA 8cYS>QC<>?{S>:9ACS|:EABZj=TFH=\b=\\IG?@:9QBXWIqS>QzAC¡=B8;S}:p79RmACI\u001eR\u001e79?@MN8wJK<>=ca=\\<>=\bI98\\=[dvw:K=mRmA MOI\u0007MOIEPKMOZpMNPK79A LaA[8;S>Q <>?qMOIE8cLN79PK=mAC?@JH=B8cS>?$QCaJH=\\<>?@Q[I9A LOM]Sf_`VACl[=[V{=\\S>:KI9MN8\\M]Sf_`VACIEP%?@Qp8\\MOQCY~=\b8\\Q[IKQ[RGMN8bF9A[8>opl <>Q[7KIEPd\n<>=\\an=\\<>=\bI98\\=MO?WACLN?@Q$MNI^E7K=\bIE8\\=\bPbF`_qXW:9ACSWMOIEP^MNZpMNPK79A LO?wFH=\bLOMO=\bZj=tA FHQ[7^SwS>:9=hR$7^Y?@MN8&S>:9=c_-:K=BAC<BVhACIEP+S>:K=\bM]<maA RGMOLOMNAC<>M]Sf_-XWM]S>:DMOSBdx<>=\\a=c<>=\bIE8\\=\nMO?MOI^E79=\\IE8\\=BPFp_mS>:K=zPK=\b?@M]<>=3S>QGFH=zA[8\b8c=\bJKS>=BPFp_bS>:9=\u0015l <>Q[7KJyd\n©Q[RG=QCa\u0015S>:K=\b?@=\u0014a=\bACS>7K<>=\\?m8\bA I+FH=&MOIE8\\QC<>JHQ <;AuS>=BP-MOI`S>Q\u0005A\nR$7K?@Mg8G<>=B8\\Q RqYRG=\\IEPK=c<3?U_^?US>=\bR¢ZpMNAS>:9=q7K?@=qQ aTp7K=\b?US>MOQ[IKIEA M]<>=\b?\bdbKQ <zRGQ`QpP2QC<?US~_^LN=`79=c<>MN=\\?\bV`?@Q[RG=Ta=\bACS>7K<>=\\?{S>:EAuS|=\\i^MN?US>MOIKl3?U_K?US>=\\RG?|7K?@=[V`RmAuS;8;:S>:KQ[?@=\u0015anQ[7KIEPmZ^MNAsJ9?U_K8;:KQ[LOQ[lC_m<>=\b?@=BAC<;8>:S>QsFH=tMORGJHQ <@S;ACI`SwMOIEP^Mg8\\ACYS>QC<>?\u0011\u0002x:9QBX}=\bZj=\\<3?@Q RG=qQ S>:K=\\<3aA 8cS>Q <>?\bV?@7E8>:\u0005AC?zPKMO?US>MOIE8cS>MOZj=\\I9=\b?@?3QCa<>:`_pS>:9R&V^P^QqI9QCSA JKJH=BAC<WS>Qq:9AuZ[=tFH=\b=\bI&7K?@=BPd\n7.ACKNOWLEDGEMENTS\u0005=&S>:EACI9o \u0001jA RG=\\?vw:KQ[R&V\u0002\u0001 79?US>MOI\u000f\u000eHQ[FH=\bLV\u0011\u0010yMOIDxA PKl :EA R&VACIEP©A I`S>:EA\n©79RmACIEA ?@=\\o[Au<;A{aQC<S>:9=\\MO<yMOIpZjQ[LOZj=\bRG=\\IpSMOItS>:9=x=\bAC<>L]_3?US;A l =\b?QCatS>:9MO?GJ^<>QC¤f=B8cSBd\u0017\u0005=&A LO?@Q%S>:9A I9o2S>:9=1ACI9Q[I`_^RGQ[7K?\u001e<>=\\an=\\<>=\b=&anQ <S>:K=\bM]<1:9=\\LNJ^a7KLz8\\Q[RGRG=\bI`S>?\bd£vw:9MO?\u0014X}Q <>oD:EAC?&FH=\b=\bIJ9AC<@S>L]_D?@79J^YJHQC<@S>=BPDF`_\u0017ACI °\u00156\u0014eUv\n©8>:9Q`Q LtQ a\nQ RGJ97KS>=c<\n©8\\MO=\bIE8\\=\nACIEP\u0017efI^YanQ <>RmAuS>MNQ Irv=B8>:9IKQ[LOQ[lC_&P^=\bJEAu<@S>RG=\bI`STl <;ACIpSBd\n8.REFERENCES®Oªc¯ \u0001Kd^º}QC<;8;:K=\\<>?wACIEPb62d^6\u00147K:9LO:EAC79?@=\\<Bd`\u0015=\b?@MOl[IbJEACS@S>=c<>I9?aQ <wMNI^YS>=\\<;A 8cS>MOZj=&R$7K?@MN8\bA LT?U_^?US>=\bRG?\bd\u0013\u0012\u0015\u0014\u0016\u0014\u0017\u0014\u0019\u0018\u001a\u0004\u001b\u0006 \r±²m·\u001d\u001c ±\u0010\t VT¬p¹`; ¹\u0016\t\u001f\u001e\u0006 \tpVª\b«[«\u0016\u0013pd® ¼B¯3\u001ed Y\nd\n:9=\\IACIEPsd \u0010|d\u0019d\n:9=\\Iyd R$7K?@Mg8&<>=B8cQ[RGRG=\bI^YP9AuS>MOQ[I%?U_^?US>=\bR F9A ?@=BP\nQ[I%R\u001e79?@MN8qP9AuS;A\u0014l <>Q 79JKMNIKl1A I9P27K?@=\\<MOI`S>=\\<>=\b?US>?\bdpefI\"!|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000f;·sµ\f\u000b%\u0012 \u000b\u001f#\bµC´c²\n\t \r±µ\f\u000b \t \u000b\u0003\u001c'& \u000bEµ\f(\u0013\u0006N·\u001d\u001c\u001f)`·\u0018 \t\f\u000b\u000e\t\u001f)`·c²m·\u0005\u000b \rV\u0015J9A l =\b?&¼C¹^ª*\u001e^¼ ¹\u0004\u0013^VtTS>LNACIpS;ApV \u000e\u0015=\bQ <>l MNA^V,+\n©zV\f\u0015QuZj=\\R$FH=\\<¼C­[­pª[d® ¹u¯ \u0001Kd\nd\nd\n:9=\bI1ACIEPsd-\u0010|d9xd\n:9=\\Iyd\t.\u001579=\\<@_bF`_m<>:p_pS>:KR0A IA JKJK<>Q[A[8>:&aQ <h?@Q[IKlq<>=\\S@<>MO=\bZ A LMOI&R\u001e79?@MN83P9AuS;A F9A ?@=\b?\bdHe~I0/}´@µ\u0007\u0002\u000f;·>·1\u001cC± \u000b2)\f\u0007bµ3#4\u0012\u0015\u0014\u0016\u0014\u0016\u00145\u0012 \u000b\u000e\r¸·c´ \u000b \t \r±µ\f\u000b \t\u0007\u000676&µC´98 \u00071\bpµc³\rµ\f\u000b\":w·\u0005\u0007\\· \tC´ \u000f;\b± \u0007 \u00071\u00049·\u0005\u0007± \u000b=< \t \r \t>\u0014 \u000b?) ± \u000b·>·c´;± \u000b2) VpJ9A l[=\\?Wª\b¹[«\u001f\u001eEª\u0007\u0006\u0016\t^d^eU»|»|»WV^ªB« «\u0016\u0013^d® \u0006C¯zv\u0015d Y\nd\n:9Q 7yV&sd4\u0010|d$xd\n:9=\bIV&ACIEP\nd Y\nd=\u0010MO7ydq6\u001479?@MN8P9AuS;A F9A ?@=\b?\b|efI9PK=\\i^MOI9ltS>=\b8;:KI9MN`79=\b?}A IEPGMORGJ9LO=\bRG=\\IpS;AuS>MOQ[Iyd[efI/}´@µ \u000f;·>·1\u001cC± \u000b2)\f\u00077\u0012\u0015\u0014\u0017\u0014\u0016\u0014@\u0012 \u000b\u000e\r¸·c´ \u000b \t\f\r±µ\f\u000b \t\f\u0006>6&µ ´*8 \u00071\bpµc³2± \u000b0\u0018\u001a\u0004\u001b\u0006 \r±²m·;\u0002\u001c ± \t=<BAC\u0018\u001aD9VªB« «\u0016\tpd® ¬B¯s62d\nLNA\b_^JHQ`Q[LVH\u0019d-\u0010y=[VE62d9%A ?@=BPVEACIEPb$dEº|<>QuXWIdEe~RGJ9LOMN8\\M]SMOI`S>=\\<>=\b?US\u0019MOI9PKMN8\bACS>QC<>?\bdBefI'E7!F\u0018\u0019\u0012 \u000b\u000e\r¸·c´ \u000b \t \r±µ \u000b \t\u0007\u0006\u0003!|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000fc·Tµ\f\u000b\u0012 \u000b\u000e\r¸·;\u0006G\u0006 ±H)`·\u0005\u000b\u000e\r4I \u0007\\·c´'\u0012 \u000b \r¸·c´\u0005# \t \u000f;·\u0005\u0007;VJ9A l =\b?t¹ ¹J\u001e \u0006[­^V\n©ACI`S;AG9=[V\u001b\ft6+\n©sV \u0001jACI^79AC<@_&¼ ­ ­^ª[dAReviewofFactorsAffectingMusicRecommender Success® \tu¯z\u0019d °zdCEAu<>I9?UXwQC<@S>:yd\u0001\u0000 \b^· DHµ \u000f\\± \t\u0007\u0006\u000b/ \u0007*\n \u000f;\bpµ\u0007\u0006Oµ1)\f\nsµ\u0015# \u0018 \u0004 \u0007>± \u000f\\dChQ[L]SBV°hMOIK=\b:EAu<@SA IEPbDMOI9?US>Q[IV \fh=\\X\u0003\u0002}QC<>oHVª\b«j¬\u0017\u0013^d® \u0010B¯ \u0001Kd+9Q`QCS>=[d\u0005\u0004\u0015MO?@7EACLOM \u0003\\MNIKlR\u001e79?@MN8A I9P\u001cAC7EP^MNQ7K?@MNIKl?@=\bL]anY?@MORGMNLNAu<>MOS~_`duefI'/}´@µ \u000f\u0007\u00062E7!F\u0018 \u0012 \u000b \r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u00062\u0018\u001a\u0004\u001b\u0006 \r±²m·\u001d\u001c ± \t4!|µ \u000b\t\u0002#B·c´>·\u0005\u000b\u000e\u000f;·cVTJEACl[=\b? \u0010\u0016\u0010$\u001e \u0013 ­^V\n\b<>LNA I9PKQ2\u0019LNQC<>MNP9A^V\u0013+\n©zV\n\b8cS>Q[FH=c<ªB« «[«^d® \u0013u¯ \u0001KdGKQ`Q S>=[dm°|vwB+\u0015°zz°=\\S@<>MO=\bZpMNIKl¥Q <;8>:9=\b?US@<;ACL\u0014R\u001e79?@MN8\u0017F`_LOQ[I9lCYS>=\\<>R¥?US@<>7E8cS>7^<>=[d efI$\u001edjº{_p<;PV \u0001Kd\n©dCtQBXWI9MO=[V`v\u0015d\n<;A\bXTYaQC<;PV dºhd\n<>Q agSBV\u0019ACIEP\nd \f\u0015=\\Z^MOLOL]Yf6&A I9IKMOI9lKVy=BP^MOS>QC<>?\bV \u0012 \u000b\t\u0002\r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u0006 D \n ²h³^µ\f\u0007>±\u0005\u0004p²µ\f\u000b \u0018\u001a\u0004 \u0007>± \u000f \u0012 \u000b\u001f#\bµC´c²\n\t \r±µ\f\u000b :w· \r´;±¸·c¶ \t\u0007\u0006 VZjQ[LO7KRG=$ª[V9|L]_^RGQ[7^S>:yV96&A ?@?>A 8;:p7K?@=\\S@S>?\bV\n\b8cS>Q FH=\\<\u0015¼ ­ ­[­^d® «u¯\n©d\t\bqd\u0016\u000e\u0015=\\<>I9=\\SBd\u001b\u0018\u001a\u0004 \u0007>± \u000f \t\u0007\u0006\t\u001cC± \u0007\u0005\u000f\\´c±²\u001e± \u000b \t\f\r±µ\f\u000b \t \ry¶ \tC´c±µ \u0004 \u0007 \t$)`· \t\f\u000b\u0003\u001c) ´ \t\u000b\u001cj·'\u0006N·c¶ ·;\u0006 \u0007>dEvw:K=\nQ[LOLO=\bl[=\u001e<>=\\?@?\bV9\u0007AC?@:9MOI9lCS>Q[IVyª\b«\u0004\u0006j­pd®]ªB­u¯ssd \u000e\u0015:KMNA ?\bV \u0001Kd7\u0010Q[l[A IyVq$d\n:9A R$FH=c<>LNMOIVGA I9Pºhd\n©RGM]S>:d.\u001579=\\<@_\u0014F`_\u0014:p79RGRGMOIKl\u000b\n R$7K?@MN8\bA LyMNI^aQC<>RmACS>MOQ[I1<>=\\S@<>MO=\bZ A L\u0019MOIA I\nA 79PKMOQP9ACS;ACFEAC?@=[defI /}´@µ \u000f\u0007\u0006 E7!F\u0018 \u0012 \u000b \r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u0006F\u0018\u001a\u0004\u001b\u0006 \r± \u0002²m·\u001d\u001c ± \t !|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000fc·cVyªB« «j¬pd®]ª[ªc¯z$d \u000e\u0015Q[LNPKFH=\\<>l^Vq$d \f\u0015MN8>:9Q LN?\bV$ºhd362d\n\bopMV\u001eA IEP$dtvy=\\<@<@_`d+\u0015?@MOI9l\n8\\Q LOLgACFHQ <;AuS>MNZ[=kEL]S>=\\<>MOIKl&S>Q&Xw=BABZj=A I9P%MNI^aQC<>RmACS>MOQ[IS;A JH=\b?US@<@_`d'!|µ ²$²'\u0004 \u000b9± \u000f \t \r±µ \u000b \u0007rµ3# \r\u0005\b^· E7!F\u0018&VT¹[¬^Uªu¼[; \t^ª9\u001e \u0010 ­pVt=B8\\=\\R$FH=\\<3ª\b«[«j¼`d®]ªu¼B¯z$d \u0001KdtAu<>l <>=BABZj=\b?sACIEP\nsd\nd \f\u0015Q <@S>:V=\bPKM]S>Q <>?\bd\f\u0000 \b^·4DHµ \u000f\\± \t\f\u0006/ \u00071\n \u000f;\bpµ\u0007\u0006Oµ1)\f\nzµ\u0015#\u0017\u0018\u001a\u0004 \u0007>± \u000f\\VuZjQ[LO7KRG=Wª d\n\bipaQC<;P>+\u0015I9MOZj=c<>?@MOS~_3x<>=\b?@?\bVªB« « \u0010pd®]ªB¹u¯ \u0001Kd \u0010|d\u0015=\\<>LOQp8>oj=\\<BV\u0015\u0001Kd\u0019zd\r\b3Q[IK?US;A IyV\u0019A I9P \u0001Kdy°MN=\bPKLdy»i^J9LNACMNI^YMOI9lz8\\Q LNLNACFHQ <;ACS>MOZj=WkEL]S>=\\<>MOIKlt<>=B8\\Q[RGRG=\\IEP9AuS>MOQ[I9?\bdCe~I /w´Uµ \u000f;·;·\u001d\u001c\u0007\u0002± \u000b2)\u0012µ\f\u000b \r\u0005\b^· E7!F\u0018\u000f\u000e\u0011\u0010\u0012\u0010\u0012\u0010 !|µ\f\u000b\u001f#B·c´@·\u0005\u000b \u000f;·\u0005µ\f\u000b5!|µ ²h³\t\u0004 \r¸·c´ D-\u0004u³\u0003\u0002³^µ ´\u0005\r¸·1\u001c !|µBµ;³K·c´ \t \r±¶ · 6\u0014µ ´98BVEJEA l =\b?z¼ \u0006Kª9\u001ep¼[¬C­^V|:KMNLNA PK=\bLOJ9:KMNA^VsVKt=B8\\=\bR\u001eFH=\\<¼C­[­[­pd®]ª\u0007\u0006C¯\u001e62d^ºhdp\u0015Q LOFK<>Q`Q[omACIEPG°zd^62d\n©8>:9MOIEP^LO=\\<Bd\n©Q RG=h=ciKJKLOQ <;ACS>QC<@_kEI9PKMOI9l ?tQ[I1S>:9=$PK=\bZ[=\bLOQ[J9RG=\\IpShQ a}R$7K?@Mg8\\A LS;AC?US>=\b?\bd\u0014\u0013pµ\f\u0004p´\u0005\u000b\u000e\t\u0007\u0006µ3# !|µ\f\u000b \u00071\u0004p²m·c´\r:w·\u0005\u0007\\· \tC´ \u000f;\bjV\u0019ª\u0007\tKUªu;Nª ªB«pV \u0001[79IK=qªB«\u0004\u0013[«^d®]ªu¬B¯z$dK\u0015QBXw=\bLOLdHLO=\\iKA IEPp<>MNA$PKMOl MOS;ACLLOM]S>=\\<;ACS>7^<>=[d9\u0005=\bFKY¸?@M]S>=[d-\u0010AC?USA[8\b8c=\b?@?@=BP1h7Kl[7K?USh¼ ­[­[¼pd®]ª\u000f\tu¯ \f\u001ed\u0015\u0001jA\b_pA I`SBV \u0001Kd \u0001 Q[:9IK?US>Q[IV}ACIEP%°zd\n©ACan<;ACI9=\boHd\n©MOl[IEACLT8\\Q RqYJK<>=\b?@?@MOQ I\rFEAC?@=BP-Q I-RGQpP^=\bLO?GQ a\u0015:^7KRmA I\u0012JH=\\<;8\\=\\JKS>MOQ[Id /}´@µ \u000f\u0007\u0006\u0012\u0015\u0014\u0017\u0014\u0016\u0014}V\u001b\u0013pªjUªB­j;OªB¹\u0016\u0013[¬\u001f\u001e9ª\u0007\u0006`¼ ¼pV\n\b8cS>Q[FH=c<sª\b«[« ¹^d®]ª \u0010B¯sv\u0015d\u0015\bzA l[=c_pA RmApV\u0016\bqd-6\u0014Qp8;:KM\u000b\u0003\b79opMVA I9P\u0017\u0002\u001ed+v\u0019A o A ?@:KMNRmApd6\u0014=\bLOQpP^_<>=\\S@<>MO=\bZ[ACLyXWM]S>:&:p79RGRGMOIKlKdpefI /w´Uµ \u000f\u0007\u0006 \u0012 \u000b \r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u0006!|µ ²\u0015³-\u0004 \r¸·c´\r\u0018\u001a\u0004 \u0007>± \u000f=!|µ\f\u000b\u001f#\b·c´>·\u0005\u000b \u000f;·cV\u0019ª\b«[« ¹^d®]ª\u000f\u0013u¯\u0018\bqd\u0019\bzA ?@:KMOI9QmA I9P1\u001edHvA IEACo[Apd ?@Q[7KIEP&?@Q 7K<;8\\=s?@=\bJEAu<;ACS>MOQ[I?U_^?US>=\bR¦XWMOS>:&S>:K=zA FKMNLOM]Sf_\u0014Q a|AC7KS>Q[RmAuS>MN8\u0015S>Q[I9=zRGQpP^=\bLOMNIKlKd9efI/w´Uµ \u000f\u0007\u0006F\u0012 \u000b \r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u0006\u0017!|µC²\u0015³\t\u0004 \r¸·c´ \u0018\u001a\u0004 \u0007>± \u000f=!|µ\f\u000b\u001f#B·c´@·\u0005\u000b \u000f;·cVEJ9A l =\b?¼\u0017\u0006\u0016\u0013J\u001e^¼ ¬[¬`VxªB«[« ¹^d®]ªB«u¯ssd\u001a\b3=\\RGJyd\u0019\u0000 \bp· \u0018 \u0004 \u0007>± \u000f \t\u0007\u0006\u001b\u0000H·c²h³K·c´ \t ²G·\u0005\u000b\u000e\rd\n\bipaQC<;P\r+hI9MOZj=\\<>?@M]S~_<>=\\?@?\bV\n\bipaQC<;PVªB«[«\u0004\t^d® ¼ ­u¯ssd\u001c\b3=\bRGJyd \u0012 \u000b\u0003\u001cC±¶B± \u001c\f\u0004 \t\u0007\u0006 <\u0015± \u001dh·c´>·\u0005\u000b\u000e\u000fc·\u0005\u00073± \u000b \u0018 \u0004 \u0007>± \u000f \t\u0007\u0006 A·9\b \t ¶B±µ \u0004^´;V8;:9A J^S>=\\<1¼`V\u0015J9A l =\b?1¼ ¬\u001f\u001e \u0006j¬pd\u001e\u0004|Q[LO79RG=%ªrQCa$\u0015AC<>l <>=\bAuZ[=\b?\u0014ACIEP\f\u0015QC<@S>:\u0005®]ªu¼B¯¸VªB«[«\u0016\u0010pd\n® ¼`ª\\¯ssd\t\b3Q[:^<>?A I9Pqºhdj6\u0014=\\<>MNA LNPKQ^d\u000b+\u0015?@MOIKl38\bACS>=\\l[Q <@_pY¸FEAC?@=BPq8\\Q LOLgACFKYQ <;AuS>MOZj=3kEL]S>=\\<>MOI9lqMOI\u0014S>:K=zA[8cS>MOZj=tX}=\bF9R\u001e79?@=\b7KR&dpefI \u0012\u0015\u0014\u0016\u0014\u0016\u0014\u000f\u0012 \u000b\t\u0002\r¸·c´ \u000b \t \r±µ \u000b \t\u0007\u0006 !|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000f;·mµ \u000b \u0018\u001a\u0004\u001b\u0006 \r±²G·\u001d\u001c ± \t \t\f\u000b\u0003\u001c \u0014\u0014\u001fc³^µ VEJEA l =\b?¹j¬`ª9\u001e`¹j¬\u0017\u0006^V \f\u0015=\\X\u0003\u0002{Q <>oHV \u0001[79L]_\u0014¼ ­ ­[­pd® ¼ ¼u¯\u0018\bqd $d6&Au<@S>MOI-ACIEP \u0002\u001ed »Td!\b3MOR&dx6\u001479?@MN8\bA L}MNIK?US@<>79RG=\bI`SsMNPK=\bI^YS>M]kH8\bAuS>MOQ[Iy{DJEAuS@S>=\\<>IKY<>=B8cQ[l[IKM]S>MNQ I&ACJ9JK<>Q[A[8>:yVªB« «\u0016\u0013pdExACJH=\\<<>=BA P2AuStS>:9=bªB¹\u0016\tuS>:2RG=\\=\\S>MOI9lbQ a|S>:9=qh8\\Q[7K?US>MN8\bA L\n©Qp8cMN=cSf_1QCahRG=c<>Mg8\\A^d® ¼C¹C¯z°zd \u0001^dw6&8 \ftA FV \u0010d}zd\n©RGM]S>:yV{ecdsdxDM]S@S>=\bIyV\nd \u0010d\u0015=\bI^YPK=c<>?@Q[IyV^ACIEP\n©d \u0001Kd\n7KI9I9MOIKl[:EACR&d`vQBXwAC<;PK?|S>:9=PKMOl MOS;ACLHR$7^Y?@MN8\u0015LOMOFK<;AC<@_{v7KI9=h<>=cS@<>MN=\\Z[ACLag<>Q[R¦A 8\\Q[7K?US>MN8tMNIKJ97^SBdpefI\u001a/w´Uµ \u000f\u0007\u0006E7!F\u0018 <\u0015±H) ± \r \t\u0007\u0006\u0019\"\u0019±$#\\´ \tC´c±¸·\u0005\u0007>VyªB«[«\u0004\t^d® ¼ \u0006 ¯sºhd|6\u0014MOLOLO=\\<BV \u0001Kdx°MN=\bPKLV|A I9P \u0001Kd%\b3Q[IK?US;A Iyd\u0019»i^JH=\\<>MO=\bI98\\=\b?sXWMOS>:l <>Q 79JKLN=\\I9?\b$6&A opMOI9l&7K?@=\bI9=cSz79?@=\\an79L{A ljACMOIyd\u0019e~I /}´@µ \u000f;·>·\u001d\u001c ± \u000b2)\f\u0007µ3# \r\u0005\b^· I \u0007\\·\u0005\u000bK±&\u001f 6b± \u000b\u000e\r¸·c´'\u0000H· \u000f;\b \u000b9± \u000f \t\u0007\u0006 !|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000fc·cV \u0001jA Ip79AC<@_ªB« « \u0010`d® ¼ ¬u¯s}dyxA[8>:9=\\SsACIEPr$d\nA\u0004\u0003\bA L]_`d\u0019¥S;ACi^Q I9Q[Rs_1Q a}R$7K?@Mg8\\A Lxl[=\\IKY<>=\b?\bdTe~I\u0019!|µ\f\u000b \r¸·\u0005\u000b\u000e\r \u0002 A \t\f\u0007\\·\u001d\u001c \u0018\u001a\u0004\u001b\u0006 \r±²m·\u001d\u001c ± \t \u0012 \u000b$#\bµ ´;² \t \r±µ \u000b E \u000f \u000f;·\u0005\u0007 \u0007!|µ\f\u000b$#B·c´>·\u0005\u000b\u000e\u000fc·)( :\u0013\u0012\u0015E\u000b*,+BV9xAC<>MO?\bVHJK<>MOL¼ ­[­ ­^d® ¼\u0017\tC¯sv=BPbxA MOI`S>=\\<WACIEPmhI9P^<>=BAC?\n©JEA IKMNA ?\bd^\u0019=\\<;8\\=\\JKS>7EACL8\\QpPKMOIKl\u001eQCaPKMOl MOS;ACL\u0019AC7EPKMOQ^d /}´@µ \u000f\u0007\u0006 \u0012\u0015\u0014\u0017\u0014\u0016\u0014wV \u0013\u0004\u0013K \u0006p; \u0006j¬pª9\u001ep¬pª\b¹^V¼ ­ ­[­^d® ¼\u0004\u0010u¯\n°QjA PK?\bd)\u0000 \bp· !|µ ²\u0015³-\u0004 \r¸·c´ \u0018\u001a\u0004 \u0007>± \u000f-\u0000\u001b\u0004 \rµC´c± \t\f\u0006 dh6\u0014eUv\u001dx<>=\b?@?\bVªB« «\u0016\tpd® ¼\u0017\u0013C¯s»Td}$d\n©8;:K=\bM]<>=\\<Bd,\u0018 \u0004 \u0007>± \u000f.\"\u0019± \u0007 \r¸·\u0005\u000b9± \u000b?)\"D \n\f\u0007 \r¸·c² \u0007>dW:9\u0003S>:9=\b?@MO?\bV6\u0014eUv\u0015VH6&A ?@?>A 8;:p79?@=cS@S>?\bV\u001c\u0001[7KI9=s¼ ­ ­[­pd® ¼C«C¯\u0018\bqd3}d\n©8>:p79=\b?@?@LO=\\<Bd'\u0018\u001a\u0004 \u0007;±\u0010\u000f \t\f\u0006/\u0000\u000e\t\f\u0007 \r¸· \t\f\u000b\u0003\u001c DHµ \u000f\\±µ\f\u0002 \u0014 \u000f>µ\f\u000bHµ ²\u001e± \u000fA \t \u000f;89) ´Uµ\f\u0004 \u000b\u0003\u001c d9:9S>:9=\b?@MO?\bVpefI9PKMNA I9A +\u0015I9MOZj=c<>?@MOS~_`Vpº}LOQ`Q[RGMOIKl YS>Q[IV9e~IEP^MgACIEApVyª\b«\u0016\u0013[­pd® ¹ ­C¯\r+\u001ed\n©:9AC<;P9ACIEACIEPd\n©Qp8\\MNA L\u0019MOIKanQ <>RmAuS>MNQ I&kEL]S>=\\<>MOI9lqanQ <hR$7K?@Mg8\\A L<>=B8\\Q RGRG=\bIEPKACS>MOQ[Id6&A ?US>=\\<B ?\u001eS>:K=\b?@MO?\bV|6&A ?@?>A 8;:p7K?@=\\S@S>?$e~I9?US>M]YS>7KS>=\u0015Q a|v=B8>:9IKQ[LOQ[lC_`V\nACR$F^<>MgP^l[=[VE6&A ?@?>A 8;:p7K?@=\\S@S>?\bVª\b«[«\u0004\u0006^d® ¹pª\\¯\u0018\bqd\n©X}=BAC<>MOIKl[=\bI\rA IEP\u0012°zd\n©MOI9:9A^d|º}=\\_jQ[IEP\u0012ACLNl Q <>M]S>:9RG?\b&hI\ne{JH=\\<>?@JH=B8cS>MOZj=\u0015Q[Im<>=B8\\Q[RGRG=\\IEPK=c<w?U_^?US>=\bRG?\bd^efI\u001aE7!F\u0018 D-\u00121\u00020\u0012\u001d: 6\u0014µ ´*8 \u0007*\b`µc³µ\f\u000b :T· \u000f>µ ²\u001e²m·\u0005\u000b\u0003\u001cj·c´ D-\n \u0007 \r¸·c² \u0007>Vbv\u0019ACRGJH=\\<>=[V\u0019MOI9LNA I9PV9h7Kl[79?USh¼C­[­pª[d® ¹[¼u¯ \u000e\u001edv \u0003BACI9=\\S;ACopMN?\bV \u000e\u001ed»?@?@LVA I9P1xd\nQ`Q[oHdh7^S>Q[RmACS>MN83R$7K?@M]Y8\bACLyl =\bIK<>=38cLgAC?@?@M]kH8\bAuS>MNQ I1Q a\u0019A 7EP^MOQ$?@MOl[I9A LO?\bdEe~I \u0001Kd\n©dK\u0015QuXWIKMO=A I9P3$duºwACMNIpF^<>MgP^l[=[V =BP^MOS>QC<>?\bV \u0012 \u000b \r¸·c´\u0005\u000b\u000e\t \r±µ\f\u000b\u000e\t\u0007\u00062D-\nC²\u0015³^µ\f\u0007>±\u0005\u0004p²µ\f\u000b\u0018\u001a\u0004 \u0007>± \u000f\u001a\u0012 \u000b\u001f#\bµC´c²\n\t \r±µ\f\u000b :T· \r´;±¸·c¶ \t\u0007\u0006 VTZ[Q[LO79RG=&¼pVTº{LNQ`Q RGMOI9l S>Q IyVefI9PKMNA I9A^V +\n©sV\n\b8cS>Q[FH=c<h¼ ­[­pª[d® ¹ ¹C¯ssdW\u0005=\bLOLN?\nACIEPsdvQ[opMOI9QB_pApdtvw:K=%l[=\\IK<>=\u0007JK<>=ca=\\<>=\bI98\\=\b?1QCaXw=\\?US>=\\<>IqJHQ[JK79LNAC<{R$7K?@MN8}F`_T¤@A J9A IK=\b?@=A[PKQ LO=\b?>8\\=\bI`S>?\bd /Tµc³\t\u0004\u001b\u0006\u0003\t ´\u0018\u001a\u0004 \u0007>± \u000f \t \u000b\u0003\u001c=DHµ \u000f\\±¸· \r\u0005\nCV¼ ¼^UªC; \u0006^ª[V\u0019ª\b«[«\u0016\u0013pd® ¹\u0017\u0006 ¯\u001e62d3%=\\LN?@:V \fsdzº}Q <>MO?@QuZHV \u0001KdzhMNLOLVq°zd3ZjQ Iº}=\b:^<>=\bIyVmACIEPsd[%Q`QKd .\u00157K=\\<@_^MOI9lzLNAC<>l[=\u00158\\Q[LOLO=B8cS>MOQ I9?WQ ayR$7K?@Mg8wanQ <T?@MORGMOLNAC<@YM]Sf_`dCv=\b8;:KI9MN8\bA L`°=\bJHQC<@SF+\nº \u001a\nT©3­ ­Y>ªB­ «\u0016\tpV\u000b+\u001ed\ndCº}=c<>oj=\bLO=\\_Q[RGJK7KS>=\\<\n©8\\MO=\bIE8c=3tMOZ^MO?@MOQ[IV \fhQuZj=\bR$FH=c<tªB« «[«^d"
    },
    {
        "title": "The CUIDADO Project.",
        "author": [
            "Hugues Vinet",
            "Perfecto Herrera",
            "François Pachet"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416940",
        "url": "https://doi.org/10.5281/zenodo.1416940",
        "ee": "https://zenodo.org/records/1416940/files/VinetHP02.pdf",
        "abstract": "The CUIDADO Project (Content-based Unified Interfaces and Descriptors for Audio/music Databases available Online) aims at developing a new chain of applications through the use of audio/music content descriptors, in the spirit of the MPEG-7 standard. The project includes the design of appropriate description structures, the development of extractors for deriving high-level information from audio signals, and the design and implementation of two applications: the Sound Palette and the Music Browser. These applications include new features, which systematically exploit high-level descriptors and provide users with content-based access to large catalogues of audio/music material.  The Sound Palette focuses on audio samples and targets professional users, whereas the Music Browser addresses a broader user target through the management of Popular music titles. After a presentation of the project objectives and methodology, we describe the original features of the two applications based on the systematic use of descriptors and the technical architecture framework on which they rely.",
        "zenodo_id": 1416940,
        "dblp_key": "conf/ismir/VinetHP02",
        "keywords": [
            "CUIDADO Project",
            "Content-based Unified Interfaces and Descriptors",
            "Audio/music Databases",
            "MPEG-7 standard",
            "Description structures",
            "Extractors",
            "High-level information",
            "Applications",
            "Sound Palette",
            "Music Browser"
        ],
        "content": "The CUIDADO Project\nThe CUIDADO Project\nHugues Vinet\nIRCAM\n1, pl ace Igor Stravinsky\nF-75004 Paris, FRANCE\nvinet@ircam.frPerfecto Herrera\nIUA-Universitat Pompeu Fa bra\nPg. C ircumva l·lació, 8\n08003 Barcelona , SPAIN\npherrera@iua.upf.esFrançois Pachet\nSONY- CSL\n6, rue Amyot\nF-75005 Paris, FRANCE\npachet@csl.sony.fr\nABSTRACT\nThe CUIDADO Project (Content-based Unified Interfaces and\nDescriptors for Audio/music Databases available Online) aims\nat developing a new chain of applications through the use of\naudio/music content descriptors, in the spirit of the MPEG-7\nstandard. The project includes the design of appropriate\ndescription structures, the development of extractors for\nderiving high-level information from audio signals, and the\ndesign and implementation of two applications: the Sound\nPalette and the Music Browser. These applications include new\nfeatures, which systematically exploit high-level descriptors\nand provide users with content-based access to large\ncatalogues of audio/music material.  The Sound Palette focuses\non audio samples and targets professional users, whereas the\nMusic Browser addresses a broader user target through the\nmanagement of Popular music titles. After a presentation of theproject objectives and methodology, we describe the original\nfeatures of the two applications based on the systematic use of\ndescriptors and the technical architecture framework on which\nthey rely.\n1. OBJECTIVES AND METHODOLOGY\nThe CUIDADO project (Content-based Unified Interfaces and\nDescriptors for Audio/music Databases available Online) aims\nat delivering new applications based on the systematic use of\naudio and music descriptors, which address new needs of the\nprofessional and consumer audio/music chain. This approach\nfalls within the scope of the MPEG-7 standardization process,\nin which CUIDADO participants have been already actively\ninvolved [21]. In this framework, descriptors and descriptor\nschemes combine knowledge bases with numerical features of\nthe audio contents, and are designed for several classes of\napplications. First, as metadata, enabling content-based\nretrieval functions on large audio/music databases, which\nwould not be feasible, due to data volumes, by directly\naccessing audio samples. Secondly, for allowing users to\nmanipulate audio/music contents through high-level\nspecification, designed to match the human cognitive\nstructures involved in auditory perception, as opposed to\ntraditional low-level, implementation-driven data structures.\nThe IRCAM Spatialisateur project, where perceptual parameters\nare used for controlling the sound quality of a simulated room\neffect, has proved the interest and usability of such an\napproach [12]. Similarly, mixing constraints have been\nintroduced as a particular form of content descriptors allowing\nreal time adaptation of complex music mixings [17]. More\ngenerally, the design of appropriate descriptors makes\npossible new application models for accessing and\nmanipulating audio contents and represents a major\nopportunity for the development of the music production and\ndistribution industries.However, in order to come up to effective results, several\nconstraints, which form the basic methodological framework ofthe project, must be taken into account:\ni) the necessity of a top-down approach, aiming at deriving\nhigh-level descriptors, which form the basic knowledge\nstructures accessible by the user in target applications. These\nknowledge structures must be adapted to application functions\nand consistent with the user cognition, i.e. the way his own\nknowledge about sound and music is organized.\nii) the interest in combining this top-down approach with a\nbottom-up approach, which extracts low-level descriptors from\nan automatic analysis of the audio signal. The objective is to\nautomatically compute required high-level descriptors,\navoiding manual input when possible. The main related issues,\ndiscussed in [10, 11] in the context of sound classification, are\nhow to choose  a set of relevant low-level descriptors and how\nto map targeted high-level descriptors onto them, by using\nappropriate machine learning techniques.\niii) designing and validating descriptors in laboratory\nconditions may however not suffice for real-world\napplications. In order to enable a complete assessment\nprocedure, it is necessary to build fully functional application\nprototypes, which take into account all technical constraints,\nand handle man-machine interaction issues. Machine learning\nfunctions enable the system to learn the user’s knowledge\nstructures; symmetrically, the design of the user interface is a\nkey factor for enabling the user to assimilate the system\nknowledge structures.\niv) such an ambitious program requires the combination of a\nmulti-disciplinary scientific and technological expertise,\nwhich has been gathered for the CUIDADO project in an\ninternational consortium including approximately 40\nresearchers and engineers from IRCAM (coordinator), IUA-UPF,\nSONY-CSL, Ben Gurion University, and industrial partners\n(Oracle, Creamware, Artspages). The project is supported by a\ngrant from the European Commission (Information Society\nTechnologies Program).\nIn order to fulfill these objectives, the project is organized\naround the development of two target applications, the Sound\nPalette, and the Music Browser, which address different needs.\nThese applications are presented in the next sections.\n2. THE SOUND PALETTE\nThe Sound Palette is dedicated to professional users in music\nand audio production. It offers audio sample management and\nediting features based on sound content description.\n2.1 Sample management features\nThe notion of audio sample is taken here in its usual\ndenomination, i.e. sounds of short duration (typically less\nthan 20 seconds), which contain individual events, such as\nsingle instrument notes. Actually, the system can handle\nsamples with more complex contents, but the main assumption\nmade on related descriptors is that they are computed for the\nwhole sound considered as a single entity, even if they reflectPermission to m ake digital or hard copies of all or part of this\nwork for personal or classroom use is granted w ithout fee\nprovided that copies are not m ade or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.\n© 2002 IRCAM – Centre PompidouThe CUIDADO Project\nits internal structure, e.g. as a decomposition into different\ntemporal phases.\nThe first available application using content-based\ndescriptions for sample management has been SoundFisher by\nMuscleFish [13]. The Studio Online project developed at\nIRCAM [3][25] provided an online server giving access to a\n117,000 instrument sample database, where high-level\nretrieval functions were experimented: browsing through a\nspecifically designed hierarchical taxonomy of samples\n(including contemporary playing modes) and a query by\nexample interface, relying on an elementary perceptual\nsimilarity distance resulting from studies on timbre spaces. A\nmore general descriptor scheme for instrument timbre was\nproposed and integrated in the MPEG-7 standard [16].\nTargeted features for the Sound Palette include various\nenhancements, in particular by enabling the management of theuser’s own sounds, and not only access to a fixed database as\nin Studio Online.\nThey combine the following functions:- management of a global sample repository, shared among\nseveral users, containing both a reference set and user samples,\nand available as a set of online Intranet/Internet services.\n- classification tools, which help the user to organize his\nsounds in classes, and provide automatic classification\nsolutions when introducing new sounds. Classes can be\nlearned from arbitrary user categories (as long as they refer to\nthe sound content), and can also be trained from more\nobjective classification schemes, such as sound sources, or\nmorphological characteristics (such as pitch or amplitude\nenvelopes, grain, etc.).\n- sample retrieval functions, combining criteria used for\nclassification (limitation of the search set) and query by\nexample functions, based on similarity distances, computedbetween each pair of samples across class boundaries.\nThe first tests of automatic classification algorithms, based on\nstatic low-level descriptors (computed for the whole sound\nduration), performed on standard instrument classes, show\npromising results [22]. New, more robust, perceptual similarity\nfunctions are also proposed, based on recent results of a meta-\nanalysis of main existing studies on timbre spaces [15].\nFurther developments foresee the us e of dynamic descriptors,\nwhich model the evolution of various sound features over\ntime, and the extension of proposed search functions through\nthe use of textual attributes, which qualify various aspects of\nthe sound contents.\nAll these features combine low-level descriptors automatically\ncomputed from audio signals an d high-level descriptors,\nwhich are either provided by the system or user-defined. The\ndesign principle for low-level descriptors is to set up an as\ncomprehensive set as possible, including characteristics which\naccount for the global temporal evolution, spectrum shape,\nenergy, harmonicity, timbre dimensions, etc. Depending on\ntargeted high-level features (e.g. for learning the characteristics\nof a given classification scheme), the most relevant low-level\ndescriptors are automatically selected from this redundant low-\nlevel descriptor set using selection criteria such as the\ncomputation of mutual information between descriptors and\nclasses [22].\nTwo versions of the Sound Palette are developed in parallel:- an “online version”, which includes all above functions, and\nis experimented as a common sample database for all musical\nproductions at IRCAM.\n- an “offline version”, integrated in an audio production\nworkstation such as the Creamware Pulsar system, which is\nrestricted to the management of the user’s samples on his local\nPC, but also includes sample editing features described in the\nnext section.\nFigure 1. Browse screen of the Online Sound Palette\nThe CUIDADO Project\nFigure 1 shows the “Browse” screen of the Online Sound\nPalette, whose interface mainly relies on HTML and Javascript\nprotocols and can be accessible from any Web browser. The\ntop-left frame is an editor for personal user data: user-created\nsample directories, reference sample collection directories he\nhas access to, etc. The bottom-left frame is dedicated to high-\nlevel description schemes, such as a taxonomy of sound\nsources, or any other classification scheme. The top-right frame\ndisplays the list of items from the selected directory (in that\ncase, the Cello directory in the System category), with the\npossibility of listening to (by streaming) and downloading\nany sound or group of sounds. Multiple selection from various\ndirectories is available. Cut-copy-paste functions between\ndirectories are also available, but, unlike file management\nsystem metaphors, a sound can be linked to any number of\ndirectories. The bottom-right frame displays details of the\nselected sample on the upper list, with all directories it is\nrelated to. Another screen enables a multi-criteria search,\nincluding full text search using sound names, directory names,\nand free user textual comments on sounds and directories. The\nsystem features a complete management of access rights (see,\nlisten, modify/delete) for samples and directories. This\napplication is designed and developed at IRCAM by the\nOnline Services Team .\nFigure 2 shows the “Sound Palette Browser” window of the\nOffline Sound Palette, integrated by Creamware in its Pulsar\nsystem, which displays similar information areas, such as\nsample categories and lists, as well as the waveform of the\nselected sample.\n2.2 Sound editing features\nThe Sound Palette can handle three different kinds of sound\nmaterials: sound samples, music monotimbral phrases, and\nrhythm tracks (for example the  “drum loops” that are\nubiquitous in popular music).\nDeriving a multi-level music description of these materials and\npopulating large databases with them paves the way for newnavigation, editing and transformation possibilities. The\nfunctionalities are even more enhanced when a synthesis\nengine is allowed to play some role in the general architecture\nof the system. Provided this system, we can talk about\n“content-based sound edition”, a concept that has much in\ncommon with old proposals of “i ntelligent sound editors” [4].\nMore recent research on which the Sound Palette (offline\nversion) relies can be found in [1, 5, 6, 7, 9, 11, 14, 24]. It\nshould also be noted that nowadays there is a bunch of\ncommercial applications that implement content-related\nfunctionalities, though in most of the cases, content\nrepresentation is hidden to the user. We could mention some\naudio-to-MIDI programs, such as Wildcat’s Canyon’s\nAutoscore , Epinoisis’s Digital Ear , or Recognisoft’s\nSoloExplorer  that perform some rough transcription, even\nattempting instrument recognition. These tools are not aimed,\nthough, to do more than MIDI approximations of some audio\ncontent, and their performance is, to put it politely, not\noptimal. Propellerhead’s ReCycle! , Digidesign’s Beat Detective ,\nand Sonic Foundry’s Acid  are tools for loop manipulation that\nuse some segmentation tricks and rhythm description in order\nto re-arrange or re-create a music file. Those “descriptions” are\nstored in proprietary file formats. Another conceptually related\npiece of software is Celemony’s Melodyne , a tool for flexible\ntransformation of audio that allows the user to transpose a\nmusical phrase or some of its individual notes without timbre\ncorruption, to stretch or compress notes along the time axis\npreserving their attacks unchanged, to change vibrato and\narticulation inflexions, and finally deriving a MIDI\nrepresentation of the musical content. As we are going to see,\nthere are some editing and transformation functionalities\n(provided by the types and levels of music content\nrepresentation they manage) that none of them are currently\nexploiting. Navigation in the Sound Palette editing\nenvironment may use note, phrase or instrument labels instead\nof the handwritten markers that are usually mandatory in\nstandard editors. “Skip to next note” or “Skip to next snare\nhit” provide better location than spotting the cursor\nsomewhere “there” by visually inspecting the waveform and\nscrub-listening to it.\nAutomatic segmentation and labeling of content also allows\nmultiple selections of those segments. Once they are selected,\nit is possible to perform editorial actions on all of them\nsimultaneously. For example, after automatically describing anelectronic bass phrase with note and sub-note descriptors, it is\npossible to select a portion of their attacks and chop them (a\nwell-known technique in certain popular styles). This is done\nin a compact way instead of manual-visual selection and\nchange of each one of the portions to be edited.\nCreation of MIDI maps from an audio rhythm loop is another\nconvenient function that cannot be completely achieved with\ncurrent commercial software devoted to this type of sonic\nmaterials (e.g. Recycle!). A Sound Palette rhythm description\ncontains not only information about onset location of sounds\nand basic pulses that underlie in the track but also generic\nlabels for the percussion instruments. This way, a MIDI\n“recreation” of an example drum loop can be set up for using\nour owned drum kit samples, instead of some illegal CD-lifted\nfragment. Thanks to a perceptual similarity-based search\nfunction, drum samples used in the recreation can even be\nperceptually very close to the original ones (provided a large\nsound database, indeed!), and not only the same in terms of\ncategory.\nAs editing and transforming sounds become operations with\nno clear boundaries between them, transformation comes into\nplay even in editorial tasks such as concatenating two\nrhythmic patterns for using the composite as a song building\nblock. Let’s suppose that pattern A has a global tempo of 100\nFigure 2. File browsing and retrieval sections of the\nOffline Sound PaletteThe CUIDADO Project\nBPM and pattern B scores at 80 BPM, and we want the\ncomposite to be at 93 BPM. Rhythm tempo matching can be\nachieved by conveniently time-stretching the original\npatterns. Seamless and continuous fusion between different-\ntempo rhythm patterns is possible without bothering the user\nwith tempo or stretch parameter adjustments.\nVariation generation is a must in the composer armamentarium,\nand the Sound Palette implements some options for that.\nTraditional variations on a musical phrase by tonality change\nof a phrase are achieved through transposition of specific\nnotes from the original phrase. Rhythm tracks can also be\naltered by swapping a specified instrument by another, or by\nmuting the specified one.\nMore original variations can be achieved by applying a\n“musaicing” process to a musical phrase as described in next\nsection [26]. Provided that we get access to a sound database of“world music” voices, a vocal track could be transformed into atimbral mosaic containing bits of those voices (instead of theoriginal one) though keeping the same pitch than the original.3. THE MUSIC BROWSER\nThe Music Browser is intended to be the first content-based\nmusic management tool for large music catalogues. This\napplication is targeted to all the actors of the music\ndistribution industry: labels who want to exploit\nsystematically their catalogues, music distributors who want\nto propose customers with personalized music playlists, radios\nwho have to schedule music programs that match user’s t astes,\ncopyright societies, who want to control illegal copies of\nmusic titles, and more generally the new businesses of\nElectronic Music Distribution yet to come.\nSeveral music browsers have been made available to the public\nrecently. Most of them are title-based, that is: the user has to\nexplicitly state what he is looking for. The few ones that\npropose some sort of content-based approach (MoodLogic,\nRelatable) suffer from two main drawbacks:\n1) The used metadata is entirely man ual. MoodLogic\naccumulated a huge database of music descriptors, built from\ncollaborative manual inputs. Manual metadata is however very\nexpensive to build and maintain. Besides, high-level\ndescriptors are usually hardly consensual (what does “Rock”\nmean?).\n2) There is no facility for building automatically consistent\nmusic playlists. Titles are retrieved individually, forcing the\nuser to organize his/her own playlists, which is time-\nconsuming, and requires a good knowledge of music\nclassifications.\nThe aim of the CUIDADO Music Browser is two-fold.\nTechnically the goal is to implement the whole processing\nchain, from the music data itself (signal, Midi, or reference\ninformation) to the user, with as much automatic processing as\npossible, and with a flexible music playlist facility.\nConceptually, the aim of the Music Browser is to build\nFigure 3. Edition and transformation windows of the Offline Sound Palette.\nThe upper section contains content-related editing functions and time-scale transformation functions. The lower  one\ncontains “loop reconstruction” and some event-based functionalities.\nFigure 4. Pitched events editing section of the Offline\nSound PaletteThe CUIDADO Project\nevidence that content-based music management does allow\nefficient personalized music access and brings substantial\nadded value in comparison to existing music distribution\nschemes.\nTechnically, the main challenge of such a music browser is to\nbe useful for the largest possible set of music behaviors.\nPreliminary experiments show indeed that user behaviors\nexposed to large music catalogue may be extremely different.\nSome users may want to browse using top down genre\ncategories, others are similarity and example oriented. Some\nknow exactly what they look for, others want explicitly only\nnew, unknown titles, etc [20]. The Music Browser consequently\nimplements all the browsing mechanisms, including:\n- Descriptor-based  search: “I want titles with fast tempo, high\nenergy, in the Rock generic genre”,\n- Similarity-based  search: “I want titles which are “close” to a\ngiven set of titles”. The similarity may be picked up from a list\nof several similarity relations, each one with a different\nsemantics, e.g. cultural similarities, metaphoric similarities,\nartist-based similarities, etc\n- Global search : “I want a playlist with instrumental music at\nthe beginning and dance music at the end, with increasing\ntempo values, and more than 60% of female singing voice”.\nNavigation within titles is also considered, through the use of\nautomatically computed music summaries, which index the\nmain title parts through a newly designed segmentation\nmethod [23].\nThe most important modules of the processing chain,\nillustrated in Figure 5, are  presented hereinafter.\n3.1 Unary descriptor extractor module\nThis module implements top-down signal processing\ntechniques that extract global unary information about music\ntitles. Music titles are considered here as 3 to 4 minute\narbitrary complex music signals. The descriptors considered\ntarget the music title in its entirety, as opposed to low-level,\nlocal descriptors that consider only small portions of a signal\n(as in the Sound Palette application).\nUnary descriptors are interesting for two reasons. First, as such,\nthey provide a basis for descriptor-based queries. Second, they\nyield similarity relations that can be exploited during music\nsearch.\nMore precisely, the descriptors targeted are of three sorts:1) Rhythm descriptors . This category includes tempo/beat, as\nwell as rhythmic information. Preliminary results about\nrhythm extraction have been obtained and documented in [8].In particular, we have focused on percussive rhythm, i.e.\nrhythm produced by repeated occurrences of percussive\nsounds.\n2) Energy-based descriptors . These descriptors aim at\nextracting rough categories of perceptual energy. A typical\nexample is the intuitive perceptive difference between a hard\nrock piece and an acoustic guitar folk song. The mapping\nbetween perceptual energy and low-level descriptors is done\nusing machine learning techniques applied on a vector of\ndescriptors such as physical energy, energy variance, tempo,\netc.\n3) Timbre-based descriptors . These descriptors aim at\nextracting a global representation of the overall timbre of the\nsong. The resulting descriptor is not meaningful in itself, but\nserves as a basis for establishing timbre similarities between\ndifferent songs.\n3.2 Similarity analysis modules\nLow-level descriptors produce low-level similarities. There are\nother means of extracting higher-level similarity relations for\nmusic. A well-known technique for producing similarities is\ncollaborative filtering . This technique builds similarities\nbetween titles based on similarities between user profiles.\nAlthough the technique does establish relevant music\nsimilarities, it carries with it a number of important drawbacks,\nwhich limit in practice its usability, such as the need for an\ninitial, large, set of user profiles, and the degradation of\nquality in recommendations for rich, eclectic profiles. We have\ninvestigated other techniques based on data mining, applied tocorpuses of textual information. The studied corpuses range\nfrom collections of album playlists (e.g. as found in CDDB),\nradio program listings, and general search engines (e.g.\nGoogle). The techniques used are based on co-occurrence, i.e.\neach time two titles are found in a similar context (radio\nprogram, album playlist or web page), their co-occurrence\ncounter is incremented. Eventually we build a similarity\nmeasure from this co-occurrence matrix. We have shown in [18]\nthat the extracted similarity is meaningful, non-trivial, and\ncomplementary to the similarities extracted through low-level\ndescriptors.\nThe extracted similarity relations are then exploited in the\nmusic browser for similarity-based search, as well as for\nproposing music discovery schemes. Through a simple user\ninterface, the user can quickly explore regions of the catalogue\nwhich are either very close to his/her set of preferred titles, or\nvery far (see [19] for a prototype implementing varying-length\nmusic catalogue exploration).\nAudio Title Database\nUser Interface\nTextual Musical InformationSimilarity ExtractorUnary Descriptor Extractors\nMetadatabaseMusic QueriesQueries Results\nNetworkAudio Title Database\nUser Interface\nTextual Musical InformationSimilarity ExtractorUnary Descriptor Extractors\nMetadatabaseMusic QueriesQueries Results\nNetwork\nFigure 5. The main content-management modules of the Music BrowserThe CUIDADO Project\n3.3 Playlist generation module\nAs mentioned in the preceding section, music titles are rarely\naccessed and listened to in isolation. We have proposed in [18]\nto consider sequence generation as the main paradigm for\nmusic retrieval, instead of the standard title-based approach. Inparticular, sequence generation allows user to explore\nefficiently the catalogue without having to learn the\nunderlying descriptor ontologies. The main idea is to let the\nuser express global properties of music playlists, such as\ncontinuity (I want a music playlist which is continuous,\ntempo-wise, or style-wise, etc.), cardinality (at least 60%\ninstrumental, Funk titles), and distribution (no repetition of\nmore than two titles by the same artist). It was shown that\nspecifying properties of playlists is indeed much easier than\nspecifying properties of individual, unknown titles.\nFor instance, a 10-title playlist can be specified with the\nfollowing properties:\n-All titles are different,-Increasing tempo,-Three cardinality constraints on genre:\no less than 50% World,o exactly 4 Rock,o more than 50% Folk.\nA solution to this play list generation “problem” found from\nour 20,000 title test catalogue is:\n1) Slip slidin' away - Simon and Garfunkel - tempo 63 - genre =\nFolk \\ Pop\n2) Un autre monde - Telephone - tempo 71 - genre = Rock \\\nSong\n3) Barbès - FFF - tempo 78 - genre = Rock \\ Funk4) Waiting for the Miracle - Leonard Cohen - tempo 82 - genre\n= Folk \\ Pop\n5) I Palami Sou – Angelique Ionatos - tempo 98 - genre =\nWorld \\ Mediterranean\n6) The Sire Of Sorrow - Joni Mitchell - tempo 113 - genre =\nFolk \\ Pop\n7) Tom’s Dinner – Suzanne Vega - tempo 123 - genre = Folk \\\nPop\n8) You're So Great - Blur - tempo 133 - genre = Rock \\ Brit9) The Boy In The Bubble - Paul Simon - tempo 138 - genre =\nFolk \\ Pop\n10) The return of the lost palmas 7 - Madness - tempo 144 -\ngenre = Rock \\ New Wave \\ Ska\nHowever, finding solutions for large catalogues is inherently\nNP-hard. We have developed a new algorithm for finding\nquickly solutions for catalogues of up to 200,000 titles [2],\nand able to handle arbitrary complex properties. This\ntechnique is efficient enough to produce solutions in a few\nseconds, as well as to form a basis for implementing relevance\nfeedback techniques. In this scheme, the user selects a few\nsequence properties off-the-shelf, gets quickly a solution, and\nthen refines interactively this solution, either by explicitly\ndeleting unwanted music titles, or by adding or removing\nsequence properties.\nThe same sequence generation module has also been applied to\nthe problem of generating sound sequences from a catalogue ofsamples, as designed and implemented in the Sound Palette\napplication. The result, coined as Musaicing [26], is a new\ngeneration of audio sequencers, in which the user specifies the\nglobal properties of the sequence to generate, instead of\nchoosing and retrieving explicitly individual samples. Apreliminary prototype has been implemented, and current work\nfocus on the design of a library of sound sequence properties,\nadapted to the generation of popular music styles.\n3.4 Web Music Monitoring System\nThis module, integrated in the Music Browser, is specifically\nintended for copyright societies. It performs the identification\nof unknown music excerpts, e.g. found on th e Web, with a\nreference copyrighted title database. So this method addresses\nthe issue of music copyright protection, in a different way than\nwatermarking, which requires the insertion of a digital\nidentifier in the audio signal and is more sensitive to\nmodifications introduced by audio coding processes, such as\ncompression. The identification principle is based on signal\ncomparison. It relies on audio signatures, stored as metadata,\nwhich capture relevant information by encoding original\nsignals with high compression rates. Tests performed with 12\nseconds excerpts on a database of 650 titles give a successful\nmatching rate greater than 97%.\n4. ARCHITECTURE\nA common hardware and software architecture has been\ndesigned and set up for both online applications. It provides\nall required services, including: massive data storage, audio\nstreaming, file uploading and downloading, audio file,\nmetadata and user data management, middleware, etc. All used\nprotocols are chosen to be as standard as possible. Access to\nonline services is provided from a variety of client terminals,\nthrough the use of HTML/XML when possible, otherwise Java\nprotocols for client interfaces. The physical architecture, based\non a 3-tier architecture, is shown in Figure 6. Logical functions\nare split between four Linux servers and a massive storage\nsystem, interconnected through two private networks: a\ncontrol data network for middleware exchanges, and a high\nbandwidth network for sound file transfers.\nAll sound files are stored in the massive storage system (from\nNetwork Appliances), and all data handled by the database\nsystem, including descriptors, are stored in the database server.\nThe sound and processing server performs all required\ncalculations on sound files, including descriptor extraction.\nThis modular architecture minimizes system administration\nand enables a selective upgrade of servers when necessary.\nFigure 6. System architectureWeb server Streaming serve r\nDatabase serve r\n(ORACLE)\nSound and Processing\nServe rStorage systemInternet network\nControl data network\nSound data networkThe CUIDADO Project\n5. CONCLUSION\nThe main objectives of the CUIDADO project have been\npresented, as well as the design principles of its main target\napplications: the Sound Palette and the Music Browser. The\nfeatures of these prototypes illustrate new possibilities in\nvarious application fields that directly result from research,\nperformed as part of the project, on audio and music sound\ndescription. At the time of this publication, the first versions\nof the applications are going to be released and tested by\nselected users. This test and assessment period will be\nessential for refining the proposed features according to users’\nreactions, testing and optimizing the system performances in\nreal situations, and converging to the final application\ndevelopment scheduled for the end of 2003. Updated\ninformation about the project can be found at\nhttp://www.cuidado.mu/     and     http://www.ircam.fr/cuidado    .\n6. ACKNOWLEDGMENTS\nThe CUIDADO project is funded by the Information Science\nand Technology Program of the European Commission.\n7. REFERENCES\n[1] Amatriain, X., J. Bonada, A.Loscos, and X. Serra,. “Spectral\nProcessing”, Udo Zölzer (Ed.), DAFX: Digital Audio\nEffects, John Wiley & Sons Publishers, 2002\n[2] Aucouturier, J.-J. and F. Pachet “Scaling up Music Playlist\nGeneration”, IEEE International Conference on\nMultimedia Expo, Lausanne (Switzerland), August 2002.\n[3] Ballet G., R. Borghesi, P. Hoffmann, F. Lévy, “Studio\nOnline 3.0: An Internet \"Killer Application\" for Remote\nAccess to IRCAM Sounds and Processing tools”, Proc. of\nJournées d’Informatique Musicale (JIM’99, 1999),\navailable online at:\nhttp://www.ai.univ–paris8.fr/~jim99/actes_html/BalletJIM99.htm     \n[4] Chafe C., B. Mont-Reynaud and L. Rush (1989) “Toward\nan Intelligent Editor of Digital Audio: Recognition of\nMusical Constructs” In C. Roads (ed.), The Music\nMachine. MIT Press, Cambridge, MA.\n[5] Dubnov, S. “Extracting sound objects by independent\nsubspace analysis”, Proceedings of AES22 International\nConference on Virtual, Synthetic and Entertainment\nAudio. Espoo, Finland, 2002.\n[6] Garcia, D., and X. Amatrian, “XML as a means of control for\naudio processing, synthesis and analysis”, Proceedings ofMOSART Workshop on Current Research Directions in\nComputer Music, Barcelona, 2001.\n[7] Gomez, E., F. Gouyon,., P. Herrera, and X. Amatriain, (in\npreparation). “Music Description Schemes and the current\nMPEG-7 standard.”\n[8] Gouyon F, O. Delerue, F. Pachet, “On the use of zero-\ncrossing rate for an application of classification of\npercussive sounds” Digital Audio Effects Conference,\nVerona (It), December 2000.\n[9] Gouyon, F. , P. Herrera, and P. Cano,”Pulse-dependent\nanalyses of percussive music”, Proceedings of AES22\nInternational Conference on Virtual, Synthetic and\nEntertainment Audio. Espoo, Finland, 2002.\n[10] Herrera P., G. Peeters, S. Dubnov, “Automatic\nClassification of Musical Instrument Sounds”, Journal ofNew Music Research, 2002 (to appear).\n[11] Herrera, P., A. Yeterian, and F. Gouyon, “Automatic\nClassification of Drum Sounds: A comparison of feature\nselection methods an d classification techniques”,\nProceedings of the 2nd International Conference on Music\nand Artificial Intelligence, Edimburgh, United Kingdom,\n2002.\n[12] Jot, J.M., \"Efficient Models for Distance and\nReverberation Rendering in Computer Music and Virtual\nAudio Reality.\" Proceedings of the International\nComputer Music Conference. San Francisco: ICMA, 1997\n[13] Keislar, D., T. Blum, T., J. Wheaton, & E. Wold,. “A content-\nware sound browser”. Proc. of the International Computer\nMusic Conference, ICMA, 1999.\n[14] Klapuri, A., “Sound Onset Detection by Applying\nPsychoacoustic Knowledge”, IEEE International\nConference on Acoustics, Speech an d Signal Processing,\nICASSP 1999.\n[15] McAdams S, S. Winsberg, “A meta-analysis of timbre\nspace. I: Multidimensional scaling of group data with\ncommon dimensions, specificities, and latent subject\nclasses” Journal of the Acoustical Society of America, (in\npreparation)\n[16] ISO/IEC FCD 15938-4 Information Technology -\nMultimedia Content Description Interface - Part 4 Audio.\n[17] Pachet, F., O. Delerue. “On-The-Fly Multi Track Mixing\",\npreprint of the Audio Engineering Society, 109th\nConvention, Los Angeles, 2001.\n[18] Pachet F., P. Roy, D. Cazaly, “A Combinatorial approach to\ncontent-based music selection”, Proc. of IEEE\nInternational Conference on Multimedia Computing and\nSystems, Firenze, Italy, Vol. 1 pp. 457-462, 1999.\n[19] Pachet, F., G. Westerman, and D. Laigre, “Musical Data\nMining for Electronic Music Distribution”, WedelMusic\nConference, Firenz, Italy,, Nov. 2001.\n[20] Pachet F., “Electronic Music Distribution: The Real\nIssues”, Communications of the ACM, to appear, 2002.\n[21] Peeters G., S. McAdams, P. Herrera, “Instrument sound\ndescription in the context of MPEG–7”, Proc. of the\nInternational Computer Music Conference, ICMA, 2000.\n[22] Peeters G., P. Tisserand, “Selecting signal features for\ninstrument sound classification”, Proc. of the\nInternational Computer Music Conference ICMA, 2002\n(Submitted).\n[23]  Peeters G., A. La Burthe, X. Rodet, “Toward Automatic\nMusic Audio Summary Generation from Signal Analysis”,\nProceedings of the International Conference on Music\nInformation Retrieval, Ircam, Paris, 2002.\n[24] Tzanetakis, G. “Manipulation, Analysis and Retrieval\nSystems for Audio Signals”, Ph. D. dissertation, Princeton\nUniversity, 2002.\n[25]  Wöhrmann, R., G. Ballet, ”Design and architecture of\ndistributed sound processing and database systems for\nweb-based computer music applications”, Computer\nMusic Journal, vol 23, Number 3, p.77-84, 1999.\n[26] Zils, A., F. Pachet, “Musical Mosaicing” Proceedings of\nDAFX 01, Limerick (Ireland), 2001."
    },
    {
        "title": "A Kind of Content-Based Music Information Retrieval Method in Peer-to-peer Environment.",
        "author": [
            "Chaokun Wang",
            "Jianzhong Li 0001",
            "Shengfei Shi"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417441",
        "url": "https://doi.org/10.5281/zenodo.1417441",
        "ee": "https://zenodo.org/records/1417441/files/WangLS02.pdf",
        "abstract": "In this paper, we propose four peer-to-peer models for content- based music information retrieval (CBMIR) and carefully evaluate them on network load, retrieval time, system update and robustness qualitatively and quantitatively. And we bring forward an algorithm to improve the speed of CBP2PMIR and a simple but effective method to filter out the replica in the final results. And we present the architecture of QUIND, a content-based peer-to- peer music information retrieval system, which can implement CBMIR. QUIND combines content-based music information retrieval technologies and peer-to-peer environments, and has strong robustness and good expansibility. Music stored and shared on each PC makes up of the whole available music resource. When a user puts forward a music request, e.g. a song or a melody, QUIND can retrieve a lot of similar music quickly and accurately according to the content of music. After the user selects his favorite ones, he can download and enjoy them.",
        "zenodo_id": 1417441,
        "dblp_key": "conf/ismir/WangLS02",
        "keywords": [
            "peer-to-peer models",
            "content-based music information retrieval",
            "network load",
            "retrieval time",
            "system update",
            "robustness",
            "algorithm",
            "filtering",
            "architecture",
            "QUIND"
        ],
        "content": "A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nA Kind of Content-Based Music Information Retrieval\nMethod in a Peer-to-Peer Environment\nChaokun Wang\nDepartment of Computer Science\nand Engineering, #318\nHarbin Institute of Technology\nHeilongjiang, 150001, P.R. China\n+86— (0)451—6415872\nwangchaokun@0451.comJianzhong Li\nDepartment of Computer Science\nand Engineering, #318\nHarbin Institute of Technology\nHeilongjiang, 150001, P.R. China\n+86— (0)451—6415827\nlijz@mail.banner.com.cnShengfei Shi\nDepartment of Computer Science\nand Engineering, #318\nHarbin Institute of Technology\nHeilongjiang, 150001, P.R. China\n+86— (0)451—6415872\nshengfei@0451.com\nABSTRACT  \nIn this paper, we propose four peer-to-peer models for content-\nbased music information retrieval (CBMIR) and carefully evaluate them on network load, retrieval time, system update and robustness qualitatively and quantitatively. And we bring forward an algorithm to improve the speed of CBP2PMIR and a simple but effective method to filter out the replica in the final results. And we present the architecture of QUIND, a content-based peer-to-peer music information retrieval system, which can implement CBMIR. QUIND combines content-based music information retrieval technologies and peer-to-peer environments, and has strong robustness and good expansibility. Music stored and shared on each PC makes up of the whole available music resource. When a user puts forward a music request, e.g. a song or a melody, QUIND can retrieve a lot of similar music quickly and accurately according to the content of music. After the user selects his favorite ones, he can download and enjoy them. \n \n1. INTRODUCTION \nAs the amount of music information increases rapidly, people \nraise higher demands for music retrieval. Instead of retrieving music by tag-like information (e.g., the title of music) people want to retrieve their favorite music by just providing a piece of similar music, such as singing a song or humming a melody. This kind of music retrieval is named Content-Based Music Information Retrieval (CBMIR), which means that given a piece of music, we should retrieve a lot of similar music from a depository of music only based on the content of music. In recent years, more and more researchers pay attention to CBMIR. And they have gotten a lot of results in this field [1~3]. \nPeer-to-peer (P2P) is another increasing research field these years, \nfor example Gnutella [4]. As the basis of the Internet, now P2P becomes another focus of people. Differing from Client/Server, in P2P architecture there is not obvious discrimination that exists between client and server in C/S. Any device connected via P2P has the same degree and has the abilities of client and server. P2P makes PC become the center of the Internet. One can share the files saved on his hard disk, enjoy the files shared and preserved on PCs of other people and directly download these files if he likes them. For convenience we consider the node or peer of a P2P system as a personal computer (PC) in the rest of this paper and think they have the same meaning. \nWe believe that Content-Based Peer-to-peer Music Information \nRetrieval (CBP2PMIR) will be an interesting field as the combination of CBMIR technologies and P2P environments. And \nit will effectively utilize the gigantic music resource distributed on numerous PCs of the Internet. So far as we know, this paper is the first one about CBP2PMIR. \nMany new problems arise in CBP2PMIR because there are a lot of \ntasks about music computation that must be finished in P2P environments. In this paper, we will research the following three \nquestions. The first is what the appropriate model of CBP2PMIR is. We should have a proper model to work on it. The second is how to improve the retrieval speed in CBP2PMIR. There are so many songs in the P2P system that it is very important to find a good method to accelerate the retrieval. The last is how to filter \nthe retrieval results. Perhaps many  same music files exist in the \noriginal results, so we’d better find them and just retain one. \nIn this paper we propose four peer-to-peer models for CBP2PMIR \nand describe the query process in each one. And we discuss these models on network load, retrieval time, system update and robustness qualitatively and quantitatively. After that, we bring forward an algorithm to improve the retrieval speed based on several useful concepts. And we also put forward a simple but effective method to filter out the replica in the final results. \nAnd we present the architecture of QUIND, a CBP2PMIR system, \nwhich implements CBMIR and consists of PCs and coordinators. Music stored and shared on each PC makes up of the whole available music resource. When a user puts forward a music request, for example a song or a melody, QUIND quickly and efficiently retrieves some similar music from the whole system according to the content of music represented as the features of music. After the user selects his favorite ones from the final results, he can download and enjoy them. \nThe rest of this paper is organized as follows.\n In next section we \npropose four different models of CBP2PMIR and describe the query process in each one. In section 3 we carefully discuss these models on network load, retrieval time, system update and robustness qualitatively and quantitatively and list the advantages and disadvantages of these models. And we present an accelerating algorithm and an effective filtering met hod in section \n4. Then we describe the architecture of our system QUIND in section 5 and review related works in section 6. Finally we summarize our contributions in section 7. \n2. MODELS \nThere are many new problems in CBP2PMIR. The first is what the model of CBP2PMIR is. The model of CBP2PMIR is very important because it is the basis of further research in the field. \nIn this section we developed four models of CBP2PMIR systems \nwhich can implement the content-based music information retrieval in peer-to-peer environm ents. The first two models have \nthe centralized architecture, the third distributed, and the last hybrid. We will compare them in the next section. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n© 2002 IRCAM – Centre Pompidou   A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \n2.1 PsC Model \nThe Peers-Coordinator Model (PsCM) of CBP2PMIR can be \nrepresented as a triple (PC, Coordinator, Query). \n(1) The PC consists of a network identifier, a music set and a same \nfeature extraction method. Any element in the music set is comprised of a unique identifier in the PC, a music file and its music feature which is computed by the feature extraction method. \n(2) The coordinator consists of a network identifier, a data \nstructure to store all music features in the P2P system, and a feature matching method to compute the distance between two music features, i.e. the similarity between the two corresponding music files. \n(3) The query is a request proposed by a user, which may be a \npiece of music uploaded, or a section of melody recorded. \nIn this model (Figure 1), any PC in the CBP2PMIR system \nconnects with the coordinator. Music data are distributed over all PCs. PCs connect with each other via the coordinator, but they directly transfer music data from one to another. \nThe main steps of a query process in the model PsCM are the \nfollowing: \n0) Each PC shares some music stored on the local hard disk, and \nregisters at the coordinator. The register information includes its network identifier, and the summary of music resource shared on itself, such as the identifiers of the music files and the features of them. \n1) A user brings forward a music request on any PC of the system \nby providing a piece of music, singing a song or humming a melody (This PC is named the request PC). Then the feature of the music request is extracted from the music request by the feature extraction method and sent to the coordinator. \n2) The coordinator receives the feature of the music request and \ncompares it with all music features uploaded in step 0, and then sends the result to the request  PC. The result contains the \nlocations (i.e. network identifiers) of the PCs which store the music similar to the request (These PCs are named destination PCs), the identifiers of the similar music files and the matching values between these files and the music request. \n3) After the request PC receives the result in step 2, the user \nselects his favorite music from it and then asks the destination PCs for connections. \n4) If the connections are established, the user can download and \nenjoy his selection. \nIn the model PsCM, the system provides the directory service and \nfeature matching is processed on the coordinator. We can get the model PsC+M, another realization of the centralized P2P system, \nif we put the feature matching process on each PC. \n2.2 PsC+ Model \nThe Peers-Coordinator+ Model (PsC+M) of CBP2PMIR can also be represented as a triple (PC, Coordinator, Query). \n(1) The PC consists of a network identifier, a music set, a same \nfeature extraction method and a same feature matching method. Any element in the music set is comprised of a unique identifier in the PC, a music file and its music feature which is computed by the feature extraction method. The music feature matching method is used to compute the distance between two music features, i.e. the similarity between the two corresponding pi eces of music. \n(2) The coordinator consists of a network identifier and a data \nstructure which stores the network identifiers of all PCs. \n(3) The query is a request proposed by a user, which may be a \npiece of music uploaded, or a section of melody recorded. \nIn this model (Figure 1), the main steps of a query process are the \nfollowing: \n0) Each PC shares some music stored on the local hard disk, and \nregisters at the coordinator. The register information is the location of this PC, i.e. its network identifier. The information of music resource shared on this PC does not be uploaded. \n1) A user brings forward a music request on any PC of the system \nby providing a piece of music, singing a song or humming a melody (This PC is named the request PC). Then the feature of the music request is extracted from the music request by the feature extraction algorithm and sent to the coordinator. \n2) The coordinator receives the feature of the request music and \nsends it to all PCs which have registered. Each PC compares it with the features of the local shared music and sends the local result to the coordinator. The local result includes the network \nidentifier of this PC, the identifie rs of some similar music files, \nand the matching values between these files and the music request. The coordinator gathers all the local results and sorts them \naccording to the matching values, then sends the final result to the request PC. The final result contains the network identifiers of PCs which store the music similar to the music request (These PCs are named destination PCs), the identifiers of the similar music files and the matching values between these files and the music request. \n3) After the request PC receives the final result in step 2, the user \nselects his favorite music from it and then asks the destination PCs for connections. \n4) If the connections are established, the user can download and \nenjoy his selection through the request PC. \n2.3 PsPs Model \nIn a distributed CBP2PMIR system, if one PC can send the same query to at most m PCs, m is said the width of the system. And if \none query can be sent through at most n hops, n is said the depth \nof the system. \nIn the PsPs model, the CBP2PMIR system is formed by a lot of \nPCs without a coordinator. One case of the model can be illustrated in the figure 2. \nThe Peers-Peers Model (PsPsM) of CBP2PMIR can be \nrepresented as a 2-tuple (PC, Query). \n(1) The PC consists of a network identifier, a music set, a same \nfeature extraction method, a same feature matching method and a data structure that stores the network identifiers of PCs with which  this PC connects (these PCs are named its neighbors). Any PC \nPC PC PC PC \nCoordinator \nFigure 1. PsCM/PsC+M of CBP2PMIR systems  A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nelement in the music set is comprised of a unique identifier in this \nPC, a music file and its music feature which is computed by the feature extraction method. The music feature matching method is used to compute the distance between two music features, i.e. the similarity between the two corresponding pi eces of music. \n (2) The query is a request proposed by a user, which may be a \npiece of music uploaded, or a section of melody recorded. \nSupposing that the width of the system is m and the depth of the \nsystem is n, the main steps of a query process in PsPsM are the \nfollowing: \n0) Each PC shares some music stored on the local hard disk, and \ngets the network identifiers of its neighbors by broadcast or other algorithms. The number of the neighbors of any PC is assumed more than m. \n1) A user brings forward a music request on any PC of the system \nby providing a piece of music, singing a song or humming a melody (This PC is named the request PC). The feature of the music request is extracted from the music request by the feature extraction algorithm. Then the request PC compares it with the features of all shared music files on this PC, and sends it  to m PCs \nrandomly selected from the neighbors of this PC. \n2) After receiving the feature of the request music, each of the m \nPCs sends it to m PCs randomly selected from its neighbors, and \ncompares it with the features of the local shared music. After that, this PC sends the local result to  the request PC. The local result \nincludes the network identifier of th is PC, the identifiers of some \nsimilar music files, and the matching values between these files and the request. \n3) Each PC which receives the feature of the music request repeats \nstep 2 until the hop number is equal to n. \n4) The request PC gathers all th e local results and sorts them \naccording to the matching valu es. After the user selects his \nfavorite music from the final result, the request PC asks the destination PCs for connections. \n5) If the connections are established, the user can download and \nenjoy his selection through the request PC. \n2.4 PsPsC Model \nIn the PsPsC model (Figure 3), there is one coordinator which each PC registers at. This coordinator collects and manages the statistical data from all PCs, then improves the retrieval speed based on these data. And the centralized architecture can be used to implement the distributed system according to this hybrid \nmodel. \nThe Peers-Peers-Coordinator Model (PsPsCM) of CBP2PMIR \ncan be represented as a triple (PC, Coordinator, Query), \n(1) Besides the 5 parts of the PC in PsPsM, the PC in PsPsCM has \nother two parts. The first is the PC feature and the second is the same PC feature extraction method. \n(2) The coordinator consists of a network identifier, a data \nstructure to store the network identifiers of all PCs, a data structure to store the PC features of all PCs, and an accelerating structure which can utilize the PC features to locate some proper PCs for faster CBMIR . \n(3) The query is a request proposed by a user, which may be a \npiece of music uploaded, or a section of melody recorded. \nThe PC feature is used to characterize a PC, i.e. the music set of \nthe PC. The accelerating structure is usually some rules between the network identifiers of all PCs and the PC features of them. Different PC features or accelerating structures will obtain different accelerating algorithms. A brief PC feature, an effective accelerating structure, and an accelerating algorithm based on them are presented in section 4. \nGiven that the width of the system is m and the depth of the \nsystem is n, the main steps of a query process in the model \nPsPsCM are the following: \n0) Each PC shares some music stored on the local hard disk, and \nregisters at the coordinator. The register information includes its network identifier and its PC feature. \n1) In the accelerating structure, the coordinator c onnects the \nnetwork identifiers of all PCs in the system with the PC features of them under some rules. \n2) A user brings forward a music request on any PC of the system \nby providing a piece of music, singing a song or humming a melody (This PC is named the request PC). The PC feature of the music request, i.e. the PC feature of the music set which includes only the music request, can be extracted from the music request by the PC feature extraction method and sent to the coordinator. \n3) Based on the PC feature of the music request and the \naccelerating algorithm, the coordinator selects some network identifiers of PCs which should have more similar music to the request (These PCs are named the likely-destination PCs), and sends them to the request PC. Then the request PC sends the feature of the music request to these likely-destination PCs. And each likely-destination PC searches its shared music files and returns the local result to the request PC. The local result includes PC\nPC PC PC PC \nPCCoordinator\nFigure 3. PsPsCM of CBP2PMIR systemsPC \nPC PC PC PC \nPC \nFigure 2. PsPsM of CBP2PMIR systems  A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nthe network identifier of this PC, the identifiers of some similar \nmusic files, and the matching values between these files and the request. \n4) The request PC receives all results and sorts them according to \nthe matching values. After the user selects his favorite music from the final result, the request PC asks the destination PCs for connections. \n5) If the connections are established, the user can download and \nenjoy his selection through the request PC. \nThere is system update in these models. For example, when a PC \ntakes part in a P2P system of PsCM, PsC+M or PsPsCM, its register information will be saved in the coordinator. And the information will be deleted if the PC exits from the system. \n \nTable 1. Parameters table \nParameter \nname Description Models used \nq the size of the music feature  \nextracted from a user’s first query all \nre the size of one record \nsatisfying the first query all \nt the number of the records \nsatisfying the first query PsCM, \nPsC+M \nn the final number of the music \nfiles the user selected all \nq' the size of a user’s second \nquery for downloading all \nm the average size of the music \nfiles all \nW the number of PCs in a P2P \nsystem all \nw the width of the system PsPsM, \nPsPsCM \nd the depth of the system PsPsM, \nPsPsCM \nt' the number of the records \nsatisfying the first query PsPsM \nW' the number of the PCs \nsatisfying the first query PsPsCM \nt'' the number of the records \nsatisfying the first query PsPsCM \n \n3. EVALUATION \nThe four models in section 2 have some common features, which \nare also the features of P2P systems. Firstly, the music resource is dispersed over the whole system. Secondly, the music data is transferred directly from one PC to another. Finally, the system can extend easily and quickly, that is any PC can take part in the P2P system handily.  \nHowever, there are still some differences among these models, \nsuch as network load, system robustness and so on. A best model should be selected from them b ecause a proper one is needed for \nmore effective working. So we will evaluate these four models qualitatively and quantitatively in this section. 3.1 The Description of CBP2PMIR \nWe can describe CBP2PMIR as the following optimization problem. \nmin COOR, LOAD, TIME sub to. RTN ≥ n \nWhere COOR is the number of coordinators, LOAD is the whole \nnetwork load, TIME is the total time used to retrieve similar music to the music request, and RTN is the number of music files as the final results. Because there is not a coordinator in PsPsM, and COOR has the same effect in the other three models, so we can simplify the above problem to the following form: \nmin LOAD, TIME sub to. RTN ≥ n \nWe will discuss LOAD and TIME separately in the following two \nsubsections. The size of the PC feature of the music request and that of likely-destination PCs are all small, and the time used to compute the likely-destination PCs is  also short, so the process of \nfinding the likely-destination PCs in PsPsCM is not considered during the evaluation. \nFirstly we do some assumptions. We don’t count the addition in \nsize during the packaging process. The distribution of the music files satisfying the music request is not singular, i.e. the music files satisfying the user’s query don’t just exist in a finite area. And we don’t think the P2P system has a limitation on the number of original results from the coordinator or other PCs, i.e. if a piece of music satisfies the matching condition, its identifier and the matching value between it and the music request will be sent to the coordinator or to the request PC and will not be lost. Along with that, if there is no file meeting the query on a PC, this PC will return nothing. And the user always selects n music files as \nthe final choice and these files will be downloaded correctly. \n3.2 Network load \n \nTable 2. LOAD of each model \n    model \npart PsCM PsC+M PsPsM PsPsCM \n1 q q ×W ( w+w2+L+\nwd)× q W' × q \n2 t×re t×re t' ×re t''×re \n3 n×q' n×q' n×q' n×q' \n4 n×m n×m n×m n×m \nLOAD q + \nt×re + \nn×q' + \nn×m q×W + \nt×re + \nn×q' + \nn×m (w+w2+L+\nwd)×q + \nt'×re + n×q' \n+ n×m W'×q + \nt''×re + \nn×q' + \nn×m \n \nIn each model, the network load consists of four parts. The first \npart is the music feature of the user’s first query. The second is the original results from the coordinator or other PCs. The third is the user’s second query for downloading. The last is the size of music files downloaded. \nConvenient for understanding, the parameters used in the \nevaluation are listed in the table 1, and we compute the network load of each model in the table 2.  A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nIt can be verified that the LOAD of PsPsCM is the least when the \nnumber of PCs in the system is sufficiently large. We should note that commonly t' ≤ t, and ( w+w\n2+L+ wd ) ≤ W when the number \nof PCs in the system is sufficiently large. \nFirstly, it follows from the table 2 that the LOAD of PsCM is less \nthan that of PsC+M. \nSecondly, the LOAD of PsPsM is less than that of PsCM when the \nnumber of PCs in the system is  sufficiently large. Since the \nnumber of PCs is sufficiently large, i.e. W→∞ , t→∞ . Note that \nw, d, q and re are all constants, w+w2+L+wd-1 is a constant, and \nthere is an upper bound for t'. Then [( w+ w2+L+wd-1) × q] / (re × \nt)→0, 1-[( w+ w2+L+wd-1) × q] / (re × t) →1, and thus t'/t ≤ 1- \n[(w+ w2+L+wd-1) × q] / (re × t). So ( w+ w2+L+wd) × q + t' × re \n+ n × q' + n × m ≤ t × re + q + n × q' + n × m. It follows that the \nLOAD of PsPsM is less than that of PsCM when the number of PCs in the system is sufficiently large. \nThirdly, the LOAD of PsPsCM is not more than that of PsPsM. It \nsuffices to show that when the final results are the same in PsPsCM and PsPsM, the LOAD of PsPsCM is not more than that of PsPsM. To simplify this case, it is assumed that the user selects all files from the final results. That means t' — the number of the \nrecords satisfying the first query  in PsPsM, is equals to t'' — that \nin PsPsCM when the final results in the two models are the same. \nAccording to the definition of PsPsCM, the accelerating structure \nin PsPsCM can accelerate CBMIR by locating proper PCs on which usually more similar music to the user’s query are stored. So if Hit is used to denote th e ratio of the number of records \nsatisfying the user’s query to the number of PCs which receive the user’s query and search their local disks, Hit\nPsPsCM  is higher than \nHit PsPsM. That means t''/W' ≥ t'/ (w +w2+L+ wd). So W' ≤ (w \n+w2+L+ wd), and thus W'×q + t''×re + n×q' + n×m ≤ \n(w+w2+L+wd)×q + t'×re + n×q' + n×m. It follows that the LOAD \nof PsPsCM is not more than that of PsPsM.  \nFinally, based to the above discussion, when the number of PCs is \nsufficiently large, LOAD PsCM < LOAD PsC+M , LOAD PsPsM < \nLOAD PsCM, LOAD PsPsCM  ≤ LOAD PsPsM. It follows that the LOAD \nof PsPsCM is the least when the number of PCs in the system is sufficiently large. \n3.3 Retrieval time \nThe retrieval time of each model is composed of 3 parts. The first part is the time for computation, i.e. the time used for feature matching. The second is the time used to merge the local results from the PCs and sort the final re sults. The last is the time used \nfor transmission, which could be measured by network load and has been discussed in the previous subsection. So the time used for computation, merging and so rting is discussed in this \nsubsection. \nBefore the discussion of retrieval time, we assume that each PC \nsorts its local result and then se nds them. As in the previous \nsubsection, we compute the retrieval time of each model in the table 3, where A\ni is the feature set of music stored on the i-th PC, \nand A is the feature set of all music stored in the P2P system. T( A) \nis the time used to compute and sort the distances between the feature of the request  and each element of A, and T( A\ni) the time \nused to compute and sort the distances between the feature of the request and each element of A\ni. Merge{ t} is the time used to \nmerge the result of t records. Merge{ t'} and Merge{ t''} have the \nsimilar meaning. \nIt is evident that TIME PsCM is more than the other three. Because \nW' ≤ W, merge{ t''} ≤ merge{ t}, TIME PsPsCM  is less than \nTIME PsC+M . And because W' ≤ (w +w2+L+ wd ) when t'' = t',  the \nreal time of computation and merging in PsPsM is more than max{T( Ai)}+ merge{ t''}, i.e. TIME PsPsCM  is less than TIME PsPsM. \nSo TIME PsPsCM  is the least. \n3.4 Others \nThere is system update in P2P systems. When one PC connects into a P2P system, it registers at the coordinator or sends location \nmessage to other PCs for finding it. In PsCM and PsC+M, system update is processed by the coordinator, but in PsPsM, it is processed by each PC. Differing from them, system update can be \nprocessed by the coordinator or PCs themselves in PsPsCM. \nWhen there is something wrong in the coordinator, PsCM and \nPsC+M will stop working, but PsPsCM will work continuously via the neighborship relation between PCs. Usually there isn’t global network paralysis in PsPsM. So PsPsM and PsPsCM have stronger robustness. \nTable 3. TIME of each model \n    model \npart PsCM PsC+M PsPsM PsPsCM \ncomputa-\ntion T(A) max{T( Ai)\n} max{T( Ai)\n} max{T( Ai)\n} \nmerge ____ merge{ t} merge{ t'} merge{ t''} \nTIME T( A) max{T( Ai)\n}+ merge{ t} max{T( A\ni)\n}+ merge{ t'} max{T( A\ni)\n}+ merge{ t''} \nremark ____ i = 1, L, \nW i = 1,L, \nw+w2+L+\nwd i = 1, L, \nW' \n3.5 Summary \nAll in all, there are many differences in the four models. In \ncentralized CBP2PMIR systems, i.e. PsCM and PsC+M, the coordinator is easily overloaded and becomes the bottleneck of the whole system. Then this kind of system has poor stability. For example, if there is something wrong with the coordinator, the \nsystem can not work correctly. Differing from centralized CBP2PMIR, the stability of distributed CBP2PMIR systems, e.g. PsPsM, is better. However the number of messages that are sent by PCs of distributed CBP2PMIR systems is numerous when the scale of the system expands. The cost of this kind of system is too expensive. Fortunately, from the above estimation, the balance of stability and cost can be obtained if we select the last model PsPsCM. And we can select the proper number of coordinators to make it better. Thereby we decide to develop our CBP2PMIR system in the light of this model, and call PsPsCM the model of CBP2PMIR systems in the rest of this paper. \n4. ACCELERATING ALGORITHM \nIn this section we will deal with two important problems in \nCBP2PMIR. The first is how to accelerate the content-based music information retrieval in peer-to-peer environments. In P2P environments there are many applications to retrieve music by meta-data. And we can easily utilize the existing CBMIR algorithms, e.g. feature extraction and feature matching, in P2P environments. But there are so much music dispensed over a P2P system and we need a fast and effective method to retrieve music information based on the content of music. In this section we propose such an accelerating algorithm to implement this \nfunction. The intuitive justification of the accelerating algorithm is \nsearching less PCs but getting more results.  A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nThe second problem in CBP2PMIR we put forward is how to \nfilter out the repeated music files. In P2P systems each PC is self-existent. It is very common that many copies of a piece of music stored in different PCs. So it is very important to filter out the replica in the results re turned to users. We bring forward a simple \nbut effective method to filter out the same music in the last of this section. \n4.1 The description of the accelerating \nproblem \nThe definition of the CBP2PMIR model is described in subsection \n2.4. We can describe the problem of accelerating retrieval as follows: \nIn CBP2PMIR system (PC, Coordinator, Query), the accelerating \nproblem is how to find PNI\nopt, i.e. a subset of the set of all PCs, \nand there is a lot of similar music to the Query in any PC which is \na member of PNI opt. \n4.2 Several concepts \nInterval is the difference in pitch between two tones of music. The \nunit of interval used in this paper is semi-tone. A piece of music can be considered as a sequence of intervals [5]. \nGiven a constant positive integer d, intervals which absolute \nvalues are not less than d are named big intervals. For a piece of \nmusic, the ratio of the number of big intervals to the number of all intervals is named the ratio of d-interval of this piece of music. \nWe assume that a proper d is predefined in this paper. \nFor convenience of the description of the accelerating method, in \nthe rest of this paper two pieces of music are said similar each other if their ratios of d-interval are close enough. For a set of music, if random variable R represents the ratio of d-\ninterval of all pieces of music in this set, <mea, std>  is said the \nfeature of d-interval of this set, where mea is the mean of R and \nstd is the standard deviation of R. It is evident that mea and std are \nbetween 0 and 1. And for a music set which has only one piece of music, the feature of d-interval of this set is just <the ratio of d-\ninterval of the piece of music, 0>. All features of d-interval make \nup of the space of d-interval features S. Each element of S is the \nfeature of d-interval of a certain music set. \nGiven\n0, 1mnab≤≤ , ,0 , 1 , 2 , 3 , ,mn v= L ,01 20 aa a=≤ ≤ ≤   \n3 1v aa≤≤=L , and  01 2 301v bb b b b=≤ ≤ ≤ ≤ ≤ = L , S is \ncompartmentalized by these points into 2v subspaces marked as \nijS, where \n(1)ijS≠Φ, 1,ij v≤≤ , \n{ } 11 ,,ij i i i iSa b a a a b b b−− =≤ < ≤ < , () 1, 1ij v≤≤ − ; \n{ } 11 ,,ij i i i iSa b a a a b b b−− =≤ ≤ ≤ < , iv=, () 11jv≤≤ − ; \n{ } 11 ,,ij i i i iSa b a a a b b b−− =≤ < ≤ ≤ , () 11iv≤≤ − ,jv=; \n{ } 11 ,,ij i i i iSa b a a a b b b−− =≤ ≤ ≤ ≤ ,ij v== ; \n(2)11 2 2ij i jSS∩= Φ , 11 , 2 , 1 , 2ii j j v≤≤ , 1 2ii≠ or12jj≠ ; \n(3)\n,1v\nij\nijSS\n==U . \nWe say { } ,1 , ,ijSi j v =L  is the partition of S and \n()01 0 1,, ,,,, ,vv aa abb bLL  the partitioning sequence (Figure 4.(a)). \nFigure 4. The accelerating algorithm \n(a) The partition of S ;                    (b) The expanding set of <k, h> ; \n(c) The minimal overlay of S<k ,h>;  (d) The illustration of the example  A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nOn the supposition that <k, h>  is the feature of d-interval of a \ncertain music set, S<k, h>  is named the expanding set of <k, h>  , α \nthe expanding factor in the mean direction, β the expanding factor \nin the standard deviation direction, where α and β are between 0 \nand 1 (Figure 4. (b) ). S<k, h>  is a subset of S. For any element of \nS<k, h> , e.g. < a, b>, a is between max( k-α, 0) and min( k+α, 1), and \nb is between max( k-β, 0) and min( k+β, 1). \nLet E be a subset of S and G a subset of the partition of S, G is \nsaid the minimal overlay of E if (1) E is a subset of the union of \nall elements in G, (2) there isn’t a proper subset of G the union of \nwhose elements is a superset of E. The minimal overlay of S<k, h>  \nis illustrated in the figure 4. (c). \nEach PC is mapped into one subspace of S according to the d-\ninterval feature of the music set preserved and shared on it. This mapping is named the pa rtition mapping. In the rest of this paper, \nwe use ff(S\nmn) to represent the set of PCs which are mapped into \nthe subspace Smn (m, n=1,…, v). \n4.3 The accelerating algorithm \nIf the constant v, the partitioning sequence, the expanding factors \nα and β are defined, based on the concepts in subsection 4.2, \nthe accelerating algorithm can be described as follows: \nInput: A query Q which is a piece of music uploaded, a song \nsung, or a melody hummed by a user; \nOutput:  PNI opt, i.e. a subset of the set of all PCs, and there is a lot \nof similar music to Q in any PC which is a member of PNI opt. \nSteps: 1. Compute the feature of d-interval of the music set which \nconsists of Q, say \n,QQab , obviously 0Qb=; \n2. Compute ,QQabS  — the expanding set of ,QQab ; \n3. Compute the minimal overlay of ,QQabS ; \n4. Accumulate all ff(S mn)s to get optPNI  where Smn is an element of \n,QQabS ; \n5. Return  PNI opt. \nWe can refine the size ofoptPNI , the accuracy and speed of \nretrieval by altering the partitioning sequence, v, α and β. \n4.4 An example \nSupposing that 5 v=, 0.2iiab i== × , 0,1, ,5i=L , 0.1α= , \n0.3β= . Q is a query that a user inputs, and the ratio of d-interval \nof Q is 0.55. Then the feature of  d-interval of the music set { Q}  \nis <0.55, 0> (Figure 4. (d) ). \nThe partition of S is{ } ,,1 , , 5ijSi j =L , and \n{ } 11 ,,ij i i i iSa b a a a b b b−− =≤ < ≤ < ,1, 4ij≤≤ ; \n{ } 51 ,0 . 8 1 ,ji iSa b a b b b− =≤ ≤ ≤ < , 5i=,14j≤≤ ; \n{ } 51 ,, 0 . 8 1ii iSa b a a a b− =≤ < ≤ ≤ , 1 4 i≤≤ , 5j=; \n{ } 55 ,0 . 8 1 , 0 . 8 1 Sa b a b=≤ ≤ ≤ ≤ , 5 ij== ; \nThe expanding set of <0.55, 0> is  \n() ( ) { 0.55, 0, max 0.55 0.1,0 ,min 0.55 0.1,1 , Sa b a =∈  −+  () ( ) } max 0 0.3,0 , min 0 0.3,1 b∈ −+   \n= [] [ ] { } , 0.45,0.65 , 0,0.3ab a b ∈∈ . \nThe minimal overlay of0.55, 0S  is { } 31 32 41 42,,,SSSS . \nThe return is () () () ()31 32 41 42 optPNI ff S ff S ff S ff S = UUU , \nwhere f is the partition mapping. \n\u0000 \n4.5 The filtering method \nIn this subsection we propose a simple and effective method to \nfilter out the repetition in the results. Because the music files with the same content must have the same size, and usually have the same date-time, we can compare the size or the date-time of music files with equal rank-values to judge whether they are repeated. \nDuring the process of merging and sorting the results from other \nPCs, if the system finds some music files with same rank-values, it compares their size and date-time, and then deletes the replica when they are same. Usually the size of file is enough. \n5. SYSTEM ARCHITECTURE \nIn this section we propose the architecture of QUIND (QUick fIND) — a CBP2PMIR system based on PsPsCM, which can retrieve music information according to the content of music in a P2P environment. QUIND consists of two kinds of parts. One is the PC, and the other is the coordinator. \n5.1 PC \nThe PC in QUIND consists of 7 components (Figure 5). The manager, short for the local music manager, is used to manage all music stored on the local hard disk and shared for the P2P system. On the one hand the manager processes the local music in advance to achieve the music features which will be used for feature matching, and produce the PC feature sent to the coordinator. On the other hand it stores the music features along with the meta-data of the local music, for example, the name of the music and the nationality of the author and so forth. Of course the information will be stored under a certain format and utilized in CBMIR. \nThe locator of a PC is a component that stores the network \nidentifier of this PC. And it can identify and store the location information of the PCs in the P2P environment, with which it can connect. The information is just the neighbors in PsPsCM and \nacquired as the accumulation of previous \noptPNI s . \nWhen a user puts forward a music request, for example singing a \nsong or humming a melody, the query generator receives the user’s input, i.e. the query in PsPsCM, and extracts music features Figure 5. The structure of the PC in QUIND\n A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \nfrom it to generate relevant music queries according to appropriate \nfeature extraction algorithms and query-constructing rules. The sender component in a PC sends the generated music queries to the coordinator or likely-destination PCs, the logon and logoff information to the coordinator, the local retrieval result and the \nselected music on the hard disk to the request PC, and the download request to the destination PCs, etc. at different steps in the system. The receiver receives information sent by the sender of the coordinator or other PCs at different steps in the system. \nOn each PC of QUIND, the calculator com ponent computes the \nmatching values, i.e. the distances between the music request and the music files stored in the mana ger, and then ranks the results \naccording to the resulted distances. The incorporator merges, sorts and filters all local results from itself and likely-destination PCs to \nform the final result, which will be sent to the user. \n5.2 Coordinator \nThe coordinator in QUIND is composed of 5 components (Figure 6). The locator of the coordinator stores the network identifier of the coordinator and the set of network identifiers of all PCs. The sender component in the coordinator sends the network identifiers of likely-destination PCs to the request PC. The receiver receives the PC feature of the music request that is sent by the request PC, and the logon, logoff and PC feature information of each PC in the system. The set of PC features of PCs in the system is stored in the library component. And the calculator component is the core of the coordinator because the accelerating algorithm pr oposed in \nsection 4 is used in it. \nQUIND has strong robustness. If there is something wrong with \nthe coordinator, the whole system can still correctly work based on the neighbor information of each PC. The difference is just the \nretrieval speed in this situation becomes that of PsPsM. And we \ncan add the number of coordinators in QUIND to improve its efficiency and robustness. It is obvious that the network identifier information in QUIND, including the set of network identifiers of all PCs at the coordinator and the network identifiers of its neighbors at each PC, s hould be updated regularly. \nQUIND has good expansibility. The number of PCs in QUIND \ncan change randomly. And we can add the number of coordinators with some modification. Feature extraction and feature matching are important issues in content-based music information retrieval, and we will discuss them in other papers. \n6. RELATED WORK \nThere are many results in the fields of CBMIR and P2P. Byrd [6] reported his work on music retrieval in Conventional Music Notation (CMN) form. Chai [7] built a query-by-humming system which could find music based on a few hummed notes. Bainbridge [8] described a comprehensive suite of tools for building the music part of New Zealand Digital Library. Rowstron [9] \ndescribed a kind of storage management and caching in a P2P environment. Stoica [10] presente d a distributed lookup protocol \nto solve the locating problem in p eer-to-peer applications. Napster \n[11] is another famous music search engine in P2P format, but it is not content-based. It can not accept a piece of music and retrieve the similar music based on content. Yang [12] developed a probabilistic model and an analytic model for “hybrid” P2P, but he didn’t consider other kinds of P2P. And his models cannot deal with content-based information retrieval. \nThough there have been many prototypes about music information \nretrieval or P2P systems, as far as we know, this paper is the first one to consider the combination of content-based music information retrieval technologies and peer-to-peer environments. \n7. CONCLUSION AND FUTURE WORK \nIn this paper, we propose four peer-to-peer models for content-based music information retrieval and describe the query process in each model. Then we carefully evaluate these models on network load, retrieval time, system update and robustness qualitatively and quantitatively. And we find PsPsCM is the best one of them. \nAfter that, we bring forward two important problems in the field \nof CBP2PMIR. Based on some useful concepts, we design an accelerating algorithm to improve the retrieval speed. And we put forward a simple and effective method to filter out the replica in the final results. \nFinally, we present the architecture of the content-based peer-to-\npeer music information retrieval system QUIND. It is based on PsPsCM and can implement content-based music information retrieval in a peer-to-peer environment. QUIND consists of PCs and coordinators. Music stored and shared on each PC makes up of the whole available music resource. When a user puts forward a music query, QUIND can retrieve some similar music quickly and accurately according to the content of music. After the user selects his favorite ones, he can download and enjoy them. \nNow we are developing the system QUIND. And we will research \nother new problems about CBP2PMIR, such as the P2P protocols for music information retrieval based on content. \n8. ACKNOWLEDGMENTS \nThis material is based on work supported in part by National Science Founds of China (69873014) and National 863 Plan of China (2001AA415410). And we thank the member of database laboratory at HIT. \n9. REFERENCES \n[1] S. Downie and M. Nelson. Evaluation and effective Music Information Retrieval Method. ACM SIGIR 2000, Athens, Greece, pp73~80 \n[2] J .  L .  H s u ,  C .  C .  L i u ,  a n d  A .  L .  P .  C h e n .  D i s c o v e r i n g  \nNontrivial Repeating Patte rns in Music Data. IEEE \ntransactions on multimedia, Vol3, No.3, September 2001, pp311~325 \n[3] G. Tzanetakis, G. Essl, and P. Cook. Automatic Musical Genre Classification of Audio Signals, ISMIR 2001, http://ismir2001.indiana.edu \n[4] Gnutella. http://gnutella.wego.com/ \n[5] Stephen Handel. Listening: An Introduction to the Perception of Auditory Events. The MIT Press, 1989. Figure 6. The structure of the coordinator in QUIND \n A Kind of Content-Based Music Information Re trieval Method in a Peer-to-Peer Environment \n[6] D. Byrd. Music-Notation Searching and Digital Libraries. \nJCDL 2001, pp239~246 \n[7] W. Chai. Melody Retrieval On The Web, Master thesis, 2001, http://web.media.mit.edu/~chaiwei/papers.html \n[8] D. Bainbridge, C. Manning, I. Witten, L. Smith, and R. McNab. Towards a Digital Library of Popular Music, ACM DL 1999, Berkeley, CA USA, pp161~169 \n[9] A. Rowstron and P. Druschel. Pastry: Scalable, Decentralized Object Location and Routing for Large-scale Peer-to-peer Systems, Proc. of the 18th IFIP/ACM International Conference on Distributed Systems Platforms (Middleware \n2001). Heidelberg, Germany, November 2001. \n[10] I. Stoica, R. Morris, M. Kaashoek, and H. Balakrishnan.  Chord. A Scalable Peer-to-peer Lookup Service for Internet Applications, ACM SIGCOMM 2001, August 27-31, 2001, San Diego, California, USA, pp149~160 \n[11] Napster. http://www.napster.com/ \n[12] B. Yang and H. Garcia-Molina. Comparing Hybrid Peer-to-Peer Systems, VLDB 2001, September 11-14, 2001, Roma, Italy, pp561~570"
    },
    {
        "title": "Combining Musical and Cultural Features for Intelligent Style Detection.",
        "author": [
            "Brian Whitman",
            "Paris Smaragdis"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1417471",
        "url": "https://doi.org/10.5281/zenodo.1417471",
        "ee": "https://zenodo.org/records/1417471/files/WhitmanS02.pdf",
        "abstract": "798\u001b:=@?\u0013A\bBCAED\u0016F\u0013GHDIA>F? =PO.BCQ(?R:9S>QPDI=@T\u0014DJ8(:QZ:\"BC8\bD\u0016W\u0018M\b?\u000bO\u0016Q[BV? ? =ZU+O.B\\:BV:\"B\u0014a%b(:4S\u0014QPDc=[T\u0014DJ8(:FFDf? O\";\bD\u0016L*DNGID*A\u0014F=PWqT\u0014FFBV:\"BcGIDtO.BCQPQ\u001au OJW\u0018L*LqM\b8\b=Z:9SNL*D\u0016:\"BCT\bB\\:\"B\u0014aPv oI;>DtBCT\bT\u0014=P:DJ? D\u000bOJM\bQZ:BV: : FFK^gD.BV:=P? ? =PL*=@Q@B\\FkLNM\b? =PO G1=Z:? =@O X DJQPW\u00188>m\u0018=P8\bm\u001b:QPDJ?Ja",
        "zenodo_id": 1417471,
        "dblp_key": "conf/ismir/WhitmanS02",
        "keywords": [
            "798",
            "798",
            "798",
            "798",
            "798",
            "798",
            "798",
            "798",
            "798",
            "798"
        ],
        "content": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\t\nCombining Musical and Cultural Features forIntellig ent\nStyle Detection\nBrianWhitman\nMIT Media Lab\nCambr idge MAU.S.A.\n+1617 253 0112\nbwhitman@media.mit.eduParisSmar agdis\nMIT Media Lab\nCambr idge MAU.S.A.\n+1617 253 0405\nparis@media.mit.edu\nABSTRA CT798\u001b:<;>=@?\u0013A\bBCAED\u0016F\u0013GHDIA>F<D\u0016?\nDJ8(:KB%LNM>?\n=PO.BCQ(?R:9S>QPDI=@T\u0014DJ8(:<=PU\bO.BV:<=PWC8N?\nO\";\bDJL*DXBC?\nD.TYWC8Y?\n=PLNM>QZ:\"BC8\bD\u0016W\u0018M\b?\u000bO\u0016Q[BV?\n?\n=ZU+O.B\\:<=@WC8]WC^KBVM+T\u0014=P:<WVF\nS\u0002BC8+T*:<D\u0016_`:<M+BVQT>BV:\"B\u0014a%b(:4S\u0014QPDc=[T\u0014DJ8(:<=ZU+O.B\\:<=@WC8]=P?dB\u001e:\"BC?\nefG1;\b=PO3;]WV^g:<DJ8h=P8`i'W\u0018QPi'D\u0016?\u001dO\u0016M\bQZj:<M\u0014F\"BCQIBV?\nAEDJO\u0016:<?k8\bWV:\u001bA>F<DJ?\nDJ8(:\u001bWCF\u001bD.BV?\n=PQPSlD\u0016_`:\nF\"BCO\u0016:<DJT-:<;>F<WCM\bm\u0018;nBCM+T\u0014=Zj:<WVF\nSlA\u0014F<W(OJDJ?\n?\n=P8\bm\u0014apoI;>Df?\nO\";\bD\u0016L*DNGID*A\u0014F<W\u0018AEW\u0018?\nD*O\u0016W\u0018L*A\bQPDJL*D\u00168`:<?kBV8(SBVM+T>=PWqT\u0014F<=Pi'DJ8*mCDJ8>F<D\u001dWCFI?R:4S\u0014QPD%T\u0014D\u0016:<DJO\u0016:<=PW\u00188r?RS\u0014?R:<DJLsG1=Z:<;\u0002BkOJQ@BC?\n?\n=ZU\bD\u0016FXBC?\nD.TrW\u00188*GID\nXj/D\u0016_`:\nF\"BCO\u0016:<DJTfT>BV:\"BcGIDtO.BCQPQ\u001au OJW\u0018L*LqM\b8\b=Z:9SNL*D\u0016:\"BCT\bB\\:\"B\u0014aPvoI;>DtBCT\bT\u0014=P:<=PWC8\u001eWC^E:<;>DJ?\nD\u000bOJM\bQZ:<M\u0014F\"BCQ>BV:\n:\nF<=\nXM\u0014:<DJ?K=@8NW\u0018M>FK^gD.BV:<M\u0014F<D\u001d?\nA\bBCOJDBV=[T\u0014?c=P8-A\u0014F<W\u0018AED\u0016FkOJQ@BV?\n?\n=PU\bO.BV:<=PWC8nWC^IBCOJWCM\b?R:<=PO.BCQPQZSlT>=P?\n?\n=PL*=@Q@B\\FkLNM\b?\n=POG1=Z:<;\b=P8f:<;\bD\u001d?<BCL*D%?R:9S\u0014Q@DCw\bBV8+T*?\n=PL*=PQ[B\\F1LNM>?\n=@O\nXDJQPW\u00188>m\u0018=P8\bm\u001b:<W\u001eT\u0014=PxyD3F\njDJ8(:1?R:9S>QPDJ?Ja\n1.INTR ODUCTION\nMusical genres aidinthelistening-and-retrie val(L&R) process by\nallowing auser orconsumer asense ofreference. Byorganizing\nphysical shelv esinrecord stores bygenres, shoppers canbrowseand\ndisco vernewmusic bywalking downanaisle. Butthedigitization of\nmusical culture carries anembarrassing problem ofhowtoorganize\ncollections: folders fullofmusic recordings, peer-to-peer virtual\nterabyte lockersandhandheld devices allneed thesame attention\ntoorganization asrooftop music stores. Asaresult, recent work\nhasapproached theproblem ofautomatic genre recognition [8][2],\ncreating top-le velclusters ofsimilar music (rock, pop, classical,\netc.) from theacoustic content.\nWhile thehigh levelseparation ofgenres isuseful, wetend to\nlook more towardstyles fordisco vering newmusic orforaccurate\nrecommendation. Styles usually deﬁne subclasses ofgenres (inthe\ngenre Country wecanchoose from ‘NoDepression, ’‘Contemporary\nCountry ,’or‘Urban Cowboy’),butsometimes jointogether artists\nacross genres. Stores (real orvirtual) normally donotpartition\ntheir space bystyle toavoidconsumer confusion (“z|{r}y~.C\n6z|\u0016@\n\u0016\nV\bz\n\u001e\nVd\n3\u001e\u0018@\u0002\n<\u00189”)buttheycanprovide cross-\nreference data(asinthecase oftheAllMusic Guide (''\u0018\rR\u0018V''\r'\u0018J\u0014>'\u0018\n\u0018J);andrecommendation engines canutilize styles for\nhigh-conﬁdence results.\nStyle isanimperati veclass ofdescription formost music retrie val\ntasks, butisusually considered a‘human’ concept andcanbehard\ntomodel. Some styles evolvedwith noacoustic underpinnings: a\nfavorite isintelligent-dance-music or‘IDM, ’inwhich theincluded\nartists range from theabstract sine-w avenoise ofPanSonic tothe\ncalm ﬁltered melodies ofBoards ofCanada. Atﬁrstglance, IDM\nwould beanintractable settomodel duetoitssimilarity being\nalmost purely cultural. Assuch, weusually relyonmark eting, print\npublications, recommendations offriends (“\ng\b3\u001a\n]z[\n*C V\n ¡z ¢\n\n\n”)tounderstand styles onourown.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotices andthefullcitation ontheﬁrstpage.O\n£\n2002 IRCAM -Centre PompidouInthispaper wepresent anautomatic style detection system that\noperates onboth theacoustic content oftheaudio andthevery\npowerful ‘cultural representation’ ofcommunity metadata, using\ndescripti vetextual features extracted from automated crawlsofthe\nweb.Thecommunity metadata feature space haspreviously shown\ntobeeffectiveinamusic similarity task onitsown[10], andhere\nweaugment itwith anaudio representation. This combined model\nperforms extremely well inidentifying asetofpreviously edited\nstyle clusters, andcanbeused tocluster arbitrarily largenewsetsof\nartists.\n2.PRIOR WORK\n2.1 Genr eClassiﬁcation\nAutomatic genre classiﬁcation techniques thatexplicitly compute\nclusters from thescore oraudio levelhavereported high results in\nmusically oracoustically separable genres such asclassical vs.rock,\nbutthehierarchical structure ofpopular music lends itself toamore\nﬁnegrained setofdivisions.\nUsing thescore levelonly,(MIDI ﬁles, transcribed music orCSound\nscores) systems canextract style orgenre using easily-e xtractable\nfeatures (once themusic isinacommon format, which may require\ncharacter recognition onascore, orparsing aMIDI ﬁle)such askey\nandfrequently used progressions. Systems normally perform genre\nclassiﬁcation byclustering similar music segments, orperforming\naone-in-n (where nisthenumber ofgenres) classiﬁcation using\nsome machine learning technique. In[5],various machine learning\nclassiﬁers aretrained onperformance characteristics ofthescore to\nlearn apiece-global ’style, ’andin[2]three types offolkmusic were\nseparated using aHidden Mark ovModel.\nApproaches thatperform genre classiﬁcation intheaudio domain\nuseacombination ofspectral features andmusically-informed in-\nferred features. Genre identiﬁcation workundertak enin[8]aims to\nunderstand acoustic content enough toclassify into asmall setof\nrelated clusters bystudying thespectra along with tempo-sensiti ve\n‘timbre grams’ with asimple beat detector inplace. Similar work\ntreating artists ascomplete genres (where similar clusters ofartist\nform a‘genre’) isstudied in[9]andthen impro vedonin[1]with\nmore musical knowledge.\n2.2 Cultural Featur eExtraction\nCultural features concerning music arenotaswell-deﬁned andvary\nwith time andinterpretation. Anyrepresentation thataims toexpress\nmusic ascommunity description isaform ofcultural features. The\nmost popular form ofcultural features, listsofpurchased music, are\nused incollaborati veﬁltering torecommend music based ontheir\npeers’ tastes. Cultural features areimportant toexpress information\nabout music thatcannot becaptured bytheactual audio content.\nManymusic retrie valtasks cannot dowell onaudio alone.\nAmore automatic andautonomous wayofcollecting cultural fea-\ntures isdescribed in[10]. There, wedeﬁne ‘community metadata’\n(which isused inthispaper) asavector space ofdescripti vetextual\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\t\nterms crawled from theweb.Forexample, anartist isrepresented\nastheir community description from albumreviewsandfan-created\npages. Animportant asset ofcommunity metadata isitsattention\ntotime: inourimplementation, community metadata vectors are\ncrawled repeatedly andfuture retrie valstakethetime ofdescription\nintoaccount. Inadomain where long-scale time isvastly important,\nthisrepresentation allowsrecommendations andclassiﬁcations to\ntakethe‘buzzfactor’ intoaccount.\nCultural features formusic retrie valarealsoexplored in[4],where\nweb crawlsfor‘my favorite artists’ lists arecollated andused ina\nrecommendation agent. The speciﬁcs ofthecommunity metadata\nfeature vector aredescribed ingreater detail below.\n3.STYLE CLASSIFICA TION\nTotestourfeature space andhypotheses concerning automatic style\ndetection, wechose asmall setofartists spanning ﬁveseparate styles\nasclassiﬁed bymusic editors. Inturn, weﬁrstmakeclassiﬁcations\nbased solely onanaudio representation, then acommunity metadata\nrepresentation, andlastly showthat thecombined feature spaces\nperform thebestinseparating thestyles.\n3.1 Data Set\nFortheresults inthispaper weoperate onaﬁxeddata setofartists\nchosen from theMinno wmatch music testbed (related workanalyzes\nthisdatabase in[9],[1],[10].) Thelistused contained twenty-ﬁv e\nartists, encapsulating ﬁveartists each across ﬁvemusic styles. The\nlistisshowninTable 1.\nEach artist inrepresented intheMinno wmatch testbed with one\nortwoalbums worth ofaudio content. The selection ofartists\ninthetestbed wasdeﬁned bytheoutput ofapeer-to-peer netw ork\nrobot which computed popularity ofsongs bywatching thousands of\nusers’ collections. Wehavepreviously crawled forthecommunity\nmetadata foreach artist intheMinno wmatch tested inJanuary of\n2002.\nThe‘ground truth’ style classiﬁcation wastakenfrom theAllMusic\nGuide at ('\u0018KR'V'\u0018\r\n''\u0016\u0014>''\n\u0018J(AMG), apopular edited\nweb resource formusic information. Weconsider AMG our‘best-\ncase’ ground truth duetoitscollecti veedited nature. Although\nAMG’ sdecisions aresubjecti ve,ourintent istoshowthatacom-\nputational metric involving both acoustic andcultural features can\napproximate anactual labeling from aprofessional.\nThesizeofourdata isintentionally small soastodemonstrate the\nissues ofacoustic versus cultural similarity presented inthispaper .\nThis simulation isnotmeant torepresent afully functioning system\ndue toitsscope, buttheapproach and results propose aviable\nsolution totheproblem.\n4.AUDIO-B ASED STYLE CLASSIFICA TION\nOne obvious feature space foramusic style classiﬁer istheaudio\ndomain. While wewillshowthatitisnotalwaysthebest wayto\ndiscern cultural labels such asstyles, wecansayitisaverygood\nindicator ofthe‘sound’ ofmusic andperhaps asahigher -levelgenre\nclassiﬁer .\nTheaudio-based style classiﬁer operates byforming each song into\narepresentation andtraining aneural netw orktoclassify anewsong\nfrom atestsetintooneoftheﬁveclasses. Below,wedescribe the\nrepresentation used andthetraining process.\n4.1 Repr esentation\nWechose afairly simple representation forthisexperiment. For\neach artist inourset,wechose onaverage 12songs randomly from\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u0006\r\u000e\u0000\b\t\u0010\u000f\u0012\u0011\u0013\u0005\b\u0007\n\t\u0014\t\u0010\u0015\u0016\u0011\u0013\u0007\n\u000f\u0006\u0017\u0018\u0011\u0013\u0001\u0004\u0019\u001b\u001a\u001d\u001c\u001e\u000f\u0012\u0011\u0013\u001f! \"\u0019\u0006\u0007#\u000f\u0006\u0005\b$\u001e\u0001\u0004\u0019\u0012%'&\b\u000f\u0006(\u0013\t)$(\u0013\u0011+*-,\u0004\t.\u0017/,\u0004\u000f\u0012(0(0\u0001213\u0017\u0018\u000f4\u00110\u0001\u0004\u0019\u001b\u001a65\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t87\u0016\r:9\u0016\u0011\u0013\u0007;\u0005\u001e\u0017\u0018\u0011\u0013\u0005\b\u0007\n\t<\u0019\u0006 =\u00110\u001f\b\t>,\u0004\t\u0010\u000f\u0012\u0007;\u001a\u001e\u0001?\u001a\b\u0003@\u001c\u001e\u0007\n\u0019\u0016\u0017\u0018\t)(0(\u00185 ABDCFEFEHGJI\n\u0001\u0004\u00110\u001f\u0014$\b\t),\u0004\u000f\u0010*\b(K\u0019\u001b\u001a\u001e,2*L\u000f\u0012\u0011M\u00110\u001f\b\tN\u0001\u0004\u001a\b\u001cO\u0005\b\u0011F,\u0004\u000f\u0010*\u001b\t)\u0007G\n\u0001?(F\u0005\b(\u0013\t4$ P\u0019\u0006\u0007Q\u0011R\u0007;\u000f\u0012\u0001?\u001a\u001e\u0001?\u001a-\u0003\b5B\n\u001f\b\tS\u0001\u0004\u001a\b\u001cO\u0005\b\u0011T\u0001?(U\u00110\u001f\b\tN\u0017\u0018\u0005\b\u0007\n\u0007;\t)\u001aV\u0011G\n\u001c\u001e,?\u0005\u001e(T\u0011\u0013\u001f\b\tS\u0011I\n\u0019\u001c\b\u0007;\t)\u0017\u0018\t)$\u001e\u0001\u0004\u001a\b\u0003W \"\t\u0010\u000f4\u00110\u0005\b\u0007\n\t.X\u001b\t4\u0017Y\u0011\u0013\u0019\u0006\u0007;(\u00185\ntheir collection. Theaudio tracks were downsampled to11,025Hz,\nconverted tomono, andtransformed tozero mean andunitvariance.\nWesubsequently extracted the512-point power spectral density\n(PSD) ofeverythree seconds ofaudio andperformed dimensionality\nreduction using principal components analysis (PCA) totheentire\ntraining data settoreduce itdowntotwenty dimensions. The\nprocess isdescribed inFigure 1.The series ofthereduced PSD\nfeatures asextracted from alltheavailable audio tracks were used\nastherepresentation ofeveryartist.\n4.2 Classiﬁcation andLear ning\nLearning forclassiﬁcation ontheaudio features wasdone using\nafeedforw ardtime-delay neural netw ork(TDNN) [3]. This isa\nstructure thatallowstheincorporation ofashort time memory for\nclassiﬁcation, byproviding asinputs samples ofprevious time points\n(Figure 2).Fortraining thisnetw orkweused theresilient backprop-\nagation algorithm [6]anditerated inbatch mode (using theentire\ntraining setasonebatch). Theinputs layer hastwenty nodes (one\nforeach dimension oftherepresentation) with amemory ofthree\nadjacent input frames. Weused onehidden layer with forty nodes,\nandtheoutput layer wasﬁvenodes, each corresponding tooneof\ntheﬁvestyles wewished torecognize. Thetraining targets were a\nvalue of1forthenode corresponding tothestyle oftheinput, and\nvalues of0foralltheother nodes.\nInthetesting phase, thefeatures ofthetestsetwere extracted (using\nthesame dimensionality reduction transform derivedfrom thetrain-\ningdata), andtheywere fedtotheclassiﬁcation netw ork. Styles\nwere assigned totheoutput node corresponding tothemaximum\nvalue.\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\tB\n\u000f\u0012&\u001e,\u0004\tN\u000b\u0006\r AU\u0007\n\u00110\u0001?(\u0013\u0011U(\u0013\t),\u0004\t)\u0017\u0018\u00110\u0001\u0004\u0019\u0006\u001a\u001e(  \"\u0019\u0012\u0007T(\u0013\u0011+*-,\u0004\t.\u0017/,\u0004\u000f\u0012(0(0\u0001213\u0017\u0018\u000f4\u00110\u0001\u0004\u0019\u001b\u001a65\u0001F\t\u0010\u000f\u0010X-*\u0003\u0002 \t)\u0011R\u000f\u0006, \u0004D\u0019\u001b\u001aV\u0011\u0013\t\u0006\u0005H\u001c3\u0019\u0006\u0007\n\u000f\u0012\u0007\n*\u0007\u0004 \u0019\u001b\u0005\u001e\u001aV\u0011\u0013\u0007\n* \u0001T\u000f\u0012\u0007;$\u001e\u0017\u0018\u0019\u0006\u0007\n\t\t\bK\u000f\u0012\u001c \nC\n\u0002 \b\f\u000b\u000e\r\u000fM\b8>?\u000b8 v\u0006\u0010tWC?\nDJ? \u0011H=PQ@QZS\u000e\u0010%BJS\u0013\u0012\u001fS`F<M\b? \u0014\u0016\u0015\u000e\u0017 \u0011HW'B\\F\"T>?1WC^\u0018\u0012IBV8+B\u0018T>B \u0019 BCM\u0014F\nS>8\u0013\u001a%=PQPQ\u001b\u0012\u001d\u001c\u001e\u0014\u001f\u0012\n\u001bQ@BC8! \u0018BCO\"e\u0014?\nWC8 79OJD\u001f\u0012HM\nXD\n\u001bA\b;>D\u0016_foHG1=P8\n\u001bBCQP=ZS`BC;b\u0014e`=@T\u0013\u0010\u001dW.G oI=@L\"\u0015YO\n\u000fF\"B.G #lM>j9o BV8\bm$\u0012HQ[BV8 b\u0006%(M+B\\F<DJA\bM>?\n;\bD\u0016F \u0014%D\nXDJQ@BC;&\u0015YWVF<m'BV8\u0019 DJT('EDJA\bAEDJQP=P8\n\u000fBVF\n:<;!\u0011\u001fF<W(WCe`? \u0015\u0002S\u0014?R:<=PO.BVQ )\rQPW\u00188>D o W\u00188\b=*\u0011\u001fF\"B\\_\u0014:<WC8\u0011HQ@BCO\"eYb>B\nX>XB\\:<; +cDJ8\b8(S\u0013\u0012H;\bD\u0016?\n8\bD\u0016S ,%M\u0014:<e\u0018BV?R: \u0015YW\u0018M\b?\nDcWC8&\u0015pBVF<? \u0015\u0002S(B\n4.3 Results\nWeranatraining andtesting scheme where each row(collection\nofﬁveartists across ﬁvestyles) inturn wasselected fortesting.\nTheremaining four rowswere used fortraining. This process was\nexecuted ﬁvetimes (one foreach rowasatest,) andtheresults for\neach permutation areshowninFigure 3.\nAsisclearly evident, theresults arenotparticularly good forthe\nIDM style. Most oftheartists havebeen misclassiﬁed, andthere is\nlittle cohesion among thatstyle. This should notbeconstrued asa\nshortcoming inthetraining method, asthisisamusic style thatex-\nhibits ahuge auditory variance, ranging from aggressi verough beats\ntoabstract andsmooth textures. What tiesthese artists together as\nastyle isnotacommon sound oftheir work, butrather acultural\nafﬁnity stemming from theuseofelectronic instruments, andcom-\nmon roots ranging back toelectronic dance music. Likewise, we\nseeinconsistent results forLauryn Hill, classiﬁed asarapartist due\ntoherrap-lik eproduction.\nSuch intra-style auditory inconsistencies areofcourse hard toover-\ncome using anyaudio based system, highlighting theneed foraddi-\ntional descriptors thatfactor inadditional cultural issues.\n5.COMMUNITY MET ADATA-BASED STYLE\nCLASSIFICA TION\nWenextdescribe using cultural features forstyle classiﬁcation solely\nusing thecommunity metadata feature vectors described earlier .\nThe cultural features forthe25artists inoursetwere computed\nduring workdone onartist similarity early in2002. Each artist\nisassociated with asetofroughly 10,000 unigram terms, 10,000\nbigram terms, 5,000 noun phrases and100adjecti ves.Each term was\nassociated with anartist byitappearing onthesame web document\nastheartists’ name– butthisalone does notproveacausal relation\nofdescription. Associated with each term isascore computed by\nthesoftw are(see Table 2)thatconsiders position from thenamed\nreferent andagaussian windo waround theterm frequenc yofthe\nterm divided byitsdocument frequenc y.(Termfrequenc yishow\noften aterm appears relating toanartist anddocument frequenc yis\nhowoften theterm appears overall.) Thegaussian weused is:-\u001e.0/214365 798:3<;>=@?A1CBD?FEG:HJI KMLON\nHere,\n-OPisthedocument frequenc yofaterm,\n-.theterm frequenc y\nofaterm, andQand\nHareparameters indicating themean and\ndeviation ofthegaussian windo w.\nThis method provedwell incomputing artist similarities (givena\nknownartist similarity list, this metric could predict them ade-\nquately) buthere weaskthesame data toarrange theartists into\nclusters.\nB\n\u000f4&O,\u0004\t 7\u0016\r 9V\u000fR\u0005H\u001cO,2\t \u000f\u0012$TS\u0013\t)\u0017\u0018\u0011\u0013\u0001\u0004X\u001b\t \u0011R\t)\u0007U\u0005S(\u001d P\u0019\u0006\u0007<\u0011\u0013\u001f\b\t \u0003\u0012\u0007;\u0019\u0006\u0005\b\u001cA\f\r$\r AI\n\u0001\u0004\u00110\u001f \u000f\u0012(0(\u0013\u0019\u0016\u0017/\u0001\u0004\u000f\u0012\u0011R\t4$I\n\t)\u0001\u0004\u0003\u001b\u001fV\u0011\u0013\t)$ (0\u0017\u0018\u0019\u0012\u0007;\t)(\u00185\u000f\u0006$TSB\n\t\u0010\u0007U\u0005 9-\u0017Y\u0019\u0006\u0007\n\t8\bW\u00188`i`=PW\u0018QPDJ8(: V`aL\nGAED\u0016F<e(S V`aL?RGID.T\u0014=@?\n; V`a V\u001eWL=@8(:<D\u0016F<8\bBV:<=PW\u00188\bBCQ V`a VGDX=@8>8\bD\u0016F V`a VGDYOJW\u00188>?\n=@?R:<D\u00168`: V`a VG\nVX=Z:\n:<D\u0016F V`a VLOZOJQ@BC?\n?\n=ZU+D.T V`a VLDL[RM\b8>=PWCF V`a VUVD\\A>F<W`T>M>OJD.T V`a VUVD]F<W\u0018LrBC8(:<=PO V`a VUVD]\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\t^-\r 9-\u0001_\u0005S\u0001?,2\u000f\u0012\u0007;\u0001\u0004\u0011+*\u0007\u0005H\u000f4\u0011\u0013\u0007;\u0001\u0004\u0015= P\u0007\n\u0019`\u0005 \u0017/\u0005\b,\u0004\u00110\u0005\b\u0007\n\u000f\u0006,\u0002(0\u0001_\u0005H\u0001?,\u0004\u000f\u0012\u0007;\u0001\u0004\u0011+*\u000657baL\u000f4\u0007\n\u00110\u0001?(\u0013\u00110(S\u000f4\u0007\n\t=\u000f4\u0007;\u0007\n\u000f\u0012\u001a\b\u0003\u0006\t)$\f\u0019\u0006\u001a \t\u0010\u000f\u0012\u0017Y\u001f \u000f4\u0015\b\u0001?(\u00185dc \u000f\u0006\u0017;\u001f (\u0013\u0011 *-,\u0004\t=\u0001?(,\u0004\u0001?(\u0013\u0011\u0013\t)$\u0014\u0019\u0006\u001a\b\tH\u000f4 \"\u0011R\t)\u0007M\u000f\u0006\u001a-\u0019\u0006\u00110\u001f\b\t\u0010\u0007G\n(\u0013\u0019 \u000f\u0012\u0017\u0018\u00110\u0001\u0004X-\u0001\u0004\u0011+*L(\u0013\u0011\u0013\u000f\u0010*\b(e\u0005 \u0019\u001b(\u0013\u00110,\u0004* \u0019\u0006\u001a\u0011\u0013\u001f\b\t $\u001e\u0001\u0004\u000f\u0012\u0003\u0012\u0019\u001b\u001a\b\u000f\u0006,P5\n5.1 Clustering Overlap Scor es\nThe community metadata system computes similarity byasimple\n‘overlap score. ’Each pair ofartists issimilar with unnormalized\nC\nP3\tfHz<g\n`ihwhere\nhisaadditi vecombination ofeveryshared\nterm’ sscore. These scalars areunimportant ontheir own,butwe\ncanrank their values using each artist inoursetastheground artist\ntoseewhich artists aremore similar each other .Using thismethod,\nwecompute thesimilarity matrix M(25,25), using each artist inthe\nﬁve-style set.(See Figure 4.)\nThis matrix isthen used topredict thestyle ofeach givenartist. For\neach term type, wetakeeach artist inturn andsorttheir overlap\nweight similarities totheother 24artists indescending order .We\nthen useprior knowledge oftheactual styles ofthe24similar artists\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\t\nGuns n' Roses\n64%33%1%1%1%Billy Ray Cyrus\n21%53%19%1%7%DMX\n6%9%65%4%17%Boards of Canada\n9%11%32%12%37%Lauryn Hill\n23%4%35%8%30%\nAC/DC\n54%9%9%8%21%Alan Jackson\n19%52%4%4%21%Ice Cube\n0%1%73%10%15%Aphex Twin\n0%0%40%31%29%Aaliyah\n6%3%31%14%46%\nSkid Row\n76%10%1%2%11%Tim McGraw\n39%56%3%0%2%Wu TangClan\n13%21%31%16%19%Squarepusher\n15%4%26%27%28%Debelah Morgan\n3%1%14%18%64%\nLed Zeppelin\n72%18%2%5%2%Garth Brooks\n25%60%8%6%1%Mystikal\n0%0%51%20%35%Plone\n17%7%27%16%33%Toni Braxton\n17%11%20%10%42%\nBlack Sabbath\n41%35%7%5%12%Kenny Chesney\n25%62%3%3%7%Outkast\n13%2%62%7%16%Mouse on Mars\n10%8%32%24%27%Mya\n7%1%31%10%51%\u0000\u0002\u0001\u0004\u0003\u001b\u0005-\u0007;\t\u0001\u0000\u0016\rDAM\u0005\u001e$\u001e\u0001\u0004\u00194%'&\u001e\u000f\u0006(\u0013\t)$L(\u0013\u0011+*\b,2\tH\u0017\u0018,\u0004\u000f\u0006(0(0\u000121O\u0017\u0018\u000f\u0012\u00110\u00012\u0019\u001b\u001a65e\rD\u000f\u0012\u0007;(K$\b\t)\u001a\b\u0019\u0006\u0011\u0013\t.\u00110\u001f-\t \t)(\u0013\u00110\u0001 \u0005H\u000f\u0012\u0011\u0013\t)$ \u001c\u001e\u0007\n\u0019\u0006&\b\u000f\u0012&O\u0001?,\u0004\u0001\u0004\u0011 * \u0019\u0006 \u00110\u001f-\u000f\u0012\u0011K\u000f\u0012\u0007\n\u00110\u0001\u0004(\u0013\u0011K& \t),\u0004\u0019\u0006\u001a\b\u0003\u001b\u0001?\u001a\b\u0003#\u0011\u0013\u0019N\u000f\u001c\u001e\u000f4\u0007\n\u00110\u0001?\u0017/\u0005\u001e,2\u000f\u0012\u0007 (\u0013\u0011+*\b,2\tV5\ntoﬁnd thetrue style ofourtargetartist: descending thesorted\nlist, once wehavecounted four other artists inthesame cluster ,\nweconsider ourtargetartist classiﬁed with anormalized score (the\namount ofcumulated overlap weights thecluster contrib uted tothe\ntotal cumulated overlap weights.) The highest cumulated score\nisdeemed thecorrect classiﬁcation, andtheﬁvestyle scores are\narranged inaprobability map. Inalarger-scale implementation,\nthisstep isakin tousing asupervised clustering mechanism which\ntries toﬁndaﬁtofanunkno wntype among already labeled data (by\nthesame algorithm). Because ofthesmall sizeofthesample set,\nwefound thismore manual method more effective.\nWedothisforeach term type inthecommunity metadata feature\nspace andaverage thereturned maps intoageneralized probability\nmap. Themap deﬁnes aconﬁdence value foreach style much like\ntheneural netw ork’sresults above,andtheprobability approach was\ncrucial inintegrating thetwomethods (which wedescribe below.)\n5.2 Results\nInFigure 5weseethattheresults forthetext-only classiﬁer performs\nverywell forthree ofthestyles andadequately butnotperfectly for\ntwooftheﬁvestyles. There seems tobeconfusion between theRap\nandR&B style sets. However,fortheprevious problem set(IDM),\nthecultural classiﬁer works perfectly andwith high conﬁdence. We\ncanattrib utethistoIDM being analmost purely culturally-deﬁned\nstyle. One oftheissues thatplague acoustically-deri vedclassiﬁers\nisthatoften human classiﬁcations havelittle statistical correlation to\ntheactual content being described. This problem alsointerferes with\ncontent-based recommendation agents thatattempt tolearn arelation\nmodel between user preference andaudio content: sometimes, the\nsound ofthemusic hasverylittle todowith howwepercei veand\nattach preference toit.\nR&B andRap’ sintrinsic crosso ver(theyboth appear onthesame\nradio mark etsandareusually geared towardthesame audiences)\nshowsthatthecultural classiﬁer canbeasconfused ashumans in\nthesame situation. Here, wepresent theinverse ofthe’description\nforcontent’ problem: just asoften, cultural inﬂuences steer us\nawayfrom treating twoalmost identical artists assimilar entities, or\nputting them inthesame class.Wepropose thatautomated systems thatattempt tomodel listening\nbehavior orprovide ‘commodity intelligence’ tomusic collections\nbemindful ofboth types ofinﬂuences. Since wecanideally model\nboth behaviors, itperhaps makesthemost sense tocombine them in\nsome manner .\n6.COMBINED CLASSIFICA TION\nAspointed outinthepreceding sections, some features which are\ncrucial forstyle identiﬁcation arebestexploited intheauditory do-\nmain andsome arebest used inthecultural domain. Sofar,given\nourchoice ofdomain, wehaveproduced coherent clusters. Musical\nstyle (and evenmore somusical similarity) requires acomplicated\ndeﬁnition thatcanfactor inmultiple observ ations ranging from au-\nditory ,historical, geographical, ideological, etc. The community\nmetadata isanefforttomakeupforthelatter features, whereas the\nauditory domain helps onamore staunch judgment onthesound\nitself. Itseems only natural thatacombination ofthese twoclassi-\nﬁers canhelp disambiguate some oftheclassiﬁcation problems that\nwehavediscussed.\nInorder tocombine thetworesults weviewourclassiﬁer data as\nposterior probabilities andcompute their average values. This isa\ntechnique thathasbeen showntobegood inpractice, when wehave\naquestionable estimate ofposterior probabilities [7],asisthecase\ninthecultural-based classiﬁcation.\n6.1 Results\nTheresults oftheaveraging areshowninFigure 6.Itisclear that\nmanyoftheproblems thatwere present intheprevious classiﬁcation\nattempts arenowresolv ed.TheIDM class, which wasproblematic\nintheaudio-based classiﬁcation, isnowcorrectly identiﬁed dueto\nstrong community metadata coherence. Likewise, theRap cluster\nwhich wasnotwell deﬁned inthemetadata classiﬁcation, wascor-\nrectly identiﬁed using theauditory inﬂuence. Overall thecombined\nclassiﬁcation wascorrect forallsamples, bypassing alltheproblems\nfound ineither audio ormetadata only classiﬁcation.\n7.FUTURE WORK\nOne lessobvious useofthissystem isa‘cultural tomusical’ ratio\nequation forrelations among artists. Anapplication thatcould know\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\t\nGuns n' Roses\n44%9%19%11%17%Billy Ray Cyrus\n5%80%5%4%5%DMX\n18%27%24%8%23%Boards of Canada\n11%5%8%68%8%Lauryn Hill\n21%13%23%11%33%\nAC/DC\n30%13%16%23%18%Alan Jackson\n5%76%9%3%7%Ice Cube\n22%18%28%15%17%Aphex Twin\n9%4%26%56%4%Aaliyah\n14%13%27%11%35%\nSkid Row\n17%38%19%13%13%Tim McGraw\n18%50%17%6%9%Wu TangClan\n10%7%28%37%18%Squarepusher\n9%6%8%72%5%Debelah Morgan\n11%10%20%11%47%\nLed Zeppelin\n21%14%19%28%18%Garth Brooks\n11%60%10%8%11%Mystikal\n17%30%16%9%28%Plone\n10%6%10%65%9%Toni Braxton\n17%16%26%11%30%\nBlack Sabbath\n52%9%18%12%10%Kenny Chesney\n6%68%16%4%7%Outkast\n14%13%32%14%27%Mouse on Mars\n10%9%7%65%9%Mya\n11%20%25%10%33%\u0000\u0002\u0001\u0004\u0003\u001b\u0005-\u0007;\t!a\u0016\r \u0004D\u0019R\u0005 \u0005 \u0005\b\u001a\u001e\u0001\u0004\u0011+* \u0002\u000e\t\u0010\u0011\u0013\u000f\u0012$\b\u000f\u0012\u0011\u0013\u000f)%'&\u001e\u000f\u0012(\u0013\t4$#(\u0013\u0011+*\b,\u0004\tF\u0017/,\u0004\u000f\u0012(0(0\u0001213\u0017\u0018\u000f4\u00110\u0001\u0004\u0019\u001b\u001a65 \rD\u000f\u0012\u0007;( $-\t4\u001a-\u0019\u0006\u0011\u0013\tF\u0011\u0013\u001f\b\tF\t)(\u0013\u00110\u0001_\u0005 \u000f\u0012\u0011\u0013\t)$ \u001c\u001e\u0007\n\u0019\u0006&\u001e\u000f\u0012&\u001e\u0001?,?\u0001\u0004\u0011+* \u0019\u0006 \u00110\u001f\b\u000f4\u0011 \u000f4\u0007;\u0011\u0013\u0001?(\u0013\u0011&3\t),\u0004\u0019\u001b\u001a\b\u0003\u0006\u0001?\u001a\b\u0003 \u0011\u0013\u0019N\u000f \u001c\u001e\u000f\u0012\u0007\n\u00110\u0001\u0004\u0017/\u0005\u001e,\u0004\u000f4\u0007 (\u0013\u0011 *-,\u0004\t\u001b5\nGuns n' Roses\n54%21%10%6%9%Billy Ray Cyrus\n13%66%12%3%6%DMX\n12%18%45%6%20%Boards of Canada\n10%8%20%40%22%Lauryn Hill\n22%8%29%9%31%\nAC/DC\n42%11%12%15%20%Alan Jackson\n12%64%6%4%14%Ice Cube\n11%10%51%13%16%Aphex Twin\n5%2%33%44%17%Aaliyah\n10%8%29%13%41%\nSkid Row\n47%24%10%8%12%Tim McGraw\n29%53%10%3%6%Wu TangClan\n12%14%29%27%19%Squarepusher\n12%5%17%50%17%Debelah Morgan\n7%5%17%14%56%\nLed Zeppelin\n46%16%10%17%10%Garth Brooks\n18%60%9%7%6%Mystikal\n9%15%33%14%31%Plone\n13%7%18%40%21%Toni Braxton\n17%13%23%11%36%\nBlack Sabbath\n47%22%12%9%11%Kenny Chesney\n15%65%10%3%7%Outkast\n14%8%47%11%21%Mouse on Mars\n10%8%20%44%18%Mya\n9%11%28%10%42%\u0000\u0002\u0001\u0004\u0003\u001b\u0005-\u0007;\t\u0001\u0000\u0016\r \u0004D\u0019R\u0005.&O\u0001\u0004\u001a\b\t)$ (\u0013\u0011 *-,\u0004\t#\u0017/,\u0004\u000f\u0012(0(0\u0001213\u0017\u0018\u000f4\u00110\u0001\u0004\u0019\u001b\u001a 5\u0007\r \u000f4\u0007;( $\b\t)\u001a\b\u0019\u0006\u0011\u0013\t \u00110\u001f\b\t \t)(\u0013\u00110\u0001_\u0005 \u000f\u0012\u0011\u0013\t)$\u000e\u001c\u001e\u0007\n\u0019\u0006&\b\u000f\u0012&O\u0001?,\u0004\u0001\u0004\u0011 * \u0019\u0012 \u00110\u001f-\u000f\u0012\u0011 \u000f\u0012\u0007\n\u00110\u0001\u0004(\u0013\u0011 &3\t),\u0004\u0019\u001b\u001a-\u0003\u001b\u0001?\u001a\b\u0003 \u0011\u0013\u0019 \u000f\u001c\u001e\u000f4\u0007\n\u00110\u0001?\u0017/\u0005\u001e,2\u000f\u0012\u0007 (\u0013\u0011+*\b,2\tV5\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\u000b\u0007\n\t\r\f\u000f\u000e\u0011\u0010\u0013\u0012\u0014\u0007\u0016\u0015\u0018\u0017\u001a\u0019\u001b\u0017\u001a\t\u001d\u001c\u0011\u0000\u001e\u0010\u001f\u0019! \"\u0010\r#\u0018\u0017\u001a\u0019%$\u001b&\u0018\u0017\u001a \"\u0010\u001f#'&(\u0012*)+\u0001\u0004#-,\u0014\t\u000b .&\u0014\u0019\n\u0019!\u0007/\f\r&\u0014\t\u000b *01 32\u0004\u00194&65\u000f&\u0014 .&(\u0015\u0014 3\u0007\u0016\u0001\u0004\t\nHeavy Metal Country Rap IDM R&B020406080100Style Classification AccuracyAccuracy (%)\nAudio\nCultural\nCombined\u0000\u0002\u0001\u0004\u0003\u001b\u0005-\u0007;\t\u0001\u0000V\r\t\bK\t)(0\u0005\u001e,\u0004\u0011\u0013(Q \"\u0019\u0006\u0007.\u000f\u0012,?, \u0017/,\u0004\u000f\u0012(0(0\u0001213\u0017\u0018\u000f4\u00110\u0001\u0004\u0019\u001b\u001a \u0011\u0013\u000f\u0012(\u0003\u0002-(\u00185 AM\u0017/\u0017/\u0005\u0016%\u0007\n\u000f\u0006\u0017Y* \u0001?( &\b\u000f\u0006(\u0013\t)$=\u0019\u001b\u001a=\u00110\u001f\b\tW\u000b/% \u0001?\u001a\u0016% aN(\u0013\u0011 *-,\u0004\t.\u0017/,2\u000f\u0006(0(0\u000121O\t\u0010\u0007 5\ninadvance howtounderstand varying types ofartist relationships\ncould beneﬁt manymusic retrie valsystems thatattempt toinject\ncommodity intelligence intotheL&R process.\nAgood caseforsuch atechnology wouldbearecommend ation agent\nthatoperates onboth acoustic andcultural data. Largescale record\nshops already compute cultural relationships using saledatafedinto\nacollaborati veﬁltering system, and music-based recommenders\nsuch asMoodlogic ( ''\r\n+'\u0005\u0004`\u0018\u0005\u0006\u0014'\n\u0018J)operate onspectral fea-\ntures. Both systems haveprovedsuccessful fordifferent types of\nmusic, andasystem thatcould deﬁne ahead oftime the‘proper’ set\noffeatures tousewould beintegraltoacombination approach.\nWecould simply deﬁne a‘culture ratio’ as\u0007K\t\b\u000b\n\r\fON\u000f\u000e\n\u0010K\u0012\u0011\u000b\u0013UK\t\b\u000b\n\u0014\f@N>N\u0010K\u0012\u0011\u0016\u0015\u0006K\t\b\u0016\n\r\fON>N\nK\nGN\ni.e. theprobability that artists \band \fwill besimilar using a\ncultural metric divided bytheprobability thatartists\band\fwillbe\nsimilar using anacoustic metric. Ahigh ‘culture ratio’ would alert a\nrecommender thatcertain musical relationships (such asalmost all\nintheIDM style) should betreated using apurely cultural feature\nspace. Lowerculture ratios would indicate thatspectral ormusically\nintelligent features should beused.\n8.CONCLUSIONS\nWehavepresented aprominent problem inmusical style classiﬁca-\ntion, andproposed amultimodal classiﬁcation scheme toovercomeit.Bycombining both acoustic andcultural artist information we\nhaveachie vedclassiﬁcation instyles thatexhibit largevariance in\neither domain.\n9.ACKNO WLEDGMENTS\nThis workwassupported bytheDigital LifeConsortium oftheMIT\nMedia Lab.\n10. REFERENCES\n[1]A.Berenzweig, D.Ellis, andS.Lawrence. Using voice seg-\nments toimpro veartist classiﬁcation ofmusic. 2002. submit-\nted.\n[2]W.Chai andB.Vercoe. Folkmusic classiﬁcation using hidden\nmark ovmodels. In \u0017I\n\n\"<\n¡z| gC{fR\u0019\u0018\"\n93\nCz C\n\u0018\u001b\u001aV.\n\u001d\u001c\n\"V\u001f\u001e\u001d\nz  \nz\n\u0018\u0018\"\n9\u0016|z<g\n\n\",2001.\n[3]D.Clouse, C.Giles, B.Horne, andG.Cottrell. Time-delay\nneural netw orks: Representation andinduction ofﬁnite state\nmachines. In\u0018\"!#!$!&%\b\n>{('+C\u0019)\n\u001d*\n\u0018)\n3_fC\nV{\n ,+.-0/(1,page\n1065, 1997.\n[4]W.W.Cohen andW.Fan.Web-collaborati veﬁltering: recom-\nmending music bycrawling theweb.2324265\u00197\n\u001a\n%8*`9)\n3_fV<V{,33(1-6):685–698, 2000.\n[5]R.B.Dannenber g,B.Thom, andD.Watson. Amachine learn-\ningapproach tomusical style recognition. In \u0018\"9\u0017I\n\n\"<\n¡z| gC{R\ng\b;:5<5>=?\u0018\"\n9\"\nVz C\nC@\u001a\n%.*\u00149BA\n*{<z\nC\u001aV.\n\n\n\",\npages 344–347. International Computer Music Association.,\n1997.\n[6]M.Riedmiller andH.Braun. Adirect adapti vemethod for\nfaster backpropagation learning: The RPROPalgorithm. In\u0017I\n\n' 4\ng>\u0018\"!$!#!D\u0018\"\n '\n\u001aCJ\u0003'EC;)\n\u001d*\nC)\n3_fV<V{,pages\n586–591, SanFrancisco, CA, 1993.\n[7]D.Tax,M.vanBreuk elen, R.Duin, andJ.Kittler .Combin-\ningmultiple classiﬁers byaveraging orbymultiplying? In\u0017\nC!93FE\n\n9gC\bz\nz C\n-\u0012G<GH1. )c>'I5 ,pages 1475–1485, 2000.\n[8]G.Tzanetakis, G.Essl, andP.Cook. Automatic musical genre\nclassiﬁcation ofaudio signals, 2001.\n[9]B.Whitman, G.Flake,andS.Lawrence. Artist detection in\nmusic with minno wmatch. In \u0017\u000bR\n\"\"\n¡zg gC{1R\ng>KJML<LN:\u0018\"!#!#!2YC<V{\n\nV&)\n\u001d*\n\u0018)\n3_fV<V{\u001eJCPO\bz<gC\n\u0018\u0017\u000bR\n\"{\"{<z|ig ,\npages 559–568. Falmouth, Massachusetts, September 10–12\n2001.\n[10] B.Whitman andS.Lawrence. Inferring descriptions andsim-\nilarity formusic from community metadata. 2002. submitted."
    },
    {
        "title": "SIA(M)ESE: An Algorithm for Transposition Invariant, Polyphonic Content-Based Music Retrieval.",
        "author": [
            "Geraint A. Wiggins",
            "Kjell Lemström",
            "David Meredith 0001"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1415960",
        "url": "https://doi.org/10.5281/zenodo.1415960",
        "ee": "https://zenodo.org/records/1415960/files/WigginsLM02.pdf",
        "abstract": "We introduce a novel algorithm for transposition-invariant content- based polyphonic music retrieval. Our SIA(M)ESE algorithm is capable of finding transposition invariant occurrences of a given template, in a database of polyphonic music called a dataset. We allow arbitrary gapping, i.e., between musical events in the dataset that have been found to match points in the template, there may be any finite number of other intervening events. SIA(M)ESE can be implemented so that it finds all transposition-invariant com- plete matches for a \u0000 -dimensional template of size \u0001 in a \u0000 - dimensional dataset of size \u0002 in a worst-case running time of \u0003\u0005\u0004 \u0000\u0001\u0006\u0002\b\u0007 \u0000\u0002 \u000e\r\u000f\u0002\u0011\u0010 ; another implementation finds even the in- complete matches in \u0003\u0005\u0004 \u0000\u0001\u0006\u0002\u0012\t\u0013\u000b\u0014 \u0004 \u0001\u0006\u0002\u0015\u0010\u0016\u0010 time. The algorithm is generalizable to any arbitrary, multidimensional translation invari- ant pattern matching problem, where the events are representable by points in a multidimensional dataset.",
        "zenodo_id": 1415960,
        "dblp_key": "conf/ismir/WigginsLM02",
        "keywords": [
            "algorithm",
            "transposition-invariant",
            "content-based",
            "polyphonic music",
            "dataset",
            "template",
            "gapping",
            "complete matches",
            "incomplete matches",
            "worst-case running time"
        ],
        "content": "SIA(M)ESE:AnAlgorithm forTransposition Invariant,Polyphonic Content-Based MusicRetrieval\nSIA(M)ESE:AnAlgorithm forTransposition Invariant,\nPolyphonicContent-Based MusicRetrieval\nGeraintA.Wiggins\nDepartmentofComputing\nCityUniversity,London\ngeraint@soi.city .ac.ukKjellLemström\nDepartmentofComputer\nScience,UniversityofHelsinki\nklemstro@cs .helsinki.ﬁDavidMeredith\nDepartmentofComputing\nCityUniversity,London\ndave@titanm usic.com\nABSTRA CT\nWeintroduce anovelalgorithm fortransposition-in variantcontent-\nbasedpolyphonic musicretrieval.OurSIA(M)ESEalgorithm is\ncapableofﬁndingtransposition invariantoccurrences ofagiven\ntemplate,inadatabase ofpolyphonic musiccalledadataset.We\nallowarbitrarygapping,i.e.,betweenmusicaleventsinthedataset\nthathavebeenfoundtomatchpointsinthetemplate, theremay\nbeanyﬁnitenumberofotherinterveningevents.SIA(M)ESE\ncanbeimplemented sothatitﬁndsalltransposition-in variantcom-\npletematches fora\n\u0000-dimensional template ofsize \u0001ina\n\u0000-\ndimensional datasetofsize\u0002inaworst-case runningtimeof\u0003\u0005\u0004\u0000\u0001\u0006\u0002\b\u0007\n\u0000\u0002\n\t\f\u000b\u000e\r\u000f\u0002\u0011\u0010;anotherimplementation ﬁndseventhein-\ncomplete matchesin\n\u0003\u0005\u0004\u0000\u0001\u0006\u0002\u0012\t\u0013\u000b\u0014\r\n\u0004\u0001\u0006\u0002\u0015\u0010\u0016\u0010time.Thealgorithm is\ngeneralizable toanyarbitrary,multidimensional translation invari-\nantpatternmatching problem, wheretheeventsarerepresentable by\npointsinamultidimensional dataset.\n1.INTRODUCTION\nTheSIAalgorithm [5]isintended fortheinduction ofsigniﬁcant\nstructures from(musical) databases. However,itcanstraightfor -\nwardlybemodiﬁed foruseinsearchingforpatternsindatabases,\nmakingassessment comparisons betweendatasets. Inthecontent-\nbasedmusicretrieval(CBMR) application, ourmatching algorithm,\nwhichwecallSIA(M)ESE,performs toalevelcomparable with\nother,morerestricted, algorithms basedon,forexample,dynamic\nprogramming (DP)workingwithinaneditdistanceframework[2,\n3,4].Specifying theproblemasoneofstringmatching hasintrin-\nsicdrawbacks: ingeneral,thereisnonaturalwayofrepresenting\npolyphonic musicexactlywithlinearstrings(exceptincertainspe-\nciﬁccases).Speciﬁcally ,thestring-based methodsrelyonlinear\ncontiguity betweensuccessivesymbolsinastring,andthenotion\nofco-occurrence issomewhatvague.Eitheroftheseconstraints\nisenoughtoprecludestraightforw ardrepresentation ofpolyphonic\ndatabases. However,SIA(M)ESEefﬁcientlyandeffectivelypro-\ncessespolyphonic musicandproduces moreusefulinformation than\ntheDPmethods. Inthispaper,weintroduce thefunctionality of\nSIA(M)ESEandhowitcanbeadaptedtosimulateotherrelated\nalgorithms.\n2.THENEWALGORITHM\nWeexpressthefunctionality ofSIA(M)ESEbyusinglogicalfor-\nmulaetomakeiteasiertounderstand theidea.Thisstageiscalled\nSIA(M),forSIA(Matching). SIA[5]isexpressed bytheequation\n(1),below.Let\u0017represent thetransposition vector.Thenthestruc-\ntureset \u0018iscomputed fromthedataset, \u0019,beinganysetofpoints\nwithanynumberofdimensions. Thedimensions maybemeasured\nonanyﬁnitelyexpressible metricsolongasitispossibletogivea\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.c\n\u001a\n2002sIRCAM-CentrePompidoutotalordering, \u001b,onallthepointsinthevectorspacedeﬁnedby \u0019.\u0018\n\u0004\u0019\u001c\u0010\u001e\u001d \u001f! \u0017#\"\u0016$&%('\n\u0004*),+$-\u0010/.\u00050 (1)\u0004*),+\u0019\u001c\u001021\n\u0004*)\u0007 \u0017\n+\u0019\u001c\u001031\n\u0004*)\u001b\n)\u0007 \u00174\u0010\u00165\nTheideaistoﬁndallthemaximalsubsetsof \u0019,whicharetranslated\nbyanon-zero vector,\u0017,inthespacedeﬁnedby\u0019’sdimensions, to\nanothersubsetof \u0019.Theordering, \u001b,preventswastageofeffort\nandduplication ofresultsbyremovingrepetition undersymmetry .\nTheoutput, 6,isintheformofasetof  vector,point-set %pairs,\nrelatingeachsubsettothevectorwhichtranslates it.\nInSIA(M),wecomputethematchset7using(2),fromatemplate\nset, 8,whoseoccurrences inthedataset, \u0019,wewishtoﬁnd:7 \u001d \u001f! \u0017#\"\u0016$&%('\n\u0004*),+$-\u00109.\u00050 (2)\u0004*):+8\u0012\u0010;1\u0006<>=\n\u0004\u0016\u0004=\n+\u0019\u001c\u001021\n\u0004*)\u0007 \u0017@?A=B\u0010\u0016\u0010\u00165DC\nTherearejusttwosmalldifferencesbetweenthesepairsofdeﬁni-\ntions.Themoresigniﬁcant isthatinSIA(M),thevectors,\u0017,arenot\nspeciﬁed intermsoftwopoints,\n)and\n)\u0007 \u0017,in \u0019orderedunder \u001b\ntoavoidduplication undersymmetry asin(1).Rather,thevectors\naregenerated fromtwoseparatesetsofpointsinhabiting thesame\nvectorspace:thetemplate,8,andthedataset,\u0019.Theseparation\nofthesetwosetsmeansthattheefﬁciencymeasureofremoving\nequivalentsundersymmetry ingenerating 8nolongermakessense,\nthusabsenceofthe\u001btermof(1)in(2).Theotherdifferenceisthat\nin(2)amorerelaxednotionofcomparison isadmitted.\nInthesimplestcasewheretheaimistoﬁndanexactoccurrence of8in \u0019, ?inequation(2)isdeﬁnedasidentityandanoccurrence is\nfoundifandonlyifcondition<4E\n\u0004\u0016\u0004 \u0017B\"FEG%\n+7H\u001031\n\u0004' EI'>\u001dG\u0001J\u0010\u0016\u0010\nisalsosatisﬁed.\nItturnsoutthattheexpressions abovecanbeimplemented very\nefﬁcientlysothatﬁndingalltransposed complete matchesfora\u0000-dimensional template ofsize \u0001ina\n\u0000-dimensional datasetof\nsize\u0002canbedoneinaworst-case runningtimeof\n\u0003\u0005\u0004\u0000\u0001\u0006\u0002K\u0007\u0000\u0002L\t\f\u000b\u000e\r\u000f\u0002\u0011\u0010,anotherversionﬁndseventheincomplete matchesin\u0003\u0005\u0004\u0000\u0001\u0006\u0002L\t\f\u000b\u0014\r\n\u0004\u0001\u0006\u0002\u0015\u0010\u0016\u0010time.Therefore, wecalltheimplementing algo-\nrithmSIA(M)E,forSIA(M)Express.\n3.FINDING MATCHING DATA\nToreasonaboutthestructures produced bytheSIA(M)Ealgorithm\ntogeneratedescriptions ofmatcheswithparticular properties be-\ntweenatemplateandadataset,weintroduce thenotionofacover.\nThisallowsusformally todescribethecorresponding elements in\namatch,andsomeusefulproperties ofcoverswhichcanbeused\ntodetermine particular kindsofmatch.Theprocessworksbyse-\nlectingcoversofatemplate8from7underaheuristicevaluation\nfunctioncomposed fromvariouspossibleelements. Sinceweare\nnowaddingSelection andEvaluationtoSIA(M)E,wecallthis\ncompleted algorithm SIA(M)ESE.\n3.1PatternCoverforaMatching Subset\nHavingcomputed 7,wesearchinitforacoverof 8.Theintuition\nbehindthecoveristhatwewantanoptimal(insomeappropriate\nsense)setoffragmentary matchestoourtemplate,8,suchthat\neachdatapoint ineachof(thelargestpossiblesubsetof) 8andtheSIA(M)ESE:AnAlgorithm forTransposition Invariant,Polyphonic Content-Based MusicRetrieval\nmatching subsetof7isaccounted forexactlyonce.Soacover\ndescribes aone-to-one mappingbetween(thelargestpossiblesubset\nof)8andthatsubsetof7againstwhich8hasbeenmatched.\nStatedmoreformally,acandidate cover Misasetof  vector,datapoint-\nset%pairs(viz.,asubsetof7),theintersection ofwhosedatapoint-\nsetsisempty,andnoneofwhosedatapoint-sets containsadatapoint\nwhichmaps,underthetranslation ofitsassociated vector,tothe\nsamepointasamemberofadifferentdatapoint-set in Munderthe\ntranslation ofitsownrespectivevector:\u0004\u0016\u0004MONP7H\u0010Q1\u0005R \u0017BST\" \u0017>U\u0014\"V$:ST\"\u0016$\u0005U\n\u0004 \u0017#SW\"\u0016$:SX%\n+MY\u0010Q1\u0004 \u0017\nU\"\u0016$\nU%\n+MY\u001021\n\u0004\u0017\nSYZ\u001d \u0017\nU\u0010\u0016\u0010[ \\<\n)SW\"\n)U\n\u0004\u0016\u0004*)S\n+$\u001cSX\u0010Q1\n\u0004*)U\n+$\u0005U]\u001021\n\u0004\u0016\u0004*)S\u000f\u001d\n)UT\u00102^\u0004*)S9\u0007 \u0017#S_\u001d\n)U/\u0007 \u0017>U]\u0010\u0016\u0010\u0016\u0010 (3)\nThecover,`,isthesetofsuchpairstheunionofwhosedatapoint-\nsetsisthelargestoutofthecandidate covers.\nHowever,(3)willtypically generate manycovers,withdifferent\nstructural properties. Weneedtobeabletopickanappropriate one.\nOften,wewillwanttousethesmallestcover–thatis,theonewhich\ndividesthepatternupintofewestsegments.Thisisbecauseweare\nworkingundertheassumption thatourdatawillbecoherentwhere\nitdoesnotcontainerrors,andbecausewewanttoﬁndtheclosest\nmatchinthecasewherethereisnotauniquesolution. Ingeneral,\nitwillbeinfeasible toﬁndtheabsolutely smallestcover.Instead,\nweuseanalgorithm whichisastraightforw ardspecialization of\nbestﬁrstsearch.Incoherentcases,thistechnique isefﬁcient,even\nthoughitissearch-based, asthedatasubsetswillbelarge,andsoit\nwillberelativelyeasytoassemble acomplete match.\nInferring Matches andDifferences.However,theremaybesev-\neralsmallestcovers,andtheymayhavedifferentproperties which\nmaybeusefulindifferentcircumstances. InthecontextofCBMR,\nwewouldwantthesetsinourcovertomatch(almost)time-contiguous\nsetsinthedata,sothattheyconstitute acomplete musicalunit.We\ncallthesethetime-minimal andthetime-maximal smallestcovers,\nrespectively.Notethatwhiletheseandotherdata-dependent prop-\nertiesmightbebuiltintothebasicalgorithms forSIA(M)(and\nSIA)wehavechosentokeepthemseparatefromthesymmetrical\nmultidimensional speciﬁcation forreasonsofclarityandneatness.\n4.SIMULA TINGRELATEDALGORITHMS\nIntheliterature, thereareonlyafewCBMRalgorithms capable\nofmatching polyphonic data.Incertaincases,someofthosemay\nbemoreefﬁcientthanSIA(M)E(as,e.g.,MONOPOLYinﬁnding\nexactoccurrences ofamonophonic templateinpolyphonic dataset).\nHowever,weclaimthatnoneofthemisasﬂexibleandversatile\nCBMRalgorithm asSIA(M)E.Inthefollowing,weillustrate this\nbyshowinghowtheworkingofthepreviousCBMRalgorithms\ncanbesimulated byusingSIA(M)E(withouttheneedofchanging\nitsoriginaltimecomplexity).Thisisnotthecasetheotherway\naround:noneofthepreviousalgorithms cansimulatetheworking\nofSIA(M)E,atleast,notwithoutmajormodiﬁcation leadingtoa\nconsiderable increaseoftimecomplexity.\nGapping Simulation. Eventhoughitisanadvantagetobeableto\ndealwithunlimited gaps,ifdesired,wecansimulatethegapping\nusedin[1,2]withSIA(M)E.Inthiscase,weuseamechanism\nthatwecalllabelling.Wedeﬁnealabeltobeanattributeattached\ntoeverydatapoint,butwhichisnotbeingconsidered asanother\ndimension bySIA(M)E.Thus,weareabletorepresent dimensions\nnotexhibiting translation invariance.\nInthiscase,SIA(M)Eattachesanextralabel, a\n\u0004cb\u0010,foreach\ntwo-dimensional point\nbinthedataset(thelabelling mechanism is\nnaturally generalizable toanyn-dimensional dataset). Letdfe\n\u0004cb\u0010\ndenotetheonsettimeofdatapoint\nb.Themapping aisasurjectiontoarangeg>\"TCWCWCX\"\u0016\u00022h,suchthata\n\u0004cbS\u0010\bija\n\u0004cbU\u0010ifandonlyif\ntheonsettimeofdatapoint\nbSoccurslaterthanthatof\nbU,anda\n\u0004cbS\u0010k\u001dla\n\u0004cbU\u0010ifandonlyifdfe\n\u0004cbS\u0010k\u001dHd\u0014e\n\u0004cbU\u0010.Moreover,ifbSand\nbUaretwodatapoints suchthat dfe\n\u0004cbSm\u0010\nnod\u0014e\n\u0004cbUT\u0010andthere\nexistsnodatapoint\nbsuchthatdfe\n\u0004cbSF\u0010\bnpd\u0014e\n\u0004cb\u0010\bnpd\u0014e\n\u0004cbUT\u0010thena\n\u0004cbU\u0010/\u001dAa\n\u0004cbS\u0010Q\u0007rq.\nNow,themodiﬁcation toSIA(M)Eisslight.Recalltheformulae\nabove.Havingcalculated equations (2),insteadofobserving if\nthereisan Ein 7suchthat ' EI's\u001dG\u0001,thetruthofthefollowing\nsentenceshouldbeobserved:<>$\n\u0004\u0016\u0004 \u0017B\"t$&%\n+7H\u0010;1\n\u0004' $u'D\u001dr\u0001&\u0010 (4)v\u0012w\u000exXyDz\u000e{ |~} X<\nbU\n\u0004\u0016\u0004cbU\n+$-\u0010Q1\n\u0004' a\n\u0004cbUT\u0010\u0015Ka\n\u0004cbSX\u0010X'sPe\u0016\u0010\u0016\u0010\u0016\u0010\nwhere\n\u0004$,\" \u00174\u0010denotesthesetofdatapoints inthedatasetthatis\nequaltothetranslation of $by \u0017and eistheallowablegap.Whereas\ntheallowingofaparametrized spacingmakesthestringmatching\napproach morecomplex,forSIA(M)Eitdoesnothavesuchan\naffect;itcausesonlyaminorpracticaloverheadfortheexecution.\nMultiple Simultaneous Templates. Simultaneous searching for\nmultiplemusicaltemplates, considered in[3],canalsobesolved\nwithSIA(M)E.Let8\nS3TW82bethetemplates tobesearchedfor\nin \u0019,simultaneously .WithSIA(M)E,thesolutionisverystraight-\nforward;itneedsnomodiﬁcation totheproblemspeciﬁcation nor\ntotheimplementation. Wesimplyconsiderthenumbering ofthe\ntemplates asanotherdimension totheproblem. Becausethepoints\ninthedatasethaveaconstantvaluealongthisdimension, itisnot\npossibleforanytwopoints\nb;and\nbsoftwodistincttemplates tobe\ntranslatable withthesamevector.\n5.CONCLUSION\nWehaveaintroduced thefunctionality ofanovelefﬁcientandversa-\ntiletransposition invariantCBMRalgorithm forpolyphonic music.\nIndeed,SIA(M)ESEisgeneralizable toanyarbitrary,multidimen-\nsionaltranslation invariantpatternmatching problem, wherethe\neventscanberepresented bypointsinamultidimensional dataset.\nThus,e.g.,itisalsoapplicable inmatching bit-mapimages.Allthe\ndetailsofSIA(M)ESEwillappearinacomplete manuscript.\n6.ACKNOWLEDGEMENTS\nThisworkwaspartlysupported bygrants#48313fromtheAcademy\nofFinlandandGR/R25316 fromEPSRC(KjellLemström), andEP-\nSRCresearchgrantGR/N08049 (DavidMeredith). Patentsapplied\nfor[6].\n7.REFERENCES\n[1]M.Crochemore, C.S.Iliopoulos, Y.J.Pinzon,andW.Rytter.\nFindingmotifswithgaps.InProc.ISMIR’2000 ,Plymouth.\n[2]M.Dovey.Atechnique forregularexpression stylesearching in\npolyphonic music.InProc.ISMIR’2001 ,Bloomington.\n[3]J.Holub,C.Iliopoulos, andL.Mouchard. Distributedstring\nmatching usingﬁniteautomata. JournalofAutomata, Lan-\nguagesandCombinatorics ,6(2):191–204, 2001.\n[4]K.Lemström andJ.Tarhio.Detecting monophonic patterns\nwithinpolyphonic sources.InContent-Based Multimedia In-\nformation AccessConferenceProceedings (RIAO’2000),pages\n1261–1279, Paris,2000.\n[5]D.Meredith, K.Lemström, andG.Wiggins.Algorithms fordis-\ncoveringrepeatedpatternsinmultidimensional representations\nofpolyphonic music.JournalofNewMusicResearch,2002.\n(Toappear).\n[6]D.Meredith, G.Wiggins,andK.Lemström. Methodof\npatterndiscovery,2002.PCTpatentapplication number\nPCT/GB02/02430, UKpatentapplication number0211914.7."
    },
    {
        "title": "MACSIS: A Scalable Acoustic Index for Content-Based Music Retrieval.",
        "author": [
            "Cheng Yang"
        ],
        "year": "2002",
        "doi": "10.5281/zenodo.1416662",
        "url": "https://doi.org/10.5281/zenodo.1416662",
        "ee": "https://zenodo.org/records/1416662/files/Yang02.pdf",
        "abstract": "\u0001\u0003\u0002\u0005\u0004\u0007\u0006\b\u0002 \b\u0002\f\u000b\u000e\r\u0003\u000f\u0010\u000b\u0011\u0002\u0013\u0012\u0015\u0014\u0013\u0016\u0017\u0002 \u000e\r\u0018\u000f\u0010\u000b\u001a\u0019\u0011\t\b\u0014\u001b\u000f\u001d\u001c\u001e\u000f\u001b\u001f \u001c\u0017\u0002!\t\b\"\u000e\t\b\r\b\u0002 #$\r\b%&\u000f\u001b\r'\u0016\u0017\u000b\u0007\u0019\u001a\u0002 (\u000e\u0002",
        "zenodo_id": 1416662,
        "dblp_key": "conf/ismir/Yang02",
        "keywords": [
            "article",
            "key",
            "aspects",
            "extracted",
            "10",
            "words",
            "or",
            "short",
            "phrases",
            "capturing"
        ],
        "content": "TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\nTheMACSISAcoustic IndexingFrameworkforMusic\nRetrieval:AnExperimental Study\nChengYang\n\u0000\nStanfordUniversity\nDepartmentofComputer Science\nStanford,CA94305,U.S.A.\nyangc@cs .stanford.edu\nABSTRA CT\u0001\u0003\u0002\u0005\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r\u0003\u000f\u0010\u000b\u0011\u0002\u0013\u0012\u0015\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\r\u0018\u000f\u0010\u000b\u001a\u0019\u0011\t\b\u0014\u001b\u000f\u001d\u001c\u001e\u000f\u001b\u001f \u001c\u0017\u0002!\t\b\"\u000e\t\b\r\b\u0002\n#$\r\b%&\u000f\u001b\r'\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(\u000e\u0002\n\t\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014-#.*\u0007\t,\u0016\u0017\u0014.\u0019&\u000f\u001b\r/\u000f102)+\u00063\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u00197#.*\u001a\t,\u0016\u0017\u00141\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c29;:<)+\r\b%\r\b%\u001a\u0002=#.*\u0007\t,\u0016\u0017\u0014>\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002.\u000f\u0010\u000b\u001a\u0019@\u0016\u0017\u000b\u001a\u0004\u0007*\u001a\r3A\u000e*\u0007\u0002\f\u0006,\u0016\u0017\u0002\n\t;\u000f\u0010\u0006\b\u0002=B\u001b\u0016\u00178+\u0002\n\u000bC\u0016\u0017\u000b'\u0006/\u000f\nDE\u000f\u001b*\u0007\u0019\u000e\u0016\u0017)02)4\u0006\b#C\u000f\u0010\r\b\tFDG\u0016\u0017\r\b%\u001a)+*\u001a\r<#H\u0002\n\r/\u000f\u0010\u0019?\u000f\u0010\r/\u000f=)4\u0006F)+\r\b%\u001a\u0002\n\u0006F\t\b\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014J\u0016\u0017\u000b\u000e02)4\u0006\b#C\u000f\u0010\r,\u0016\u0017)4\u000bLK?\u0006\b\u0002\u00135\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cM\u0016\u0017\tG\r/\u000f\u001b\u0006\bB+\u0002\n\r\b\u0002\n\u0019N\u000f\u0010\rG#.*\u001a\t,\u0016\u0017\u0014>\u0004 \u0016\u0017\u0002\n\u0014\f\u0002\n\t3DJ%\u000e\u0016\u0017\u0014/%O\u000f\u0010\u0006\b\u0002\u0015PQ\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006\bR'\r\b).\r\b%\u0007\u0002A\u000e*\u001a\u0002\n\u0006\b\"'\t\b)4*\u0007\u000b\u001a\u0019O\u0014\u0013\u001cI\u0016\u0017\u0004S9T=*\u001a\u0006102\u0006/\u000f\u001b#-\u0002\nDG)4\u0006\bUV\u0016\u0017\t-\u0019\u001a\u0002\n\t,\u0016\u0017B4\u000b\u0007\u0002\n\u0019W\u000f\u001b\tC\u000f'\t\b\u0002\n\u0006,\u0016\u0017\u0002\n\tH)\u00100>#H)\u000e\u0019\u0007*\u000e\u001c\u001e\u000f\u0010\u0006C\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002\t\b\r/\u000f\u001bB+\u0002\n\tH\u000f\u0010\u000b\u001a\u0019\u0005\u0004\u0007%?\u000f\u0010\t\b\u0002\n\t\n9VX<\u000f\u001b\u0014\u0013%Y#.*\u001a\t,\u0016\u0017\u0014\u0015Z\u001a\u001c\u0017\u0002'\u0002\f\u000b\u000e\r\b\u0002\n\u0006,\u0016\u0017\u000b\u0007B\u0003\r\b%\u001a\u0002@\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002'\u0016\u0017\t\r\b\u0006/\u000f\u001b\u000b\u0007\t,02)4\u0006\b#-\u0002\n\u0019[\u0016\u0017\u000b\u000e\r\b)\u0005\t\b\u0004&\u0002\n\u0014\f\r\b\u0006\b)+B+\u0006/\u000f\u001b#\\84\u0002\n\u0014\n\r\b)4\u0006\b\t'\u000f\u001b\u000b\u0007\u0019E\r\b%\u001a\u0002\n\u000b\u0005\u0016\u0017\u000b\u000e\r\b)^]/_&`\u000ea\bb`\u000e]\u001bc2d\u001ba\u0013egf\fc2eh]'f\fd\u0013i\u001bj\u001ad\u0010k?]\u0013d\nf/l<\u0006\b\u0002\f\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r,\u0016\u0017\u000b\u0007BW\t\b#C\u000f\u001d\u001cI\u001c.\t\b\u0002\fB+#-\u0002\n\u000b\u000e\r\b\tO)\u001b0C\u000f\u001b*\u0007\u0019\u000e\u0016\u0017)02\u0002\u001d\u000f\u0010\r\b*\u001a\u0006\b\u0002\n\t=\r\b%?\u000f\u0010\r;\u0014\u001b\u000f\u0010\u000bO\r\b)\u001b\u001c\u0017\u0002\n\u0006/\u000f\u0010\r\b\u0002-\t\b)4#-\u0002.\u000b\u0007)\u001b\u0016\u0017\t\b\u0002C\u000f\u0010\u000b\u001a\u0019O\r\b\u0002\n#-\u0004&)C84\u000f\u0010\u0006,\u0016\u001e\u000f\u001b\r,\u0016\u0017)+\u000b\u001a\t\n9m3%\u001a\u0002\n\t\b\u0002C\t\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\u0002\f\tC\u000f\u0010\u0006\b\u0002C\u0004\u000e\u001c\u001e\u000f\u0010\u0014\n\u0002\n\u0019Y\u0016\u0017\u000b!\u000f'%\u000e\u0016\u0017B+%\u000e5,\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000bn\u000f\u001d\u001c=\u0016\u0017\u000b\u001a\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u0007B\t\b\r\b\u0006\b*\u001a\u0014\n\r\b*\u001a\u0006\b\u0002+9poq\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cF\u0006\b\u0002\f\t\b* \u001c\u0017\r\b\t302\u0006\b)+#r\r\b%\u0007\u00023\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(O\u000f\u001b\u0006\b\u0002=\u0006/\u000f\u001b\u000b\u0007U4\u0002\n\u0019'\u001f?\u000f\u0010\t\b\u0002\n\u0019)4\u000b\u0005\u000f\u001d\u001cI\u0016\u0017B4\u000b\u0007#-\u0002\f\u000b\u000e\r-)\u001003\t\b%\u001a)+\u0006\b\r.#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u0007BO\t\b\u0002\fB+#-\u0002\n\u000b\u000e\r\b\t\n9NXs\u000f\u0010\u0014/%7#-)\u000e\u0019\u001a*\u000e\u001c\u0017\u0002')\u00100\r\b%\u001a\u0002'02\u0006/\u000f\u0010#H\u0002\nDG)+\u0006\bU\u0005\u0014\u001b\u000f\u001b\u000b[\u001f&\u0002'\u0016\u0017\u000b\u001a\u0019\u001a\u0002\n\u0004&\u0002\n\u000b\u001a\u0019\u0007\u0002\f\u000b\u000e\r,\u001c\u0017\"^\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\u0019E)+\u0006C\u0006\b\u0002\n\u0004\u000e\u001c\u001e\u000f\u0010\u0014\f\u0002\n\u0019Ll\u000f\u001b\u000b\u0007\u0019ODG\u0002;\t\b\r\b*\u0007\u0019\u001a\"'\r\b%\u001a\u0002\u0013\u0016\u0017\u0006;\u0002\u0013tS\u0002\n\u0014\n\r\b\t;\u001f\u000e\"u\u000f1\t\b\u0002\n\r3)\u00100s\u0002\n(\u000e\u0004?\u0002\n\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\t\n9\n1.INTRODUCTIONvF\u000f\u001b\t\b\rG\u0006\b\u0002\n\t\b\u0002\u001b\u000f\u001b\u0006\b\u0014\u0013%C)+\u000bC\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u0019'#.*\u001a\t,\u0016\u0017\u0014=\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cM%?\u000f\u0010\tp\u0004\u0007\u0006,\u0016\u0017#C\u000f\u001b\u0006,\u0016I\u001c\u0017\"02)\u000e\u0014\f*\u0007\t\b\u0002\n\u0019\u0005)4\u000b7\t\b\"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014O\u0019?\u000f\u0010\r/\u000f'\u0006/\u000f\u001b\r\b%\u0007\u0002\f\u00061\r\b%?\u000f\u0010\u000b\u0005\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014'\u0019?\u000f\u0010\r/\u000f\u000e9O\u0001w\u0016\u0017\r\b%\t\b\"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014\u0018\u0019&\u000f\u001b\r/\u000f7\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)4\u000b^)\u001b0>#.*\u0007\t,\u0016\u0017\u00144l=\r\b%\u001a\u0002O\u0019&\u000f\u001b\r/\u000fVZ\u001a\u001c\u0017\u0002OU+\u0002\n\u0002\n\u0004\u001a\t\r\b\u0006/\u000f\u001b\u0014\u0013UY)\u001b0;\u0002\u001b\u000f\u0010\u0014/%Y\u000b\u001a)+\r\b\u0002+x \t-\u0004\u000e\u0016\u0017\r\b\u0014\u0013%SlG\u0019\u001a*\u0007\u0006/\u000f\u001b\r,\u0016\u0017)+\u000bWy2\t\b\r/\u000f\u0010\u0006\b\r.\r,\u0016\u0017#-\u0002\u001bz\u0010\u0002\f\u000b\u0007\u0019!\r,\u0016\u0017#-\u0002\u001b{\u0013l\t\b\r\b\u0006\b\u0002\n\u000b\u001aB+\r\b%Sls\u000f\u001b\t>DG\u0002\u0013\u001cI\u001cq\u000f\u0010\t>)+\r\b%\u001a\u0002\n\u0006>\u0004&\u0002\f\u0006\b\r,\u0016\u0017\u000b\u0007\u0002\n\u000b\u000e\r.\u0016\u0017\u000b 02)4\u0006\b#C\u000f\u0010\r,\u0016\u0017)4\u000bL9uXF(\u001a\u000f\u0010#H\u0004 \u001c\u0017\u0002\n\t)\u001b0=\t\b*\u001a\u0014/%[\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r/\u000f\u001b\r,\u0016\u0017)+\u000b\u001a\t\u0015\u0016\u0017\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019\u0007\u0002\u0018|'}\b~3}-\u000f\u001b\u000b\u0007\u0019!3*\u0007#-\u0019\u001a\u0006\b*\u001a#'lsDq\u0016\u0017\r\b%|'}~J}=\u001f?\u0002\u0013\u0016\u0017\u000b\u0007BO\r\b%\u001a\u0002-#-)4\t\b\r>\u0004&)+\u0004\u001a*\u000e\u001c\u001e\u000f\u0010\u0006102)4\u0006\b#C\u000f\u0010\r\n9H|O\u000f\u001b\u000b\u000e\"7)4\u0004&\u0002\n\u0006/\u000f\u0010\r,\u0016\u0017)4\u000b\u0007\tH)+\u000b\t\b*\u001a\u0014\u0013%\u0003\u0019?\u000f\u0010\r/\u000f'\u000f\u001b\u0006\b\u0002184\u000f\u001b\u0006,\u0016\u001e\u000f\u0010\r,\u0016\u0017)+\u000b\u001a\t-)\u001b0G\t\b\r\b\u0006,\u0016\u0017\u000b\u001aB'#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aBO#-\u0002\n\r\b%\u001a)\u000e\u0019\u0007\t\n9.\u0001w\u0016\u0017\r\b%\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014>\u0019&\u000f\u001b\r/\u000f\u0006\b\u0002\f\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)+\u000bSl&%\u001a)\u001bDG\u0002\n84\u0002\n\u0006\nl\u001a\u000b\u0007)>\u0016\u0017\u000b 0)+\u0006\b#C\u000f\u001b\r,\u0016\u0017)+\u000bO\u000f\u001b\u001f&)+*\u001a\r\u0016\u0017\u000b\u001a\u0019 \u0016\u00178+\u0016\u0017\u0019\u001a*S\u000f\n\u001cO\u000b\u001a)+\r\b\u0002\n\tO\u0016\u0017\tOB\u0010\u0016\u001784\u0002\n\u000bLKu)+\u000b\u000e\u001c\u0017\"\u000f\u001b*\u0007\u0019\u000e\u0016\u0017)\u0016\u0017\u000b\u000e\r\b\u0002\f\u000b\u0007\t,\u0016\u0017\r6\"\u001184\u000f\u001d\u001c\u0017*\u001a\u0002\n\t\u0005\u000f\u0010\u0006\b\u0002\u0006\b\u0002\n\u0014\f)+\u0006\b\u0019\u001a\u0002\n\u0019E\u000f\u001b\t'\u000f'02*\u001a\u000b\u0007\u0014\f\r,\u0016\u0017)+\u000b[)\u001b0>\r,\u0016\u0017#-\u00024l3\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\u0019W\u000f\u0010\r\u0015\u000fV\u0014\n\u0002\f\u0006\b\r/\u000f\u001d\u0016\u0017\u000b[\u0006/\u000f\u001b\r\b\u0002+l)\u001b02\r\b\u0002\n\u000b'\u0014\f)+#-\u0004\u001a\u0006\b\u0002\n\t\b\t\b\u0002\n\u0019O\r\b)-\t/\u000f\n84\u0002>\t\b\u0004&\u000f\u001b\u0014\n\u0002+93XF(\u001a\u000f\u0010#H\u0004 \u001c\u0017\u0002\n\t=\u0016\u0017\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019\u0007\u0002u9 D3\u000f\n8&lS9 \u000f\u0010*\u000f\u001b\u000b\u0007\u0019\u0003|Ovs 9q \"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014C#.*\u0007\t,\u0016\u0017\u0014.\u0019?\u000f\u0010\r/\u000f-\u0014\u001b\u000f\u0010\u000bO\u001f?\u0002>\t\b\"\u000e\u000b\u000e\r\b%\u0007\u0002\n\t,\u0016\u0017\f\u0002\n\u00197\u0016\u0017\u000b\u000e\r\b)u\u000f\u0010*\u000e5\u0019\u000e\u0016\u0017)C\t,\u0016\u0017B+\u000b?\u000f\u001d\u001c\u0017\t>\u0002\u001b\u000f\u001b\t,\u0016I\u001c\u0017\"+lM\u001f\u001a*\u001a\rJ\r\b%\u001a\u0002\n\u0006\b\u0002=\u0016\u0017\t;\u000b\u0007)-U\u000e\u000b\u001a)\u001bD3\u000b\u0003\u000f\u001d\u001c\u0017B+)4\u0006,\u0016\u0017\r\b%\u0007#\r\b)-\u0019\u0007)H\u0006\b\u0002\u00135\u001cI\u0016\u001e\u000f\u001b\u001f \u001c\u0017\u0002.\u0014\n)+\u000b\u000e84\u0002\n\u0006\b\t,\u0016\u0017)+\u000bC\u0016\u0017\u000bC\r\b%\u0007\u00023)+\r\b%\u001a\u0002\n\u0006s\u0019 \u0016\u0017\u0006\b\u0002\f\u0014\n\r,\u0016\u0017)+\u000bYyh\u001629 \u0002+9\u0017l\u000e#1*\u001a\t,\u0016\u0017\u0014=\r\b\u0006/\u000f\u001b\u000b\u0007\t\b\u0014\f\u0006,\u0016\u0017\u0004 5\r,\u0016\u0017)4\u000b[02\u0006\b)+#\u0006/\u000f\nD\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014Y\t,\u0016\u0017B4\u000b&\u000f\n\u001c\u0017\t/{\u00139m3%\u001a\u0002\n\u0006\b\u0002\u001302)4\u0006\b\u0002+l3\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u0019#.*\u0007\t,\u0016\u0017\u0014C\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001cG)4\u000b\u0003\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014C\u0019?\u000f\u0010\r/\u000fCDG)+*\u000e\u001c\u0017\u00197\u0006\b\u0002\fA\u000e* \u0016\u0017\u0006\b\u0002u\u000f-\u000b\u0007\u0002\fD\t\b\u0002\f\r=)\u00100\r\b\u0002\n\u0014/%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002\n\t3\r\b%&\u000f\u001b\rG\u0019\u000e\u0016ItS\u0002\n\u0006G\t,\u0016\u0017B4\u000b \u0016IZ?\u0014\u001b\u000f\u0010\u000b\u000e\r,\u001c\u001e\"@0\u0006\b)+#\t\b\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014>#.*\u0007\t,\u0016\u0017\u0014;\u0019&\u000f\u001b\r/\u000f\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001cp\u000f\u001d\u001c\u0017B4)+\u0006,\u0016\u0017\r\b%\u001a#-\t\n9:<\u0002\f\u0014\u001b\u000f\u0010*\u001a\t\b\u0002>\r\b%\u0007\u00023#C\u000f\n\b)4\u0006,\u0016\u0017\r,\"-)\u001b0F#.*\u0007\t,\u0016\u0017\u0014;\u0019&\u000f\u001b\r/\u000f1\u000f\n84\u000f\u001d\u0016I\u001c\u001e\u000f\u001b\u001f \u001c\u0017\u0002.)+\u000bC\r\b%\u001a\u0002J\u0016\u0017\u000b\u000e\r\b\u0002\n\u0006\b\u000b\u001a\u0002\n\r\u000f\u001b\u0006\b\u0002G\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\u0002\n\u00191\u0016\u0017\u000b.)+\u000b\u001a\u0002G)\u001b0&\r\b%\u001a\u00023\u000f\u0010\u0014\f)+*\u001a\t\b\r,\u0016\u0017\u0014q0)+\u0006\b#C\u000f\u001b\r\b\t\nl\u0010\u0016\u0017\rM\u0016\u0017\tM\u0016\u0017#-\u0004?)+\u0006\b\r/\u000f\u0010\u000b\u000e\r\r\b)O%?\u000f\n8+\u0002'\t\b\u0002\u001d\u000f\u0010\u0006\b\u0014/%[\u000f\u001b\u000b\u0007\u0019\u0005\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c\u000f\u001d\u001c\u0017B+)4\u0006,\u0016\u0017\r\b%\u0007#H\t')+\u000b\u0005\t\b*\u001a\u0014\u0013%\u0005\u0019?\u000f\u0010\r/\u000f\u000e9YvM)\u001b5\r\b\u0002\n\u000b\u000e\r,\u0016\u001e\u000f\n\u001c.\u000f\u001b\u0004\u0007\u0004\u000e\u001cI\u0016\u0017\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000b\u0007\t'\u0016\u0017\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019\u0007\u0002\u0018\u000f\u0010*\u001a\r\b)+#\u0015\u000f\u0010\r,\u0016\u0017\u0014O#.*\u0007\t,\u0016\u0017\u0014'\u0016\u0017\u0019\u001a\u0002\n\u000b\u000e\r,\u0016IZ?\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000bLl#.*\u0007\t,\u0016\u0017\u0014.\u000f\u001b\u000b&\u000f\n\u001c\u0017\"\u000e\t,\u0016\u0017\t\nlS\u0004 \u001c\u001e\u000f\u001bB\u0010\u0016\u001e\u000f\u001b\u0006,\u0016\u0017\t\b#\u0019\u0007\u0002\f\r\b\u0002\n\u0014\n\r,\u0016\u0017)+\u000bSl&\u0014\f)+\u0004\u000e\"\u000e\u0006,\u0016\u0017B+%\u000e\r;\u0002\n\u000b\u000e02)+\u0006\b\u0014\n\u0002\f#-\u0002\n\u000b\u000e\r\nl\u0002\n\r\b\u001449\r\b\u0006\b*\u000e\u001c\u0017\"E\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u0019^#.*\u001a\t,\u0016\u0017\u0014Y\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c.\t\b\"\u000e\t\b\r\b\u0002\f#\\\t\b%\u001a)+*\u000e\u001c\u0017\u0019\u0000\u000e*\u0007\u0004\u001a\u0004&)4\u0006\b\r\b\u0002\n\u0019E\u001f\u000e\"\u0005\u000f\u0003L\u0002\n)4\u000b&\u000f\u001b\u0006\b\u0019^\u000e9p %\u000e*\u001a\t\b\r\b\u0002\nU!\u0007\u0002\u0013\u001cI\u001c\u0017)\u001bD3\t\b%\u000e\u0016\u0017\u0004Ll;\u0004&\u000f\u001b\u0006\b\r-)\u00100\r\b%\u001a\u0002. \r/\u000f\u001b\u000b 0)+\u0006\b\u0019O=\u0006/\u000f\u001b\u0019\u0007*?\u000f\u0010\r\b\u0002.\u001a\u0002\u0013\u001cI\u001c\u0017)\u001bDJ\t\b%\u000e\u0016\u0017\u0004\u0003\u0004\u0007\u0006\b)4B+\u0006/\u000f\u0010#ulS\u000f\u0010\u000b\u001a\u0019O;\u0007\u0005=\u0006/\u000f\u001b\u000b\u000e\r}\b}5+4+4++\u000e9\nPermission tomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroom useisgrantedwithoutfeeprovided\nthatcopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.\n\u00142002IRCAM-CentrePompidou\n%?\u000f\n8+\u0002;\r\b%\u0007\u0002\u000f\u0010\u001f\u000e\u0016I\u001cI\u0016\u0017\r,\"'\r\b)=Z?\u000b\u001a\u0019'\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006J\t\b)4\u000b\u0007B4\t3\u001f&\u000f\u001b\t\b\u0002\n\u0019C)+\u000bC\r\b%\u0007\u0002/\u0016\u0017\u0006G*\u0007\u000b\u001a\u0019\u001a\u0002\n\u0006,\u001c\u0017\"+5\u0016\u0017\u000b\u001aB-\t\b\u0014\n)4\u0006\b\u00023)+\u0006G#-\u0002/\u001c\u0017)\u000e\u0019\u0007\"+l\u001a\u0006\b\u0002\nB\u000f\u0010\u0006\b\u0019\u000e\u001c\u0017\u0002\n\t\b\t3)\u00100\r\b%\u001a\u0002\u0013\u0016\u0017\u0006G#-\u0002\n\r/\u000f\u001b\u0019&\u000f\u001b\r/\u000f\u0019\u001a\u0002\n\t\b\u0014\n\u0006,\u0016\u0017\u0004\u001a\r,\u0016\u0017)+\u000b)4\u0006GZ\u0007\u001c\u0017\u0002.\u000b&\u000f\u001b#-\u0002\n\t\n9|')4\t\b\r=#.*\u0007\t,\u0016\u0017\u0014H\u0004 \u0016\u0017\u0002\n\u0014\f\u0002\n\t=%?\u000f\n8+\u0002-#.* \u001c\u0017\r,\u0016\u0017\u0004\u000e\u001c\u0017\u0002C\u000b\u0007)4\r\b\u0002\n\t=)\u000e\u0014\f\u0014\n*\u0007\u0006\b\u0006,\u0016\u0017\u000b\u001aB'\t,\u0016\u0017#.*\u000e\u001c\u0017\r/\u000f\u0010\u000b\u001a\u0002\u00135)4*\u0007\t,\u001c\u0017\"4l\u000f\t\b\u0014\n\u0002\f\u000b&\u000f\u001b\u0006,\u0016\u0017)@U\u000e\u000b\u001a)\u001bD3\u000bO\u000f\u0010\ts&+ \u0017¡\b?_&\u000ek&¡\u00109nvM)\u0010\u001c\u0017\"\u000e\u0004\u001a%\u0007)4\u000b\u000e\"'\u0016\u0017\t3%&\u000f\u001b\u000b\u0007\u0019\u000e\u001c\u0017\u0002\n\u0019\u0016\u0017\u000b!\t\b\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014\u0003\u0006\b\u0002\n\u0004\u0007\u0006\b\u0002\f\t\b\u0002\n\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)4\u000b\u0007\tO\u001f\u000e\"\u0005\u000f\n\u001cI\u001c\u0017)\u001bDq\u0016\u0017\u000b\u001aB[\r\b%\u001a\u0002'\u000b\u001a)+\r\b\u0002\f\t\nx<\r,\u0016\u0017#.\u0016\u0017\u000b\u0007B4\t\r\b).)\u001b8+\u0002\n\u0006,\u001c\u001e\u000f\u001b\u0004L9p},\u000bu\u000f\u0010\u0014\f)+*\u001a\t\b\r,\u0016\u0017\u00141\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r/\u000f\u001b\r,\u0016\u0017)+\u000b\u001a\t\nlS\u0004&)\u0010\u001c\u0017\"\u000e\u0004\u001a%\u001a)+\u000b\u000e\"O\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r\b\tG\u0016\u0017\u000b\t,\u0016\u0017B4\u000b&\u000f\n\u001c\u0017\t3\r\b%&\u000f\u001b\rG\u000f\u001b\u0006\b\u0002G\r\b%\u0007\u00023\t\b*\u001a#)\u00100S\t,\u0016\u0017B4\u000b&\u000f\n\u001c\u0017\tG\r\b%&\u000f\u001b\r<Dp)+*\u000e\u001c\u0017\u0019@%?\u000f\n8+\u00023\u001f&\u0002\n\u0002\f\u000b-B+\u0002\n\u000b\u000e5\u0002\f\u0006/\u000f\u0010\r\b\u0002\n\u0019-\u001f\u000e\"=\u0016\u0017\u000b\u001a\u0019\u000e\u0016\u00178+\u0016\u0017\u0019&*&\u000f\n\u001cF\u000b\u001a)+\r\b\u0002\n\t\n9¢\u0001w\u0016\u0017\r\b%u\u000f\u0010\u0014\n\u0014\f\u0002\n\t\b\tF)+\u000b\u000e\u001c\u0017\"-\r\b)=\r\b%\u001a\u0002<Z?\u000b&\u000f\n\u001cS\t\b*\u001a#)\u001b03\t,\u0016\u0017B+\u000b?\u000f\u001d\u001c\u0017\t\nls\u000b\u001a)+\r=\u0016\u0017\u000b\u001a\u0019 \u0016\u00178+\u0016\u0017\u0019?*&\u000f\n\u001c>\u0014\n)4#-\u0004&)+\u000b\u001a\u0002\n\u000b\u000e\r\b\t\nlM\u0016\u0017\r=\u0016\u0017\t.8+\u0002\f\u0006\b\"O\u0019 \u0016I\u0012C\u0014\f* \u001c\u0017\r-\r\b)\u0006\b\u0002\f\u0014\n)+\u000b\u001a\t\b\r\b\u0006\b*\u0007\u0014\f\rJ\r\b%\u001a\u0002J\u0016\u0017\u000b\u001a\u0019\u000e\u0016\u00178+\u0016\u0017\u0019&*&\u000f\n\u001cs\u000b\u0007)4\r\b\u0002\n\tG02\u0006\b)+#r\u0004&)\u0010\u001c\u0017\"\u000e\u0004\u001a%\u001a)+\u000b\u000e\u0016\u0017\u0014C\u000f\u0010*\u001a\u0019 \u0016\u0017)\u001a93\u0007)4\u0006\u0019\u001a\u0002\n\u0014\u001b\u000f\u001b\u0019\u0007\u0002\f\t-\u0004&\u0002\n)4\u0004 \u001c\u0017\u0002u%&\u000f\n8+\u0002C\u001f&\u0002\f\u0002\n\u000bY\r\b\u0006\b\"+\u0016\u0017\u000b\u001aB7\r\b)u\u0019\u0007\u0002\n\t,\u0016\u0017B4\u000b!\u000f\u0010*\u001a\r\b)+#C\u000f\u001b\r,\u0016\u0017\u0014'\r\b\u0006/\u000f\u0010\u000b\u000e5\t\b\u0014\f\u0006,\u0016\u0017\u0004\u0007\r,\u0016\u0017)4\u000b'\t\b\"\u000e\t\b\r\b\u0002\n#-\tp\r\b%&\u000f\u001b\r<\u0002\n(\u000e\r\b\u0006/\u000f\u001b\u0014\n\rs#1*\u001a\t,\u0016\u0017\u0014\u001b\u000f\u001d\u001c\t\b\u0014\n)4\u0006\b\u0002\n\tF02\u0006\b)+#\u0006/\u000f\nD!\u000f\u0010*\u001a\u0019 \u0016\u0017)\u0006\b\u0002\f\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u0007B4\t\nl&\u001f\u001a*\u0007\rs%?\u000f\u001d84\u00023)+\u000b\u000e\u001c\u0017\"-\t\b*\u0007\u0014\f\u0014\n\u0002\n\u0002\n\u0019\u001a\u0002\n\u0019-\u0016\u0017\u000bC#-)+\u000b\u001a)+\u0004\u001a%\u0007)4\u000b \u0016\u0017\u0014.\u000f\u001b\u000b\u0007\u0019C84\u0002\n\u0006\b\"\t,\u0016\u0017#H\u0004 \u001c\u0017\u0002u\u0004&)\u0010\u001c\u0017\"\u000e\u0004\u001a%\u0007)4\u000b \u0016\u001e\u0014O\u0014\u001b\u000f\u001b\t\b\u0002\n\t@£ ¤ l¥\u0007lp¦\n¥\u0010§6lF\u000b\u001a)+\r=\u0016\u0017\u000bY\r\b%\u001a\u0002CB+\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\n\u001c3\u0004&)\u0010\u001c\u0017\"+5\u0004\u001a%\u001a)+\u000b\u000e\u0016\u0017\u00147\u0014\u001b\u000f\u001b\t\b\u0002+9!},\u000b\u0005)+*\u001a\u0006-DG)4\u0006\bU&lGDG\u0002u\u0019\u0007\u0002\u001b\u000f\n\u001c;Dq\u0016\u0017\r\b%!\u0004&)\u0010\u001c\u0017\"\u000e\u0004\u001a%\u001a)+\u000b\u000e\u0016\u0017\u0014Y#1*\u001a\t,\u0016\u0017\u0014\u0019?\u000f\u0010\r/\u000f-DG\u0016\u0017\r\b%\u001a)+*\u001a\r>\u000f\u0010\r\b\r\b\u0002\f#-\u0004\u0007\r,\u0016\u0017\u000b\u001aBC\r\b\u0006/\u000f\u0010\u000b\u001a\t\b\u0014\n\u0006,\u0016\u0017\u0004\u001a\r,\u0016\u0017)+\u000bS9\n2.BACKGROUND|'*\u001a\u0014/%@)\u001b0\u0004\u0007\u0006\b\u0002\n8+\u0016\u0017)4*\u0007\tpDG)+\u0006\bU.)+\u000bC\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u0019C#1*\u001a\t,\u0016\u0017\u0014=\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001cS06\u000f\u001d\u001cI\u001c\u0017\t\u0016\u0017\u000b\u000e\r\b)H\r,DG).\u0014\u001b\u000f\u0010\r\b\u0002\fB+)+\u0006,\u0016\u0017\u0002\f\t\n¨G\t\b\"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014.A\u000e*\u0007\u0002\f\u0006\b\"-)+\u000bN\u000f\t\b\"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014.\u0019?\u000f\u0010\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u00024l)4\u00063#-)+\u000b\u001a)+\u0004\u001a%\u0007)4\u000b \u0016\u0017\u0014u\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u00141A\u000e*\u001a\u0002\n\u0006\b\"u)+\u000bO\u000f.\t\b\"\u000e#1\u001f&)\u001b\u001cI\u0016\u0017\u0014-\u0019?\u000f\u0010\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u000249©\n\u000e\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014-A\u000e*\u001a\u0002\n\u0006\b\"')4\u000bO\u000f1\t\b\"\u000e#.\u001f&)\u001b\u001cI\u0016\u0017\u0014-\u0019&\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002+¨q:<)+\r\b%u\r\b%\u0007\u0002>A\u000e*\u001a\u0002\n\u0006\b\"\u000f\u001b\u000b\u0007\u0019W\r\b%\u0007\u0002\u0018*\u0007\u000b\u001a\u0019\u0007\u0002\f\u0006,\u001c\u0017\"+\u0016\u0017\u000b\u0007B\u0011\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002!\u000f\u0010\u0006\b\u0002Y\u0016\u0017\u000b^\t\b\"\u000e#.\u001f&)\u001b\u001cI\u0016\u0017\u0014w02)4\u0006\b#C\u000f\u0010\r\b\t\nl\t\b*\u001a\u0014\u0013%H\u000f\u0010\tp|'}\b~3}F\u000f\u001b\u000b\u0007\u0019HJ*\u001a#-\u0019\u001a\u0006\b*\u0007#'9poG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001c&\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n#-\tG)4\u000b1\t\b*\u001a\u0014/%\t\b\"\u000e#.\u001f&)\u0010\u001cI\u0016\u0017\u0014>\u0019?\u000f\u0010\r/\u000f>\u0014\u001b\u000f\u001b\u000b-\r,\"\u000e\u0004\u000e\u0016\u0017\u0014\u001b\u000f\u001d\u001cI\u001c\u0017\"u\u001f&\u0002>\u000f\u001b\u0019\u0007\u0019\u001a\u0006\b\u0002\n\t\b\t\b\u0002\n\u0019C\u001f\u000e\"-#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\tG\u0019\u001a\u0002\u00135\u0006,\u0016\u001784\u0002\n\u00190\u0006\b)+#\r\b\u0002\n(\u000e\ru\t\b\u0002\u001b\u000f\u0010\u0006\b\u0014/% \u0016\u0017\u000b\u001aBE\r\b\u0002\n\u0014/%\u0007\u000b\u000e\u0016\u0017A\u000e*\u001a\u0002\n\t\n9ª\u000e\u0002\n8+\u0002\n\u0006/\u000f\n\u001c.\t\b\"\u000e\t\b\r\b\u0002\n#-\t)\u001b0G\r\b%\u000e\u0016\u0017\t>\r,\"\u000e\u0004&\u0002H%&\u000f\n8+\u0002C\u001f&\u0002\f\u0002\n\u000bV\u0016\u0017#H\u0004 \u001c\u0017\u0002\n#H\u0002\n\u000b\u000e\r\b\u0002\n\u0019Sl«\u0016\u0017\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019 \u0016\u0017\u000b\u001aBY\r\b%\u001a\u00021m3%\u001a\u0002\u00135#-\u0002/Z&\u000b\u001a\u0019\u0007\u0002\f\u0006G\u0004\u0007\u0006\b)\u001b\b\u0002\n\u0014\f\r=yh¬\u001a­\u000e­+®p¯\b°\u000e°+±\u000e±+±s²,­\u001b¬?³\u001d´?³+µ\u0007¶\n·¸\u001a³+¹F²\bº+¹4»\u001a{\u0013l\nD3%\u0007\u0002\f\u0006\b\u0002\r\b%\u001a\u0002O\t\b\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014Y\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002Y\u0014\u001b\u000f\u0010\u000b!\u001f&\u0002O\t\b\u0002\u001b\u000f\u001b\u0006\b\u0014\u0013%\u001a\u0002\n\u0019E*\u001a\t,\u0016\u0017\u000b\u0007B!\u0004 \u0016\u0017\r\b\u0014/%E\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\t\nl&\u0016\u0017\u000b\u000e\r\b\u0002\n\u0006\b84\u000f\u001d\u001c\u0017\t\nl\u000f\u001b\u0004\u0007\u0004\u001a\u0006\b)\u001b(+\u0016\u0017#C\u000f\u0010\r\b\u0002H\u0014\n)+\u000b\u000e\r\b)4*\u0007\u0006\b\t\nlS\u0002\n\r\b\u001449©\n|')4\u000b\u0007)4\u0004\u0007%\u001a)+\u000b\u000e\u0016\u0017\u0014>\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u00143A\u000e*\u0007\u0002\n\u0006\b\".)+\u000bH\u000fJ\t\b\"\u000e#.\u001f&)\u001b\u001cI\u0016\u0017\u0014=\u0019?\u000f\u0010\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u00024¨Fm3% \u0016\u0017\t\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n#'l\u0015\u000f\u001d\u001c\u0017\t\b)EU\u000e\u000b\u001a)\u0010D3\u000b¼\u000f\u0010\tE½pj\u0007d\u001ba/¡[¾\u001b¡Y¿>j\u001aÀ1À.ehk\u000eÁ7)4\u0006E½<ÂG¿;l%?\u000f\u0010\t.\u0006\b\u0002\n\u0014\n\u0002\u0013\u0016\u001784\u0002\n\u0019\u0005\u0014\n)+\u000b\u001a\t,\u0016\u0017\u0019\u001a\u0002\n\u0006/\u000f\u0010\u001f\u000e\u001c\u0017\u0002Y\u000f\u001b\r\b\r\b\u0002\n\u000b\u000e\r,\u0016\u0017)+\u000b\u0005\u0019\u001a*\u0007\u0006,\u0016\u0017\u000b\u001aB\u0003\r\b%\u0007\u0002C\u0004?\u000f\u0010\t\b\r102\u0002\fD\"4\u0002\u001b\u000f\u0010\u0006\b\t\u0003£I\u000el>\u000el>Ã\u000el>\u000el>¦+¦+lH¦\u001b¤\u001d§69},\u000bE\t\b*\u001a\u0014/%E\t\b\"\u000e\t\b\r\b\u0002\n#-\t\nl>\r\b%\u001a\u0002V\u0016\u0017\u000b\u001a\u0004\u001a*\u0007\rA\u000e*\u001a\u0002\n\u0006\b\"@\u0016\u0017\t>\r6\"\u000e\u0004 \u0016\u0017\u0014\u001d\u000f\u001d\u001cI\u001c\u0017\"\u0005\u000f1*\u001a\t\b\u0002\n\u0006,56%\u000e*\u001a#-#-\u0002\n\u0019Y#H\u0002\u0013\u001c\u0017)\u000e\u0019\u0007\"O\r\b%\u001a\u0006\b)+*\u001aB+%\u0018\u000f-#1\u0016I5\u0014\n\u0006\b)4\u0004\u0007%\u001a)+\u000b\u001a\u0002+l\u000f\u0010\u000b\u001a\u00191\r\b%\u001a\u0002s#-\u0002\u0013\u001c\u0017)\u000e\u0019\u001a\"=\u0016\u0017\tF\u000f\u0010\u000b?\u000f\u001d\u001c\u0017\"\u000e\f\u0002\n\u0019C\u000f\u0010\u000b\u001a\u0019>#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\u0019H\u000f\u0010B\u000e\u000f\n\u0016\u0017\u000b\u0007\t\b\r\u000fC\t\b\"\u000e#1\u001f&)\u001b\u001cI\u0016\u0017\u0014'\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002+9uJ*\u001a#C\u000f\u0010\u000b\u000e56%\u000e*\u0007#H#-\u0002\n\u0019\u0005\r\b*\u0007\u000b\u001a\u0002\n\t.\u000f\u001b\u0006\b\u0002@#H)+\u000b\u001a)\u00105\u0004\u001a%\u0007)4\u000b \u0016\u0017\u0014\u0005#-\u0002/\u001c\u0017)\u000e\u0019 \u0016\u0017\u0002\n\t\u0018\u000f\u0010\u000b\u001a\u0019W\u0014\u001b\u000f\u0010\u000bE\u001f&\u0002Ä\u000f\u0010*\u001a\r\b)+#C\u000f\u001b\r,\u0016\u0017\u0014\u001b\u000f\u001d\u001cI\u001c\u0017\"\u0011\r\b\u0006/\u000f\u001b\u000b\u0007\t\b\u0014\f\u0006,\u0016\u0017\u001f&\u0002\n\u0019\u0016\u0017\u000b\u000e\r\b)u\u0004 \u0016\u0017\r\b\u0014/%\u0007\u0002\f\t1DG\u0016\u0017\r\b%\u0003\u0006\b\u0002\u001b\u000f\u0010\t\b)4\u000b&\u000f\u001b\u001f \u001c\u0017\u0002N\u000f\u0010\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\u0014\f\"+9-}6\u000bO)+\u0006\b\u0019\u001a\u0002\n\u00061\r\b)C\u0014\n)4#15\u0004&\u0002\f\u000b\u0007\t/\u000f\u001b\r\b\u0002<02)+\u0006S\u0004&)4\t\b\t,\u0016\u0017\u001f \u001c\u0017\u0002G\u0016\u0017\u000b?\u000f\u0010\u0014\n\u0014\f*\u0007\u0006/\u000f\u001b\u0014\u0013\u0016\u0017\u0002\n\tG)\u001b0\u001a%\u000e*\u0007#C\u000f\u001b\u000b 56%\u000e*\u001a#-#-\u0002\n\u0019.\r\b*\u001a\u000b\u0007\u0002\n\t\nl\t\b)4#-\u00021\t\b\"\u000e\t\b\r\b\u0002\f#-\t=*\u001a\t\b\u0002.\u000f\u0010\u0004\u001a\u0004\u0007\u0006\b)\u001b(+\u0016\u0017#C\u000f\u001b\r\b\u0002C\u0014\n)+\u000b\u000e\r\b)4*\u0007\u00063\u0016\u0017\u000b 02)4\u0006\b#C\u000f\u0010\r,\u0016\u0017)4\u000b[y2*\u001a\u0004Ll\u0019\u001a)\u001bDJ\u000bSlF\u0002\n\r\b\u0014\u001d{J)+\u0006.\u001f&\u0002\u001b\u000f\u001b\r=\u0016\u0017\u000b 0)+\u0006\b#C\u000f\u001b\r,\u0016\u0017)+\u000b\u0005\r\b)\u0003\u000f\u001d\u0016\u0017\u0019Y\r\b%\u001a\u0002-\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c3\u0004\u001a\u0006\b)\u00105\u0014\n\u0002\f\t\b\t\n9:s)+\r\b%-)\u001b0S\r\b%\u0007\u0002\f\t\b\u0002J\u0014\u001d\u000f\u0010\r\b\u0002\nB4)+\u0006,\u0016\u0017\u0002\n\tG\u0019\u001a\u0002\u001b\u000f\n\u001cSDG\u0016\u0017\r\b%-\t\b\"\u000e#1\u001f?)\u0010\u001cI\u0016\u0017\u0014=#.*\u001a\t,\u0016\u0017\u0014=\u0019?\u000f\u0010\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002\f\t\n93\u0002\nD[#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\tG\u000b\u001a\u0002\n\u0002\n\u0019-\r\b)>\u001f?\u0002=\u0019\u001a\u0002\n8+\u0002\u0013\u001c\u0017)4\u0004&\u0002\n\u0019u\r\b)>%&\u000f\u001b\u000b\u0007\u0019\u000e\u001c\u0017\u0002.\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014;#1*\u001a\t,\u0016\u0017\u0014\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002\n\t\n9;T=*\u001a\u0006=\u0006\b\u0002\n\t\b\u0002\u001b\u000f\u001b\u0006\b\u0014\u0013%'0)\u000e\u0014\n*\u0007\t\b\u0002\f\tJ)4\u000bO\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"7\u0006\b\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cF02\u0006\b)4#\u000f-\u0004&)\u001b\u001c\u0017\"\u000e\u0004\u0007%\u001a)+\u000b\u000e\u0016\u0017\u0014O\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014-\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u00021\u0016\u0017\u000bO\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\t\b\u0002-\r\b)u\u000f-\u0004&)\u001b\u001c\u0017\"\u000e\u0004\u0007%\u001a)+\u000b\u000e\u0016\u0017\u0014\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014YA\u000e*\u001a\u0002\n\u0006\b\"+9Åoq\u0002/\u001c\u001e\u000f\u0010\r\b\u0002\n\u0019WDG)+\u0006\bUY\u0016\u0017\u000bE\r\b%\u000e\u0016\u0017\tO\u000f\u001b\u0006\b\u0002\u001b\u000fY\u0014\u001b\u000f\u0010\u000b!\u001f&\u00027\u0014\u001d\u000f\u0010\r\b\u0002\nB4)\u00105\u0006,\u0016\u0017\f\u0002\n\u0019'\u001f?\u000f\u0010\t\b\u0002\n\u0019u)+\u000bC\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r3\u0019\u001a\u0002\u0013Z&\u000b\u000e\u0016\u0017\r,\u0016\u0017)+\u000b\u001a\t>)\u00100qP\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u001e\r,\"+l RCD3% \u0016\u0017\u0014/%O\u000f\u001b\u0006\b\u0002\u0019\u000e\u0016\u0017\t\b\u0014\n*\u001a\t\b\t\b\u0002\n\u0019\u0003\u000b\u0007\u0002\n(\u000e\r\n9TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental StudyÆ2Ç ÈÊÉ\u001aË,ÌgÍ\bÎÊÏSÇ Ì Ð&Ì ËÎÊÏSÍÑQÒÌgÈ\fÓÔ\nÕ,Ö\f×hØÙ«Ú\nÛ Ü\u001aÝgØÞ Ü6ßàá/Ù«Ú\nÝgÜ6×h×â áQãÔ\nÕ,Ö\fã\fá/â ×hÜÔ\nÕ,Ö\fâ ã\f×Þ Ýgä\u001dÙFÜ6ã\fÞ ×hßå/á/à2Ø,ÛÔ\nÕ,Ö\fÞ Ü6ÙFÚ\u001dáQßã\fáQÞ Ü\u001aÜ6Ù«Ú\næ\fØ×hâ ×Ô\nÕ,Ö\fÚ\u001dâ Þ à2æGç Þ ÝgØã\f×hÚ\u001dáQ×hâ Þ â áQã\fèÔ\nÕ,Ö\fÙ«Ü6Û á/é\u001dâ àå\u0013ØÝgâ Ø,Þ â áQãêÎÊë@È?Ð&ÈÊÉ ì\u0013È\níâ ã\fî/Ü6ÝgÚ\u001dÝgâ ã\fÞ â ã\fî×hÚ\nÜ6à2Þ ÝgØÛQâ ã\fé\u001dÜ6ï\bâ ã\fîî/Ü6ã\fÝgÜ\u0007àÛ Ø×h×hâ\níâ à2Ø,Þ â á/ã\nÔ\nÕ,Ö\u000eð ð ðñGòÊó?ôFõ\u0010ö\u0003÷&ø.ùCòúpö\u000eõ\u001bö\u000eûMü.ýFö+þsûFò2ü\nò2ÿ&û\u0001\u0000>ÿ\u0003\u0002\u0005\u00047ô\u0001\u0000\fò\u0007\u0006\b\u0000\nò\t\u0004Yò\t\n\t\u000b\u001aõ\u0010ò2ü\r\f\n2.1MusicSimilarity De®nitionsm3%\u001a\u0002\n\u0006\b\u0002-\u0016\u0017\t-\u000b\u0007)O\u0014\f)+\u000b\u001a\t\b\u0002\n\u000b\u0007\t\b*\u001a\t-)4\u000bY\r\b%\u0007\u0002C\u0002\n(\u001a\u000f\u001b\u0014\n\r-#-\u0002\u001d\u000f\u0010\u000b\u000e\u0016\u0017\u000b\u0007B\u0003)\u00100;#.*\u0007\t,\u0016\u0017\u0014C\t,\u0016\u0017#15\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"+lH\u000f\u0010\u000b\u001a\u0019[\u0016\u0017\r'\u0014\u001d\u000f\u0010\u000b[\u001f?\u00027\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019W)\u001b8+\u0002\n\u0006N\u000f7\u001f\u001a\u0006\b)\u000e\u000f\u0010\u0019E\u0014\f)+\u000b\u000e\r,\u0016\u0017\u000b\u000e*\u001a*\u0007#'lH\u000f\u001b\t\u0016\u0017\u000bWL\u0016\u0017B4*\u0007\u0006\b\u0002\u0005¦+9¼T=\u000b[)4\u000b\u0007\u0002O\u0002\f(\u000e\r\b\u0006\b\u0002\n#-\u0002+l;DG\u0002'\u0014\u001d\u000f\u0010\u000b[\u0006\b\u0002\fB\u000e\u000f\u0010\u0006\b\u0019E)4\u000b \u001c\u0017\"\u0005\u0016\u0017\u0019\u0007\u0002\f\u000b\u000e\r,\u0016I5\u0014\u001b\u000f\n\u001c\u0019 \u0016\u0017B\u001b\u0016\u0017\r/\u000f\u001d\u001cs\u0014\n)4\u0004 \u0016\u0017\u0002\n\t;)\u00100s#.*\u0007\t,\u0016\u0017\u0014.\u000f\u001b\tHPQ\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006\nl RN\u000f\u0010\u000b\u001a\u0019O\u000f\u0010\u000b\u000e\"\u000e\r\b%\u000e\u0016\u0017\u000b\u001aB@\u0002/\u001c\u0017\t\b\u0002-\u000f\u001b\t\u0019\u000e\u0016\u0017\t\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f+\u0006\n93\u0001w\u0016\u0017\r\b%O\r\b%\u000e\u0016\u0017\t=\u0019\u001a\u0002\u0013Z?\u000b \u0016\u0017\r,\u0016\u0017)4\u000bLl\r\b%\u001a\u0002>#1*\u001a\t,\u0016\u0017\u00141\u0006\b\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cF\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n#\u0006\b\u0002\n\u0019\u001a*\u001a\u0014\n\u0002\n\tC\r\b)\u0005\u000fOB+\u0002\n\u000b\u001a\u0002\n\u0006,\u0016\u0017\u0014\u0003\u0019&\u000f\u001b\r/\u000fV\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c>\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n#'l;D3% \u0016\u0017\u0014/%[\u0014\u001b\u000f\u001b\u000b[\u001f&\u0002\u000f\u001b\u0014\u0013%\u000e\u0016\u0017\u0002\n8+\u0002\f\u0019[\r\b%\u001a\u0006\b)+*\u001aB+%\u0005\u0006\b\u0002\nB+*\u000e\u001c\u001e\u000f\u0010\u0006H%&\u000f\u001b\t\b% \u0016\u0017\u000b\u001aB\u0005\u000f\u0010\u000b\u001a\u0019Y\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u001aBY\r\b\u0002\n\u0014/%\u0007\u000b\u000e\u0016\u0017A\u000e*\u001a\u0002\n\t\n93\u001c\u0017\r\b\u0002\n\u0006\b\u000b&\u000f\u001b\r,\u0016\u00178+\u0002\u0013\u001c\u0017\"4l&DG\u0002s\u0014\u001b\u000f\u0010\u000b.\r\b)\u0010\u001c\u0017\u0002\f\u0006/\u000f\u0010\r\b\u00023\t\b)+#-\u0002p\u0019 \u0016\u0017\t\b\r\b)4\u0006\b\r,\u0016\u0017)+\u000b\u001a\tG\u0019\u0007*\u001a\u0002G\r\b);\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\u0006/\u000f\u001b\r\b\u0002@\u0014/%?\u000f\u0010\u000b\u001aB+\u0002+ls\u0014\n)4#-\u0004\u0007\u0006\b\u0002\f\t\b\t,\u0016\u0017)+\u000bSlG)+\u0006.\u000b\u001a)\u0010\u0016\u0017\t\b\u0002+lp\u000f\u0010\u000b\u001a\u0019Y\t\b\r,\u0016I\u001cI\u001c;\u0006\b\u0002\nB\u000f\u0010\u0006\b\u0019Y\r\b%\u001a\u0002C\u0006\b\u0002\u00135\t\b*\u000e\u001c\u0017\r\b\t;\u000f\u0010\t3\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006\n9;T=\u0006\nlDG\u0002=\u0014\u001b\u000f\u001b\u000b'\r\b)\u001b\u001c\u0017\u0002\n\u0006/\u000f\u0010\r\b\u0002>\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\tJ\u0016\u0017\u000bC\u0016\u0017\u000b\u0007\t\b\r\b\u0006\b*\u001a#-\u0002\n\u000b\u000e\r\b\t\nl84)\u000e\u0014\u001b\u000f\u001d\u001cs\u0004?\u000f\u0010\u0006\b\r\b\t3)+\u00063\r\b\u0002\n#H\u0004&)\u0007l\u000f\u001b\u000b\u0007\u0019u\t\b\r,\u0016I\u001cI\u001cs\u0006\b\u0002\nB\u000e\u000f\u001b\u0006\b\u0019O\r\b%\u0007\u0002>\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r\b\t.\u000f\u0010\t3\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006\nl\u000f\u001b\t=\u001c\u0017)+\u000b\u001aB\u0003\u000f\u0010\t>\r\b%\u001a\u0002@#.*\u001a\t,\u0016\u0017\u0014\u001b\u000f\u001d\u001cPQ\t\b\u0014\n)4\u0006\b\u0002\nRO\u0006\b\u0002\n#C\u000f\n\u0016\u0017\u000b\u0007\t.*\u001a\u000b\u0007\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\u0019S9OT=\u000b\u0003\r\b%\u0007\u0002)4\r\b%\u0007\u0002\n\u0006p\u0002\n(\u000e\r\b\u0006\b\u0002\n#-\u0002+lDG\u00023\u0014\u001b\u000f\u0010\u000b-\r\b)4\r/\u000f\u001d\u001cI\u001c\u0017\"C\u0019\u000e\u0016\u0017\t\b\u0006\b\u0002\nB\u000e\u000f\u001b\u0006\b\u0019'\r\b%\u001a\u0002=\t\b\u0014\n)4\u0006\b\u0002+l\u001a\u000f\u0010\u000b\u001a\u0019C\u0019\u0007\u0002/Z&\u000b\u001a\u0002\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u0005\u000f\u001b\tCP\bDq\u0016\u0017\r\b%\u000e\u0016\u0017\u000b\u0003\r\b%\u0007\u0002.\t/\u000f\u0010#-\u0002.B+\u0002\f\u000b\u0007\u0006\b\u0002\nRC53\u000f-\t\b)4#-\u0002\nD3%&\u000f\u001b\r=\t\b*\u001a\u001f\u000e\b\u0002\n\u0014\u00135\r,\u0016\u001784\u0002O\u000b\u0007)4\r,\u0016\u0017)+\u000bS9^m3%\u001a\u0002'\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u001a\u0014\n\u00027\u0016\u0017\u000b!\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"E\u0019\u0007\u0002/Z&\u000b\u000e\u0016\u0017\r,\u0016\u0017)+\u000b\u001a\tO\u001f\u0007\u0006,\u0016\u0017\u000b\u001aB+\t\u000f\u001b\u001f&)+*\u001a\r=\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r>\u0019\u000e\u0016\u0017\u0006\b\u0002\n\u0014\n\r,\u0016\u0017)4\u000b\u0007\t=\u0016\u0017\u000bO#.*\u001a\t,\u0016\u0017\u00141\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001cF\u0006\b\u0002\n\t\b\u0002\u001d\u000f\u0010\u0006\b\u0014/%L9P/L\u0016\u0017\u000b\u0007B4\u0002\n\u0006\b\u0004\u0007\u0006,\u0016\u0017\u000b\u000e\r,\u0016\u001e\u000b\u001aB+R\u0005\r\b\u0002\n\u0014/%\u0007\u000b\u000e\u0016\u0017A\u000e*\u0007\u0002\f\t\u00150)\u000e\u0014\n*\u0007\t.)+\u000bYZ?\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u001aBY#1*\u001a\t,\u0016\u0017\u0014'\u0006\b\u0002\f\u0014\n)+\u0006\b\u0019\u000e5\u0016\u0017\u000b\u001aB+\t;\r\b%&\u000f\u001b\r=\u000f\u001b\u0006\b\u0002>\u000f\n\u001c\u0017#-)+\t\b\r3\u0016\u0017\u0019\u0007\u0002\n\u000b\u000e\r,\u0016\u0017\u0014\u001d\u000f\u001d\u001csD3% \u0016I\u001c\u0017\u0002.\r\b)\u0010\u001c\u0017\u0002\f\u0006/\u000f\u0010\r,\u0016\u0017\u000b\u001aB@\t\b#\u0015\u000f\u001d\u001cI\u001cs\u000f\u001b#-)+*\u001a\u000b\u000e\r)\u001b0G\u000b\u001a)\u0010\u0016\u0017\t\b\u0002C\u0019\u000e\u0016\u0017\t\b\r\b)+\u0006\b\r,\u0016\u0017)+\u000b\u001a\t'£\u0017¦\u0013§69>m3%\u0007\u0002.\u0014\n\u0002\n\u000b\u000e\r\b\u0006/\u000f\n\u001c<\u0016\u0017\u0019\u001a\u0002\u001b\u000f-\u0016\u0017\t>\r\b)'\u0002\n(\u000e\r\b\u0006/\u000f\u001b\u0014\n\r>\t\b)+#-\u0002\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r/\u000f\u001b\r,\u0016\u00178+\u0002¼PQ\u0019\u000e\u0016\u0017B\u0010\u0016\u0017\r/\u000f\n\u001cC\t,\u0016\u0017B+\u000b?\u000f\u0010\r\b*\u001a\u0006\b\u0002\n\t\bRE02\u0006\b)+# \r\b%\u001a\u0002\u0005\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014\u0018\u0019&\u000f\u001b\r/\u000fD3%\u000e\u0016\u0017\u0014\u0013%-\u0014\u001d\u000f\u0010\u000b.\u001f&\u0002G%?\u000f\u0010\t\b%\u001a\u0002\n\u0019C\u000f\u0010\u000b\u001a\u0019.\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n8+\u0002\f\u0019-\u0002\u0013\u0012C\u0014\u0013\u0016\u0017\u0002\f\u000b\u000e\r,\u001c\u0017\"+93\u000e\u0002\n8+\u0002\f\u0006/\u000f\u001d\u001c?\u0004\u0007\u0006\b)4\u0004\u0007\u0006,\u0016I5\u0002\n\r/\u000f\u001b\u0006\b\"1\t\b\"\u000e\t\b\r\b\u0002\f#-\tG%?\u000f\u001d84\u00023\u001f&\u0002\n\u0002\n\u000b-\u0019\u001a\u0002\n84\u0002\u0013\u001c\u0017)+\u0004&\u0002\f\u0019O\u000f\u0010\rs\u0014\n)4#-\u0004&\u000f\u001b\u000b \u0016\u0017\u0002\f\tJ\t\b*\u001a\u0014/%C\u000f\u0010\tpoG\u0002\u00135\u001c\u001e\u000f\u001b\r/\u000f\u0010\u001f\u000e\u001c\u0017\u00021yh¬\u001a­\u000e­+®p¯\b°\u000e°+±\u000e±\u000e±F²,¹³\u000f\u000e\u000f\u0010\u0010­\u0011\u0010\u0013\u0012\u0014\u000e\u000e³M²\u0016\u0015+º\u0010´?{\u0013l/D3%\u000e\u0016\u0017\u0014\u0013%1\u0016\u0017\tFDp)+\u0006\bU+\u0016\u0017\u000b\u001aB>Dq\u0016\u0017\r\b%;\u000f\u0010\u0004\u001a\t\b\r\b\u0002\n\u00061)4\u000b7\u0016\u0017#-\u0004\u000e\u001c\u0017\u0002\n#-\u0002\n\u000b\u000e\r,\u0016\u0017\u000b\u001aB\u0005#1*\u001a\t,\u0016\u0017\u0014@Z\u001a\u001c\u0017\u0002-Z\u0007\u001c\u0017\r\b\u0002\f\u0006\b\t-\r\b)'\u0002\n\u000b\u000e02)4\u0006\b\u0014\n\u0002'\u0014\n)4\u0004\u000e\"+5\u0006,\u0016\u0017B4%\u000e\r<\u0004\u001a\u0006\b)+\r\b\u0002\n\u0014\f\r,\u0016\u0017)+\u000bSl&\u000f\u001b\u000b\u0007\u0019\u0018\u0017\u001a\u00193~^yh¬\u001a­\u000e­+®p¯\b°\u000e°+±+±\u000e±s²\u001c\u001b+­\u000f\u0010+¹\u0011\u0015\u0010¸s²\u0016\u00154º\u0010´\u001a{\u0013l\fDJ%\u001a)+\t\b\u0002#.*\u0007\t,\u0016\u0017\u00143\u0016\u0017\u0019\u0007\u0002\f\u000b\u000e\r,\u0016IZ&\u0014\u001b\u000f\u001b\r,\u0016h)4\u000bO\t\b\"\u000e\t\b\r\b\u0002\n#\r\b\u0006/\u000f\u001b\u0014\u0013U\u000e\tp\t\b)+\u000b\u001aB+\tG\u0004\u000e\u001c\u001e\u000f\n\"+\u0002\n\u0019u)+\u000b-\u0006/\u000f\u001b\u0019 \u0016\u0017)-\t\b\r/\u000f\n5\r,\u0016\u0017)4\u000b\u0007\t\u000f\u0010\u000b\u001a\u0019O\u0004\u0007\u0006\b)\u001b8+\u0016\u0017\u0019\u001a\u0002\n\t>*\u0007\t\b\u0002\f\u0006\b\tJDG\u0016\u0017\r\b%'\u0016\u0017\u0019\u001a\u0002\n\u000b\u000e\r,\u0016IZ?\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000bY\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r\b\t\n9=\u0002\n\u000b\u001a\u0006\b\u0002[\u0014/\u001c\u001e\u000f\u0010\t\b\t,\u0016IZ?\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000b \r\b\u0002\n\u0014\u0013%\u001a\u000b\u000e\u0016\u0017A\u000e*\u0007\u0002\n\tY02)\u000e\u0014\f*\u0007\t\u0003)+\u000b\u0011\u0014/\u001c\u001e\u000f\u0010\t\b\t,\u0016I02\"+\u0016\u0017\u000b\u001aB#1*\u001a\t,\u0016\u0017\u0014\u0019?\u000f\u0010\r/\u000fOy2\t\b)+#-\u0002\n\r,\u0016\u0017#H\u0002\n\t1\t\b\u0004?\u0002\n\u0002\n\u0014/%7\u0019?\u000f\u0010\r/\u000f')4\u0006=)+\r\b%\u001a\u0002\n\u0006=02)4\u0006\b#-\t>)\u00100q\u000f\u0010*\u001a\u0019\u000e\u0016\u0017)7\u0019?\u000f\u0010\r/\u000f4{\u001f?\u000f\u0010\t\b\u0002\n\u0019!)+\u000b\u0005%\u000e\u0016\u0017B+%\u000e5Ê\u001c\u0017\u0002\f8+\u0002\u0013\u001c.\u0004\u001a\u0006\b)+\u0004&\u0002\f\u0006\b\r,\u0016\u0017\u0002\n\tC\t\b*\u0007\u0014/%!\u000f\u0010\t-\u0002\n\u000b\u001a\u0002\n\u0006\bB4\"w\u0019\u000e\u0016\u0017\t\b\r\b\u0006,\u0016\u0017\u001f\u001a*\u0007\r,\u0016\u0017)4\u000bLl\r,\u0016\u0017#.\u001f\u0007\u0006\b\u0002>02\u0002\u001b\u000f\u001b\r\b*\u0007\u0006\b\u0002\f\t\nl&\u001f\u001a\u0006,\u0016\u0017B+%\u000e\r\b\u000b\u001a\u0002\n\t\b\t\nlS\r\b\u0002\n(\u000e\r\b*\u001a\u0006\b\u0002+lS\u0006\b%\u000e\"\u000e\r\b%\u0007#.\u0016\u0017\u00141\u0004?\u000f\u0010\r\b\r\b\u0002\n\u0006\b\u000b\u001a\t\nlS\u000f\u001b\u000b\u0007\u0019\t\b)!)+\u000b£g¦\u001d l-¦\u001d l-¦\u001d l-¦\u001dÃ\u001d§69 }6\u000bE#C\u000f\u0010\u000b\u000e\"E\u0014\u001d\u000f\u0010\t\b\u0002\n\t\nl.#C\u000f\u001b\u0014\u0013%\u000e\u0016\u0017\u000b\u0007\u0002Y\u001c\u0017\u0002\u001b\u000f\u001b\u0006\b\u000b \u0016\u0017\u000b\u001aB\r\b\u0002\n\u0014/%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002\n\t3\t\b*\u0007\u0014/%C\u000f\u001b\tG\u000f\u001b*\u0007\r\b)4#C\u000f\u0010\r,\u0016\u0017\u0014;\u0014\u0013\u001c\u0017*\u0007\t\b\r\b\u0002\f\u0006,\u0016\u0017\u000b\u0007BH\u000f\u0010\u0006\b\u00023\u0002\n#-\u0004\u000e\u001c\u0017)\u001b\"+\u0002\n\u0019C\u0019\u001a*\u0007\u0006,\u0016\u0017\u000b\u001aB\r\b\u0006/\u000f\n\u0016\u0017\u000b \u0016\u0017\u000b\u001aB\u00079\u000e)+#-\u0002\fDJ%\u001a\u0002\n\u0006\b\u0002G\u001f?\u0002\n\r,DG\u0002\f\u0002\n\u000b=Z&\u000b\u001aB+\u0002\f\u0006\b\u0004\u0007\u0006,\u0016\u0017\u000b\u000e\r,\u0016\u0017\u000b\u001aB.\u000f\u0010\u0004\u001a\u0004\u0007\u0006\b)\u000f\u0010\u0014/%\u0007\u0002\f\tG\u000f\u001b\u000b\u0007\u0019>B4\u0002\n\u000b\u0007\u0006\b\u0002s\u0014\u0013\u001c\u001e\u000f\u001b\t,5\t,\u0016IZ?\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000b\u000f\u0010\u0004\u001a\u0004\u001a\u0006\b)\u000e\u000f\u0010\u0014/%\u001a\u0002\n\t\nl.\r\b%\u0007\u0002\n\u0006\b\u0002O\u0016\u0017\tO\u000fO\u001c\u001e\u000f\u0010\u0006\bB+\u0002Ä\u000f\u0010\u0006\b\u0002\u001b\u000fYD3% \u0016\u0017\u0014/%W\u0006\b\u0002\n#C\u000f\u001d\u0016\u0017\u000b\u001a\t#-)4\t\b\r,\u001c\u0017\"7*\u001a\u000b\u001a\u0002\n(\u000e\u0004 \u001c\u0017)4\u0006\b\u0002\n\u0019S9CmJ%\u001a\u0002-\u0014\n)4\u0006\b\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u0007B\u0018\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"Y\u0019\u0007\u0002/Z&\u000b\u000e\u0016\u0017\r,\u0016\u0017)+\u000b\u0016\u0017\u000b\u000e84)\u0010\u001c\u00178+\u0002\f\t=\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"@\u0016\u0017\u000bu#1*\u001a\t,\u0016\u0017\u0014\u001b\u000f\n\u001cM\t\b\u0014\f)+\u0006\b\u0002\n\t\nl\u001a\u0006\b\u0002\nB\u000f\u0010\u0006\b\u0019\u000e\u001c\u0017\u0002\n\t\b\t3)\u00100\r\b\u0002\n#-\u0004&)\u001al\u000e\u0016\u0017\u000b 5\t\b\r\b\u0006\b*\u001a#-\u0002\n\u000b\u000e\r/\u000f\u001b\r,\u0016\u0017)+\u000bY)4\u0006=\u0004&\u0002\n\u0006,0)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u0002H\t\b\r,\"+\u001c\u0017\u0002+9;m3% \u0016\u0017\t>\u0019\u001a\u0002\u0013Z?\u000b \u0016\u0017\r,\u0016\u0017)4\u000bY\u0006\b\u0002\u001e\u001d?\u0002\n\u0014\n\r\b\t\u000f\u001b\u000bÄ\u0016\u0017\u000b\u000e\r\b*\u000e\u0016\u0017\r,\u0016\u00178+\u0002O\u000b\u001a)+\r,\u0016\u0017)4\u000b[)\u001b0CP\b\t/\u000f\u0010#-\u0002C\t\b)4\u000b\u0007B4RY\u000f\u001b\t-\u0004&\u0002\n\u0006\b\u0014\n\u0002/\u0016\u00178+\u0002\n\u0019!\u001f\u000e\"7%\u000e*\u001a#C\u000f\u0010\u000b\u001cI\u0016\u0017\t\b\r\b\u0002\n\u000b\u001a\u0002\n\u0006\b\t\n93\u0001\u0003\u0002=02)\u000e\u0014\n*\u001a\t3)+\u000bO\r\b%\u000e\u0016\u0017\t=\u0019\u001a\u0002\u0013Z&\u000b\u000e\u0016\u0017\r,\u0016\u0017)+\u000b\u0003%\u001a\u0002\n\u0006\b\u0002+92.2ProblemStatement}6\u000b 0)+\u0006\b#C\u000f\n\u001cI\u001c\u0017\"+l=)4*\u0007\u0006C\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n# \u0014\u001b\u000f\u001b\u000b[\u001f&\u0002O\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\f\u0019W\u000f\u0010\t\n¨OB\u001b\u0016\u00178+\u0002\n\u000bW\u000f7A\u000e*\u001a\u0002\n\u0006\b\"#.*\u001a\t,\u0016\u0017\u0014-\u0014\u0013\u001cI\u0016\u0017\u0004O\u0016\u0017\u000bO\u0006/\u000f\u001dDÅ\u000f\u0010*\u001a\u0019 \u0016\u0017)C02)+\u0006\b#C\u000f\u001b\r\nl\u0007Z?\u000b\u0007\u0019\u0003\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u00061\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\t302\u0006\b)+#ª\r\b%\u001a\u0002\u000f\u001b*\u0007\u0019\u000e\u0016\u0017)[\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002+l;DJ%\u001a\u0002\n\u0006\b\u0002O\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"E\u0016\u0017\t'\u001f?\u000f\u0010\t\b\u0002\n\u0019E)4\u000b[\r\b%\u001a\u0002'\u0016\u0017\u000b\u000e\r\b* \u0016\u0017\r,\u0016\u001784\u0002\u000b\u001a)+\r,\u0016\u0017)4\u000b\u0011)\u00100H\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u0011\u0004&\u0002\f\u0006\b\u0014\n\u0002\u0013\u0016\u00178+\u0002\f\u0019^\u001f\u000e\"E%\u000e*\u001a#C\u000f\u0010\u000b\u001a\t\n¨!\r,DG)\u0005\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\t\u0003\u000f\u001b\u0006\b\u0002\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006J\u0016I0¢\r\b%\u0007\u0002\f\"C\u000f\u0010\u0006\b\u0002302* \u001cI\u001c\u0017\"C)4\u0006G\u0004&\u000f\u001b\u0006\b\r,\u0016\u001e\u000f\u001d\u001cI\u001c\u0017\"u\u001f&\u000f\u001b\t\b\u0002\n\u0019')4\u000b@\r\b%\u001a\u0002=\t/\u000f\u001b#-\u0002=\t\b\u0014\f)+\u0006\b\u0002+l\u0002\f8+\u0002\n\u000bC\u0016I0F\r\b%\u001a\u0002\n\"C\u000f\u0010\u0006\b\u0002;\u0004&\u0002\n\u0006,02)4\u0006\b#-\u0002\n\u0019u\u001f\u000e\"-\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\r3\u0004&\u0002\n)4\u0004 \u001c\u0017\u0002.)+\u00063\u000f\u001b\r<\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u000e\r\r\b\u0002\f#-\u0004&)\u00079qoG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cs\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r\b\t=\t\b%\u001a)+*\u000e\u001c\u0017\u0019\u0003\u001f&\u0002.\u000f\u001cI\u0016\u0017\t\b\r>)\u00100s\t\b)4\u000b\u0007B4\t=\u0006/\u000f\u001b\u000b\u0007U4\u0002\n\u0019O\u001f\u000e\"\u0014\f)+#-\u0004\u001a*\u0007\r\b\u0002\f\u0019O\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"7\u0002\n\t\b\r,\u0016\u0017#\u0015\u000f\u0010\r\b\u0002+9\u0001\u0003\u0002G\u0016\u0017\u0019\u0007\u0002\f\u000b\u000e\r,\u0016I02\"1Z?8+\u00023\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rG\r6\"\u000e\u0004&\u0002\n\tG)\u001b0pPQ\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006\bR-#.*\u0007\t,\u0016\u0017\u0014;\u0004&\u000f\n\u0016\u0017\u0006\b\t\nl\u000eDG\u0016\u0017\r\b%\u0016\u0017\u000b\u001a\u0014\n\u0006\b\u0002\u001b\u000f\u001b\t,\u0016\u0017\u000b\u0007B'\u001c\u0017\u0002\f8+\u0002\u0013\u001c\u0017\t;)\u00100s\u0019\u000e\u0016I\u0012C\u0014\n*\u000e\u001c\u0017\r,\"&¨©\nmG\"\u000e\u0004?\u0002>}\u0013¨\u001a},\u0019\u001a\u0002\n\u000b\u000e\r,\u0016\u0017\u0014\u001b\u000f\n\u001cs\u0019 \u0016\u0017B\u001b\u0016\u0017\r/\u000f\u001d\u001cp\u0014\n)+\u0004\u000e\"©\nmG\"\u000e\u0004?\u0002G}\b}\u0013¨4\u001a\u000f\u0010#-\u0002;\u000f\u001b\u000b&\u000f\n\u001c\u0017)+B.\t\b)+*\u001a\u0006\b\u0014\n\u0002+l\u000e\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\rs\u0019\u000e\u0016\u0017B\u0010\u0016\u0017\r/\u000f\u001d\u001c\u0014\n)4\u0004 \u0016\u0017\u0002\f\t\nl\u0007\u0004?)+\t,5\t,\u0016\u0017\u001f\u000e\u001c\u0017\"ODG\u0016\u0017\r\b%'\u000b\u001a)\u0010\u0016\u0017\t\b\u0002©\nmG\"\u000e\u0004?\u0002G}\b}\b}\u0013¨\u0007\u000f\u001b#-\u0002<\u0016\u0017\u000b\u001a\t\b\r\b\u0006\b*\u0007#H\u0002\n\u000b\u000e\r/\u000f\u001d\u001c\u0004&\u0002\n\u0006,0)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u00024l\u000e\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rG84)\u000e\u0014\u001b\u000f\u001d\u001c\u0014\n)4#-\u0004&)4\u000b\u0007\u0002\n\u000b\u000e\r\b\t©\nmG\"\u000e\u0004?\u0002J}\u001c\u001f1¨\u001a\u001a\u000f\u0010#H\u0002J\t\b\u0014\f)+\u0006\b\u0002+l\u001a\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r3\u0004&\u0002\n\u0006,0)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u0002\f\t=y2\u0004&)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\"O\u000f\u001b\r\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r;\r\b\u0002\n#-\u0004&){©\nmG\"\u000e\u0004?\u0002 \u001f.¨;\u0007\u000f\u001b#-\u00027*\u001a\u000b\u001a\u0019\u0007\u0002\n\u0006,\u001c\u0017\"+\u0016\u0017\u000b\u001aB\u0011#-\u0002/\u001c\u0017)\u000e\u0019\u0007\"+l\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\r7)4\r\b%\u0007\u0002\f\u0006\bDq\u0016\u0017\t\b\u00024lDG\u0016\u0017\r\b%'\u0004?)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\u0002@\r\b\u0006/\u000f\u001b\u000b\u0007\t\b\u0004?)+\t,\u0016\u0017\r,\u0016\u0017)+\u000bm3%\u001a\u0002GZ&\u0006\b\t\b\rG\r6DG)=\r6\"\u000e\u0004&\u0002\n\tG)\u001b84\u0002\n\u0006,\u001c\u001e\u000f\u0010\u0004uDq\u0016\u0017\r\b%HDJ%?\u000f\u0010\rp\u0002\n(+\u0016\u0017\t\b\r,\u0016\u0017\u000b\u0007B.Z&\u000b\u001aB+\u0002\n\u0006\b\u0004\u001a\u0006,\u0016\u0017\u000b\u000e\r,\u0016\u0017\u000b\u001aB\r\b\u0002\f\u0014\u0013%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002\n\t>\u0014\u001b\u000f\u001b\u000bO%&\u000f\u001b\u000b\u0007\u0019\u000e\u001c\u0017\u0002+9;T=*\u001a\u00063)+\u001f\u000e\b\u0002\n\u0014\n\r,\u0016\u001784\u00021\u0016\u0017\t3\r\b)-%?\u000f\u0010\u000b\u001a\u0019 \u001c\u0017\u0002-\r\b%\u001a\u0002.)+\r\b%\u001a\u0002\n\u0006\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"\u0003\r,\"\u000e\u0004?\u0002\n\t>\u000f\u001b\tJDp\u0002\u0013\u001cI\u001cs\u000f\u001b\tJ\r\b%\u001a\u0002=Z?\u0006\b\t\b\rJ\r6DG)\u00079\u001a)\u000e)+\r\b\u0002\u0005£I\n§.\u0002\n(\u000e\u0004&\u0002\n\u0006,\u0016\u0017#-\u0002\f\u000b\u000e\r\b\u0002\n\u0019\u0011DG\u0016\u0017\r\b%E\r\b% \u0016\u0017\tOU+\u0016\u0017\u000b\u001a\u0019W)\u00100H#1*\u001a\t,\u0016\u0017\u0014Y\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u0019\u001a\u0002\n\r\b\u0002\n\u0014\f\r,\u0016\u0017)+\u000bE\u001f\u000e\"Y#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aBY\u0004&)\u001bDG\u0002\n\u0006C\u000f\u001b\u000b\u0007\u0019!\t\b\u0004&\u0002\n\u0014\n\r\b\u0006\b)4B+\u0006/\u000f\u001b# 84\u000f\u001d\u001c\u0017*\u001a\u0002\n\t')\u001b84\u0002\n\u0006\r,\u0016\u0017#H\u0002@*\u001a\t,\u0016\u0017\u000b\u001aB\u0005\u000f@\u0019\u001a\"\u000e\u000b&\u000f\u001b#1\u0016\u0017\u0014O\u0004\u001a\u0006\b)+B+\u0006/\u000f\u001b#-#1\u0016\u0017\u000b\u001aBY#-\u0002\n\r\b%\u001a)\u000e\u0019S9O3\u0002@\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\f\u0019[\u000f\u0014\f)+\t\b\r-#-)\u000e\u0019\u001a\u0002\u0013\u001c302)4\u00061#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aBY\r,DG)O\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\f\t@\u0004?)\u0010\u0016\u0017\u000b\u000e\r,56\u001f\u000e\"+56\u0004&)\u0010\u0016\u0017\u000b\u000e\r\nl;DG\u0016\u0017\r\b%[\u000f\u0004?\u0002\n\u000b&\u000f\n\u001c\u0017\r,\"N\u000f\u0010\u0019\u001a\u0019\u0007\u0002\n\u0019C02)+\u0006p\u000b\u0007)4\u000b 56#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u0007BC\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t\n93S)\u0010Dp\u0002\n\u0006<\u0014\f)+\t\b\rG#-\u0002\u001d\u000f\u0010\u000b\u001a\t=\u000f\u0014/\u001c\u0017)+\t\b\u0002\n\u0006.#C\u000f\u0010\r\b\u0014/%'\u0016\u0017\u000bY\r\b%\u001a\u00021\u0006\b\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cp\u0006\b\u0002\n\t\b* \u001c\u0017\r\n9HmF\u0002\n\t\b\r=\u0006\b\u0002\f\t\b* \u001c\u0017\r\b\t>)4\u000bY\u000f-\t\b#C\u000f\n\u001cI\u001c\r\b\u0002\f\t\b\rM\u0014\n)4\u0006\b\u0004\u0007*\u001a\tM\u0016\u0017\u000b\u0007\u0019\u000e\u0016\u0017\u0014\u001b\u000f\u001b\r\b\u0002\n\u0019C\r\b%&\u000f\u001b\rM\r\b%\u001a\u0002<#-\u0002\f\r\b%\u0007)\u000e\u0019>\u0016\u0017\tM02\u0002\u001b\u000f\u001b\t,\u0016\u0017\u001f \u001c\u0017\u0002G02)4\u0006M\u0019\u001a\u0002\n\r\b\u0002\n\u0014\n\r,\u0016\u0017\u000b\u001aB\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"\u0003\u0016\u0017\u000b7)4\u0006\b\u0014\u0013%\u001a\u0002\n\t\b\r\b\u0006/\u000f\u001d\u001cs#.*\u001a\t,\u0016\u0017\u0014+9=}6\u000bO)+*\u001a\u0006=\u0004\u001a\u0006\b\u0002\n8+\u0016\u0017)+*\u001a\t.DG)+\u0006\bU\u0003£g¦\u001d\u001d§FDp\u0002\u000f\n\u001c\u0017\t\b)O\u0002\n#-\u0004\u000e\u001c\u0017)\u001b\"+\u0002\n\u0019!\u000f'\u0019\u001a\"\u000e\u000b&\u000f\u001b#1\u0016\u0017\u0014'\u0004\u001a\u0006\b)+B4\u0006/\u000f\u0010#-#1\u0016\u0017\u000b\u001aB\u0003#C\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB\u0018\u000f\u0010\u0004\u001a\u0004\u0007\u0006\b)\u000f\u0010\u0014/%\u001f?\u000f\u0010\t\b\u0002\f\u0019')+\u000bN\u000f\u0014\f)+\t\b\rG#-)\u000e\u0019\u001a\u0002\u0013\u001c2l?\u001f\u0007*\u001a\r3\u0004\u0007\u0006\b\u0002\n\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t\b\u0002\f\u0019'\r\b%\u001a\u0002>\t,\u0016\u0017B+\u000b?\u000f\u001d\u001c\u0017\t=\r\b)>\u0016\u0017\u0019\u001a\u0002\n\u000b 5\r,\u0016I0\"7\u0004&\u0002\u001d\u000f\u0010U\u000e\tC\u000f\u001b\u000b\u0007\u0019\u0005)+\u000b\u000e\u001c\u0017\"Y#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\u0019Y\t\b\u0004?\u0002\n\u0014\n\r\b\u0006\b)+B4\u0006/\u000f\u0010#-\tH\u000b\u0007\u0002\u001b\u000f\u001b\u00061\r\b%\u001a\u0002C\u0004&\u0002\u001b\u000f\u001bU\u000e\t\n9\u001a*\u0007\u0006\b\r\b%\u001a\u0002\n\u0006\b#-)4\u0006\b\u0002+l&DG\u0002;*\u0007\t\b\u0002\f\u0019'\t\b)+#H\u0002=\u001cI\u0016\u0017\u000b\u0007\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\"'Z\u001a\u001c\u0017\r\b\u0002\n\u0006,\u0016\u0017\u000b\u001aB'\u0014\n\u0006,\u0016\u0017\r\b\u0002\n\u0006,\u0016\u001e\u000f-\r\b).\u0019 \u0016\u0017\t,5\r,\u0016\u0017\u000b\u001aB+*\u000e\u0016\u0017\t\b%[\u001f?\u0002\n\r,DG\u0002\f\u0002\n\u000bY\u000f'B4)\u000e)\u000e\u0019Y#C\u000f\u001b\r\b\u0014\u0013%\u0018\u000f\u0010\u000b\u001a\u0019\u0005\u000f'\u001f?\u000f\u0010\u0019\u0003#C\u000f\u0010\r\b\u0014/%L9u:s)+\r\b%Y)\u001b0\r\b%\u001a\u0002\n\t\b\u0002\u0018\u000f\u0010\u0004\u001a\u0004\u0007\u0006\b)\u000f\u0010\u0014/%\u0007\u0002\f\tV\u001c\u001e\u000f\u001b\u0014\u0013UE\t\b\u0014\u001b\u000f\n\u001c\u001e\u000f\u0010\u001f\u000e\u0016I\u001cI\u0016\u0017\r,\"+l\u0015\u000f\u0010\u000b\u001a\u0019E\u0004&\u0002\n\u0006,02)4\u0006\b#C\u000f\u0010\u000b\u001a\u0014\n\u0002Y\u0019\u001a\u0002\n\r\b\u0002\n\u0006,\u0016I5)4\u0006/\u000f\u0010\r\b\u0002\n\tO\u0006/\u000f\u001b\u0004 \u0016\u0017\u0019\u000e\u001c\u0017\"\u0011D3%\u001a\u0002\n\u000bE\r\b%\u0007\u0002\u0018\u0019&\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002YB4\u0002\n\r\b\t'\u001c\u001e\u000f\u0010\u0006\bB4\u0002+9ªmF)[\u000f\u001b\u0019\u0007\u0019\u001a\u0006\b\u0002\n\t\b\t\r\b%\u000e\u0016\u0017\t@\u0016\u0017\t\b\t\b*\u001a\u0002+l3DG\u0002O\u0004\u001a\u0006\b)+\u0004?)+\t\b\u0002\n\u0019W\u000f7\u0004\u001a\u0006\b)+\r\b)4\r,\"\u000e\u0004&\u0002O\t\b\"\u000e\t\b\r\b\u0002\f#*\u0007\t,\u0016\u0017\u000b\u001aB[\t\b\u0004&\u0002\f\u0014\n\r\b\u0006/\u000f\u001d\u001c\u0016\u0017\u000b\u001a\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u0007BO£\u0017¦\u001b\n§,l\u001a\u000f\u001b\u000b\u0007\u0019-02*\u001a\u0006\b\r\b%\u001a\u0002\n\u0006<\u0019\u001a\u0002\n84\u0002\u0013\u001c\u0017)+\u0004&\u0002\f\u0019'\u0016\u0017\r<\u0016\u0017\u000b\u000e\r\b)-\u000f=\t\b\u0014\u001d\u000f\u001d\u001c\u001e\u000f\u0010\u001f\u000e\u001c\u0017\u0002=02\u0006/\u000f\u001b#-\u0002\u00135Dp)+\u0006\bUEU\u000e\u000b\u001a)\u0010D3\u000b¼\u000f\u0010\t\u0003|'\u0005\u00193 }Åy\u0016!Nj f\feh]\nb\t\">j\u0011#\u000eeh%$S_&`\u000ea,`\u000e]\u001bc2d\u001ba\u0013egf\fc2eh]'&Sd\nbi\u001bj\u001ad\u001bk&]\u0013d)(\nk*#\u000ed\r+\u000eehk\u000eÁ,&S¡\u0010f\fc2d\u001bÀ={M£ ¤+\u001d§69mJ%\u001a\u0002F02\u0006/\u000f\u0010#-\u0002\fDG)+\u0006\bU3\u0016\u0017\u000b\u000e8+)\u0010\u001c\u001784\u0002\n\tG#.* \u001c\u0017\r,\u0016I5\u0004\u000e\u001c\u0017\u0002>\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002-#-)\u000e\u0019\u001a* \u001c\u0017\u0002\n\t3D3%\u000e\u0016\u0017\u0014\u0013%C\u0014\u001b\u000f\u001b\u000b'\u001f&\u00023\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n\u0004&\u0002\n\u000b\u001a\u0019\u001a\u0002\n\u000b\u000e\r,\u001c\u0017\"O\u0006\b\u0002\n\u0019\u001a\u0002\n\t,\u0016\u0017B+\u000b\u001a\u0002\n\u0019\u000f\u001b\u000b\u0007\u0019\u0003\u0006\b\u0002\n\u0004\u000e\u001c\u001e\u000f\u0010\u0014\n\u0002\f\u0019L9C}6\u000bO\r\b% \u0016\u0017\t.\u0004&\u000f\u001b\u0004&\u0002\n\u0006\nlDG\u0002.\t\b\r\b*\u0007\u0019\u001a\"7\r\b%\u001a\u0002.\u0002\u0013tS\u0002\n\u0014\n\r\b\t>)\u001b03\t\b\u0002\n84\u0002\n\u0006/\u000f\u001d\u001c\u0019\u001a\u0002\n\t,\u0016\u0017B4\u000b7\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002\n\t3\u0016\u0017\u000b\u000e8+)\u0010\u001c\u00178+\u0016\u0017\u000b\u001aBO\t\b*\u001a\u0014\u0013%O#-)\u000e\u0019\u001a*\u000e\u001c\u0017\u0002\n\t\n9\n3.ACOUSTIC INDEXING FRAMEWORKL\u0016\u0017B4*\u0007\u0006\b\u0002\u0018¤Y\t\b%\u0007)\u001bD3\tC\r\b%\u0007\u0002O\u001f?\u000f\u0010\t,\u0016\u0017\u0014\u0018\t\b\r\b\u0006\b*\u0007\u0014\n\r\b*\u001a\u0006\b\u0002O)\u00100.)4*\u0007\u0006C\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(+56\u001f?\u000f\u0010\t\b\u0002\n\u0019W\u0006\b\u0002\u00135\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c3\t\b\"\u000e\t\b\r\b\u0002\n#'l-!\u0018\"\u0018$)&\u0011(\u001e&&9S},\r>\u0014\f)+\u000b\u001a\t,\u0016\u0017\t\b\r\b\t-)\u001003\r\b%\u001a\u0006\b\u0002\n\u0002C\u0004\u001a%&\u000f\u001b\t\b\u0002\n\t\nlsDJ%\u000e\u0016\u0017\u0014/%\u000f\u001b\u0006\b\u0002>\t\b*\u0007#H#C\u000f\u0010\u0006,\u0016\u0017\n\u0002\f\u00197\u001f&\u0002/\u001c\u0017)\u0010D>¨©\nvF%&\u000f\u001b\t\b\u0002O¦Oy2\u0004\u0007\u0006\b\u0002\n\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t,\u0016\u0017\u000b\u001aBY\u0004\u001a%&\u000f\u001b\t\b\u0002\u001b{\u0013¨Co3\u000f\nD \u000f\u001b*\u0007\u0019\u000e\u0016\u0017)O\u0016\u0017\t1\u0014\n)4\u000b\u000e8+\u0002\n\u0006\b\r\b\u0002\f\u0019\u0016\u0017\u000b\u000e\r\b)\u0015P\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n(\u001a\u000f\u001b\u001f \u001c\u0017\u0002\fR1\u0016\u0017\r\b\u0002\n#-\tFU\u000e\u000b\u001a)\u001bDJ\u000b\u0015\u000f\u0010\t3]/_&`\u000ea,`\u000e]\u001bc2d\u001ba\u0013egf\fc2eg]Ff\fd\u0013i\u001bj\u001ad\u0010k?]\u0013d\nf\blD3%\u000e\u0016\u0017\u0014\u0013%C\u0014\u001b\u000f\u001b\u000b@\u001f?\u0002=8+\u0016\u0017\u0002\nDG\u0002\f\u0019'\u000f\u001b\t<\u0004?)\u0010\u0016\u0017\u000b\u000e\r\b\tG\u0016\u0017\u000bu\u000f%\u000e\u0016\u0017B+%\u000e56\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)\u000e\u000b?\u000f\u001d\u001cs84\u0002\n\u0014\u00135\r\b)4\u0006-\t\b\u0004&\u000f\u001b\u0014\n\u0002ODG\u0016\u0017\r\b%!\u000f\u0010\u000bE\u000f\u001b\u0004\u0007\u0004\u001a\u0006\b)+\u0004\u001a\u0006,\u0016\u001e\u000f\u0010\r\b\u0002\u0003\u0019\u000e\u0016\u0017\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u00027#H\u0002\u001b\u000f\u0010\t\b*\u001a\u0006\b\u0002+9!Xs\u000f\u0010\u0014/%\u0014/%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\f\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014=\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002=\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\tp\u000f=\t\b%\u001a)+\u0006\b\rs\t\b\u0002\nB+#-\u0002\f\u000b\u000e\r<)\u001b0S#1*\u000e5\t,\u0016\u0017\u0014;\u0019&\u000f\u001b\r/\u000f 9s:<)4\r\b%C\r\b%\u0007\u0002;\u000f\u0010*\u001a\u0019\u000e\u0016\u0017)-\u0019&\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002\u000f\u0010\u000b\u001a\u0019@\r\b%\u001a\u00023A\u000e*\u0007\u0002\f\u0006\b\"C\u000f\u0010\u0006\b\u00023\u0004\u001a\u0006\b)\u00105\u0014\n\u0002\f\t\b\t\b\u0002\n\u0019w\u0016\u0017\u000bE\r\b%\u000e\u0016\u0017\tCD3\u000f\n\"+9Em3% \u0016\u0017\tu\u0004\u0007%?\u000f\u0010\t\b\u0002O\u0014\u001d\u000f\u0010\u000b[\u001f?\u0002'02*\u0007\u0006\b\r\b%\u001a\u0002\n\u0006C\u0019\u000e\u0016\u00178+\u0016\u0017\u0019\u0007\u0002\n\u0019\u0016\u0017\u000b\u000e\r\b)H\u000f\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002.)\u00100F#.*\u000e\u001c\u0017\r,\u0016\u0017\u0004 \u001c\u0017\u0002.\t\b\r\b\u0002\n\u0004\u001a\t\n¨M02\u0006\b)+#r\u0006/\u000f\nDE\u000f\u0010*\u001a\u0019\u000e\u0016\u0017)-\r\b)1\t\b\u0004?\u0002\n\u0014\u00135\r\b\u0006\b*\u001a#'KS02\u0006\b)+#ª\t\b\u0004&\u0002\n\u0014\n\r\b\u0006\b*\u001a#\r\b)C\u0002\n84\u0002\n\u000b\u000e\r>8+\u0002\n\u0014\f\r\b)+\u0006\nKS02\u0006\b)+#ª\u0002\n8+\u0002\f\u000b\u000e\r>8+\u0002\n\u0014\n\r\b)4\u0006\r\b)-\u0014/%?\u000f\u0010\u0006/\u000f\u0010\u0014\f\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014-\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002+93Xs\u000f\u0010\u0014/%'\t\b\r\b\u0002\f\u0004'\u0016\u0017\u000b'\r\b%\u001a\u00021\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002C\u0014\u001b\u000f\u0010\u000bTheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study.0/ 1 2\u000f35476\u001e398\u001e.,3;:</ =?>\u001e1 >Audio Query Audio Database\nPreprocessing@\n:<AB:5C\u0016D\u000f1 EF>\u001e1 2\u000f3;:</G91 2\u000f3;:</IHKJ :53\u0016> L\tE<J\t4M:<8N1 E<3G;O?6\u001eP\u001c8QJ\tC94RKS\n6\u001e398\u001aT76?8 6\u001eP\u001c8Q1 E<3G\u00166\u001eU;C;6\u001e35PV6XW9E53\u0016>?8QJ\tC9P\u00078N1 E53\nRKS\n6\u001e358S\n6\u001eP\u001c8 E5JPhase 1W9Y?:5J :<P\u001c8 6?J\t1 >\u001e8N1 P5>\u00076\u001eU;C\u00166\u001e39PZ6\nstore lookup\nPhase 2Indexing / LookupW\u001aY?:<J :5P\u00078 6\u001eJ\t1 >\u001e8N1 P9>\t6\u001eU;C;6\u001e35PV6S\n6\u001eP\u00078 E5J[\\\n]^ _\n`\na b_\ncd\ne[\\\n]^ _\n`\na b_\ncd\nf[\n\\\n]^\n_\n`\nab_\nc\nd\ng[\\\n]^ _\n`\na b_\ncd\nhi?i?i\nmatching items\nRetrieval\nRetrieval Result\nj\n:58QPVY\u000f1 3\u00162k1 8 6\u001e4M>\u0003E<3)>\tY?E<J\t8\u001e>\t6\t2\u000f476?358 >2\u000f/ E<lm:</I47:58NPZY?6\t>Phase 3ñqò2ó&ôFõ\u001böonLøqpMü\nõ\u001bô\u0001\u0006\u000eü\nôFõ\u001böOÿ\u0003\u0002srut\bvwp\u0001x\u0013p\u001f&\u00023\u0016\u0017#-\u0004 \u001c\u0017\u0002\f#-\u0002\n\u000b\u000e\r\b\u0002\n\u0019'\u0016\u0017\u000bu\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\r3D3\u000f\n\"\u000e\t\nl \u001c\u0017\u0002\u001d\u000f\u0010\u0019\u000e\u0016\u0017\u000b\u0007BC\r\b).\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\r3\u0006\b\u0002\u00135\t\b*\u000e\u001c\u0017\r\b\t\n9©\nvF%&\u000f\u001b\t\b\u0002Y¤[yh\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n(+\u0016\u0017\u000b\u001aBWz7\u001c\u0017)\u000e)+U\u000e*\u001a\u0004^\u0004\u001a%?\u000f\u0010\t\b\u0002\u001b{\u0013¨y\u0019G%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014\u0018\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002\n\t102)4\u00061\r\b%\u001a\u0002u\u000f\u0010*\u001a\u0019 \u0016\u0017)Y\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u00027y2B4\u0002\n\u000b\u0007\u0002\f\u0006/\u000f\u0010\r\b\u0002\n\u0019!\u001f\u000e\"YvF%&\u000f\u001b\t\b\u00027¦\u001d{\u000f\u0010\u0006\b\u0002F\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n(\u000e\u0002\f\u00191\u0016\u0017\u000b-\u000fs% \u0016\u0017B4% 56\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016h)4\u000bS\u000f\u001d\u001c&\u0016\u0017\u000b\u001a\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u0007B.\t\b\u0014/%\u0007\u0002\n#H\u0002<U\u000e\u000b\u001a)\u001bDJ\u000b\u000f\u0010\t\bzS\u001b]/`+ \u0017ehc2¡\u0010b{&nd\u0010k\u001af/ehc2e}|\nd>¿>`+f_&ehk\u000eÁ+lS)+\u0006'z)&&¿£ \u001d§69Om3%\u000e\u0016\u0017\t1\t\b\u0014/%\u0007\u0002\f#-\u0002\u0006\b*\u001a\u000b\u0007\ts#C\u000f\u0010\u000b\u000e\".%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007B>\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\f\u0002\n\t<\u0016\u0017\u000b-\u0004?\u000f\u0010\u0006/\u000f\n\u001cI\u001c\u0017\u0002\u0013\u001c2K&\u0016\u0017\u000b-\u0002\u001b\u000f\u001b\u0014\u0013%1\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002+l\r\b%\u001a\u0002=8+\u0002\f\u0014\n\r\b)+\u0006<\u0016\u0017\tp%&\u000f\u001b\t\b%\u0007\u0002\n\u0019u\u001f\u000e\"C\u000f=02*\u0007\u000b\u001a\u0014\n\r,\u0016\u0017)4\u000bO\t\b)>\r\b%&\u000f\u001b\rG\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006;8+\u0002\n\u0014\f\r\b)+\u0006\b\t\u000f\u0010\u0006\b\u0002P\u001cI\u0016\u0017U4\u0002\u0013\u001c\u0017\"\u000eR.\r\b)=\u001f&\u0002s%&\u000f\u001b\t\b%\u0007\u0002\f\u0019-\r\b)3\r\b%\u0007\u0002G\t/\u000f\u001b#-\u0002G84\u000f\u001d\u001c\u0017*\u001a\u0002+l\u000eDG\u0016\u0017\r\b%.\u000f3\u0014\n\u0002\n\u0006\b\r/\u000f\u001d\u0016\u0017\u000b\u0004\u001a\u0006\b)+\u001f?\u000f\u0010\u001f\u000e\u0016I\u001cI\u0016\u0017\r,\"+9\u0018J\r>A\u000e*\u001a\u0002\n\u0006\b\"\u0003\r,\u0016\u0017#-\u0002+lF\u0014/%?\u000f\u0010\u0006/\u000f\u0010\u0014\f\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014O\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002\n\t102)4\u0006A\u000e*\u001a\u0002\n\u0006,\u0016\u0017\u0002\n\t.\u000f\u0010\u0006\b\u0002>\u001c\u0017)\u000e)+U4\u0002\n\u0019Y*\u001a\u0004'\u0016\u0017\u000b7\r\b%\u001a\u0002HM\u001a¼\u0016\u0017\u000b\u001a\u0019\u001a\u0002\n(7\r\b)HB+\u0002\n\r;#C\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB\u0016\u0017\r\b\u0002\n#-\t\n9©\nvF%&\u000f\u001b\t\b\u0002CVy2\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001cG\u0004\u001a%?\u000f\u0010\t\b\u0002\u001b{\u0013¨H|O\u000f\u0010\r\b\u0014/%\u001a\u0002\n\t>)+\u000b\u0003\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014u\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002\n\tY\u000f\u001b\u0006\b\u0002Y\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\u0019\r\b)+B4\u0002\n\r\b%\u001a\u0002\n\u0006O\r\b)wZ?\u000b\u0007\u0019 PQB\u0010\u001c\u0017)4\u001f&\u000f\n\u001c\u0017R\u0011#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\t\n9~J*\u001a\u0006,\u0016\u0017\u000b\u001aBO\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c2ls\u000fC\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014C\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002-02\u0006\b)+# \u000f-A\u000e*\u001a\u0002\n\u0006\b\"#C\u000f\n\"C#C\u000f\u0010\r\b\u0014/%'#\u0015\u000f\u0010\u000b\u000e\"C\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rJ\u0016\u0017\r\b\u0002\f#-\tJ\u0016\u0017\u000bu\r\b%\u0007\u0002>\u0019?\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002+K&\u0016\u0017\r3#C\u000f\n\"\u0002\n84\u0002\n\u000bC%&\u000f\n8+\u0002;#1*\u000e\u001c\u0017\r,\u0016\u0017\u0004\u000e\u001c\u0017\u00021#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\tGDG\u0016\u0017\r\b%\u000e\u0016\u0017\u000bC\r\b%\u0007\u0002;\t/\u000f\u0010#-\u0002;#1*\u001a\t,\u0016\u0017\u0014=\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002+l\t,\u0016\u0017\u000b\u001a\u0014\n\u0002'#C\u000f\u001b\u000b\u000e\"7#.*\u001a\t,\u0016\u0017\u0014'\u0004?\u000f\u0010\r\b\r\b\u0002\n\u0006\b\u000b\u001a\t-\r\b\u0002\n\u000b\u001a\u0019Y\r\b)O\u0006\b\u0002\n\u0004&\u0002\u001d\u000f\u0010\r>\u0016\u0017\r\b\t\b\u0002\u0013\u001cI0\b9\u0018vM)+\t\b\t,\u0016I5\u001f\u000e\u001c\u0017\u0002O\r\b\u0002\n#-\u0004&)Y\u0014/%?\u000f\u0010\u000b\u001aB+\u0002\n\tN\u000f\u0010\u0019\u001a\u0019[\r\b)Y\r\b%\u001a\u0002O\u0014\n)+#H\u0004 \u001c\u0017\u0002\n(+\u0016\u0017\r6\"[)\u001b0.\r\b%\u0007\u0002u\u0004\u0007\u0006\b)4\u001f 5\u001c\u0017\u0002\n#'9\u00017\u0002.#C\u000f\u0010U4\u0002-*\u001a\t\b\u00021)\u001b0G\r\b%\u001a\u0002106\u000f\u001b\u0014\n\r3\r\b%&\u000f\u001b\r>\r\b\u0002\n#-\u0004&)C\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\t1#.*\u001a\t\b\r\u001f&\u0002O*\u001a\u000b \u0016I0)+\u0006\b# \u0016\u0017\u000bE\r,\u0016\u0017#-\u0002+lq\u000f\u0010\u000b\u001a\u0019E\u0019 \u0016\u0017\t\b\u0014\f*\u0007\t\b\tu\r,DG)\u0003\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\r'#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\t\ny6&\u0016\u0017\u000b\u0007\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\"\u0005L\u0016I\u001c\u0017\r\b\u0002\f\u0006,\u0016\u0017\u000b\u0007B\u00038\u000e\t\n9sJ)4*\u0007B4%7mF\u0006/\u000f\u001b\u000b\u0007\t,0)+\u0006\b#C{=\r\b)O\u0019\u001a\u0002\n\r\b\u0002\n\u0006\b#1\u0016\u0017\u000b\u001a\u0002P\bB\u0010\u001c\u0017)+\u001f?\u000f\u001d\u001c\u0017RO#\u0015\u000f\u0010\r\b\u0014/%\u0007\u0002\n\t3\u001f?\u000f\u0010\t\b\u0002\n\u0019\u0003)+\u000bu\u0004&\u000f\u001b\u0006\b\r,\u0016\u001e\u000f\u001d\u001cp#C\u000f\u0010\r\b\u0014/%\u001a\u0002\n\t3)\u00100p\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,5\u0016\u0017\t\b\r,\u0016\u0017\u0014.\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002\n\t\n93\rFA\u000e*\u001a\u0002\n\u0006\b\"-\r,\u0016\u0017#-\u00024l\u000e\r\b%\u0007\u00023\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u0002\u0016\u0017\u000buvF%&\u000f\u001b\t\b\u0002>¦3\u0014\u001b\u000f\u0010\u000b-\r/\u000f\u001bU+\u00023\u0006\b\u0002\u001b\u000f\n\u001cI5,\r,\u0016\u0017#H\u0002>\u000f\u0010*\u000e5\u0019\u000e\u0016\u0017)C\u000f\u001b\t«\u0016\u0017\u000b\u001a\u0004\u0007*\u001a\r;\u000f\u0010\u000b\u001a\u0019C\u0004\u0007\u0006\b)\u000e\u0019\u001a*\u0007\u0014\f\u0002>\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014>\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\tG02)+\u0006F\u001c\u0017)\u000e)+U\u000e*\u001a\u0004\u000f\u001b\u000b\u0007\u0019O\u0006\b\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cF)4\u000b 56\r\b%\u001a\u0002\u00135\t\u001d&\"49Xs\u000f\u0010\u0014/%'\u0004\u001a%?\u000f\u0010\t\b\u0002>%?\u000f\u0010\t3\t\b\u0002\n8+\u0002\f\u0006/\u000f\u001d\u001cF\u0019\u001a\u0002\n\t,\u0016\u0017B4\u000bO\u0014\u0013%\u001a)\u0010\u0016\u0017\u0014\n\u0002\f\tJD3%\u000e\u0016\u0017\u0014\u0013%N\u000f\u001dtS\u0002\f\u0014\n\rGZ&\u000b?\u000f\u001d\u001c¢\u0004&\u0002\n\u0006,50)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u0002493},\u000bO\r\b%\u001a\u0002=02)\u0010\u001cI\u001c\u0017)\u001bDG\u0016\u0017\u000b\u0007BO\t\b\u0002\f\u0014\n\r,\u0016\u0017)+\u000b\u001a\t>DG\u0002>\u0019\u000e\u0016\u0017\t\b\u0014\n*\u0007\t\b\t>\t\b*\u001a\u0014/%O\u0014\u0013%\u001a)\u0010\u0016\u0017\u0014\n\u0002\f\t\n9\u001a)+\u0006;\u000f1\u0019\u001a\u0002\n\r/\u000f\u001d\u0016I\u001c\u0017\u0002\n\u0019\u0003\r\b\u0006\b\u0002\u001b\u000f\u001b\r\b#-\u0002\n\u000b\u000e\r3)\u00100p\u000f\n\u001c\u0017B+)+\u0006,\u0016\u0017\r\b%\u001a#-\t>\u0004\u000e\u001c\u0017\u0002\u001b\u000f\u0010\t\b\u0002H\u0006\b\u0002\u001302\u0002\n\u0006G\r\b)u£I¤4\u001d§69\n3.1Generation ofCharacteristic Sequences302\r\b\u0002\n\u0006G\u0019\u001a\u0002\n\u0014\n)4#-\u0004\u001a\u0006\b\u0002\n\t\b\t,\u0016\u0017)+\u000b\u0003\u000f\u001b\u000b\u0007\u0019u\u0004&\u000f\u001b\u0006\b\t,\u0016\u0017\u000b\u0007B\u001alS\u0002\u001b\u000f\u0010\u0014/%C\u0006/\u000f\u001dDW\u000f\u001b*\u0007\u0019\u000e\u0016\u0017)-Z\u0007\u001c\u0017\u0002>\u0014\u001d\u000f\u0010\u000b'\u001f?\u0002\u0006\b\u0002\fB\u000e\u000f\u0010\u0006\b\u0019\u001a\u0002\n\u0019u\u000f\u0010\ts\u000fJ\u001cI\u0016\u0017\t\b\rs)\u00100n\t,\u0016\u0017B+\u000b?\u000f\u001d\u001cS\u0016\u0017\u000b\u000e\r\b\u0002\n\u000b\u001a\t,\u0016\u0017\r,\"-84\u000f\u001d\u001c\u0017*\u001a\u0002\n\t\nl\u001a\t/\u000f\u0010#H\u0004 \u001c\u0017\u0002\n\u0019u\u000f\u001b\rs\u000f=\t\b\u0004&\u0002\u00135\u0014/\u0016IZ&\u0014=02\u0006\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\"+9~\u00193~q56A\u000e*?\u000f\u001d\u001cI\u0016\u0017\r6\"'\t\b\r\b\u0002\n\u0006\b\u0002\n)H\u0006\b\u0002\n\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u001aB+\t3%&\u000f\n84\u0002>\r,DG)>\u0014/%&\u000f\u001b\u000b 5\u000b\u001a\u0002\u0013\u001c\u0017\t\nl\u001a\u0002\u001b\u000f\u001b\u0014\u0013%-\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\u0019u\u000f\u0010\rs¥+¥\u00079\u0017¦\nU\u001a3+l\u000eDG\u0016\u0017\r\b%-\u0002\u001b\u000f\u001b\u0014\u0013%H\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002=\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\u0002\f\u0019\u000f\u001b\tG\u000f-¦\u001b\n56\u001f \u0016\u0017\r<\u0016\u0017\u000b\u000e\r\b\u0002\fB+\u0002\n\u0006\n9s},\u000bH)+*\u001a\u0006<\u0002\n(\u000e\u0004?\u0002\n\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\t3DG\u00023*\u001a\t\b\u0002J\t,\u0016\u0017\u000b\u001aB\u0010\u001c\u0017\u0002/5,\u0014/%?\u000f\u0010\u000b\u001a\u000b\u0007\u0002\u0013\u001c\u0006\b\u0002\f\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u0007B4\tG)\u00100n\u000fq\u001c\u0017)\u001bDG\u0002\f\u0006<A\u000e*?\u000f\u001d\u001cI\u0016\u0017\r6\"+l\u001a\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\u0019C\u000f\u001b\rs¤+¤ 9 +\u001bU\u001aJ+l4DG\u0016\u0017\r\b%-\u0002\u001b\u000f\u001b\u0014\u0013%\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u00021\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\u0002\f\u0019Y\u000f\u001b\t=\u000f\u001b\u000bO\n5,\u001f\u000e\u0016\u0017\rJ\u0016\u0017\u000b\u000e\r\b\u0002\fB+\u0002\n\u0006\n9m3%\u000e\u0016\u0017\t1\u0004\u001a%?\u000f\u0010\t\b\u0002C\u0014\u001b\u000f\u0010\u000b\u0018\u001f&\u0002C\u0006\b\u0002\nB\u000e\u000f\u001b\u0006\b\u0019\u0007\u0002\n\u0019Ä\u000f\u0010\tH\u000f@\u0004\u000e\u0016\u0017\u0004&\u0002\u0013\u001cI\u0016\u0017\u000b\u001a\u00027)\u001b03\r\b%\u0007\u0006\b\u0002\f\u0002-\t\b*\u0007\u001f?\u0014\n)+#15\u0004?)+\u000b\u001a\u0002\n\u000b\u000e\r\b\t\n9\n3.1.1FromRawAudiotoSpectrum\u0001\u0003\u0002O*\u001a\t\b\u00027\r\b%\u001a\u0002\u0003 %\u001a)+\u0006\b\r,56mG\u0016\u0017#-\u0002\u0018\u0007)+*\u001a\u0006,\u0016\u0017\u0002\n\u0006umF\u0006/\u000f\u0010\u000b\u001a\t,02)+\u0006\b# y6\u000em;Mm={.\r\b)Y\u0002\n(+5\r\b\u0006/\u000f\u001b\u0014\n\r<\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u000e\r/\u000f\u001b\u000b\u0007\u0002\n)4*\u0007\tG02\u0006\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\"C\u0019\u000e\u0016\u0017\t\b\r\b\u0006,\u0016\u0017\u001f\u0007*\u001a\r,\u0016\u0017)+\u000b\u001a\t=02\u0006\b)+#\r\b%\u001a\u00023\t,\u0016\u0017B+\u000b?\u000f\u001d\u001c\u0017\t\n9\u0001\u0003\u0002>\t\b\u0004\u000e\u001cI\u0016\u0017\r>\u0002\u001b\u000f\u0010\u0014/%'\t,\u0016\u0017B4\u000b&\u000f\n\u001cF\u0016\u0017\u000b\u000e\r\b)V¦\u001d+¤\u001b¥\u001056\u001f\u000e\"\u000e\r\b\u0002\u001352\u001c\u0017)+\u000b\u001aB'\t\b\u0002\nB4#-\u0002\n\u000b\u000e\r\b\t3DG\u0016\u0017\r\b%\u0003+9)\u001b84\u0002\n\u0006,\u001c\u001e\u000f\u0010\u0004SlFDG\u0016\u0017\u000b\u0007\u0019\u001a)\u001bD\u0002\u001b\u000f\u0010\u0014/%7\t\b\u0002\fB+#-\u0002\n\u000b\u000e\r\u000f\u0010\u000b\u001a\u00197\u0004?\u0002\n\u0006,02)+\u0006\b# ¤+\u0010¥\u001d56\u001f\u000e\"\u000e\r\b\u0002-\n\u0002\f\u0006\b)\u00105\u0004?\u000f\u0010\u0019\u001a\u0019\u001a\u0002\n\u0019EFMmr)+\u000b\u0005\u0002\u001b\u000f\u0010\u0014/%\u0005DG\u0016\u0017\u000b\u0007\u0019\u001a)\u001bDG\u0002\n\u0019[\t\b\u0002\fB+#-\u0002\n\u000b\u000e\r\n9\u0018ms\u000f\u0010U+\u0016\u0017\u000b\u001aBY\u000f\u001b\u001f\u0007\t\b)\u001b\u001c\u0017*\u0007\r\b\u000284\u000f\n\u001c\u0017*\u0007\u0002\n\t.y2#C\u000f\u0010B4\u000b \u0016\u0017\r\b*\u001a\u0019\u001a\u0002\n\t/{J)\u001b0F\r\b%\u001a\u0002.FMmE\u0006\b\u0002\n\t\b* \u001c\u0017\r\nl?DG\u0002=)4\u001f\u0007\r/\u000f\n\u0016\u0017\u000b\u0003\u000f1\t\b\u0004&\u0002\n\u0014\f\r\b\u0006\b)\u00105B4\u0006/\u000f\u0010# B\u001b\u0016\u00178+\u0016\u0017\u000b\u0007BC\u001c\u0017)\u000e\u0014\u001b\u000f\u001d\u001cI\u0016\u0017\n\u0002\f\u00197\t\b\u0004&\u0002\f\u0014\n\r\b\u0006/\u000f\u001d\u001c¢\u0014\n)+\u000b\u000e\r\b\u0002\n\u000b\u000e\r;\u000f\u001b\t3\u000f02*\u001a\u000b\u0007\u0014\f\r,\u0016\u0017)+\u000bO)\u00100¢\r,\u0016\u0017#-\u0002+9T=\r\b%\u001a\u0002\n\u0006.\r,\"\u000e\u0004&\u0002\f\t1)\u001b03\t,\u0016\u0017B+\u000b?\u000f\u001d\u001c;\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t,\u0016\u0017\u000b\u001aBY#C\u000f\n\"\u0003\u000f\u001d\u001c\u0017\t\b)\u0003\u001f&\u0002C*\u0007\t\b\u0002\f\u0019Y%\u0007\u0002\f\u0006\b\u0002+KF\u0016\u0017\u000b\r\b%\u000e\u0016\u0017\t=\u0004?\u000f\u0010\u0004?\u0002\n\u0006=DG\u0002;)+\u000b\u000e\u001c\u0017\"'02)\u000e\u0014\n*\u001a\t3)+\u000b'\r\b%\u001a\u0002H m;Mm>9\n3.1.2FromSpectrum toEventVectorm3%\u000e\u0016\u0017\t3\t\b\r\b\u0002\n\u0004-\u0016\u0017\u000b\u000e8+)\u0010\u001c\u001784\u0002\n\t;\u000f\u0010\u000b?\u000f\u001d\u001c\u0017\"\u000e\u0013\u0016\u0017\u000b\u001aB'\r\b%\u001a\u0002=\t\b\u0004?\u0002\n\u0014\n\r\b\u0006\b)+B4\u0006/\u000f\u0010#r\r\b)\u0016\u0017\u0019\u001a\u0002\n\u000b\u000e\r,\u0016I02\"C\t,\u0016\u0017B\u00105\u000b\u000e\u0016IZ?\u0014\u001b\u000f\u0010\u000b\u000e\rNPQ\u0002\n84\u0002\n\u000b\u000e\r\b\t\bR-\u0016\u0017\u000b7\r\b%\u001a\u0002.\u0006\b\u0002\n\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u001aB\u0007l¢\u000f\u0010\u000b\u001a\u00197\r\b)-)4\u001f\u0007\r/\u000f\n\u0016\u0017\u000b\u0005\u000f1\u0014\n)4#-\u0004?\u000f\u0010\u0014\n\r\u0006\b\u0002\f\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)+\u000bu)\u00100S84\u0002\n\u0014\n\r\b)4\u0006\b\t<\u0014\n)4\u0006\b\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u0007BH\r\b)>\u0002\n8+\u0002\n\u000b\u000e\r\b\t\n9¢},\u000b-)4*\u0007\u0006F\u0016\u0017#15\u0004\u000e\u001c\u0017\u0002\n#-\u0002\f\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)+\u000bO\u0019\u001a\u0002\n\t\b\u0014\n\u0006,\u0016\u0017\u001f?\u0002\n\u0019O\u001f&\u0002\u0013\u001c\u0017)\u001bD>l¢PQ\u0002\n84\u0002\n\u000b\u000e\r\b\t\bRC\u000f\u0010\u0006\b\u00023\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019N\u000f\u0010\tG\u0004?\u0002\u001b\u000f\u0010U\u000e\t\u0016\u0017\u000bO\t,\u0016\u0017B4\u000b&\u000f\n\u001cs\u0004&)\u001bDG\u0002\n\u0006\n9¦+9;vM\u001c\u0017)+\r;\r\b%\u0007\u0002=\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u000e\r/\u000f\u001b\u000b\u0007\u0002\n)4*\u0007\t>\u0004?)\u0010Dp\u0002\n\u0006;\u000f\u0010\t;\u000f02*\u001a\u000b\u0007\u0014\f\r,\u0016\u0017)+\u000b\u0003)\u00100s\r,\u0016\u0017#-\u0002+9¤ 93}6\u0019\u0007\u0002\n\u000b\u000e\r,\u0016I0\"1\u0004&\u0002\u001d\u000f\u0010U\u000e\t<\u0016\u0017\u000b-\r\b%\u001a\u00023\u0004&)\u001bDG\u0002\n\u0006F\u0004\u000e\u001c\u0017)+\r\nl\u001aD3%\u0007\u0002\f\u0006\b\u0002J\u0004?\u0002\u001b\u000f\u0010U>\u0016\u0017\t<\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\f\u0019'\u000f\u001b\t\u000f1\u001c\u0017)\u000e\u0014\u001b\u000f\n\u001cG#C\u000f\u001b(+\u0016\u0017#1*\u001a#84\u000f\u001d\u001c\u0017*\u001a\u0002CDG\u0016\u0017\r\b% \u0016\u0017\u000b\u0018\u000f-\u000b\u0007\u0002\u0013\u0016\u0017B4%\u000e\u001f&)+\u0006\b%\u001a)\u000e)\u000e\u0019[)\u001b03\u000f-\u0004\u001a\u0006\b\u0002\u00135\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019\u0018\t,\u0016\u0017\n\u0002+9.m3% \u0016\u0017\t>\u0019\u001a\u0002\u0013Z?\u000b \u0016\u0017\r,\u0016\u0017)4\u000bw%\u001a\u0002\u0013\u001c\u0017\u0004\u001a\t1\u0006\b\u0002\f#-)\u001b8+\u0002.\u0014\n\u0002\n\u0006\b\r/\u000f\u001d\u0016\u0017\u000b\u0018\u001f&)+B4*\u0007\t\u001c\u0017)\u000e\u0014\u001b\u000f\n\u001csP\b\u0004&\u0002\u001b\u000f\u001bU\u000e\t\bR>DJ%\u000e\u0016\u0017\u0014/%-\u000f\u001b\u0006\b\u0002<\u0016\u0017#-#-\u0002\n\u0019\u000e\u0016\u001e\u000f\u0010\r\b\u0002/\u001c\u0017\"102)\u0010\u001cI\u001c\u0017)\u001bDG\u0002\f\u00191)4\u0006F\u0004\u0007\u0006\b\u0002\n\u0014\f\u0002\n\u0019\u0007\u0002\f\u0019\u001f\u000e\"u% \u0016\u0017B4%\u0007\u0002\n\u0006.84\u000f\u001d\u001c\u0017*\u001a\u0002\n\t\n9.},\u000b\u000e\r\b*\u000e\u0016\u0017\r,\u0016\u00178+\u0002\u0013\u001c\u0017\"4lF\r\b%\u0007\u0002\f\t\b\u0002-\u0004&\u0002\u001b\u000f\u001bU\u000e\t>\u0006\b)+*\u001aB+%\u000e\u001c\u0017\"7\u0014\n)4\u0006\b\u0006\b\u0002\u00135\t\b\u0004&)4\u000b\u0007\u0019C\r\b);\u0019 \u0016\u0017\t\b\r,\u0016\u0017\u000b\u001a\u0014\n\r,\u0016\u00178+\u0002.\u000b\u001a)+\r\b\u0002\n\ts)+\u0006s\u0006\b%\u000e\"\u000e\r\b%\u0007#.\u0016\u0017\u0014=\u0004?\u000f\u0010\r\b\r\b\u0002\n\u0006\b\u000b\u001a\t\nl\u000e\u001f\u001a*\u0007\rsDG\u0016\u0017\r\b%\t\b)4#-\u0002>\u0002\n\u0006\b\u0006\b)+\u0006\b\t\nl?D3% \u0016\u0017\u0014/%ODG\u0016I\u001cI\u001cF\u001f&\u0002.\u0014\n)+#H\u0004&\u0002\n\u000b\u001a\t/\u000f\u0010\r\b\u0002\n\u0019'\u001c\u001e\u000f\u001b\r\b\u0002\n\u0006\n9 9;XF(\u000e\r\b\u0006/\u000f\u0010\u0014\n\r302\u0006\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\"'\u0014\n)4#-\u0004&)4\u000b\u0007\u0002\n\u000b\u000e\r\b\t>\u000b\u001a\u0002\u001b\u000f\u001b\u0006=\u0002\u001b\u000f\u001b\u0014\u0013%O\u0004&\u0002\u001d\u000f\u0010U&9;\u00017\u0002.\r/\u000f\u0010U4\u0002¦\u001d+J\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\t<)\u001b0\u00070\u0006\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\".\u0014\n)4#-\u0004&)+\u000b\u001a\u0002\n\u000b\u000e\r\b\ts\u001f&\u0002\n\r6DG\u0002\n\u0002\n\u000bH¤+4+Jq\u000f\u0010\u000b\u001a\u0019¤4++4J+9p=84\u0002\n\u0006/\u000f\u0010B4\u0002184\u000f\u001d\u001c\u0017*\u001a\u0002\n\t;)\u001084\u0002\n\u0006>\u000f.\t\b%\u0007)4\u0006\b\r=\r,\u0016\u0017#-\u0002>\u0004&\u0002\f\u0006,\u0016\u0017)\u000e\u0019V02)\u001b\u001cI\u001c\u0017)\u001bDq5\u0016\u0017\u000b\u001aBY\r\b%\u001a\u0002'\u0004&\u0002\u001b\u000f\u001bU!\u000f\u0010\u0006\b\u0002O*\u001a\t\b\u0002\n\u0019Y\u0016\u0017\u000b[)+\u0006\b\u0019\u001a\u0002\n\u0006C\r\b)\u0003\u0006\b\u0002\n\u0019\u0007*\u001a\u0014\n\u0002O\t\b\u0002\n\u000b\u001a\t,\u0016\u0017\r,\u0016\u00178+\u0016\u0017\r,\"E\r\b)\u000b\u001a)\u0010\u0016\u0017\t\b\u0002;\u000f\u0010\u000b\u001a\u0019.\r\b)>\u000f\n8+)\u001b\u0016\u0017\u0019-\r\b%\u0007\u0002P\u0013\u000f\u0010\r\b\r/\u000f\u001b\u0014\u0013U\u000eR>\u0004?)+\u0006\b\r,\u0016\u0017)+\u000b\u001a\tG\u0004\u001a\u0006\b)\u000e\u0019\u0007*\u001a\u0014\n\u0002\n\u0019H\u001f\u000e\">\u0014\n\u0002\n\u0006,5\r/\u000f\n\u0016\u0017\u000b1\u0016\u0017\u000b\u0007\t\b\r\b\u0006\b*\u001a#-\u0002\n\u000b\u000e\r\b\t3y2\t\b%\u001a)+\u0006\b\r\nl\u000e\u000b\u001a)+\u000b\u000e56%&\u000f\u001b\u0006\b#-)+\u000b\u000e\u0016\u0017\u0014=\t,\u0016\u0017B4\u000b&\u000f\n\u001cS\t\b\u0002\nB+#H\u0002\n\u000b\u000e\r\b\tG\u000f\u001b\r\r\b%\u001a\u0002=)4\u000b\u0007\t\b\u0002\n\rp)\u00100\u0002\u001b\u000f\u0010\u0014/%C\u000b\u0007)4\r\b\u0002\u001b{\u00139sm3% \u0016\u0017\tG\t\b\r\b\u0002\f\u0004@B4\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u0010\r\b\u0002\n\t;\u000fJ\u001cI\u0016\u0017\t\b\r3)\u00100p¦\u001b4\u001d5\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001cp8+\u0002\n\u0014\f\r\b)+\u0006\b\t\n9¥\u00079\u0005\u0019G)+\u000b\u000e84\u0002\n\u0006\b\rF\u0002\u001b\u000f\u001b\u0014\u0013%'¦\u001d+\n5,\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c¢8+\u0002\n\u0014\f\r\b)+\u0006~K<\u0016\u0017\u000b\u000e\r\b).\u000f=#H)+\u0006\b\u0002G\u0014\n)4#15\u0004?\u000f\u0010\u0014\n\r;\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)4\u000bY)\u00100~\u0005\u0019 \u0016\u0017#H\u0002\n\u000b\u0007\t,\u0016\u0017)4\u000b\u0007\t\n9H\u00017\u0002.B\u0010\u0016\u001784\u0002-\r,Dp)C\u000f\u001d\u001c\u0017\r\b\u0002\n\u0006,5\u000b?\u000f\u0010\r,\u0016\u001784\u0002\n\t=%\u001a\u0002\n\u0006\b\u0002+¨©\n\u0019G)4#1\u001fu#-\u0002\n\r\b%\u001a)\u000e\u0019L¨q\u0007)+\u00063\u0002\u001b\u000f\u001b\u0014\u0013%Y¦\u001d+\u001d56\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c384\u0002\n\u0014\n\r\b)4\u0006B\u000f+l\u0014\f)+\u000b\u000e8+\u0002\f\u0006\b\r\u0016\u0017\r1\u0016\u0017\u000b\u000e\r\b)\u0018\u000f'L56\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c8+\u0002\n\u0014\n\r\b)4\u0006\b\n\r\b%&\u000f\u001b\r1\u0006\b\u0002\n\u0004\u000e5\u0006\b\u0002\f\t\b\u0002\n\u000b\u000e\r\b\t!\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rC\u0004 \u0016\u0017\r\b\u0014/%Y\u001c\u0017\u0002\n8+\u0002/\u001c\u0017\t\n¨I\r 7y\u001c ¦9\u0013\u0013\u0013)\u0016F{\u0006\b\u0002\f\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r\b\t=\r\b%\u001a\u0002>\u0004 \u0016\u0017\r\b\u0014/%\b\n3.\u000f\u0010\u000b\u001a\u0019'\u0016\u0017\t=\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019O\u001f\u000e\"&¨0\r \nTheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\nv5v6v7\nv8 v1\n2vv4v3\nv12vv3v3v4v6\nv1v3v4v7v8v6time\nsequence 2sequence 1d=1\nd=2ñGòÊó?ôFõ\u0010öoLøqv1ÿ?û\u0001\u0000\nü\nõ\u001bô\u0001\u0006\u000eü\nò2ÿ&û\u0005ÿ\u0003\u0002qvq\u0001\u000b\u0007õ<\u000b\u0011\u0006\u000eü\nö\u000eõ\u001bò\u0007\u0000\fü\nò\t\u0006pMö\u000fSôFö\u000eû\u0001\u0006\u000eö\u000f\u0000 QM\r\n <lsD3%\u0007\u0002\n\u0006\b\u0002\n\r\n <\u0016\u0017\tC\r\b%\u0007\u0002C02\u0006\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\"Y\u0014\n)+#H\u0004&)\u00105\u000b\u001a\u0002\n\u000b\u000e\rC)\u00100.84\u0002\n\u0014\n\r\b)+\u0006oK\u0003\u000f\u001b\r-02\u0006\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\"V\u0014YJ49\u0011m3% \u0016\u0017\tu\t\b\r\b\u0002\n\u0004\u0002\u0013tn\u0002\n\u0014\n\r,\u0016\u00178+\u0002/\u001c\u0017\"\u0011\u0014\n)+\u000b\u000e84)\u0010\u001c\u001784\u0002\n\t7\r\b%\u001a\u0002Y)4\u0006,\u0016\u0017B\u0010\u0016\u0017\u000b?\u000f\u001d\u001cu\t\b\u0004&\u0002\n\u0014\n\r\b\u0006\b*\u001a#$DG\u0016\u0017\r\b%\u0011\u000f06\u000f\u001b#1\u0016I\u001c\u0017\"E)\u00100.\u0014\f)+#.\u001f 52\u001cI\u0016\u0017U+\u0002YZ\u001a\u001c\u0017\r\b\u0002\n\u0006\b\t\n9rmJ%\u001a\u0002O\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r'\u0016\u0017\tO\u000f7\u001cI\u0016\u0017\t\b\rO)\u00100S5,\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c;84\u0002\n\u0014\n\r\b)+\u0006\b\t\nl~\nlo ¦9/¤K\u0013?\u0013\rMlFD3%\u001a\u0002\n\u0006\b\u0002\u0016\u0017\t-\r\b%\u001a\u0002'\u000b\u000e*\u001a#.\u001f&\u0002\n\u0006-)\u001b0=\u0004?\u0002\u001b\u000f\u0010U\u000e\tC)+\u001f\u001a\r/\u000f\u001d\u0016\u0017\u000b\u001a\u0002\n\u0019S9[}6\u000b\u000e\r\b* \u0016\u0017\r,\u0016\u001784\u0002\u0013\u001c\u0017\"+l3\u0002\u001b\u000f\u001b\u0014\u0013%84\u0002\n\u0014\n\r\b)+\u00063\u0002\n\t\b\r,\u0016\u0017#\u0015\u000f\u0010\r\b\u0002\n\t>\r\b%\u001a\u0002>\u0004 \u0016\u0017\r\b\u0014/%\u0003\u0019 \u0016\u0017\t\b\r\b\u0006,\u0016\u0017\u001f\u001a*\u0007\r,\u0016\u0017)4\u000b!\u000f\u0010\r3\r\b%\u0007\u0002.\u0014\n)4\u0006\b\u0006\b\u0002\u00135\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u001aB'\r,\u0016\u0017#-\u0002=\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u000e\r\n9©\n\u0019G\u0002\n\u0004\u001a\t\b\r\b\u0006\b*\u0007# #-\u0002\n\r\b%\u001a)\u000e\u0019S¨!\u0007)+\u0006C\u0002\u001b\u000f\u001b\u0014\u0013%\u0011¦\u001d+\u001d56\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000bS\u000f\n\u001cC8+\u0002\f\u0014\u00135\r\b)4\u0006B\nl&\r/\u000f\u001bU+\u0002>\u001c\u0017)+B\u000e\u000f\u001b\u0006,\u0016\u0017\r\b%\u0007# \u000f\u001b\u000b\u0007\u0019\u0003\u0004&\u0002\n\u0006,02)4\u0006\b#\u000f\u001b\u000b\u0007)4\r\b%\u0007\u0002\f\u0006.\u0007)+*\u001a\u0006,\u0016\u0017\u0002\n\u0006mF\u0006/\u000f\u001b\u000b\u0007\t,02)4\u0006\b#)4\u000b@\u0016\u0017\r\n9sm3%\u0007\u0002>\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\rJ\u0016\u0017\t;\u000f=\u001cI\u0016\u0017\t\b\r=)\u001b0s\u0014\n\u0002\n\u0004\u001a\t\b\r\b\u0006\b*\u0007# \u0014\n)\u001b5\u0002\u0013\u0012\u0015\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\r\b\t\n9;\u00017\u0002>\r/\u000f\u001bU+\u0002H\u000f\u0015\t\b*\u001a\u001f\u001a\t\b\u0002\n\r=)\u001b0,Y\u0014\f)\u000e\u0002\u0013\u0012C\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\r\b\t=0\u0006\b)+# \u0016\u0017\r\r\b)>02)+\u0006\b#ª\u000fqL56\u0019 \u0016\u0017#H\u0002\n\u000b\u0007\t,\u0016\u0017)4\u000b&\u000f\n\u001cG84\u0002\n\u0014\n\r\b)4\u0006B\u0013+9m3%\u001a\u0002\u0015Z?\u000b?\u000f\u001d\u001c3\u001cI\u0016\u0017\t\b\r-)\u00100;84\u0002\n\u0014\n\r\b)+\u0006\b\t\u0018\nl ¡ ¦9/¤K?\u0013\u0013;¢lp\u000f\u001b\u0006\b\u0002@\r\b%\u001a\u0002\u0018PQ\u0002\n84\u0002\n\u000b\u000e\r84\u0002\n\u0014\n\r\b)+\u0006\b\t\bR.\r\b%?\u000f\u0010\rs\u0006\b\u0002\n\u0004\u0007\u0006\b\u0002\f\t\b\u0002\n\u000b\u000e\r,C\u0002\n8+\u0002\f\u000b\u000e\r\b\t<\u0019\u001a\u0002\n\r\b\u0002\n\u0006\b#1\u0016\u0017\u000b\u001a\u0002\n\u0019C\u001f\u000e\".\u0004&)\u001bDG\u0002\n\u0006s\u0004&\u0002\u001b\u000f\u0010U\u000e\t\n9\n3.1.3FromEventVectortoCharacteristic SequenceXF8+\u0002\n\u000b\u000e\r>84\u0002\n\u0014\n\r\b)4\u0006\b\t>)+\u000b\u000e\u001c\u0017\"7B\u001b\u0016\u00178+\u0002-\u0016\u0017\u000b 0)+\u0006\b#C\u000f\u001b\r,\u0016\u0017)+\u000bY)4\u000bV\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u000e\r/\u000f\u001b\u000b\u0007\u0002\n)4*\u0007\tC\t\b\u0004&\u0002\f\u0014\u00135\r\b\u0006/\u000f\n\u001cS\u0014\n)+\u000b\u000e\r\b\u0002\f\u000b\u000e\r\b\t3\u000f\u0010\u0006\b)4*\u0007\u000b\u001a\u0019C\u0002\u001b\u000f\u0010\u0014/%C\u0002\n8+\u0002\n\u000b\u000e\r\nK\u001a\r\b%\u001a\u0002\n\".\u0019\u0007).\u000b\u0007)4\r<\t\b\u0004?\u0002\n\u0014\u0013\u0016I02\"C%\u001a)\u0010D!\r\b%\u0007\u0002#.*\u0007\t,\u0016\u0017\u0014\u0003\u0004\u001a\u0006\b)+B+\u0006\b\u0002\f\t\b\t\b\u0002\n\t\n9\u0011mF)\u0003\u0014\u001b\u000f\u0010\u0004\u001a\r\b*\u001a\u0006\b\u0002V\u0016\u0017\u000b\u000e02)+\u0006\b#\u0015\u000f\u0010\r,\u0016\u0017)+\u000bW)+\u000b!\u0006\b%\u000e\"\u000e\r\b%\u0007# \u000f\u001b\u000b\u0007\u0019#.*\u0007\t,\u0016\u0017\u0014H\u0004\u0007\u0006\b)4B+\u0006\b\u0002\n\t\b\t,\u0016\u0017)4\u000bLlFDp\u0002>\u0004\u0007*\u001a\r>\r\b)+B4\u0002\n\r\b%\u0007\u0002\f\u0006>\u000f-\t\b\u0002\n\u0006,\u0016\u0017\u0002\n\t>)\u001b0s\u0002\n8+\u0002\n\u000b\u000e\r;8+\u0002\n\u0014\f\r\b)+\u0006\b\t\r\b)-\u0014\f)+\u000b\u001a\t\b\r\b\u0006\b*\u0007\u0014\n\r\u0015PQ\u0014/%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\f\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014-\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002\n\t\n9 R\u0001!%\u0007\u0002\n\u000b\u0018\u0004&\u0002\u001b\u000f\u0010U\u000e\tuy2\u0002\n8+\u0002\n\u000b\u000e\r\b\t/{>\u000f\u001b\u0006\b\u0002C\u0019\u0007\u0002\n\r\b\u0002\f\u0014\n\r\b\u0002\n\u00197\u0016\u0017\u000bY\r\b%\u001a\u0002-\u0004\u001a\u0006\b\u0002\n8+\u0016\u0017)+*\u001a\tC\t\b\r\b\u0002\n\u0004SlMDG\u0002U4\u0002\n\u0002\n\u0004\u0003\r\b\u0006/\u000f\u0010\u0014/UO)\u00100p\r,\u0016\u0017#-\u0002-)\u0010tn\t\b\u0002\n\r\b\t>)\u00100p\r\b%\u0007\u0002\u00187\u0004&\u0002\u001b\u000f\u001bU\u000e\t\nl\u0001¢\n£¢{¤\u001a\u0013?\u0013\u0016¢£¥nl\u000f\u001b\u000b\u0007\u0019\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002w¦*§1\r\b)-\u001f?\u0002>\r\b%\u0007\u0002>\u0002\n84\u0002\n\u000b\u000e\r38+\u0002\n\u0014\f\r\b)+\u0006\u0005\u001a>\t\b*\u0007\u0014/%O\r\b%?\u000f\u0010\r-¢\u0016©¨«ª­¬¢£\u0013®\nl\u001629 \u0002+9\u0017l\u0007\r\b%\u001a\u0002J\u001c\u001e\u000f\u001b\t\b\r3\u0004&\u0002\u001b\u000f\u0010UC\u000b\u001a)1\u001c\u001e\u000f\u0010\r\b\u0002\f\u0006J\r\b%?\u000f\u0010\u000bC\r,\u0016\u0017#-\u0002qª&9s},\rF02)\u0010\u001cI\u001c\u0017)\u001bD3\t=\r\b%?\u000f\u0010\r0¦*¯\u0007°X\nl?\u000f\u001b\u000b\u0007\u0019¦§\n\u0016\u0017\t3*\u001a\u000b\u0007\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019O02)4\u0006Bª\b¬%¢\n9\u0001\u0005\u0016\u0017\r\b%w\r\b%\u000e\u0016\u0017\t.\u000b\u0007)4\r/\u000f\u0010\r,\u0016\u0017)+\u000bSlGDp\u0002-\u0014\n)+\u000b\u001a\t\b\r\b\u0006\b*\u001a\u0014\n\r-\u000f'\t\b\u0002\f\r>)\u001003\u0014/%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\f\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014O\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\t>\u000f\u001b\tG02)\u0010\u001cI\u001c\u0017)\u001bD3\t\n¨<02)4\u00063\u000f\u0010\u000b\u000e\"C\r6DG)1\u000b\u001a\u0002\u001b\u000f\u001b\u0006\b\u001f\u000e\"'\u0004&\u0002\u001b\u000f\u001bU\u000e\t3DJ%\u000e\u0016\u0017\u0014/%\u0003\u000f\u0010\u0006\b\u0002>\t\b\u0002\n\u0004\u000e5\u000f\u001b\u0006/\u000f\u0010\r\b\u0002\n\u0019W\u001f\u000e\"[02\u0002\nDG\u0002\f\u0006'\r\b%?\u000f\u0010\u000b²±\\)+\r\b%\u001a\u0002\n\u0006O\u0004&\u0002\u001b\u000f\u001bU\u000e\t\nl=\u0016\u0017\u0019\u001a\u0002\n\u000b\u000e\r,\u0016I02\"¼\u000fw\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002)\u001b0P02)\u001b\u001cI\u001c\u0017)\u0010DG56*\u001a\u0004\u0007Ru\u0004&\u0002\u001b\u000f\u0010U\u000e\t3D3%\u000e\u0016\u0017\u0014\u0013%O#C\u000f\n\u0016\u0017\u000b\u000e\r/\u000f\u001d\u0016\u0017\u000b\u0003\u0006\b)+*\u001aB+%\u000e\u001c\u0017\"O\u0002\nA\u000e*?\u000f\u001d\u001cF\u0019\u000e\u0016\u0017\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u000202\u0006\b)4# \u0002\u001d\u000f\u0010\u0014/%7)4\r\b%\u0007\u0002\n\u0006\nl\t\b\r/\u000f\u0010\u0006\b\r,\u0016\u0017\u000b\u001aB'02\u0006\b)4# \r\b%\u001a\u0002-\r,Dp)@)4\u0006,\u0016\u0017B\u0010\u0016\u0017\u000b?\u000f\u001d\u001c3\u0004?\u0002\u001b\u000f\u0010U\u000e\t\n9.m3%\u0007\u0002\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t3\u0016\u0017\tJ\u0016I\u001cI\u001c\u0017*\u001a\t\b\r\b\u0006/\u000f\u0010\r\b\u0002\n\u0019O\u0016\u0017\u000b\u0003L\u0016\u0017B+*\u001a\u0006\b\u0002-\u000e9\u0007)4\u0006\b#C\u000f\u001d\u001cI\u001c\u0017\"4l\u000f1\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014-\t\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\u00021\u0016\u0017\t3B\u001b\u0016\u00178+\u0002\n\u000bO\u001f\u000e\"³\u000f´\u0013\r\u000f´\u001c®Fµ<\r¦¯\u0007¶\r®\n¤\u0016·¯¶\t¸\u0011¹»º\n¯\u0007¶\u001c¼\n\r¦¯\t¶\u0016®*½\n·¯¶m¸\u0003¹\u001aº\n¯\u0007¶\u0016¼\n\u0013\u0013\u0013M¦¯\n¶®\n·\t¾º\n¼\n·¯¶\t¸\u0011¹º\n¯\n¶¼;¿D3%\u001a\u0002\n\u0006\b\u0002­À \u0016\u0017\t>\r\b%\u001a\u00021\u0004\u001a\u0006\b\u0002\n\u0019\u001a\u0002\u0013Z&\u000b\u001a\u0002\n\u0019[ \u0017d\u001bk\u000eÁ\u000ec\u0017_1)\u001b0G\u0002\u001b\u000f\u001b\u0014\u0013%\u0003\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002+lXÁ\u0016\u0017\t>\r\b%\u0007\u0002f\fc2`\u000ea\u0013c2ehk\u000eÁ-?\u000eehk&chlFD3% \u0016\u0017\u0014/%E\u0006/\u000f\u0010\u000b\u001aB+\u0002\f\t@)\u001b84\u0002\n\u0006u\u000f\u001d\u001cI\u001c;\u0004&)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\u00027\u0016\u0017\u000b\u001a\u0019\u0007\u0002\f(\u000e\u0002\n\t\nl>\u000f\u001b\u000b\u0007\u0019Â[\u0016\u0017\t7\r\b%\u001a\u0002W¾\u001ba,`\u000e]\rÃ\u000ed\u0010c2ehkÁw]/\u000ek&c2a,+ Il>D3%\u000e\u0016\u0017\u0014\u0013%¼\r/\u000f\u0010U+\u0002\f\t7)4\u000b\t\b#\u0015\u000f\u001d\u001cI\u001c-\u0016\u0017\u000b\u000e\r\b\u0002\nB4\u0002\n\u000684\u000f\u001d\u001c\u0017*\u001a\u0002\n\t-\u0016\u0017\u000b[\r\b%\u001a\u0002@\u0016\u0017\u000b\u000e\r\b\u0002\f\u0006\b8+\u000f\n\u001c-£\u0017¦\u001a\r±1§69\u0018J)+\r\b\u0002u\r\b%&\u000f\u001b\r1\u0016I0>\u0002\u001d\u000f\u0010\u0014/%\u00038+\u0002\n\u0014\n\r\b)4\u00061\u0016\u0017\tS5,\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c2lp\r\b%\u0007\u0002H\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b&\u000f\n\u001cg\u0016\u0017\r6\"w)\u001b03\u0002\u001b\u000f\u001b\u0014\u0013%\u0003\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014u\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\u0016\u0017\t\u0005\u0001À\u00119vector0.8  1.1  1.4  0.9  0.6  2.0  0.8  1.1  1.2  0.5  1.7  1.2Ä\u001cÅ\u001cÆ Ç È{É Ê ËQÈ\u001cÌVÍ\rÎ Ä\u001cÅ{Æ Ç È\u001cÉ Ê ËNÈ\u001cÌZÍ\rÏ Ä\u001cÅ\u001cÆ Ç È{É Ê ËQÈ\u001cÌVÍ\rÐ Ä\u001cÅ{Æ Ç È\u001cÉ Ê ËNÈ\u001cÌZÍ\rÑÆ\u001aËNÉ Ò9ÓmËQÔ Õ\u001cÍ\rÏ Æ»ËQÉZÒ5ÓmËNÔ Õ{Í\rÐ Æ\u001aËNÉ Ò9ÓmËQÔ Õ\u001cÍ\rÑÖ\u0011×mØVÙ\u0011Ú\u0016×mÛ\u001cÜ Ý5Þ Ö\u0011×mØVÙ\u0011Ú\u0016×mÛ\u001cÜ Ý5ß ÖK×mØVÙ\u0011Ú;×mÛ\u001cÜ Ý5à\námámámá}ámáámámámá}ámá\nÆ»ËQÉZÒ5ÓmËNÔ Õ{Í\rÎÖK×mØVÙ\u0011Ú;×mÛ\u001cÜ Ý<âñGò2ó&ôFõ\u001böãMør^ô\u0001\n2ü\nò\tä\u0001\n2öå\u0001\u000b\u0011\u0000\u0013Fò2ûFó\u0005ò2û\u0001\u0000\nü\u0013\u000b\u001aû\u0001\u0006\u000eö\u000f\u0000åæHòÊü?EòÊûFýFö\u000fäsö\u000eûçýFö\u000eûü\u0001\u000b\u0011\u0000\u0013Eü\u0013\u000b\u0011è\u0001\n2ö\u000f\u0000\u001a)+\u0006C\r\b%\u0007\u0002O\u0002\f(\u001a\u000f\u0010#-\u0004\u000e\u001c\u0017\u00027\u0016\u0017\u000bWL\u0016\u0017B+*\u001a\u0006\b\u0002\u0018 l3\r\b%\u001a\u0002O\t\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\u0002\u0003DG\u0016\u0017\r\b%«Âé ¦'\u0016\u0017\t³\n;5¤\u001a\r½\n\r½\n\r5ê9\r\n¿\nl\u000eD3% \u0016\u0017\u0014/%-\u0019\u001a)\u000e\u0002\n\tF\u000b\u001a)+\rp\u000f\u001d\u001cI\u0016\u0017B+\u000bCDG\u0002/\u001cI\u001c&DG\u0016\u0017\r\b%.\r\b%\u0007\u00023\u001f&\u0002\u001b\u000f\u001b\ry2\u0019\u001a*\u001a\u0002>\r\b)>\r\b%\u0007\u0002;\u001f&)+B4*\u0007\t3\u0004&\u0002\u001b\u000f\u001bU5¤\u001b{\u0013K\u001a)+\u000bC\r\b%\u001a\u0002=)4\r\b%\u0007\u0002\n\u0006p%&\u000f\u001b\u000b\u0007\u0019Sl&\r\b%\u001a\u0002=\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002DG\u0016\u0017\r\b%Â% ¤O\u0016\u0017\t\n³\n?<½5?\nê?\n?\u001aë\u0013\u0013<ì¿\nlpD3% \u0016\u0017\u0014/%W\u000f\u001d\u001cI\u0016\u0017B4\u000b\u0007\tO\u0004&\u0002\f\u0006,02\u0002\n\u0014\n\r,\u001c\u0017\"DG\u0016\u0017\r\b%-\r\b%\u001a\u0002G\u001f&\u0002\u001d\u000f\u0010\r\n9s}Ê0S\u0002\u001d\u000f\u0010\u0014/%­=84\u0002\n\u0014\n\r\b)+\u0006F\u0016\u0017\tíL56\u0019 \u0016\u0017#H\u0002\n\u000b\u0007\t,\u0016\u0017)4\u000b&\u000f\n\u001c2l&\u0002\u001b\u000f\u001b\u0014\u0013%.\u001c\u0017\u0002\n\u000b\u0007B4\r\b% 5'\t\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\u0002O%\u001a\u0002\n\u0006\b\u0002-\u0016\u0017\tC5S56\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b&\u000f\n\u001c29\u0005mJ%\u001a\u0002C\u0004\u0007*\u001a\u0006\b\u0004&)4\t\b\u0002')\u00100;%?\u000f\n8+\u0016\u0017\u000b\u0007B\r\b%\u001a\u0002O\u001f\u001a\u0006/\u000f\u0010\u0014/U+\u0002\n\r,\u0016\u0017\u000b\u001aBY\u0014\n)4\u000b\u000e\r\b\u0006\b)\u0010\u001cqÂ%î£\u0017¦\u001a;±H§3\u0016\u0017\t-\r\b)\u0018)\u0010tS\t\b\u0002\n\rH\r\b%\u0007\u0002O\u0002\u0013tn\u0002\n\u0014\n\r\b\t-)\u001b0\u0004?)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\u0002O\u001f&)+B4*\u0007\t-\u0004?\u0002\u001b\u000f\u0010U\u000e\t-\r\b%?\u000f\u0010\r.\t\b*\u001a\u0006\b8+\u0016\u00178+\u0002\n\u0019\u0005\t\b\r\b\u0002\n\u0004\u0005¤@\u0016\u0017\u000bÄ \u0002\n\u0014\f\r,\u0016\u0017)+\u000bE\u000e9g¦49I¤\u000el\t\b*\u001a\u0014/%\u0003\u000f\u0010\t\u0005\n¤%\u001a\u0002\n\u0006\b\u0002+9\n3.2IndexingXs\u000f\u0010\u0014/%C\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014>\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002>B+\u0002\f\u000b\u0007\u0002\n\u0006/\u000f\u001b\r\b\u0002\n\u0019C\u001f\u000e\"CvF%&\u000f\u001b\t\b\u00021¦3\u0016\u0017\tG\u000f>% \u0016\u0017B4% 5\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c;84\u0002\n\u0014\n\r\b)+\u0006.\u000f\u001b\u000b\u0007\u0019\u0003\u0014\u001b\u000f\u001b\u000bO\u001f&\u00021\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n(\u000e\u0002\f\u0019L9HmJ%\u001a\u0002.\u0019\u0007\u0002\n\t,\u0016\u0017B4\u000bYB+)\u000f\u001d\u001cs)\u001b0\t\b*\u001a\u0014/%[\u000f\u001b\u000bY\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(7\u0016\u0017\t-\r\b)'0Ê\u000f\u0010\u0014\u0013\u0016I\u001cI\u0016\u0017\r/\u000f\u001b\r\b\u0002O\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c;)\u00100\u0015P\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006\bR\u00058+\u0002\f\u0014\n\r\b)+\u0006\b\t\nlD3%\u001a\u0002\n\u0006\b\u00023\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"@\u0016\u0017\t<\u0016\u0017\u000b\u001a\u0019\u000e\u0016\u0017\u0014\u001b\u000f\u0010\r\b\u0002\n\u0019u\u001f\u000e\"1%\u000e\u0016\u0017B+%C\u0014\n)4\u0006\b\u0006\b\u0002\u0013\u001c\u001e\u000f\u0010\r,\u0016\u0017)4\u000bO\u000f\n\u001c\u0017)+\u000b\u001aBC\u000f=\t\b*\u0007\u001f\u000e5\t\b\u0002\f\r<)\u001b0M\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u001a\t\n93J\u0016\u001784\u0002\n\u000bu\u000f=A\u000e*\u0007\u0002\f\u0006\b\"-8+\u0002\n\u0014\f\r\b)+\u0006\nl\u001a\u000f>S)\u000e\u0014\u001b\u000f\u001d\u001cI\u0016\u0017\r6\"+5\b\u000e\u0002\n\u000b\u001a\t,\u0016\u0017\r,\u0016h84\u0002;\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007BH\t\b\u0014\u0013%\u001a\u0002\n#-\u0002.y6M\u001a={J£ \u001d§&\u0016\u0017\tq\u000f06\u000f\u001b\t\b\r<\u0004\u001a\u0006\b)+\u001f?\u000f\u0010\u001f\u000e\u0016I\u001cI\u0016\u0017\t/\r,\u0016\u0017\u0014-\t\b\u0014\u0013%\u001a\u0002\n#-\u0002;\r\b%&\u000f\u001b\r\u0006\b\u0002\f\r\b*\u0007\u0006\b\u000b\u001a\t3\u000f\u0010\u0004\u001a\u0004\u001a\u0006\b)\u0010(+\u0016\u0017#\u0015\u000f\u0010\r\b\u0002>#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\tGDG\u0016\u0017\r\b%C\u0014\n)+\u000b\u000e\r\b\u0006\b)\u001b\u001cI\u001c\u001e\u000f\u0010\u001f\u000e\u001c\u0017\u00020Ê\u000f\u001d\u001c\u0017\t\b\u0002=\u0004?)+\t,\u0016\u0017\r,\u0016\u00178+\u0002\u000f\u001b\u000b\u0007\u0019'0Ê\u000f\u001d\u001c\u0017\t\b\u0002>\u000b\u001a\u0002\nB\u000e\u000f\u001b\r,\u0016\u00178+\u0002-\u0006/\u000f\u001b\r\b\u0002\n\t\n9L\u0016\u0017B4*\u0007\u0006\b\u0002C¥-\u0016I\u001cI\u001c\u0017*\u001a\t\b\r\b\u0006/\u000f\u0010\r\b\u0002\n\t-)4*\u0007\u0006.\u0007\u0011\u0016\u0017#-\u0004\u000e\u001c\u0017\u0002\n#-\u0002\n\u000b\u000e\r/\u000f\u001b\r,\u0016\u0017)+\u000bS9C},\r=\u0014\f)+\u000b\u001a\t,\u0016\u0017\t\b\r\b\t1)\u001b0#\u0015\u000f\u0010\u000b\u000e\"O\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\rNPQ%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007BO\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002\n\t\nl RODJ%\u001a\u0002\n\u0006\b\u0002.\u0002\u001b\u000f\u0010\u0014/%\u0003%&\u000f\u001b\t\b% \u0016\u0017\u000b\u001aB'\u0016\u0017\u000b 5\t\b\r/\u000f\u001b\u000b\u0007\u0014\f\u0002@\u0016\u0017\t.\u0019\u0007\u0002\n\t,\u0016\u0017B4\u000b\u0007\u0002\f\u0019[\r\b)O#C\u000f\u001b\u0004Y8+\u0002\n\u0014\f\r\b)+\u0006\b\t1\u0016\u0017\u000b\u000e\r\b)\u0003%&\u000f\u001b\t\b%Y84\u000f\u001d\u001c\u0017*\u001a\u0002\n\tC\t\b)O\r\b%&\u000f\u001b\rP\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006\bR\u00038+\u0002\f\u0014\n\r\b)+\u0006\b\t.\u000f\u001b\u0006\b\u0002-%&\u000f\u001b\t\b%\u0007\u0002\f\u0019V\u0016\u0017\u000b\u000e\r\b)u\r\b%\u0007\u0002-\t/\u000f\u001b#-\u0002-%?\u000f\u0010\t\b%\u000384\u000f\u001d\u001c\u0017*\u001a\u0002-DG\u0016\u0017\r\b%%\u000e\u0016\u0017B+%u\u0004\u0007\u0006\b)4\u001f&\u000f\u001b\u001f \u0016I\u001cI\u0016\u0017\r6\"+9;Xs\u000f\u0010\u0014/%@%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u001aB1\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\f\u0002>%&\u000f\u001b\t<\u0016\u0017\r\b\tG)\u001bD3\u000b-%&\u000f\u001b\t\b%C\r/\u000f\u001d5\u001f\u000e\u001c\u0017\u0002=\r\b);\t\b\r\b)+\u0006\b\u00023%?\u000f\u0010\t\b%C84\u000f\u001d\u001c\u0017*\u001a\u0002\n\t3\u000f\u0010\tsDG\u0002\u0013\u001cI\u001cn\u000f\u0010\ts\r\b%\u001a\u0002J\u0014\f)+\u0006\b\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u001aB-\u0004&)\u0010\u0016\u0017\u000b\u000e\r\b\u0002\f\u0006\b\t\r\b);)+\u0006,\u0016\u0017B\u001b\u0016\u0017\u000b&\u000f\n\u001cS8+\u0002\n\u0014\f\r\b)+\u0006\b\t\n9F}20&\r6DG)G8+\u0002\f\u0014\n\r\b)+\u0006\b\ts\u000f\u001b\u0006\b\u0002G\r\b\u0006\b* \u001c\u0017\">\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006\nl\u001a\r\b%\u0007\u0002/\u0016\u0017\u0006F%&\u000f\u001b\t\b%84\u000f\n\u001c\u0017*\u0007\u0002\n\t.\u000f\u001b\u0006\b\u0002\u001cI\u0016\u0017U4\u0002\u0013\u001c\u0017\"\u0003\r\b)C\u000f\u0010B4\u0006\b\u0002\n\u0002\u0016\u0017\u000bO#\u0015\u000f\u0010\u000b\u000e\"')\u001b0G\t\b*\u001a\u0014/%'\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\n\u0002\f\t\n9=m3%\u001a\u0002\n\u0006\b\u0002\u001350)+\u0006\b\u0002+l?DJ%\u001a\u0002\n\u000bODG\u0002.\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t>\u000f-A\u000e*\u001a\u0002\n\u0006\b\"@\u001c\u0017)\u000e)4U\u000e*\u0007\u0004\u000302\u0006\b)+# \r\b%\u001a\u00021%?\u000f\u0010\t\b%O\r/\u000f\u001b\u001f \u001c\u0017\u0002\f\t\nlDp\u0002@02)\u000e\u0014\n*\u001a\t')4\u000b[\r\b%\u001a)+\t\b\u0002O84\u0002\n\u0014\n\r\b)+\u0006\b\tC\r\b%?\u000f\u0010\rC#C\u000f\u001b\r\b\u0014\u0013%!\r\b%\u0007\u0002OA\u000e*\u001a\u0002\n\u0006\b\"\u0005)+\u000b!#C\u000f\u0010\u000b\u000e\"\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u000e\r>%&\u000f\u001b\t\b% \u0016\u0017\u000b\u001aB@\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002\n\t\n9m3%\u001a\u00023U+\u0002\n\"-\r\b)>\r\b%\u001a\u00023\u0019\u0007\u0002\f\t,\u0016\u0017B+\u000b-\u0016\u0017\tG\r\b)=Z?\u000b\u0007\u0019u\u000fJ06\u000f\u001b#1\u0016I\u001c\u0017\"-)\u00100%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007B.\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\n\u0002\f\t\t\b)C\r\b%?\u000f\u0010\ruP\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006\bRO84\u0002\n\u0014\n\r\b)+\u0006\b\t>\u0014\u001d\u000f\u0010\u000bO\u001f&\u0002-%?\u000f\u0010\t\b%\u001a\u0002\n\u0019O\u0016\u0017\u000b\u000e\r\b)'\r\b%\u001a\u00021\t/\u000f\u001b#-\u0002.%&\u000f\u001b\t\b%84\u000f\n\u001c\u0017*\u0007\u0002>DG\u0016\u0017\r\b%'%\u000e\u0016\u0017B+%u\u0004\u0007\u0006\b)4\u001f&\u000f\u001b\u001f \u0016I\u001cI\u0016\u0017\r6\"+9;},\u000b-)4*\u0007\u00063%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007BC\u0019\u001a\u0002\n\t,\u0016\u0017B+\u000bSl&\u0002\u001b\u000f\u001b\u0014\u0013%C\u0006/\u000f\nD84\u0002\n\u0014\n\r\b)4\u0006Z?\u0006\b\t\b\r>B+)\u000e\u0002\n\t.\r\b%\u001a\u0006\b)+*\u001aB+%!\u000f@\t,\u0016\u0017#-\u0004\u000e\u001c\u0017\u0002'\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c\u0017\u0016\u0017\r,\"+56\u0006\b\u0002\n\u0019?*\u0007\u0014\n\r,\u0016\u001e)+\u000b\u0006\b)4*\u0007\r,\u0016\u0017\u000b\u001a\u00023D3% \u0016\u0017\u0014/%1\u0004\u000e\u0016\u0017\u0014/U\u000e\ts\u000fq\u0006/\u000f\u001b\u000b\u0007\u0019\u001a)+#¼\t\b*\u0007\u001f\u001a\t\b\u0002\n\rF)\u00100\u001a\u0016\u0017\r\b\tF\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u001a\t\n9Gm3%\u0007\u0002\f\u000bLl\r\b%\u001a\u0002C\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\u0019!\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b\u0007\tN\u000f\u0010\u0006\b\u0002C\u000b\u0007)4\u0006\b#C\u000f\u001d\u001cI\u0016\u0017\f\u0002\n\u0019^y2\r\b)O\n\u0002\f\u0006\b)O#-\u0002\u001b\u000f\u0010\u000bÄ\u000f\u0010\u000b\u001a\u0019*\u001a\u000b\u000e\u0016\u0017\rG84\u000f\u0010\u0006,\u0016\u001e\u000f\u0010\u000b\u001a\u0014\n\u0002\u001b{s\u000f\u0010\u000b\u001a\u0019-\u0004?\u000f\u0010\t\b\t\b\u0002\n\u0019C\r\b%\u001a\u0006\b)+*\u001aB+%C\u000fG\u001c\u0017)\u001bDq56\u0006\b\u0002\n\t\b)\u001b\u001c\u0017*\u0007\r,\u0016\u0017)4\u000b'A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001b\u000f\n5\r,\u0016\u0017)4\u000b'B4\u0006,\u0016\u0017\u0019Ll\u001a\t\b).\r\b%&\u000f\u001b\rG\u0002\u001b\u000f\u0010\u0014/%C\u0019 \u0016\u0017#H\u0002\n\u000b\u0007\t,\u0016\u0017)4\u000b'\u0016\u0017\tGA\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\n\u0002\n\u0019\u0003\u000f\u001b\u000b\u0007\u0019C\u0014\n)4\u000b\u000e8+\u0002\n\u0006\b\r\b\u0002\f\u0019\u0016\u0017\u000b\u000e\r\b)u\u000f1\t\b#\u0015\u000f\u001d\u001cI\u001cF\u0016\u0017\u000b\u000e\r\b\u0002\nB4\u0002\n\u0006\n93L\u0016\u0017\u000b?\u000f\u001d\u001cI\u001c\u0017\"+l\r\b%\u001a\u0002>\u0006\b\u0002\n\t\b* \u001c\u0017\r,\u0016\u0017\u000b\u001aBO8+\u0002\f\u0014\n\r\b)+\u00063)\u00100F\u0016\u0017\u000b\u000e\r\b\u0002\fB+\u0002\n\u0006\b\t\u0016\u0017\tp\u0004&\u000f\u001b\t\b\t\b\u0002\n\u0019C\r\b%\u0007\u0006\b)4*\u0007B4%O\u000f=*\u001a\u000b \u0016\u001784\u0002\n\u0006\b\t/\u000f\u001d\u001c¢%&\u000f\u001b\t\b% \u0016\u0017\u000b\u001aB102*\u001a\u000b\u0007\u0014\f\r,\u0016\u0017)+\u000b-\u0016\u0017\u000bCDJ%\u000e\u0016\u0017\u0014/%'\u0002\u001b\u000f\u001b\u0014\u0013%\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u0005\u0016\u0017\t1#.*\u000e\u001c\u0017\r,\u0016\u0017\u0004 \u001cI\u0016\u0017\u0002\n\u0019!\u001f\u000e\"\u0003\u000f@\u0006/\u000f\u001b\u000b\u0007\u0019\u001a)+# DG\u0002\u0013\u0016\u0017B4%\u000e\r-\u000f\u001b\u000b\u0007\u0019\u0018\r\b%\u0007\u0002\u0013\u0016\u0017\u0006.\t\b*\u001a#\u0016\u0017\t;\r/\u000f\u0010U4\u0002\n\u000b'\r\b)10)+\u0006\b#\r\b%\u001a\u0002=Z?\u000b&\u000f\n\u001cF%?\u000f\u0010\t\b%O84\u000f\u001d\u001c\u0017*\u001a\u0002+l&#H)\u000e\u0019\u0007*\u000e\u001c\u0017)'\r\b%\u001a\u0002>\u000b\u000e*\u0007#.\u001f&\u0002\n\u00063)\u001b0%?\u000f\u0010\t\b%O\u001f\u001a*\u001a\u0014\u0013U4\u0002\n\r\b\t\n93\u001cI\u001c\u001a\r\b%\u0007\u0002s\u0006/\u000f\u0010\u000b\u001a\u0019\u001a)+#\u0011\u0004?\u000f\u0010\u0006/\u000f\u001b#-\u0002\n\r\b\u0002\n\u0006\b\tsy2\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b-\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t\nl\u000eA\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001d\u000f\u0010\r,\u0016\u0017)+\u000bB4\u0006,\u0016\u0017\u00197\u001cI\u0016\u0017\u000b\u001a\u0002\n\t\nls%&\u000f\u001b\t\b% \u0016\u0017\u000b\u001aB7Dp\u0002\u0013\u0016\u0017B+%\u000e\r\b\t/{>\u000f\u001b\u0006\b\u0002-\u0004\u0007\u0006\b\u0002/5,B4\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u0010\r\b\u0002\n\u0019Ä\u000f\u0010\u000b\u001a\u00197Z&(\u000e\u0002\f\u0019V02)4\u0006TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\nr\ns\n1 7 234568910123457\n68fitted linemost out-lying pointñGòÊó?ôFõ\u0010öoïLøqx\u001e\n\t\n2ô\u0001\u0000\nü\nõ<\u000b\u001aü\nò2ÿ&û\u0005ÿ\u0003\u0002sð<ò2ûFö\u000f\u000b\u0007õ\u001bò2ü\r\f\u0005ñGò\u0007\n2ü\nö\u000eõ\u001bò2ûFó\u0002\u001b\u000f\u001b\u0014\u0013%C%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007B-\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002+l?\u001f\u0007*\u001a\rG84\u000f\u0010\u0006\b\"C\u000f\u001b\u0014\n\u0006\b)+\t\b\tp\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\rG\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\n\u0002\f\t\n93}Ê0\r6DG)=\u0006/\u000f\nDY84\u0002\n\u0014\n\r\b)+\u0006\b\ts\u000f\u001b\u0006\b\u0002.PQ\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006\nl R1\u001629 \u000249gl4DG\u0016\u0017\r\b%-\u000f3% \u0016\u0017B4%-\u0014\n)+\u0006\b\u0006\b\u0002/\u001c\u001e\u000f\u0010\r,\u0016\u0017)+\u000bC\u0014\n)\u001b5\u0002\u0013\u0012\u0015\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\rGDG\u0016\u0017\r\b%C\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\u0019C\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u001a\t\nl&\u0016\u0017\rG\u0014\u001b\u000f\u0010\u000bH\u001f&\u0002=\t\b%\u001a)\u001bDJ\u000bH\r\b%&\u000f\u001b\r<\r\b%\u001a\u0002\n\"#.*\u0007\t\b\rH\u001f&\u0002C\u0014\u0013\u001c\u0017)+\t\b\u0002u\r\b)7\u0002\u001b\u000f\u001b\u0014\u0013%\u0018)+\r\b%\u001a\u0002\n\u00061\u0016\u0017\u000b!X«*\u001a\u0014\u0013\u001cI\u0016\u0017\u0019\u001a\u0002\u001b\u000f\u0010\u000bE\t\b\u0004?\u000f\u0010\u0014\f\u0002'DG\u0016\u0017\r\b%\u0005\r\b%\u0007\u0002\n\t\b\u0002\t/\u000f\u001b#-\u0004 \u001c\u0017\u0002\f\u00197\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u001a\t.\u000f\u001d02\r\b\u0002\f\u0006J\u000b\u001a)+\u0006\b#\u0015\u000f\u001d\u001cI\u0016\u0017\u001b\u000f\u001b\r,\u0016\u0017)+\u000b[£ ¤ ¦/§,l\u001a\t\b)-\r\b%\u001a\u0002\n\"C%&\u000f\n84\u0002-\u000fB4)\u000e)\u000e\u0019O\u0014\u0013%?\u000f\u0010\u000b\u001a\u0014\n\u0002>)\u00100s\u001f?\u0002\u0013\u0016\u0017\u000b\u0007BC#C\u000f\u001b\u0004\u0007\u0004?\u0002\n\u0019O\r\b)1\r\b%\u001a\u0002>\t/\u000f\u0010#H\u0002>\u0004&)\u0010\u0016\u0017\u000b\u000e\r\u000f\u001d02\r\b\u0002\n\u0006pA\u000e*&\u000f\u001b\u000b 5\r,\u0016\u0017\u001b\u000f\u001b\r,\u0016\u0017)+\u000bS93TJ0¢\u0014\n)+*\u001a\u0006\b\t\b\u0002=\r\b%\u001a\u0002\n\"-#C\u000f\n\"C\u000f\n\u001c\u0017\t\b)-\u001f&\u0002;*\u0007\u000b\u000e\u001c\u0017*\u0007\u0014/U\u000e\"O\u000f\u001b\u000b\u0007\u0019u%&\u000f\u001b\u0004\u0007\u0004?\u0002\n\u000b'\r\b)\u001cI\u0016\u0017\u0002\u0003\u000f\u001b\u0014\n\u0006\b)+\t\b\t.\r\b%\u0007\u0002uA\u000e*&\u000f\u001b\u000b\u000e\r,\u0016\u0017\u001b\u000f\u0010\r,\u0016\u0017)4\u000bEB+\u0006,\u0016\u0017\u0019Y\u001cI\u0016\u0017\u000b\u001a\u0002\n\tC\u0002\n8+\u0002\n\u000b\u0005\r\b%\u001a)+*\u001aB+%\u0005\r\b%\u001a\u0002\n\"\u0005\u000f\u0010\u0006\b\u0002\u0014\u0013\u001c\u0017)4\t\b\u0002C\r\b)@\u0002\u001d\u000f\u0010\u0014/%7)4\r\b%\u0007\u0002\n\u0006\n9.}6\u000bO\t\b*\u0007\u0014/%7\u0014\u001d\u000f\u0010\t\b\u0002\n\t>\r\b%\u001a\u0002\n\"ODG\u0016I\u001cI\u001cG\u000b\u001a)+\r>\u001f?\u0002-#C\u000f\u0010\u0004\u001a\u0004&\u0002\n\u0019\r\b).\r\b%\u0007\u0002;\t/\u000f\u0010#-\u0002>%?\u000f\u0010\t\b%u84\u000f\u001d\u001c\u0017*\u001a\u0002+9pJ)\u001bDG\u0002\n84\u0002\n\u0006\nl\u001a\t,\u0016\u0017\u000b\u0007\u0014\n\u0002>Dp\u0002=%?\u000f\u001d84\u0002>#C\u000f\u0010\u000b\u000e\"H%&\u000f\u001b\t\b% 5\u0016\u0017\u000b\u001aB-\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\n\u0002\f\t>\u0006\b*\u0007\u000b\u001a\u000b \u0016\u0017\u000b\u001aB'\u0016\u0017\u000bO\u0004?\u000f\u0010\u0006/\u000f\u001d\u001cI\u001c\u0017\u0002/\u001c2lM\r\b%\u001a\u0002\n\u0006\b\u0002>\u0016\u0017\t>\u000f1%\u000e\u0016\u0017B+%\u0003\u0004\u0007\u0006\b)4\u001f&\u000f\u001b\u001f \u0016I\u001cI\u0016\u0017\r6\"\r\b%?\u000f\u0010\rF\r\b%\u001a\u0002\n\".DG)+*\u000e\u001c\u0017\u0019-\u001f&\u0002G#\u0015\u000f\u0010\u0004\u001a\u0004&\u0002\n\u0019-\r\b);\r\b%\u0007\u00023\t/\u000f\u0010#H\u0002G%&\u000f\u001b\t\b%-84\u000f\u001d\u001c\u0017*\u001a\u0002G\u0016\u0017\u000b-\t\b\u0002\n8+\u0002\f\u0006/\u000f\u001d\u001c\t\b*\u001a\u0014\u0013%O\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002\n\t\n9u\u000e*\u0007\u0014/%7\u0004\u001a\u0006\b)+\u001f?\u000f\u0010\u001f\u000e\u0016I\u001cI\u0016\u001e\r,\u0016\u0017\u0002\n\tC\u0014\u001b\u000f\u0010\u000b\u001a\u000b\u0007)4\r1\u001f?\u0002-A\u000e*&\u000f\u001b\u000b\u000e\r,\u0016\u0017\r/\u000f\u0010\r,\u0016\u001784\u0002\u0013\u001c\u0017\"\u000f\u001b\u000b&\u000f\n\u001c\u0017\"\u000e\n\u0002\n\u0019\u0011DG\u0016\u0017\r\b%\u001a)+*\u001a\rO\u000f\u0010\u000bE\u0002\f(\u001a\u000f\u0010\u0014\n\ru#C\u000f\u0010\r\b%\u001a\u0002\n#C\u000f\u001b\r,\u0016\u0017\u0014\u001b\u000f\u001d\u001c.\u0019\u001a\u0002\u0013Z?\u000b \u0016\u0017\r,\u0016\u0017)4\u000b)\u001b0.\r\b%\u0007\u0002\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"!#-\u0002\u001b\u000f\u0010\t\b*\u001a\u0006\b\u0002+9u3)\u0010Dp\u0002\n8+\u0002\n\u0006\nl¢\r\b%\u0007\u0002C\u000f\u001b\u001f&)\u001b8+\u0002u\u0006\b\u0002\u001b\u000f\u0010\t\b)4\u000b \u0016\u0017\u000b\u001aB7\t\b*\u001aB+B4\u0002\n\t\b\r\b\t\r\b%?\u000f\u0010\r-\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"E\u001f?\u0002\n\r,DG\u0002\f\u0002\n\u000b\u0005\u0006/\u000f\u001dDr8+\u0002\n\u0014\f\r\b)+\u0006\b\t-\u0014\u001b\u000f\u001b\u000b[\u001f&\u0002u\u0006\b\u0002\n\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\u0002\n\u0019!\u001f\u000e\"\r\b%\u001a\u0002C\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006.)\u00100;%?\u000f\u0010\t\b%\u000e\u0016\u0017\u000b\u0007BO\u0016\u0017\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\f\u0002\n\tC\r\b%&\u000f\u001b\r1\r\b%\u001a\u0002\n\"\u0003#C\u000f\u0010\r\b\u0014/%V\u0016\u0017\u000b\u0005\r\b\u0002\n\u0006\b#H\t)\u001b0s%&\u000f\u001b\t\b%O8+\u000f\n\u001c\u0017*\u0007\u0002\f\t\n9}6\u000b')+*\u001a\u0006=\u0002\f(\u000e\u0004&\u0002\n\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\t=\u0016\u0017\u000b\u0003\u000e\u0002\n\u0014\n\r,\u0016\u0017)4\u000b7¥\u001al&DG\u0002;DG\u0016I\u001cI\u001cs\t\b\r\b*\u0007\u0019\u001a\"'\r\b%\u001a\u0002>\u0002\u0013tS\u0002\n\u0014\f\r\b\t=)\u00100\u000f\u001b\u0019\u0010\b*\u001a\t\b\r,\u0016\u0017\u000b\u001aBY\r\b%\u0007\u0002H\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006-)\u001b0;\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b[\t/\u000f\u0010#H\u0004 \u001c\u0017\u0002\n\t\nlp\u000f\u001b\t1DG\u0002/\u001cI\u001c;\u000f\u0010\t.\r\b%\u0007\u0002A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001b\u000f\u001b\r,\u0016\u0017)+\u000bYB4\u0006,\u0016\u0017\u0019O\t,\u0016\u0017\n\u0002+9\n3.3Alignment AnalysisXs\u000f\u0010\u0014/%C\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014.\t\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\u0002>\u0006\b\u0002\f\u0004\u0007\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r\b\t=\u000f>\t\b%\u001a)+\u0006\b\rG\t\b\u0002\nB4#-\u0002\n\u000b\u000e\rG)\u001b0s\u000f#.*\u0007\t,\u0016\u0017\u0014C\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002+9HmF)-Z&\u000b\u001a\u0019Y\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006-#1*\u001a\t,\u0016\u0017\u0014CB\u0010\u0016\u001784\u0002\n\u000b\u0005\u000f-A\u000e*\u0007\u0002\f\u0006\b\"+lFDG\u0002.\u001f\u0007\u0006\b\u0002\u001d\u000f\u0010U\u0019\u001a)\u001bDJ\u000b!\r\b%\u0007\u0002OA\u000e*\u001a\u0002\n\u0006\b\"\u0005\u0016\u0017\u000b\u000e\r\b)[\u000f\u0003\t\b\u0002\n\r-)\u00100.\u0014/%?\u000f\u0010\u0006/\u000f\u0010\u0014\f\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014Y\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\tO\u000f\u001b\u000b\u0007\u0019\u0004&\u0002\f\u0006,02)+\u0006\b# \u000f\u0010\u000bO\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n(\u0003\u001c\u0017)\u000e)+U\u000e*\u001a\u0004L9NX<\u000f\u001b\u0014\u0013%\u0003\u001c\u0017)\u000e)+U\u000e*\u001a\u0004w#\u0015\u000f\u001d\"OB4\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u0010\r\b\u0002u\u000f@\t\b\u0002\f\r)\u001b03#C\u000f\u0010\r\b\u0014/%\u001a\u0002\n\t\n9OT=*\u001a\u0006>\r/\u000f\u0010\t\bUO\u0016\u0017\u000bY\r\b%\u000e\u0016\u0017\t1\t\b\r\b\u0002\f\u00047\u0016\u0017\t1\r\b)C\u0016\u0017\u000b\u000e\r\b\u0002\n\u0006\b\u0004\u001a\u0006\b\u0002\n\r1\r\b%\u001a\u0002C\t\b\u0002\n\r>)\u00100\t\b%\u001a)+\u0006\b\ru\t\b\u0002\nB+#-\u0002\f\u000b\u000e\r'#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\t'\r\b)\u0018\u0019\u0007\u0002\n\r\b\u0002\f\u0006\b#1\u0016\u0017\u000b\u0007\u0002\u0003D3% \u0016\u0017\u0014/%W#1*\u001a\t,\u0016\u0017\u00147\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002O\u0016\u0017\t\r\b%\u001a\u0002\u0003PQB\u001b\u001c\u0017)+\u001f?\u000f\u001d\u001c\u0017R\u0003#C\u000f\u0010\r\b\u0014/%S9>m3%\u0007\u0002.U+\u0002\n\"'0Ê\u000f\u0010\u0014\n\r;\r\b)'\u001f&\u0002.*\u0007\t\b\u0002\f\u0019Y%\u0007\u0002\f\u0006\b\u0002\u0016\u0017\t>\r\b%?\u000f\u0010\r.\u000fB\u001b\u001c\u0017)+\u001f?\u000f\u001d\u001cF#\u0015\u000f\u0010\r\b\u0014/%@#.*\u001a\t\b\rG%&\u000f\n84\u0002.\u000f\t\b\u0002\n\rp)\u00100\t\b%\u0007)4\u0006\b\r3\t\b\u0002\nB+#-\u0002\f\u000b\u000e\rG#C\u000f\u0010\r\b\u0014/%\u001a\u0002\n\t3\r\b%&\u000f\u001b\r\u000f\u001b\u0006\b\u0002>DG\u0002\u0013\u001cI\u001cs\u000f\n\u001cI\u0016\u0017B+\u000b\u001a\u0002\n\u0019SKM\r\b\u0002\n#H\u0004&)-\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\t>#1*\u001a\t\b\r3\u001f&\u0002>*\u001a\u000b \u0016I02)4\u0006\b# \u0016\u0017\u000bO\r,\u0016\u0017#H\u0002+9Xs\u000f\u0010\u0014/%C#C\u000f\u0010\r\b\u0014/%C\u001f&\u0002\n\r,Dp\u0002\n\u0002\n\u000bC\r,DG)>#.*\u001a\t,\u0016\u0017\u0014=\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\t\u0005Á=\u000f\u001b\u000b\u0007\u0019oñJ\u0014\n)4\u000b\u000e\r/\u000f\u001d\u0016\u0017\u000b\u001a\t;\u000f\r\b*\u000e5\u0004\u000e\u001c\u0017\u0002Oy2A\u000e*\u0007\u0002\n\u0006\b\"+56)\u001btS\t\b\u0002\n\r\nlF#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aB\u001056)\u0010tS\t\b\u0002\f\r/{1D3% \u0016\u0017\u0014/%7\u0016\u0017\u000b\u001a\u0019 \u0016\u0017\u0014\u001b\u000f\u001b\r\b\u0002\n\t-\r,\u0016\u0017#-\u0002C)\u001bt&5\t\b\u0002\n\r\b\tu)\u00100.\r\b%\u001a\u00027\r6DG)Y#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aB[\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t\n9}20.DG\u0002O\u0004 \u001c\u0017)4\r\u0003\u000fY¤\u001d5~B+\u0006/\u000f\u001b\u0004\u0007%DG\u0016\u0017\r\b%C\r\b%\u0007\u00023#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aB-\r,\u0016\u0017#-\u0002=\u0004?)\u0010\u0016\u0017\u000b\u000e\r\b\t3)\u00100XÁJ)4\u000b-\r\b%\u001a\u0002=%\u001a)+\u0006,\u0016\u0017\n)4\u000b\u000e\r/\u000f\u001d\u001cs\u000f\u001b(+\u0016\u0017\tG8\u000e\t\n9\r\b%\u001a\u00021\u0014\f)+\u0006\b\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u001aBO\u0004&)\u0010\u0016\u0017\u000b\u000e\r\b\t>)\u001b0~ñ1)+\u000bu\r\b%\u0007\u0002.8+\u0002\n\u0006\b\r,\u0016\u0017\u0014\u001d\u000f\u001d\u001cp\u000f\u0010(+\u0016\u0017\t\nl\u000f\u0003P\bB+)\u000e)\u000e\u0019\u001aR#C\u000f\u001b\r\b\u0014\u0013%!DG)+*\u000e\u001c\u0017\u0019E\u0014\n)4\u000b\u000e\r/\u000f\u001d\u0016\u0017\u000b¼\u000f7\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B4%\u000e\r@\u001cI\u0016\u0017\u000b\u001a\u00027D3%\u000e\u0016I\u001c\u0017\u0002\u0005\u000fEPQ\u001f?\u000f\u0010\u0019\u001aR\u0005#C\u000f\u0010\r\b\u0014/%DG)4* \u001c\u0017\u0019\u0003\u000b\u001a)+\r\n9;\u0001w\u0016\u0017\r\b%\u001a)+*\u001a\r>\r\b\u0002\n#-\u0004&)C\u0014/%&\u000f\u001b\u000b\u0007B4\u0002+lM\r\b%\u001a\u0002.\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B+%\u000e\r=\u001cI\u0016\u0017\u000b\u001a\u0002-\t\b%\u001a)+*\u000e\u001c\u0017\u0019\u001f&\u0002u\u000f\u001b\r-\u000f'¥\u001d56\u0019\u0007\u0002\fB+\u0006\b\u0002\n\u0002N\u000f\u0010\u000b\u001aB\u0010\u001c\u0017\u0002+9\u0003\u0001w\u0016\u0017\r\b%!\u0004&)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\u0002O\r\b\u0002\n#-\u0004?)O\u0014\u0013%?\u000f\u0010\u000b\u001aB+\u0002+lp\r\b%\u0007\u0002\u001cI\u0016\u0017\u000b\u001a\u0002=DG\u0016I\u001cI\u001cS\u001f&\u0002q\u000f\u0010\rs\u000f=\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\rp\u000f\u0010\u000b\u001aB\u0010\u001c\u0017\u0002+l\u001a\u001f\u001a*\u0007\rF\u0016\u0017\rF\t\b\r,\u0016I\u001cI\u001c\t\b%\u0007)4* \u001c\u0017\u0019C\u001f&\u0002p\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B+%\u000e\r\n9\u0001\u0003\u0002-\u0019\u000e\u0016\u0017\t\b\u0014\n*\u0007\t\b\tH\r,DG)u\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\r1#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\t.\r\b)@Z?\u000b\u0007\u0019Y)4*\u0007\r>%\u001a)\u001bDDG\u0002\u0013\u001cI\u001cp\r,DG)\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\t;#C\u000f\u0010\r\b\u0014/%S¨3n\u0016\u0017\u000b\u001a\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\"OL\u0016I\u001c\u0017\r\b\u0002\f\u0006,\u0016\u0017\u000b\u0007B\u0003\u000f\u001b\u000b\u0007\u0019NJ)+*\u001aB+%OmF\u0006/\u000f\u001b\u000b\u0007\t,0)+\u0006\b#'9\n3.3.1Linearity Filtering&\u0016\u0017\u000b\u0007\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\" L\u0016I\u001c\u0017\r\b\u0002\n\u0006,\u0016\u0017\u000b\u0007B\u0005\u0016\u0017\t-\u0016I\u001cI\u001c\u0017*\u0007\t\b\r\b\u0006/\u000f\u001b\r\b\u0002\n\u0019\u0016\u0017\u000b L\u0016\u0017B+*\u001a\u0006\b\u0002\u0003 9!\u00017\u0002u\u0002\n(\u001a\u000f\u0010#.\u0016\u0017\u000b\u0007\u0002\r\b%\u001a\u0002>¤\n5\b~EB+\u0006/\u000f\u001b\u0004\u0007%u)\u00100¢#C\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB-\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t\nl\u0007Z?\r3\u000f>\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B+%\u000e\rG\u001cI\u0016\u0017\u000b\u001a\u00021\r\b%\u001a\u0006\b)+*\u001aB+%r\ns\n1 7 234568910123457\n68\nb\n234\n1\na\n-4-3-2-10 1 2 3r = a s + b\n1 = 3a + bs=3, r=1(3,1)\n1 = 3a + b\nwinner:a=1, b=-2\n( Hidden line equation:  r = s - 2 )vote forñGò2ó&ôFõ\u001böòLøóx\u001e\n\t\nÊô\u0001\u0000\fü\nõ<\u000b\u0007ü\nò2ÿ&û\u0005ÿ\u0003\u0002óô'ÿ&ôFó*éõ<õ<\u000b\u001aû\u0001\u0000\u001e\u0002\bÿ&õ<\u0004\r\b%\u001a\u0002O\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\tVy2*\u001a\t,\u0016\u0017\u000b\u001aBÄ\u001c\u0017\u0002\u001d\u000f\u0010\t\b\rC#-\u0002\u001b\u000f\u001b\u000b 56\t\bA\u000e*?\u000f\u0010\u0006\b\u0002\u0003\u0014\n\u0006,\u0016\u0017\r\b\u0002\n\u0006,\u0016\u001e\u000f+{\u0013l;\u000f\u001b\u000b\u0007\u0019!\u0014\u0013%\u001a\u0002\n\u0014/UY\u0016I0\u000f\u001b\u000b\u000e\"'\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t=06\u000f\u001d\u001cI\u001c¢\r\b)\u000e)106\u000f\u001b\u0006=\u000f\nD3\u000f\n\"C02\u0006\b)+# \r\b%\u001a\u0002=\u001cI\u0016\u0017\u000b\u0007\u0002493}Ê0s\t\b)\u0007l?\u0006\b\u0002\n#-)\u001b8+\u0002.\r\b%\u001a\u0002#H)+\t\b\r3)+*\u001a\r,52\u001c\u0017\"+\u0016\u0017\u000b\u0007Bu\u0004&)\u0010\u0016\u0017\u000b\u000e\r.\u000f\u001b\u000b\u0007\u0019CZ&\r;\u000f1\u000b\u001a\u0002\nDE\u001cI\u0016\u0017\u000b\u0007\u0002-\r\b%\u001a\u0006\b)+*\u001aB+%O\r\b%\u001a\u0002>\u0006\b\u0002\n#C\u000f\n\u0016\u0017\u000b 5\u0016\u0017\u000b\u001aBC\u0004&)\u0010\u0016\u0017\u000b\u000e\r\b\t\n9;oG\u0002\n\u0004?\u0002\u001b\u000f\u0010\r;\r\b%\u0007\u0002>\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t;*\u0007\u000b\u000e\r,\u0016I\u001cp\u000f\n\u001cI\u001cF\u0006\b\u0002\n#C\u000f\n\u0016\u0017\u000b \u0016\u0017\u000b\u001aB'\u0004?)\u0010\u0016\u0017\u000b\u000e\r\b\t=\u001cI\u0016\u0017\u0002DG\u0016\u0017\r\b%\u000e\u0016\u0017\u000b\u0003\u000f\t\b#\u0015\u000f\u001d\u001cI\u001c\u000b\u0007\u0002/\u0016\u0017B+%\u000e\u001f&)4\u0006\b%\u0007)\u000e)\u000e\u0019\u0003)\u00100¢\r\b%\u0007\u00023Z&\r\b\r\b\u0002\n\u0019C\u001cI\u0016\u0017\u000b\u0007\u000249=y2},\u000bC\r\b%\u001a\u0002=DG)4\u0006\b\t\b\r\u0014\u001d\u000f\u0010\t\b\u0002+l?)+\u000b\u000e\u001c\u0017\"'\r,Dp)1\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t.\u000f\u0010\u0006\b\u0002=\u001c\u0017\u0002\u00130\r=\u000f\u001b\rG\r\b%\u001a\u0002>\u0002\n\u000b\u0007\u0019S93:s*\u0007\rG\u0016\u0017\u000b'\u0004\u001a\u0006/\u000f\u0010\u0014\f\r,\u0016\u0017\u0014\n\u00021Dp\u0002\t\b\r\b)4\u0004OD3%\u0007\u0002\n\u000bu\r\b)\u000e)102\u0002\nDE\u0004?)\u0010\u0016\u0017\u000b\u000e\r\b\t>\u0006\b\u0002\n#C\u000f\n\u0016\u0017\u000bL9 {m3%\u001a\u0002=\r\b)4\r/\u000f\u001d\u001c\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006p)\u00100F#\u0015\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB-\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t=\u000f\n02\r\b\u0002\n\u0006G\r\b%\u000e\u0016\u0017\tGZ\u0007\u001c\u0017\r\b\u0002\f\u0006,\u0016\u0017\u000b\u0007BC\t\b\r\b\u0002\n\u0004\u0016\u0017\t;\r/\u000f\u0010U4\u0002\n\u000bO\u000f\u001b\t>\u000f\u0010\u000bC\u0016\u0017\u000b\u0007\u0019\u000e\u0016\u0017\u0014\u001b\u000f\u001b\r\b)+\u0006>)\u00100s%\u001a)\u001bD^Dp\u0002\u0013\u001cI\u001cF\r6DG)1\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\f\t=#C\u000f\u001b\r\b\u0014\u0013%S9\n3.3.2HoughTransform3)+*\u001aB+%-mF\u0006/\u000f\u001b\u000b\u0007\t,0)+\u0006\b#E\u0016\u0017\tG\u000fG\u0014\n)4#-\u0004\u0007*\u001a\r\b\u0002\n\u0006s8+\u0016\u0017\t,\u0016\u0017)+\u000b-\r\b\u0002\f\u0014\u0013%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002=\r\b%?\u000f\u0010\rM\u001c\u0017)\u000e\u0014\u001d\u000f\u0010\r\b\u0002\n\tP\b% \u0016\u0017\u0019\u001a\u0019\u001a\u0002\n\u000b\u0007RC\u001cI\u0016\u0017\u000b\u0007\u0002\f\t>)+\u00063\t\b%&\u000f\u001b\u0004&\u0002\n\t302\u0006\b)+# \u000f\u0010\u000b@\u0016\u0017#\u0015\u000f\u0010B+\u0002C£\u0017¦\u001b\u001d§69F\u0001\u0003\u0002>*\u0007\t\b\u0002=\u0016\u0017\r;%\u0007\u0002\f\u0006\b\u0002\r\b);%\u0007\u0002/\u001c\u0017\u0004-*\u0007\tM\u001c\u0017)\u000e\u0014\u001d\u000f\u0010\r\b\u00023\r\b%\u0007\u0002p\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B+%\u000e\rF\u001cI\u0016\u0017\u000b\u001a\u0002=%\u000e\u0016\u0017\u0019\u0007\u0019\u001a\u0002\n\u000b-\u0016\u0017\u000b1\r\b%\u001a\u00023¤\n5\b~\u0005#C\u000f\u0010\r\b\u0014/%\u000e\u0016\u0017\u000b\u0007B\u0004\u000e\u001c\u0017)+\r\n9}6\u000bCL\u0016\u0017B+*\u001a\u0006\b\u0002. l\r\b%\u0007\u00023\r\b)+\u0004HB+\u0006/\u000f\u0010\u0004\u001a%C\t\b%\u0007)\u001bD3\t3\u000f=\u0004&)4\t\b\t,\u0016\u0017\u001f \u001c\u0017\u0002>\u0004\u000e\u001c\u0017)+\rpDq\u0016\u0017\r\b%C#C\u000f\u001b\r\b\u0014\u0013%\u000e5\u0016\u0017\u000b\u001aB\u0005\u0004&)\u0010\u0016\u0017\u000b\u000e\r\b\t\n9Wm3%\u001a\u0002'\u001c\u0017)+\u000b\u001aB+\u0002\n\t\b\rÄP\u001cI\u0016\u0017\u000b\u001a\u0002\nREy2\t\b\r/\u000f\u0010\u0006\b\r,\u0016\u0017\u000b\u001aB[\u000f\u001b\rOy6K\n¦\u001d{@\u000f\u001b\rC\u000f7¥\u001d5\u0019\u001a\u0002\nB4\u0006\b\u0002\n\u0002C\u000f\u0010\u000b\u001aB\u0010\u001c\u0017\u0002\u001d{=\u0014\n)+\u0006\b\u0006\b\u0002\n\t\b\u0004?)+\u000b\u001a\u0019\u0007\t.\r\b)-\r\b%\u001a\u0002-\u000f\u001b\u0014\n\r\b*&\u000f\n\u001cG#\u0015\u000f\u0010\r\b\u0014/%LlSD3%\u000e\u0016I\u001c\u0017\u0002-\u0004&)+\t\b\t,\u0016I5\u001f\u000e\u001c\u0017\u0002'\t\b%\u001a)+\u0006\b\r\b\u0002\f\u0006\u001cI\u0016\u0017\u000b\u001a\u0002\n\t'y2\t\b*\u001a\u0014\u0013%!\u000f\u001b\t1\r\b%\u001a\u0002-\u0004?\u000f\u0010\u0006/\u000f\u001d\u001cI\u001c\u0017\u0002/\u001c>)+\u000b\u001a\u0002C\r\b)'\r\b%\u001a\u0002C\u0006,\u0016\u0017B+%\u000e\r/{-\u000f\u001b\u0006\b\u0002\u0006\b\u0002\f\t\b* \u001c\u0017\r\b\t\u0003)\u00100H\u0006\b\u0002\n\u0004&\u0002\u001b\u000f\u001b\r\b\u0002\n\u0019W\u0004&\u000f\u001b\r\b\r\b\u0002\n\u0006\b\u000b\u0007\tO\u0014\f)+#-#-)4\u000b\u0016\u0017\u000b¼\u0006\b\u0002\u001b\u000f\u001d\u001cI56DG)4\u0006,\u001c\u0017\u0019^#.*\u0007\t,\u0016\u0017\u001449T=\r\b%\u001a\u0002\n\u00063\t\b\u0014\u001b\u000f\u001b\r\b\r\b\u0002\n\u0006\b\u0002\n\u0019O\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t.\u000f\u0010\u0006\b\u0002>\u0019\u001a*\u0007\u0002>\r\b)-\u0002\f\u0006\b\u0006\b)+\u0006=)4\u00063\u0006/\u000f\u0010\u000b\u001a\u0019\u0007)4# #C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\t\n9Xs\u000f\u0010\u0014/%\u0003\u0004&)\u0010\u0016\u0017\u000b\u000e\r>%?\u000f\u0010\t.\u000f\u001b\u000b\u0003\u000f\u0010\t\b\t\b)\u000e\u0014\u0013\u0016\u001e\u000f\u001b\r\b\u0002\n\u0019YDG\u0002\u0013\u0016\u0017B4%\u000e\r\nlMD3%\u000e\u0016\u0017\u0014\u0013%O\u0016\u0017\u000b\u001a\u0019 \u0016\u0017\u0014\u001b\u000f\u001b\r\b\u0002\n\t.\u0014\n)+\u000b\u000eZ\u00075\u0019\u001a\u0002\n\u000b\u001a\u0014\n\u0002.)\u00100s\r\b%\u001a\u0002>#C\u000f\u0010\r\b\u0014/%7y2)4\u001f\u0007\r/\u000f\n\u0016\u0017\u000b\u0007\u0002\n\u0019O02\u0006\b)4# \r\b%\u0007\u0002>\u0004\u001a\u0006\b\u0002\n8+\u0016\u0017)+*\u001a\t>\t\b\r\b\u0002\n\u0004?{\u00139TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental StudymF)1Z?\u000b\u001a\u00197\r\b%\u001a\u0002>% \u0016\u0017\u0019\u001a\u0019\u001a\u0002\n\u000bY\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B4%\u000e\r=\u001cI\u0016\u0017\u000b\u0007\u0002.02\u0006\b)+# \r\b%\u001a\u00021#\u0015\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB'\u0004\u000e\u001c\u0017)+\r\nlSDG\u0002#-)\u000e\u0019\u001a\u0002\u0013\u001cp\r\b%\u0007\u0002H% \u0016\u0017\u0019\u001a\u0019\u0007\u0002\f\u000bw\t\b\r\b\u0006/\u000f\n\u0016\u0017B+%\u000e\r>\u001cI\u0016\u0017\u000b\u0007\u0002u\u001f\u000e\"O\r\b%\u0007\u0002H\u0002\nA\u000e*&\u000f\u001b\r,\u0016\u0017)+\u000béñ­÷ö\u000fÁkø«ù\nlD3%\u001a\u0002\n\u0006\b\u0002ñ7\u000f\u001b\u000b\u0007\u0019yÁ7\u000f\u001b\u0006\b\u0002'\r\b%\u001a\u0002O#C\u000f\u0010\r\b\u0014/%\u000e\u0016\u0017\u000b\u0007B\u0005\r,\u0016\u0017#-\u0002\u0003)\u0010tS\t\b\u0002\f\r@84\u000f\n\u001c\u0017*\u0007\u0002\n\tu)\u00100\r,DG)#.*\u0007\t,\u0016\u0017\u0014\u0003\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\n\t\n9Em3%\u001a\u0002O84\u000f\u001d\u001c\u0017*\u001a\u0002\n\tC)\u00100wöY\u000f\u001b\u000b\u0007\u0019yù'\u000f\u0010\u0006\b\u0002O\u0002\f\t\b\r,\u0016\u0017#C\u000f\u0010\r\b\u0002\n\u0019\u0005\u0016\u0017\u000bE\r\b%\u0007\u000202)\u001b\u001cI\u001c\u0017)\u001bDq\u0016\u0017\u000b\u001aBYD3\u000f\n\"&¨H\u0002\u001b\u000f\u0010\u0014/%Y#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u0007B\u0003\r\b*\u001a\u0004 \u001c\u0017\u0002\u0003y\u001cÁIú\u000f\rñ\rú\u001b{84)+\r\b\u0002\n\tCy2DG\u0016\u0017\r\b%7\u0016\u0017\r\b\t)\u001bD3\u000b>DG\u0002\u0013\u0016\u0017B4%\u000e\r/{L02)+\u0006¢\u000fJ\t\b\u0002\n\r)\u00100?\u0004&)+\t\b\t,\u0016\u0017\u001f\u000e\u001c\u0017\u00021y\u001cö\u0003\rù/{M84\u000f\u001d\u001c\u0017*\u001a\u0002\n\tF\r\b%?\u000f\u0010\rF\t/\u000f\u0010\r,\u0016\u0017\t,0\"qñ\u001eú\u0005ö\u000fÁ\u0013úMøù\u001d9s},\u000b.\r\b%\u000e\u0016\u0017\tG\u0014\u001b\u000f\u0010\t\b\u00024l\u000e\u0002\u001b\u000f\u0010\u0014/%C#C\u000f\u0010\r\b\u0014/%\u000e\u0016\u0017\u000b\u0007B.\r\b*\u0007\u0004\u000e\u001c\u0017\u0002=84)+\r\b\u0002\n\tF02)4\u0006G\u000f=\t\b\r\b\u0006/\u000f\n\u0016\u0017B+%\u000e\r\u001cI\u0016\u0017\u000b\u001a\u0002J\u0016\u0017\u000bCy\u001cö\u0003?ù\u0013{M\t\b\u0004?\u000f\u0010\u0014\n\u000249FJ02\r\b\u0002\f\u0006F\u000f\u001d\u001cI\u001cn#C\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB>\r\b*\u001a\u0004 \u001c\u0017\u0002\n\ts\u0014\u001b\u000f\u001b\t\b\rF\r\b%\u0007\u0002\u0013\u0016\u0017\u0006s8+)4\r\b\u0002\n\t\nl\r\b%\u001a\u0002wy\u001cö\u0003;ù\u0013{C\u0004&\u000f\n\u0016\u0017\u0006O\r\b%&\u000f\u001b\r@\u0006\b\u0002\f\u0014\n\u0002\u0013\u0016\u00178+\u0002\f\tO\r\b%\u0007\u0002\u0003% \u0016\u0017B4%\u0007\u0002\f\t\b\rO8+)+\r\b\u0002O\u001f?\u0002\n\u0014\n)+#H\u0002\n\tO\r\b%\u0007\u0002DG\u0016\u0017\u000b\u001a\u000b\u0007\u0002\n\u0006\n9u}Ê0p\r\b%\u001a\u0002@84)+\r\b\u00021\u0016\u0017\t>\u001c\u001e\u000f\u0010\u0006\bB4\u0002C\u0002\n\u000b\u0007)4*\u0007B4%Lls\r\b%\u001a\u0002\n\u000bYDG\u0002H\u0014\n)+\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019\u0007\u0002u\r\b%&\u000f\u001b\rñqûö\u000fÁ,ø«ùJ\u0016\u0017\t>\r\b%\u0007\u0002.%\u000e\u0016\u0017\u0019\u0007\u0019\u001a\u0002\n\u000b\u0005\t\b\r\b\u0006/\u000f\u001d\u0016\u0017B4%\u000e\r=\u001cI\u0016\u0017\u000b\u0007\u0002CDG\u0002.\u000f\u001b\u0006\b\u0002-\r\b\u0006\b\"+\u0016\u0017\u000b\u0007Bu\r\b)-Z&\u000b\u001a\u0019L9m3%\u001a\u0002=\u001f&)4\r\b\r\b)+# B+\u0006/\u000f\u001b\u0004\u0007%C)\u001b0sL\u0016\u0017B+*\u001a\u0006\b\u0002.=\u0016I\u001cI\u001c\u0017*\u0007\t\b\r\b\u0006/\u000f\u001b\r\b\u0002\n\t>\r\b%\u0007\u0002;8+)4\r,\u0016\u0017\u000b\u0007B-\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t\n9m3%\u000e\u0016\u0017\tF8+)+\r\b\u0002p\u0014\u001b\u000f\u0010\u000bH\u000f\u001d\u001c\u0017\t\b);\u001f&\u0002G*\u001a\t\b\u0002\n\u00191\r\b)3#-\u0002\u001d\u000f\u0010\t\b*\u001a\u0006\b\u0002G\u0014\n)+\u000b\u000eZ?\u0019\u0007\u0002\n\u000b\u001a\u0014\n\u0002G\u0016\u0017\u000b.#C\u000f\u0010\r\b\u0014/% \u0016\u0017\u000b\u001aB\r\b%\u001a\u0002\n\t\b\u0002.\r,DG).#1*\u001a\t,\u0016\u0017\u0014.\u0004 \u0016\u0017\u0002\n\u0014\f\u0002\n\t\n9\n4.EXPERIMENTSXF(\u000e\u0004&\u0002\n\u0006,\u0016\u0017#-\u0002\f\u000b\u000e\r\b\tO%&\u000f\n8+\u0002\u0003\u001f&\u0002\n\u0002\f\u000b[\u0014\n)4\u000b\u0007\u0019\u001a*\u0007\u0014\f\r\b\u0002\n\u0019^)4\u000bE\u000fY\u0019?\u000f\u0010\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002\u0003)\u00100H¤\u000elI4+#1\u0016\u0017\u000b\u000e*\u001a\r\b\u0002\n\t>)\u001b0G#.*\u0007\t,\u0016\u0017\u0014H\u0006\b\u0002\n\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u001aB+\t\n9.T=*\u001a\u0006=\u0019?\u000f\u0010\r/\u000f-\u0014\n)\u001b\u001cI\u001c\u0017\u0002\n\u0014\n\r,\u0016\u0017)+\u000bY\u0016\u0017\t;\u0019\u0007)4\u000b\u0007\u00021\u0016\u0017\u000b\r6DG)7Dq\u000f\u001d\"\u000e\t\nl3\u001f\u000e\"\u0018\u0019 \u0016\u0017B\u001b\u0016\u0017\r/\u000f\u001d\u001cI\u001c\u001e\"E\u0006,\u0016\u0017\u0004\u0007\u0004\u000e\u0016\u0017\u000b\u0007By\u00193~J\t-\u0016\u0017\u000b\u000e\r\b)[9 D3\u000f\n8702)+\u0006\b#\u0015\u000f\u0010\r\nl3\u000f\u001b\u000b\u0007\u0019\u001f\u000e\"\u0003\u0006\b\u0002\n\u0014\n)4\u0006\b\u0019 \u0016\u0017\u000b\u001aB%\u00193~J\t.)+\u0006.\r/\u000f\u0010\u0004&\u0002\f\t\u0016\u0017\u000b\u000e\r\b)\u0018vü\u0019G\t.\r\b%\u0007\u0006\b)4*\u0007B4%!\u000f@\u001c\u0017)\u0010DG56A\u000e*?\u000f\u001d\u001cI\u0016\u0017\r,\"#1\u0016\u0017\u0014\f\u0006\b)+\u0004\u001a%\u0007)4\u000b\u0007\u0002+9\u0003m3%\u001a\u00021\u001c\u001e\u000f\u0010\r\b\r\b\u0002\n\u0006>\u0016\u0017\t>\u0016\u0017\u000b\u000e\r\b\u0002\n\u000b\u0007\u0019\u001a\u0002\n\u0019!\r\b)O\u000f\u0010\u0019\u001a\u0019Y\t\b)4#-\u0002C\u0006\b\u0002\u001b\u000f\u001d\u001cI\u0016\u0017\t\b\r,\u0016\u0017\u0014\u001b\u000f\n\u001c\u000b\u001a)\u0010\u0016\u0017\t\b\u0002u\r\b)'\r\b\u0002\n\t\b\r>\r\b%\u001a\u0002-\t\b\"\u000e\t\b\r\b\u0002\n#'x \t>\u0006\b)+\u001f\u001a*\u001a\t\b\r\b\u000b\u0007\u0002\n\t\b\t\u0015\u000f\u0010\u000b\u001a\u0019Y\u0004&\u0002\n\u0006,02)4\u0006\b#C\u000f\u0010\u000b\u001a\u0014\n\u0002-\u0016\u0017\u000b\u0005\u000f\u0004\u001a\u0006/\u000f\u0010\u0014\n\r,\u0016\u0017\u0014\u001d\u000f\u001d\u001cF\u0002\f\u000b\u000e8+\u0016\u0017\u0006\b)+\u000b\u001a#-\u0002\n\u000b\u000e\r\n9;:<)4\r\b%@\u0014/\u001c\u001e\u000f\u0010\t\b\t,\u0016\u0017\u0014\u001b\u000f\n\u001cp\u000f\u0010\u000b\u001a\u0019@#H)\u000e\u0019\u0007\u0002\n\u0006\b\u000bu#1*\u001a\t,\u0016\u0017\u0014.\u000f\u0010\u0006\b\u0002\u0016\u0017\u000b\u001a\u0014\u0013\u001c\u0017*\u001a\u0019\u0007\u0002\n\u0019SlGDG\u0016\u0017\r\b%Y\u0014/\u001c\u001e\u000f\u0010\t\b\t,\u0016\u0017\u0014\u001b\u000f\n\u001c=#.*\u001a\t,\u0016\u0017\u0014@\u001f?\u0002\u0013\u0016\u0017\u000b\u0007B\u0003\r\b%\u001a\u0002102)\u000e\u0014\n*\u001a\t\n9ý=*\u001a\u0002\n\u0006,\u0016\u0017\u0002\n\tH\u000f\u0010\u0006\b\u0002B\u001b\u0016\u00178+\u0002\n\u000b'\u0016\u0017\u000b\u00034\u001d5p\r\b)C+\n5,\t\b\u0002\f\u0014\n)+\u000b\u001a\u0019O\u0014\u0013\u001cI\u0016\u0017\u0004\u001a\t\n9L\u0016\u001784\u0002>\r,\"\u000e\u0004&\u0002\n\t3)\u001b0F\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u0003\u000f\u0010\u0006\b\u0002;\u0019\u0007\u0002/Z&\u000b\u001a\u0002\n\u0019\u0003\u000f\u0010\u0014\f\u0014\n)+\u0006\b\u0019\u000e\u0016\u0017\u000b\u0007BC\r\b)H \u0002\f\u0014\n\r,\u0016\u0017)+\u000bN¤ 9 ¤ 9mG\"\u000e\u0004?\u00021}<\u0016\u0017\t;\r\b%\u0007\u0002.\u0002\u001b\u000f\u001b\t,\u0016\u0017\u0002\n\t\b\r-yh\u0016\u0017\u0019\u0007\u0002\f\u000b\u000e\r,\u0016\u0017\u0014\u001b\u000f\u001d\u001cp\u0019 \u0016\u0017B\u001b\u0016\u0017\r/\u000f\u001d\u001cp\u0014\n)+\u0004\u000e\u0016\u0017\u0002\n\t/{=D3%\u000e\u0016I\u001c\u0017\u0002-mG\"\u000e\u0004&\u0002©\u001f\u0016\u0017\t.\r\b%\u0007\u0002H#-)+\t\b\r>\u0019\u000e\u0016I\u0012C\u0014\n*\u000e\u001c\u0017\r'y2\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"Y\u0016\u0017\u000bY#-\u0002\u0013\u001c\u0017)\u000e\u0019\u001a\"\u001a{\u00139N )4*\u0007\u000b\u001a\u0019\u0005\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t)\u001b0=\u0002\u001d\u000f\u0010\u0014/%w\r6\"\u000e\u0004&\u0002'\u0014\u001d\u000f\u0010\u000b\u0005\u001f&\u0002@0)+*\u001a\u000b\u0007\u0019!\u000f\u0010\rH\r\b%\u0007\u0002CDG\u0002\n\u001f\u001a\t,\u0016\u0017\r\b\u0002@¬\u001a­\u000e­+®p¯\b°\u000e°+±\u000e±\u000e±\u0011þ\u000e¸\u001a\u0012s²\u001b+­\u0011\u0010\u0010·\u001aµ\u001aº+¹\u000e¸s²\b³\u001b¸\u001aÿ?°\u0001\u0000\u0002\n\u0010\u0010·\u001a»»\u0015\u000e°\u001d´\u000fÿ\u0014\u001b\u000e¶9\u0015\u0007¶\n¹\u001a°<9qP\u001e\u0019G)+\u0006\b\u0006\b\u0002\n\u0014\f\r\bRJ\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"H\u0004&\u000f\n\u0016\u0017\u0006\b\ty2\u001f?\u000f\u0010\t\b\u0002\n\u0019.)+\u000b>\r\b%\u001a\u0002s#1*\u001a\t,\u0016\u0017\u0014<\r,\u0016\u0017\r,\u001c\u0017\u0002\u001d{F\u000f\u0010\u0006\b\u0002s%&\u000f\u001b\u000b\u0007\u0019\u000e5\u000f\u0010\u000b\u001a\u000b\u0007)4\r/\u000f\u0010\r\b\u0002\n\u00191\u0016\u0017\u000b>\r\b%\u001a\u0002s\u0019&\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u000202)4\u0006F\u0002\n84\u000f\u001d\u001c\u0017*?\u000f\u0010\r,\u0016\u0017)4\u000b@\u0004\u001a*\u001a\u0006\b\u0004&)+\t\b\u0002\n\t\nl\u001a\u001f\u001a*\u0007\rs\u000f\u001b\u0006\b\u0002G\u000b\u001a)+\rF#C\u000f\u001b\u0019\u0007\u0002;\u000f\n84\u000f\u001d\u0016I\u001c\u001e\u000f\u0010\u001f\u000e\u001c\u0017\u0002=\r\b);\r\b%\u0007\u0002p\u0006\b\u0002\u00135\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cs\u000f\u001d\u001c\u0017B4)+\u0006,\u0016\u0017\r\b%\u001a#'9;\u0007)4\u0006J\u0002\u001d\u000f\u0010\u0014/%'A\u000e*\u001a\u0002\n\u0006\b\"+l&\r\b%\u001a\u0002>\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cs\u0002\n\u000b\u0007B\u001b\u0016\u0017\u000b\u0007\u0002.\u0006/\u000f\u0010\u000b\u001aU\u000e\t\u0014\u001b\u000f\u001b\u000b\u0007\u0019\u000e\u0016\u0017\u0019?\u000f\u0010\r\b\u0002>\u0016\u0017\r\b\u0002\n#-\tG02\u0006\b)+#r\r\b%\u0007\u0002;\u0019&\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002+9p}Ê0¢\r\b%\u0007\u0002\u0015PQ\u0014\n)4\u0006\b\u0006\b\u0002\n\u0014\n\r\bRC\u000f\u001b\u000b\u0007\t\bDG\u0002\f\u0006\b\t\u000f\u001b\u0004\u0007\u0004&\u0002\u001d\u000f\u0010\u0006.Dq\u0016\u0017\r\b%\u000e\u0016\u0017\u000bY\r\b%\u001a\u00021\r\b)4\u0004YC#C\u000f\u0010\r\b\u0014/%\u001a\u0002\n\t\nlM\r\b%\u001a\u0002\n\u000b'\u0016\u0017\r=\u0016\u0017\t>\u0014\n)4\u000b\u0007\t,\u0016\u0017\u0019\u001a\u0002\n\u0006\b\u0002\n\u0019\u0005\u0014\n)4\u0006,5\u0006\b\u0002\n\u0014\f\r\n9C\u0007)4\u0006>\u0002\u001b\u000f\u0010\u0014/%Y\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u0005\r,\"\u000e\u0004&\u00024lM\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c;\u000f\u0010\u0014\f\u0014\n*\u0007\u0006/\u000f\u001b\u0014\n\"O\u0016\u0017\t1\u0019\u001a\u0002\u0013Z?\u000b\u0007\u0002\n\u0019\u000f\u001b\tJ\r\b%\u001a\u0002>\u0004&\u0002\n\u0006\b\u0014\f\u0002\n\u000b\u000e\r/\u000f\u0010B4\u0002-)\u00100JPQ\u0014\n)4\u0006\b\u0006\b\u0002\n\u0014\n\r\bRu\u000f\u0010\u000b\u001a\t\bDG\u0002\n\u0006\b\t3\u0006\b\u0002\n\r\b*\u001a\u0006\b\u000b\u0007\u0002\f\u0019L9T=*\u001a\u00061\u0006\b\u0002\f\t\b* \u001c\u0017\r-B4\u0006/\u000f\u0010\u0004\u001a%\u0007\t\u0015\u000f\u0010\u0006\b\u0002C\u001f&\u000f\u001b\t\b\u0002\n\u0019Y)4\u000b\u0005\u000f'\t\b\u0002\n\r.)\u00100;¤4¤+OA\u000e*\u0007\u0002\n\u0006,\u0016\u0017\u0002\f\t\n9\u0003:<)+\r\b%\u0014\u0013\u001c\u001e\u000f\u001b\t\b\t,\u0016\u0017\u0014\u001b\u000f\u001d\u001cJ\u000f\u0010\u000b\u001a\u0019Y#-)\u000e\u0019\u001a\u0002\n\u0006\b\u000b7#.*\u001a\t,\u0016\u0017\u0014@A\u000e*\u001a\u0002\n\u0006,\u0016\u0017\u0002\n\tH\u000f\u0010\u0006\b\u0002H*\u0007\t\b\u0002\n\u0019S9H:<\u0002\n\u0014\u001b\u000f\u001b*\u0007\t\b\u0002u\u0007\u0016\u0017\u000b\u001a\u0019\u0007\u0002\f(+\u0016\u0017\u000b\u0007B'\u0016\u0017\t;\u0004\u0007\u0006\b)4\u001f&\u000f\u001b\u001f \u0016I\u001cI\u0016\u0017\t\b\r,\u0016\u001e\u0014-\u0016\u0017\u000b'\u000b?\u000f\u0010\r\b*\u001a\u0006\b\u0002+lS\u0002\u001b\u000f\u0010\u0014/%'\u0006\b*\u001a\u000bO#C\u000f\n\"CB+\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u001b\r\b\u0002-\u000f\t,\u001cI\u0016\u0017B4%\u000e\r,\u001c\u0017\"@\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u000e\rJ\u0006\b\u0002\f\t\b* \u001c\u0017\rs\u0019\u0007*\u001a\u0002=\r\b);\u0006/\u000f\u0010\u000b\u001a\u0019\u0007)4#\u0016\u0017\u000b\u000e\u0016\u0017\r,\u0016\u001e\u000f\u001d\u001cI\u0016\u001e\u001b\u000f\u0010\r,\u0016\u0017)\u000e\u000b\u001a\t\n93m3%\u0007\u0002\f\u0006\b\u0002\u0013502)4\u0006\b\u0002+l3DG\u0002u\u0006\b*\u0007\u000bEA\u000e*\u001a\u0002\n\u0006,\u0016\u0017\u0002\n\tu#1*\u000e\u001c\u0017\r,\u0016\u0017\u0004 \u001c\u0017\u0002\u0005\r,\u0016\u0017#-\u0002\f\t@DG\u0016\u0017\r\b%!\u0019 \u0016ItS\u0002\f\u0006\b\u0002\n\u000b\u000e\rO\u0006/\u000f\u0010\u000b\u001a\u0019\u0007)4#\t\b\u0002\n\u0002\f\u0019\u0007\t\nl\u000f\u001b\u000b\u0007\u0019\u0003\u000f\n8+\u0002\f\u0006/\u000f\u0010B+\u0002>\r\b%\u001a\u0002.\u0006\b\u0002\n\t\b* \u001c\u0017\r\b\t\n9q~J*\u001a\u00021\r\b)H\r\b%\u0007\u0002>\u001c\u001e\u000f\u0010\u0014/U')\u001b0s\t\b\r/\u000f\u0010\u000b\u001a\u0019&\u000f\u001b\u0006\b\u0019 5\u0016\u0017\n\u0002\f\u0019E\r\b\u0002\n\t\b\r\b\u001f&\u0002\n\u0019!\u0014\n)\u0010\u001cI\u001c\u0017\u0002\n\u0014\f\r,\u0016\u0017)+\u000b\u001a\t\u0003\u000f\u0010#-)4\u000b\u0007B\u0005#.*\u0007\t,\u0016\u0017\u0014O\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c>\u0006\b\u0002\n\t\b\u0002\u001d\u000f\u0010\u0006\b\u0014/%\u0007\u0002\n\u0006\b\t\nlDG\u0002N\u000f\u0010\u0006\b\u0002O\u000b\u001a)+\ru\u000f\u0010\u001f\u000e\u001c\u0017\u0002\u0003\r\b)Y\u0004\u0007\u0006\b)\u001b8+\u0016\u0017\u0019\u001a\u00027\u0004?\u0002\n\u0006,02)+\u0006\b#\u0015\u000f\u0010\u000b\u001a\u0014\n\u00027\u0014\f)+#-\u0004?\u000f\u0010\u0006,\u0016\u0017\t\b)4\u000b\u0007\tuDq\u0016\u0017\r\b%)4\r\b%\u0007\u0002\n\u0006;\t\b\"\u000e\t\b\r\b\u0002\n#-\t\n9;\t>DG\u00021\u0019\u000e\u0016\u0017\t\b\u0014\n*\u001a\t\b\t\b\u0002\n\u0019Y\u0016\u0017\u000bY\u000e\u0002\n\u0014\n\r,\u0016\u0017)4\u000b\u0005 l\r\b%\u001a\u0002C|'\u0005\u0019J\u000e}\b'02\u0006/\u000f\u001b#-\u0002\nDG)4\u0006\bUO\u0014\n)+\u000b\u000e5\t,\u0016\u0017\t\b\r\b\tF)\u001b0\u001a#C\u000f\u0010\u000b\u000e\">#-)\u000e\u0019\u001a*\u000e\u001c\u001e\u000f\u0010\u0006F\u0014\n)4#-\u0004&)+\u000b\u001a\u0002\n\u000b\u000e\r\b\tF\r\b%?\u000f\u0010\r\u0014\u001b\u000f\u0010\u000b>\u001f&\u0002F\u0016\u0017\u000b\u001a\u0019\u0007\u0002\n\u0004?\u0002\n\u000b\u0007\u0019\u001a\u0002\n\u000b\u000e\r,\u001c\u001e\"#-)\u000e\u0019\u000e\u0016IZ?\u0002\n\u00197\r\b)u\u000f\ntS\u0002\n\u0014\n\r;\u0004&\u0002\n\u0006,02)4\u0006\b#C\u000f\u0010\u000b\u001a\u0014\n\u0002+93}6\u000b'\r\b%\u000e\u0016\u0017\t=\t\b\u0002\n\u0014\n\r,\u0016\u0017)4\u000bLlDG\u0002=02)\u000e\u0014\f*\u0007\t;)+\u000b\r\b%\u001a\u0002V0)\u0010\u001cI\u001c\u0017)\u001bDG\u0016\u0017\u000b\u0007B!8+\u000f\u001b\u0006,\u0016\u001e\u000f\u0010\u001f\u000e\u001c\u0017\u0002Y\u0014\n)4#-\u0004&)+\u000b\u001a\u0002\n\u000b\u000e\r\b\t\nK.\u0002\u001b\u000f\u001b\u0014\u0013%E\u0014\n)4#-\u0004&)4\u000b\u0007\u0002\n\u000b\u000e\ru%&\u000f\u001b\tO\u000f\u0019\u001a\u0002\u001306\u000f\u001b* \u001c\u0017\r;\u0014\u0013%\u001a)\u0010\u0016\u0017\u0014\n\u0002.D3%\u0007\u0002\f\u000bODG\u0002=#-)\u000e\u0019\u000e\u0016I02\"u)+\r\b%\u001a\u0002\n\u0006=\u0014\n)4#-\u0004&)4\u000b\u0007\u0002\n\u000b\u000e\r\b\t\n¨©\n3\rFA\u000e*\u0007\u0002\f\u0006\b\"1\r,\u0016\u0017#-\u00024l\u000e)+\u000b\u001a\u00023D3\u000f\n\">\r\b)=\t\b\u0004&\u0002\n\u0002\f\u0019@*\u001a\u0004-\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c&\u0016\u0017\ts\r\b)=\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t)+\u000b\u000e\u001c\u0017\"O\u000f-\t\b#C\u000f\n\u001cI\u001cs\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002.)\u00100s\u0014/%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014C\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\t=02\u0006\b)4# \r\b%\u0007\u0002A\u000e*\u001a\u0002\n\u0006\b\"'\u0006/\u000f\u001b\r\b%\u0007\u0002\n\u0006>\r\b%?\u000f\u0010\u000bO\r\b%\u001a\u0002.\u0002\n\u000b\u000e\r,\u0016\u0017\u0006\b\u0002-A\u000e*\u001a\u0002\n\u0006\b\"+9;mJ%\u001a\u0002.\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t>\u0014\u001b\u000f\u001b\u000b7\u001f&\u0002\r/\u000f\u0010U4\u0002\n\u000b'\u0006/\u000f\u001b\u000b\u0007\u0019\u001a)+#.\u001c\u0017\"'DG\u0016\u0017\r\b%\u0003\u000fZ?(\u000e\u0002\n\u0019O\t/\u000f\u001b#-\u0004 \u001cI\u0016\u0017\u000b\u001aB'\u0006/\u000f\u001b\r\b\u0002+9s\u00017\u0002;\u0002\n84\u000f\u001d\u001c\u0017*?\u000f\u0010\r\b\u0002\u0004&\u0002\n\u0006,0)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u0002pDq\u0016\u0017\r\b%.\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\rsA\u000e*\u0007\u0002\f\u0006\b\"+5,\t/\u000f\u001b#-\u0004\u000e\u001cI\u0016\u0017\u000b\u0007B-\u0006/\u000f\u001b\r\b\u0002\n\t\n9s~3\u0002\u001306\u000f\u0010*\u000e\u001c\u0017\r\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002+¨G\t/\u000f\u001b#-\u0004\u000e\u001cI\u0016\u0017\u000b\u0007Bu\u0006/\u000f\u0010\r\b\u0002q\u001a@9©\nJ0\r\b\u0002\n\u0006S\u0004&\u0002\u001b\u000f\u0010U\u000e\ts\u000f\u001b\u0006\b\u0002<\u0002\f(\u000e\r\b\u0006/\u000f\u0010\u0014\n\r\b\u0002\n\u0019>\u0019\u001a*\u001a\u0006,\u0016\u0017\u000b\u0007B>\u0002\n84\u0002\n\u000b\u000e\rM\u0019\u001a\u0002\n\r\b\u0002\n\u0014\f\r,\u0016\u0017)+\u000bSl\u000eDG\u0002F\u0014\n)4#15\u0004?\u000f\u0010\u0006\b\u0002-\r6DG)C#-\u0002\n\r\b%\u001a)\u000e\u0019\u0007\t.\r\b)'\u0014\n)4\u000b\u0007\t\b\r\b\u0006\b*\u001a\u0014\n\r>\u0002\n84\u0002\n\u000b\u000e\r.8+\u0002\n\u0014\n\r\b)4\u0006\b\t\n¨>\r\b%\u001a\u0002-\u0014\n)+#.\u001f#-\u0002\n\r\b%\u001a)\u000e\u0019\u0005\u000f\u001b\u000b\u0007\u0019Y\r\b%\u001a\u0002-\u0014\n\u0002\f\u0004\u0007\t\b\r\b\u0006\b*\u001a# #-\u0002\n\r\b%\u001a)\u000e\u0019Lls\u000f\u001b\t1\u0019\u001a\u0002\n\t\b\u0014\n\u0006,\u0016\u0017\u001f?\u0002\n\u0019Y\u0016\u0017\u000b\u0005 \u0002\f\u0014\u00135\r,\u0016\u0017)+\u000b¼ 9\u0017¦+9 ¤ 9 mJ%\u001a\u0002\u0003\u0019 \u0016\u0017#-\u0002\f\u000b\u0007\t,\u0016\u0017)4\u000b&\u000f\n\u001cg\u0016\u0017\r6\")\u001b0-\u0002\f8+\u0002\n\u000b\u000e\ru8+\u0002\n\u0014\n\r\b)4\u0006\b\t\nl©<l3\u0016\u0017\t\u000f\u0010\u000b\u0003\u000f\u001b\u0019\u0010*\u0007\t\b\r/\u000f\u001b\u001f \u001c\u0017\u0002-\u0004?\u000f\u0010\u0006/\u000f\u001b#-\u0002\n\r\b\u0002\n\u0006\n9;~3\u0002\u001306\u000f\u001b* \u001c\u0017\r>\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002+¨3\u0014\n)4#1\u001fu#-\u0002\n\r\b%\u001a)\u000e\u0019Ll\no^¤\u001b¥\u00079©\n~3*\u0007\u0006,\u0016\u0017\u000b\u001aB1\u0014\f)+\u000b\u001a\t\b\r\b\u0006\b*\u0007\u0014\n\r,\u0016\u0017)4\u000bC)\u00100?\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014>\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\ts\u0019\u0007\u0002\n\t\b\u0014\f\u0006,\u0016\u0017\u001f&\u0002\n\u0019\u0016\u0017\u000b\u0003\u000e\u0002\n\u0014\n\r,\u0016\u0017)4\u000bY\u000e9g¦49I\u000el&DG\u0002.\u000f\u001b\u0019\u0010\b*\u001a\t\b\r=\r\b%\u001a\u0002>02)\u0010\u001cI\u001c\u0017)\u001bDG\u0016\u0017\u000b\u0007Bu\u0004&\u000f\u001b\u0006/\u000f\u0010#-\u0002\f\r\b\u0002\n\u0006\b\t\n¨3\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002«\u001c\u0017\u0002\f\u000b\u0007B4\r\b%\bÀ \u000f\u001b\u000b\u0007\u0019.\u001f\u0007\u0006/\u000f\u001b\u0014\u0013U4\u0002\n\r,\u0016\u0017\u000b\u001aB=\u0014\n)4\u000b\u000e\r\b\u0006\b)\u0010\u001c\u0014±'9F~3\u0002\u001306\u000f\u001b* \u001c\u0017\rF\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002+¨À E\u000elM±ûW 9©\n~3*\u0007\u0006,\u0016\u0017\u000b\u001aBY\u0002\u001b\u000f\u001b\u0014\u0013%!M\u001a\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002O\u0019\u0007\u0002\f\t\b\u0014\n\u0006,\u0016\u0017\u001f&\u0002\n\u0019\u0005\u0016\u0017\u000bE\u000e\u0002\n\u0014\n\r,\u0016\u0017)+\u000b  9 ¤ lFDp\u0002\u000f\u001b\u0019\u0010\b*\u001a\t\b\rM\r\b%\u001a\u0002s\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006F)\u001b0&\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b-\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\tG\u000f\u001b\u000b\u0007\u0019>A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001d\u000f\u0010\r,\u0016\u0017)+\u000bB4\u0006,\u0016\u0017\u0019Y\t,\u0016\u0017\n\u0002+9\u0015~J\u0002\u001306\u000f\u001b* \u001c\u0017\r>\u0014/%\u001a)\u0010\u0016\u0017\u0014\n\u0002+¨.\u000b\u000e*\u001a#1\u001f?\u0002\n\u00061)\u001b0G\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b!\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t\u0011\u000el&A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001b\u000f\u001b\r,\u0016\u0017)+\u000bYB4\u0006,\u0016\u0017\u0019O\t,\u0016\u0017\n\u0002w¦+9 ¤ 9©\n~3*\u0007\u0006,\u0016\u0017\u000b\u001aB\u0011\u000f\n\u001cI\u0016\u0017B+\u000b\u001a#-\u0002\n\u000b\u000e\r!\u000f\u0010\u000b?\u000f\u001d\u001c\u0017\"\u000e\t,\u0016\u0017\t\nluDG\u0002Y\u0014\f)+#-\u0004?\u000f\u0010\u0006\b\u0002\u0005\r,Dp)[#-\u0002\n\r\b%\u001a)\u000e\u0019\u001a\t\n¨&\u0016\u0017\u000b\u0007\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\"CL\u0016I\u001c\u0017\r\b\u0002\f\u0006,\u0016\u0017\u000b\u0007BC\u000f\u001b\u000b\u0007\u0019HJ)4*\u0007B4%-mF\u0006/\u000f\u0010\u000b\u001a\t,02)4\u0006\b#'l\u001a\u000f\u0010\tF\u0019\u001a\u0002\n\t\b\u0014\n\u0006,\u0016\u0017\u001f?\u0002\n\u0019-\u0016\u0017\u000b\u000e\u0002\n\u0014\n\r,\u0016\u0017)4\u000b\u0007\t.\u000e9I\u000e9g¦.\u000f\u001b\u000b\u0007\u0019\u0018 9  9 ¤ 9p\u00017\u0002.\t\b\r\b*\u001a\u0019\u0007\"u%\u0007)\u001bD\u0011\r\b%\u001a\u0002\n\"'\u0006\b\u0002\f\t\b\u0004&)+\u000b\u001a\u00197\r\b)\u0014/%&\u000f\u001b\u000b\u0007B4\u0002\n\t>\u0016\u0017\u000bYA\u000e*\u0007\u0002\f\u0006\b\"+5,\t/\u000f\u001b#-\u0004\u000e\u001cI\u0016\u0017\u000b\u0007B\u0005\u0006/\u000f\u0010\r\b\u000249C~J\u0002/06\u000f\u0010*\u000e\u001c\u0017\r-\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002+¨HJ)4*\u0007B4%mF\u0006/\u000f\u001b\u000b\u0007\t,02)4\u0006\b#'9L\u0016\u0017B4*\u0007\u0006\b\u0002HÃ y6\u000f4{G\t\b%\u0007)\u001bD3\tG\r\b%\u001a\u0002>\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c«\u000f\u0010\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\u0014\f\"')\u00100¢*\u0007\t,\u0016\u0017\u000b\u001aB\u0018PQ\t/\u000f\u001b#-\u0004 \u001c\u0017\u0002\f\u0019\u0007RA\u000e*\u001a\u0002\n\u0006,\u0016\u0017\u0002\n\t.\u000f\u001b\r=\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r.\t/\u000f\u0010#H\u0004 \u001cI\u0016\u0017\u000b\u001aBO\u0006/\u000f\u0010\r\b\u0002\n\t\n93\t\b*\u001a\u001f\u0007\t\b\u0002\f\r=)\u00100p\u0014/%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t,5\r,\u0016\u0017\u0014.\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\tJ0\u0006\b)+# \r\b%\u001a\u0002>A\u000e*\u0007\u0002\n\u0006\b\"u\u0014\u0013\u001cI\u0016\u0017\u0004'\u0016\u0017\t=\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\u0019\u0003\u000f\u0010\r3\u0006/\u000f\u0010\u000b\u001a\u0019\u001a)+#\u000f\u0010\u000b\u001a\u00190\u0002\n\u0019w\u0016\u0017\u000b\u000e\r\b)Y\r\b%\u001a\u0002O\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001c>\u0002\n\u000b\u001aB\u0010\u0016\u0017\u000b\u001a\u0002+9WL\u0016\u001784\u0002V\u001cI\u0016\u0017\u000b\u001a\u0002\n\t@\u0016\u0017\u000b!\r\b%\u0007\u0002O\u0004\u000e\u001c\u0017)+\rC\u0014\n)4\u0006\b\u0006\b\u0002\u00135\t\b\u0004?)+\u000b\u001a\u0019[\r\b)'Z?8+\u0002'\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u000e\r@\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"!\r,\"\u000e\u0004&\u0002\f\t\n9\u0005L\u0016\u0017B+*\u001a\u0006\b\u0002OÃ\u000ey2\u001f&{.\t\b%\u0007)\u001bD3\t\r\b%\u001a\u0002'\u0014\f)+\u0006\b\u0006\b\u0002\n\t\b\u0004&)4\u000b\u0007\u0019\u000e\u0016\u0017\u000b\u001aBw\u0002\f(\u000e\u0002\n\u0014\n*\u001a\r,\u0016\u0017)+\u000b[\r,\u0016\u0017#H\u0002\n\t1\u0016\u0017\u000b\u0005\t\b\u0002\n\u0014\n)4\u000b\u0007\u0019\u001a\t102)4\u0006-\r\b%\u0007\u0002C\u0014\n)4#15\u0004\u000e\u001c\u0017\u0002\n\r\b\u0002>\t\b\u0002\n\rp)\u00100¢+41\r\b\u0002\n\t\b\rsA\u000e*\u0007\u0002\n\u0006,\u0016\u0017\u0002\f\t\n9Gm3%\u0007\u0002\u0015PQ\t/\u000f\u001b#-\u0004 \u001c\u0017\u0002\f\u0019\u0007RCA\u000e*\u001a\u0002\n\u0006\b\"u\u000f\u0010\u0004\u001a\u0004\u0007\u0006\b)\u000f\u0010\u0014/%\u0006\b\u0002\f\t\b* \u001c\u0017\r\b\t1\u0016\u0017\u000b!\u000f'\t,\u0016\u0017B4\u000b \u0016IZ?\u0014\u001b\u000f\u0010\u000b\u000e\ru\t\b\u0004&\u0002\n\u0002\n\u0019\u001a*\u001a\u0004LlpDJ%\u000e\u0016I\u001c\u0017\u0002O\u000b\u001a)+\r.\t/\u000f\u0010\u0014\n\u0006,\u0016IZ?\u0014\u0013\u0016\u0017\u000b\u001aBw#.*\u001a\u0014\u0013%\u0016\u0017\u000bO\u0004?\u0002\n\u0006,02)+\u0006\b#\u0015\u000f\u0010\u000b\u001a\u0014\n\u00021D3%\u001a\u0002\n\u000b'\r\b%\u001a\u0002>\t/\u000f\u0010#H\u0004 \u001cI\u0016\u0017\u000b\u001aBO\u0006/\u000f\u0010\r\b\u0002=\u0016\u0017\t;\u000f\u0010\u001f?)\u001084\u0002-9@9L\u0016\u0017B4*\u0007\u0006\b\u0002qq\u0014\f)+#-\u0004?\u000f\u0010\u0006\b\u0002\n\t\r\b%\u0007\u0002s\u0014\n)4#1\u001f>#-\u0002\f\r\b%\u0007)\u000e\u0019H\u000f\u0010\u000b\u001a\u0019>\r\b%\u0007\u0002s\u0014\n\u0002\n\u0004\u001a\t\b\r\b\u0006\b*\u001a#\u0011#-\u0002\n\r\b%\u001a)\u000e\u0019\u0019\u001a*\u001a\u0006,\u0016\u0017\u000b\u0007B-\u0002\f8+\u0002\n\u000b\u000e\rs8+\u0002\n\u0014\f\r\b)+\u0006s\u0014\n)+\u000b\u001a\t\b\r\b\u0006\b*\u0007\u0014\f\r,\u0016\u0017)+\u000bSl\u0007D3%\u000e\u0016I\u001c\u0017\u0002=84\u000f\u0010\u0006\b\"+\u0016\u0017\u000b\u001aB-\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001cI5\u0016\u0017\r6\"C)\u001b0S\u0002\n8+\u0002\f\u000b\u000e\r<84\u0002\n\u0014\n\r\b)+\u0006\b\t\n9¢},\u000b.B+\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u001d\u001c2l\u001a\r\b%\u001a\u0002G\u0014\n)4#1\u001f-#H\u0002\n\r\b%\u0007)\u000e\u0019H\u0004&\u0002\n\u0006,02)4\u0006\b#-\t\u001f?\u0002\n\r\b\r\b\u0002\n\u0006p\u000f\u0010\u000b\u001a\u0019106\u000f\u0010\t\b\r\b\u0002\f\u0006F\r\b%&\u000f\u001b\u000b-\r\b%\u0007\u00023\u0014\n\u0002\f\u0004\u0007\t\b\r\b\u0006\b*\u001a##-\u0002\n\r\b%\u001a)\u000e\u0019L9s\u0001w\u0016\u0017\r\b%H\r\b%\u0007\u00023\u0014\n)4#1\u001f#H\u0002\n\r\b%\u0007)\u000e\u0019Sl;\u000f\u0010\tH\u0002\n8+\u0002\n\u000b\u000e\rH8+\u0002\n\u0014\n\r\b)4\u0006-\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c\u0017\u0016h\r6\"[\u0016\u0017\t1\u0016\u0017\u000b\u0007\u0014\f\u0006\b\u0002\u001b\u000f\u0010\t\b\u0002\n\u0019Sl\u000f\u0010\u0014\n\u0014\n*\u000e5\u0006/\u000f\u001b\u0014\n\"YB4)\u000e\u0002\n\t-*\u001a\u0004[\u000f\u001b\u000b\u0007\u0019\u0005\r\b%\u001a\u0002\n\u000b[\t,\u001cI\u0016\u0017B4%\u000e\r,\u001c\u0017\"[\u0019\u001a)\u001bDJ\u000bSls\u001f\u0007*\u001a\r-\r\b%\u001a\u0002'\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u001a\u0014\n\u0002'\u0016\u0017\t\u000b\u001a)+\r>\t,\u0016\u0017B4\u000b \u0016IZ?\u0014\u001b\u000f\u001b\u000b\u000e\r\n91\u0001\u0003\u0002-\t\b\u0002\n\r;\r\b%\u0007\u0002.\u0019\u001a\u0002\u001306\u000f\u0010*\u000e\u001c\u0017\r>\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001c\u0017\u0016h\r6\"Y\r\b)C\u001f&\u0002-¤\u001b¥\u0007lD3%\u001a\u0002\n\u0006\b\u0002>\r\b%\u001a\u0002.\u000f\u001d84\u0002\n\u0006/\u000f\u0010B4\u00021\u0006\b\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cp\u000f\u0010\u0014\f\u0014\n*\u0007\u0006/\u000f\u001b\u0014\n\"@\u0016\u0017\t3%\u000e\u0016\u0017B+%S9L\u0016\u0017B4*\u0007\u0006\b\u0002ÄÄ\t\b\r\b*\u001a\u0019\u000e\u0016\u0017\u0002\n\t7\r\b%\u001a\u0002\u0003\u0002\u0013tS\u0002\n\u0014\n\r\b\tu)\u00100H\u0014\u0013%?\u000f\u0010\u000b\u001aB\u0010\u0016\u0017\u000b\u001aBÀ \u000f\u0010\u000b\u001a\u0019±\\84\u000f\u001d\u001c\u0017*\u001a\u0002\n\t\u0019\u001a*\u001a\u0006,\u0016\u0017\u000b\u0007B\u0003\u0014\n)+\u000b\u001a\t\b\r\b\u0006\b*\u001a\u0014\n\r,\u0016\u0017)+\u000b\u0005)\u00100q\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014'\t\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\u0002\n\t\n9u=\tq±ªB+)\u000e\u0002\n\t*\u001a\u0004SlS\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cp\u000f\u001b\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\u0014\n\"'\u0016\u0017#-\u0004\u001a\u0006\b)\u001b8+\u0002\n\t\nlS\u001f\u001a*\u001a\r=\u0002\n(\u000e\u0002\n\u0014\f*\u0007\r,\u0016\u0017)4\u000bY\r,\u0016\u0017#-\u0002H\u000f\u001d\u001c\u0017\t\b)-\u0016\u0017\u000b 5\u0014\f\u0006\b\u0002\u001b\u000f\u0010\t\b\u0002\n\t\nl\u0019\u0007*\u001a\u00027\r\b)\u0005\r\b%\u001a\u0002V\u0016\u0017\u000b\u001a\u0014\n\u0006\b\u0002\u001b\u000f\u001b\t\b\u0002\n\u0019W\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006O)\u001b0.\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014\u0005\t\b\u0002\u00135A\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\t-\r\b)u\u001f&\u0002C\u0004\u0007\u0006\b)\u000e\u0014\f\u0002\n\t\b\t\b\u0002\n\u0019S9O=\t.\u000fC\u0014\n)+#-\u0004\u001a\u0006\b)+#.\u0016\u0017\t\b\u0002+lGDG\u0002H\t\b\u0002\n\r>\r\b%\u0007\u0002C\u0019\u001a\u0002\u001350Ê\u000f\u0010*\u000e\u001c\u0017\rq± 84\u000f\u001d\u001c\u0017*\u001a\u0002-\r\b)C\u001f&\u0002H l?DJ%\u001a\u0002\n\u0006\b\u0002.\u0002\n(\u000e\u0002\n\u0014\n*\u001a\r,\u0016\u0017)+\u000b\u0018\r,\u0016\u0017#-\u0002\u0016\u0017\t>\u000b\u001a)+\r;\r\b)\u000e)C% \u0016\u0017B4%\u000f\u001b\u000b\u0007\u0019\u0005\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c;\u000f\u0010\u0014\f\u0014\n*\u0007\u0006/\u000f\u001b\u0014\n\"7\u0016\u0017\t.\u0006\b\u0002\u001b\u000f\u0010\t\b)4\u000b&\u000f\u001b\u001f \u001c\u0017\u000249Y=\tqÀ \u0016\u0017\u000b\u0007\u0014\n\u0006\b\u0002\u001d\u000f\u0010\t\b\u0002\n\t\nlp\u0002\n(\u000e\u0002\u00135\u0014\f*\u0007\r,\u0016\u0017)4\u000b[\r,\u0016\u0017#-\u0002u\u0019\u0007\u0002\n\u0014\f\u0006\b\u0002\u001b\u000f\u0010\t\b\u0002\n\tH\t,\u001cI\u0016\u0017B+%\u000e\r,\u001c\u0017\"[\u001f?\u0002\n\u0014\u001b\u000f\u0010*\u001a\t\b\u0002'\r\b%\u001a\u0002'\r\b)4\r/\u000f\u001d\u001c3\u000b\u000e*\u001a#.\u001f&\u0002\n\u0006-)\u001b084\u000f\n\u001cI\u0016\u0017\u0019Y\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014-\t\b\u0002\fA\u000e*\u0007\u0002\n\u000b\u001a\u0014\n\u0002\n\t.B+)\u000e\u0002\f\t=\u0019\u001a)\u0010D3\u000bS9;J)\u001bDG\u0002\n84\u0002\n\u0006\nlS\u000f1\u001c\u001e\u000f\u001b\u0006\bB+\u0002\n\u0006À #-\u0002\u001b\u000f\u0010\u000b\u001a\tF\u001c\u0017\u0002\n\t\b\t<\r\b)\u001b\u001c\u0017\u0002\n\u0006/\u000f\u0010\u000b\u001a\u0014\n\u0002>\r\b);#1*\u001a\t,\u0016\u0017\u0014=84\u000f\u0010\u0006,\u0016\u001e\u000f\u001b\r,\u0016\u0017)+\u000bu\u000f\u0010\u000b\u001a\u0019C\u000b\u0007)\u001b\u0016\u0017\t\b\u0002+9pT=\u000b-\r\b%\u001a\u0002)4\r\b%\u0007\u0002\f\u0006G%&\u000f\u001b\u000b\u0007\u0019Sl?\u000f84\u0002\n\u0006\b\".\t\b#C\u000f\u001d\u001cI\u001cXÀ #C\u000f\n\"1\r\b)\u001b\u001c\u0017\u0002\n\u0006/\u000f\u0010\r\b\u0002;\r\b)\u000e)1#.*\u001a\u0014\u0013%C84\u000f\u0010\u0006,\u0016\u001e\u000f\u001b\r,\u0016\u0017)+\u000b\u000f\u001b\u000b\u0007\u0019\u0005\u0014\u001b\u000f\u001b*\u0007\t\b\u0002u\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rC#1*\u001a\t,\u0016\u0017\u0014'\u0004\u000e\u0016\u0017\u0002\n\u0014\n\u0002\f\t@\r\b)O\u001f?\u0002@\u0006\b\u0002\f\r\b*\u0007\u0006\b\u000b\u001a\u0002\n\u0019E\u000f\u001b\t1\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006\n9;\t;\u000f1\u0006\b\u0002\n\t\b*\u000e\u001c\u0017\r\nlSDG\u0002=\u0014/%\u0007)\u000e)4\t\b\u0002-H\u000f\u0010\t3\r\b%\u0007\u0002>\u0019\u001a\u0002\u001306\u000f\u001b* \u001c\u0017\r;84\u000f\u001d\u001c\u0017*\u001a\u0002102)+\u00060ÀÅ9L\u0016\u0017B4*\u0007\u0006\b\u0002Y¦\u001d'\t\b%\u0007)\u001bD3\t-\r\b%\u001a\u0002C\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c\u000f\u0010\u0014\f\u0014\n*\u0007\u0006/\u000f\u001b\u0014\n\"\u0005\u000f\u001b\u000b\u0007\u0019\u0005\r,\u0016\u0017#-\u0002uDJ%\u001a\u0002\n\u000b!\u000f\u0010\u0019\u000e5*\u0007\t\b\r,\u0016\u0017\u000b\u001aB'\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u001aB7\u0004?\u000f\u0010\u0006/\u000f\u001b#-\u0002\n\r\b\u0002\n\u0006\b\t>DG\u0016\u0017\r\b%\u000e\u0016\u0017\u000b\u0005M\u001aÅ\u0016\u0017\u000b\u001a\t\b\r/\u000f\u0010\u000b\u001a\u0014\n\u0002\n\t\n¨\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006)\u001b0G\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u0005\t/\u000f\u001b#-\u0004 \u001c\u0017\u0002\f\t.\u000f\u0010\u000b\u001a\u00197A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001b\u000f\u001b\r,\u0016\u0017)+\u000b\u0005B+\u0006,\u0016\u0017\u0019\u0003\t,\u0016\u0017\n\u0002+9\u0011\t\b#C\u000f\u001d\u001cI\u001c\u0017\u0002\f\u0006\u000b\u000e*\u001a#.\u001f&\u0002\n\u0006=)\u001b0F\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u0003\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t3\u001c\u0017\u0002\u001b\u000f\u0010\u0019\u001a\t=\r\b).% \u0016\u0017B4%\u0007\u0002\f\u0006=\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cp\u000f\u0010\u0014\n\u0014\n*\u000e5\u0006/\u000f\u001b\u0014\n\"+l?\u001f&\u0002\n\u0014\u001b\u000f\u001b*\u0007\t\b\u0002-)\u001b0F\r\b%\u001a\u00021%\u000e\u0016\u0017B+%\u001a\u0002\n\u00063\u001cI\u0016\u0017U+\u0002\u0013\u001cI\u0016\u0017%&)\u000e)\u000e\u0019\u0018\r\b%&\u000f\u001b\r=\r,Dp)1\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u00061\u0014/%?\u000f\u0010\u0006,5\u000f\u001b\u0014\n\r\b\u0002\n\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014C\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\t>B+\u0002\f\rJ%?\u000f\u0010\t\b%\u001a\u0002\n\u0019O\u0016\u0017\u000b\u000e\r\b)'\r\b%\u001a\u00021\t/\u000f\u001b#-\u0002.\u001f\u0007*\u001a\u0014\u0013U4\u0002\n\r\n9J)\u001bDG5\u0002\f8+\u0002\n\u0006\nl?\u0002\n(\u000e\u0002\n\u0014\n*\u001a\r,\u0016\u0017)+\u000b\u0003\r,\u0016\u0017#-\u0002.\t\bU\u000e\"\u000e\u0006\b)\u000e\u0014\u0013U4\u0002\n\r\b\t>\u000f\u001b\tJ\r\b%\u001a\u0002.\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006;)\u00100F\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\t=\u0016\u0017\t=\u0006\b\u0002\n\u0019\u001a*\u001a\u0014\n\u0002\n\u0019SlS\t,\u0016\u0017\u000b\u0007\u0014\n\u0002H\r\b%\u0007\u0002.%&\u000f\u001b\t\b%O\t\b\r\b\u0006\b*\u0007\u0014\f\r\b*\u0007\u0006\b\u0002>\u0016\u0017\t=\u000b\u001a)-\u001c\u0017)+\u000b\u001aB+\u0002\n\u0006>\u0002\u00130\u001e50\u0002\n\u0014\n\r,\u0016\u00178+\u0002>D3%\u001a\u0002\n\u000b'\r\b)\u000e).#C\u000f\u001b\u000b\u000e\"\u0015\u0016\u0017\r\b\u0002\f#-\tG06\u000f\u001d\u001cI\u001cS\u0016\u0017\u000b\u000e\r\b)-\r\b%\u001a\u0002>\t/\u000f\u0010#-\u0002;%&\u000f\u001b\t\b%O\u001f\u0007*\u001a\u0014/U+\u0002\n\r\n9\u001a\u000f\u0010#H\u0002\u00150)+\u0006>\r\b%\u001a\u0002@A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001d\u000f\u0010\r,\u0016\u0017)+\u000b!B+\u0006,\u0016\u0017\u0019\u0005\t,\u0016\u0017\n\u0002+¨H\u000f@\u001c\u001e\u000f\u001b\u0006\bB+\u0002\n\u0006.B+\u0006,\u0016\u0017\u0019\u0005\r\b)\u0010\u001c\u0017\u0002\f\u0006/\u000f\u0010\r\b\u0002\n\t#H)+\u0006\b\u0002G84\u000f\u0010\u0006,\u0016\u001e\u000f\u001b\r,\u0016\u0017)+\u000bu\u000f\u0010\u000b\u001a\u0019-#C\u000f\u001bU+\u0002\n\tF\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006G\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002\n\tG#H)+\u0006\b\u0002<\u001cI\u0016\u0017U4\u0002\u0013\u001c\u0017\"-\r\b)=\u001f?\u0002%?\u000f\u0010\t\b%\u001a\u0002\n\u00197\u0016\u0017\u000b\u000e\r\b)u\r\b%\u0007\u0002H\t/\u000f\u0010#-\u0002.\u001f\u0007*\u001a\u0014/U+\u0002\n\r\n9H\u000e)O\u000f1\u001c\u001e\u000f\u0010\u0006\bB4\u0002\n\u00061A\u000e*?\u000f\u0010\u000b\u000e\r,\u0016\u0017\u001d\u000f\u0010\r,\u0016\u0017)+\u000b\u0005B4\u0006,\u0016\u0017\u0019\t,\u0016\u0017\f\u0002\u0015\u001c\u0017\u0002\u001d\u000f\u0010\u0019\u001a\t1\r\b)u% \u0016\u0017B4%\u0007\u0002\n\u0006H\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cq\u000f\u0010\u0014\n\u0014\f*\u0007\u0006/\u000f\u001b\u0014\n\"7\u001f\u001a*\u001a\r\u001c\u0017)4\u000b\u0007B4\u0002\n\u00061\u0002\f(\u000e\u0002\n\u0014\n*\u001a\r,\u0016\u0017)+\u000b\r,\u0016\u0017#H\u0002+9ET=\u000b\u0005\r\b%\u001a\u0002')4\r\b%\u0007\u0002\n\u0006H%&\u000f\u001b\u000b\u0007\u0019Sl3D3%\u0007\u0002\n\u000b!\r\b%\u0007\u0002u\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006C)\u001b0=\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\t/\u000f\u001b#-\u0004\u000e\u001c\u0017\u0002\n\t1\u0016\u0017\t>\t\b\u0002\n\r>\r\b)\u000e)u\t\b#C\u000f\u001d\u001cI\u001c3)4\u0006>\r\b%\u0007\u0002HA\u000e*&\u000f\u001b\u000b\u000e\r,\u0016\u0017\u001b\u000f\u0010\r,\u0016\u0017)4\u000b[B+\u0006,\u0016\u0017\u0019\u0018\t,\u0016\u0017\n\u00021\u0016\u0017\t.\t\b\u0002\n\r\r\b)\u000e)-\u001c\u001e\u000f\u001b\u0006\bB+\u0002+l\u000f\u001b\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\u0014\n\"O\u001f&\u0002\fB\u0010\u0016\u0017\u000b\u001a\t>\r\b)@\t\b*\u000etS\u0002\n\u0006\nlS\t,\u0016\u0017\u000b\u001a\u0014\n\u0002-\u0019\u000e\u0016\u0017\t\b\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006-\u0014\u0013%?\u000f\u0010\u0006/\u000f\u001b\u0014\u00135\r\b\u0002\f\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014O\t\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\u0002\f\t-\t\b\r/\u000f\u0010\u0006\b\r.\r\b)\u0003\u000f\u0010\u0004\u001a\u0004&\u0002\u001b\u000f\u001b\u0006\u0003PQ\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006\n9 R ;\t.\u000fV\u0014\n)4#-\u0004\u001a\u0006\b)\u00105TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\n020406080100\n00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n\u0003\u0004\n\u0005\n\u0006 \u0007\n\b\n\u0005 \t\n\n\u000b \f\n\r\n\r\u000e\n\u0007\n\n\r \u000f\u0010\u0012\u0011\u0014\u0013\u0016\u0015\u0018\u0017 \u0019\u001b\u001a\u001c\u0011\u0014\u001d \u0019Type I\nType II\nType III\nType IV\nType V\n020004000600080001000012000\n00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n\u001e \u001f\n\u0005\n\r\u000e\n\u0006\n\b  \n!\n\"\n\b #\n\u0005\n$ %\n\u0005\n\r'&\n(\u0010)\u0011\u0014\u0013\u0016\u0015\u0018\u0017 \u0019\u001b\u001a\u001c\u0011\u0014\u001d \u0019ñGòÊó?ôFõ\u0010ö+*Lø-,-ö\u000eü\nõ\u001bò2ö/.\u0003\u000b\u0011\n-\u000b\u0003\u0006\u000f\u0006\u000eôFõ<\u000b\u0011\u0006\u000f\f%\u000b\u0007ûFý!ü\nò\t\u0004Yöæ-ò2ü\u0013%\u0000\u0013\u000b\u0011\u0004 ä\u0001\n2ö\u000eý«LôFö\u000eõ\u001bò2ö\u000f\u0000\n020406080100\n15 20 25 30 35\n\u0003\u0004\n\u0005\n\u0006 \u0007\n\b\n\u0005 \t\n\n\u000b \f\n\r\n\r\u000e\n\u0007\n\n\r \u000f021\n\u00133\u001954761 8\n49\u0011\u0014\u00171\n\u001d :8<;7=?>\n\u0019547\u001d7@\u001b\u0019BAC\u001d8<DType I\nType II\nType III\nType IV\nType V\n050010001500200025003000\n15 20 25 30 35\n\u001e \u001f\n\u0005\n\r\u000e\n\u0006\n\b  \n!\n\"\n\b #\n\u0005\n$ %\n\u0005\n\r'&\n(021\n\u0013\u0016\u001954761 8\n49\u0011\u0014\u00171\n\u001d :8<;7=?>\n\u0019547\u001d7@E\u00195AC\u001d8<D\u0019q)4#1\u001fu#-\u0002\n\r\b%\u001a)\u000e\u0019\n020406080100\n15 20 25 30 35\n\u0003\u0004\n\u0005\n\u0006 \u0007\n\b\n\u0005 \t\n\n\u000b \f\n\r\n\r\u000e\n\u0007\n\n\r \u000f021\n\u00133\u001954761 8\n49\u0011\u0014\u00171\n\u001d :8<;7=?>\n\u0019547\u001d7@\u001b\u0019BAC\u001d8<DType I\nType II\nType III\nType IV\nType V\n050010001500200025003000\n15 20 25 30 35\n\u001e \u001f\n\u0005\n\r\u000e\n\u0006\n\b  \n!\n\"\n\b #\n\u0005\n$ %\n\u0005\n\r'&\n(021\n\u0013\u0016\u001954761 8\n49\u0011\u0014\u00171\n\u001d :8<;7=?>\n\u0019547\u001d7@E\u00195AC\u001d8<D\u0019G\u0002\n\u0004\u0007\t\b\r\b\u0006\b*\u001a# #-\u0002\n\r\b%\u001a)\u000e\u0019ñGò2ó&ôFõ\u001böGFSø-,-ö\u000eü\nõ\u001bòÊö/.\u0011\u000b\u0011\nk\u000b\u0011\u0006\u000f\u0006\u000eôFõ<\u000b\u0003\u0006\u000f\f%\u000b\u001aûFý[ö/HSö\u000f\u0006\u000eôFü\nò2ÿ&û!ü\nò\t\u0004Yö'\u0002\bÿ&õw\u0006\u000eÿ*\u0004 èé\u000b\u0007ûFýé\u0006\u000eö\u000fä\u0001\u0000\nü\nõ\u001bô\u0001\u0004 \u0004Yö\u000eü?FÿLý\u0001\u0000TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\n020406080100\n2 3 4 5 6 7 8 9\nIJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<W\u0014X7Y?W5Z\u0018[CW/\\]W\u0014Z9^7_ `baType I\nType II\nType III\nType IV\nType V\n0100200300400500600\n2 3 4 5 6 7 8 9\nc d\nK\nST\nL\nN ef\ng\nN h\nK\ni'j\nK\nS'k\nlV<W5X7Y?W\u0014Z\u0018[CW/\\?W\u0014Z\u0018^m_ `ba~ ¦\n020406080100\n2 3 4 5 6 7 8 9\nIJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<W\u0014X7Y?W5Z\u0018[CW/\\]W\u0014Z9^7_ `baType I\nType II\nType III\nType IV\nType V\n0100200300400500600\n2 3 4 5 6 7 8 9\nc\nd\nK\nST\nL\nN\nef\ng\nN\nh\nK\ni'j\nK\nS'k\nlV<W5X7Y?W\u0014Z\u0018[CW/\\?W\u0014Z\u0018^m_ `ba~ ¤\n020406080100\n2 3 4 5 6 7 8 9\nIJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<W\u0014X7Y?W5Z\u0018[CW/\\]W\u0014Z9^7_ `baType I\nType II\nType III\nType IV\nType V\n0100200300400500600\n2 3 4 5 6 7 8 9\nc d\nK\nST\nL\nN ef\ng\nN h\nK\ni'j\nK\nS'k\nlV<W5X7Y?W\u0014Z\u0018[CW/\\?W\u0014Z\u0018^m_ `ba~ \n020406080100\n2 3 4 5 6 7 8 9\nIJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<W\u0014X7Y?W5Z\u0018[CW/\\]W\u0014Z9^7_ `baType I\nType II\nType III\nType IV\nType V\n0100200300400500600\n2 3 4 5 6 7 8 9\nc\nd\nK\nST\nL\nN\nef\ng\nN\nh\nK\ni'j\nK\nS'k\nlV<W5X7Y?W\u0014Z\u0018[CW/\\?W\u0014Z\u0018^m_ `ba~ \u0011¥ñGò2ó&ôFõ\u001böGnSø-,-ö\u000eü\nõ\u001bòÊö/.\u0011\u000b\u0011\n,\u000b\u0003\u0006\u000f\u0006\u000eôFõ<\u000b\u0011\u0006\u000f\f%\u000b\u0007ûFýEö/HSö\u000f\u0006\u000eôFü\nò2ÿ&û!ü\nò\t\u0004Yöå\u0002ÿ&õCýFòúpö\u000eõ\u001bö\u000eûMüqrpo«ùq.\u0011\u000b\u0003\n2ôFö\u000f\u0000TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study\n020406080100\n4 6 8 10 12 14 16 18 20\nIJ\nK\nLM\nN\nKO\nP\nQ R\nS\nS\nT\nMP\nSUr\nY\u0018sutmW\u0014v\u001cwmx9y{z s|W\u0014Z9}~z wmZbV<)s|? W\u0014}Type I\nType II\nType III\nType IV\nType V\n05001000150020002500300035004000\n4 6 8 10 12 14 16 18 20\nc\nd\nK\nS\nT\nL\nN e\nf\ng\nN h\nK\ni'j\nK\nS'k\nlr\nY?s|t7W5v\u001cwmx\u0018yz s|W\u0014Z\u0018}~z wmZbV<)s|? W\u0014};\u0019\u0010*\u0007\t\b\r,\u0016\u0017\u000b\u001aBC\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006;)\u00100s\u0019\u000e\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b\u0003\t/\u000f\u0010#-\u0004\u000e\u001c\u0017\u0002\n\t\n020406080100\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\nIJ\nK\nLM\nN\nKO\nP\nQ R\nS\nS\nT\nMP\nSU{Y?)Z\u0018_ z ))_ z w7Zb2vz \u0016V<z )WType I\nType II\nType III\nType IV\nType V\n0500100015002000250030003500\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\nc\nd\nK\nS\nT\nL\nN e\nf\ng\nN h\nK\ni'j\nK\nS'k\nl{Y?)Z\u0018_ z ))_ z wmZb2vz \u0016V\u0012z )W=\u0019\u001b\b*\u0007\t\b\r,\u0016\u0017\u000b\u001aBCA\u000e*&\u000f\u001b\u000b\u000e\r,\u0016\u0017\u001b\u000f\u0010\r,\u0016\u0017)4\u000bYB+\u0006,\u0016\u0017\u0019O\t,\u0016\u0017\f\u0002ñGò2ó&ôFõ\u001bö7÷2Sø-,-ö\u000eü\nõ\u001bòÊö/.\u0011\u000b\u0011\nk\u000b\u0011\u0006\u000f\u0006\u000eôFõ<\u000b\u0003\u0006\u000f\f%\u000b\u001aûFý[ü\nò\t\u0004Yö\b\u0002\bÿ&õCýFòhúpö\u000eõ\u0010ö\u000eûü1ò2ûFýFö/HSò2ûFó'ä\u0001\u000b\u001aõ<\u000b\u0003\u0004Yö\u000eü\fö\u000eõ5\u0000\n020406080100\n00.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\nI\nJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<)s|? W\u0001)_ WType I\nType II\nType III\nType IV\nType V\n05001000150020002500\n00.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\nc d\nK\nST\nL\nN ef\ng\nN h\nK\ni'j\nK\nS\nk\nlV<)su\u0018 W/)_ Wn\u0016\u0017\u000b\u001a\u0002\u001b\u000f\u001b\u0006,\u0016\u0017\r,\"\u0003L\u0016I\u001c\u0017\r\b\u0002\n\u0006,\u0016\u0017\u000b\u001aB\n020406080100\n00.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\nI\nJ\nK\nL M\nN\nKOP\nQ R\nS\nST\nMP\nSUV<)s|? W\u0001)_ WType I\nType II\nType III\nType IV\nType V\n05001000150020002500\n00.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\nc d\nK\nST\nL\nN ef\ng\nN h\nK\ni'j\nK\nS\nk\nlV<)su\u0018 W/)_ W3)+*\u001aB+%OmF\u0006/\u000f\u0010\u000b\u001a\t,02)4\u0006\b#ñGò2ó&ôFõ\u001bö7÷&÷?ø-,-ö\u000eü\nõ\u001bòÊö/.\u0011\u000b\u0011\nk\u000b\u0011\u0006\u000f\u0006\u000eôFõ<\u000b\u0003\u0006\u000f\f%\u000b\u001aûFý[ü\nò\t\u0004Yö'\u0002ÿ&õ-ýFòhúpö\u000eõ\u0010ö\u000eûü\u000b\u0011\n2ò2ó&û\u0001\u0004Yö\u000eûMü©\u000b\u001aû\u0001\u000b\u0011\n\u0007\fM\u0000\nò\t\u0000­\u0004\u0018ö\u000eü\u0013FÿSý\u0001\u0000TheMACSISAcousticIndexingFrameworkforMusicRetrieval:AnExperimental Study#1\u0016\u0017\t\b\u0002H\u001f&\u0002\n\r,Dp\u0002\n\u0002\n\u000b\u0003\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cp\u000f\u001b\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\u0014\n\"\u0018\u000f\u0010\u000b\u001a\u00197\u0002\f(\u000e\u0002\n\u0014\n*\u001a\r,\u0016\u0017)+\u000bY\r,\u0016\u0017#-\u00024lSDG\u00021\t\b\u0002\f\r\r\b%\u001a\u0002C\u0019\u0007\u0002\u00130Ê\u000f\u0010*\u000e\u001c\u0017\r-\u000b\u000e*\u0007#.\u001f&\u0002\n\u0006H)\u00100;\u0019 \u0016\u0017#H\u0002\n\u000b\u0007\t,\u0016\u0017)4\u000b[\t/\u000f\u001b#-\u0004 \u001c\u0017\u0002\f\t@\r\b)O\u001f?\u0002'\u000els\u000f\u0010\u000b\u001a\u0019\u0005\r\b%\u0007\u0002\u0019\u001a\u0002\u001306\u000f\u001b* \u001c\u0017\r;A\u000e*&\u000f\u001b\u000b\u000e\r,\u0016\u0017\u001b\u000f\u0010\r,\u0016\u0017)4\u000bYB+\u0006,\u0016\u0017\u0019O\t,\u0016\u0017\n\u0002.\r\b)-\u001f&\u0002-¦49I¤\u000e9L\u0016\u0017B4*\u0007\u0006\b\u0002Y¦4¦C\u0014\n)+#-\u0004?\u000f\u0010\u0006\b\u0002\f\tCn\u0016\u0017\u000b\u001a\u0002\u001b\u000f\u0010\u0006,\u0016\u0017\r6\"!L\u0016I\u001c\u0017\r\b\u0002\n\u0006,\u0016\u0017\u000b\u0007B\u0005DG\u0016\u0017\r\b%\u00053)+*\u001aB+%\u0005mF\u0006/\u000f\u0010\u000b\u001a\t,502)4\u0006\b# \u000f\u0010\r-\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u000e\rOA\u000e*\u0007\u0002\n\u0006\b\"+56\t/\u000f\u001b#-\u0004 \u001cI\u0016\u0017\u000b\u001aBE\u0006/\u000f\u0010\r\b\u0002\f\t\n9Em3%\u0007\u0002O\r6DG)7#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\t\r/\u000f\u001bU+\u0002-\u0006\b)4*\u0007B4% \u001c\u0017\"\u0003\u000f\u001b\u000b7\u0002\nA\u000e*?\u000f\u001d\u001cp\u000f\u001b#-)+*\u001a\u000b\u000e\r>)\u00100p\r,\u0016\u0017#-\u0002-\r\b)C\u0002\n(\u000e\u0002\f\u0014\n*\u0007\r\b\u000249>},\u000bO\r\b\u0002\n\u0006\b#H\t)\u001b03\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cq\u000f\u0010\u0014\f\u0014\n*\u0007\u0006/\u000f\u001b\u0014\n\"+l\r\b%\u0007\u0002\u0015J)+*\u001aB+%\u0003mF\u0006/\u000f\u0010\u000b\u001a\t,02)4\u0006\b# \u0016\u0017\t>\u001f&\u0002\n\r\b\r\b\u0002\n\u0006\nl¢\u000f\u0010\r\u000f\u001d\u001cI5#-)4\t\b\r;\u000f\u001d\u001cI\u001cs\t/\u000f\u0010#H\u0004 \u001cI\u0016\u0017\u000b\u001aB'\u0006/\u000f\u001b\r\b\u0002\n\t\n9smJ%\u000e\u0016\u0017\t=\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\f\u000b\u0007\u0014\n\u0002>\u0016\u0017\t=\u0004?\u000f\u0010\u0006\b\r,\u001c\u0017\"u\u0019\u0007*\u001a\u0002.\r\b)-\r\b%\u0007\u000206\u000f\u001b\u0014\n\rs\r\b%&\u000f\u001b\r3n\u0016\u0017\u000b\u001a\u0002\u001b\u000f\u0010\u0006,\u0016\u0017\r6\"uL\u0016I\u001c\u0017\r\b\u0002\n\u0006,\u0016\u0017\u000b\u0007B-\u0016\u0017\tG\t\b\u0002\f\u000b\u0007\t,\u0016\u0017\r,\u0016\u001784\u0002>\r\b)1)4*\u0007\r,\u001c\u0017\"+\u0016\u0017\u000b\u001aB-\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\tq\u0016\u0017\u000b\r\b%\u001a\u0002=#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u001aB-\u0004 \u001c\u0017)4\r\nl?\u000f\u0010\u000b\u001a\u0019@#\u0015\u000f\u001d\"H\u000b\u0007)4\r<\u001f&\u0002\u000f\u0010\u001f\u000e\u001c\u0017\u0002>\r\b)=\u001c\u0017)\u000e\u0014\u001b\u000f\u0010\r\b\u0002;\r\b%\u0007\u0002;% \u0016\u0017\u0019\u001a\u0019\u001a\u0002\n\u000b\t\b\r\b\u0006/\u000f\n\u0016\u0017B+%\u000e\r@\u001cI\u0016\u0017\u000b\u001a\u0002\u0003DJ%\u001a\u0002\n\u000b!\r\b)\u000e)7#C\u000f\u001b\u000b\u000e\"[\u0006/\u000f\u001b\u000b\u0007\u0019\u001a)+#\u0004&)\u001b\u0016\u0017\u000b\u000e\r\b\t'\u0002\n(+\u0016\u0017\t\b\r\n9J)+*\u001aB+%mF\u0006/\u000f\u001b\u000b\u0007\t,02)4\u0006\b#\u0016\u0017\t=#-)4\u0006\b\u0002>\u0006\b)+\u001f\u001a*\u0007\t\b\r;\r\b)-\r\b%\u000e\u0016\u0017\tJ\u0004\u001a\u0006\b)+\u001f\u000e\u001c\u0017\u0002\n#'9}6\u000b7#-)4\t\b\r1)\u001b03)+*\u001a\u0006>\u0002\n(\u000e\u0004&\u0002\f\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\t\nlG\r\b%\u001a\u0002-\t\b\"\u000e\t\b\r\b\u0002\n# \u0004&\u0002\f\u0006,02)+\u0006\b#-\t>84\u0002\n\u0006\b\"\u0003DG\u0002\u0013\u001cI\u001c02)4\u00063\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r6\"7\r6\"\u000e\u0004&\u0002\n\t3*\u0007\u0004u\r\b)-mG\"\u000e\u0004&\u0002>}\u001c\u001fy2\t/\u000f\u0010#H\u0002>\t\b\u0014\n)+\u0006\b\u0002>\u001f\u001a*\u0007\r3\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r\u0004&\u0002\f\u0006,02)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\f\u0002\n\t/{\u00139GmG\"\u000e\u0004?\u0002B\u001f[\u0016\u0017\t3\u000b\u0007)4\rGDG\u0002\u0013\u001cI\u001cn%&\u000f\u001b\u000b\u0007\u0019\u000e\u001c\u0017\u0002\n\u0019SlS\t,\u0016\u0017\u000b\u0007\u0014\n\u0002=\u0016\u0017\rs\u0016\u0017\u000b\u0007\u0014\u0013\u001c\u0017*\u001a\u0019\u001a\u0002\n\t\u0004\u000e\u0016\u0017\r\b\u0014\u0013%\u0005\r\b\u0006/\u000f\u001b\u000b\u0007\t\b\u0004&)4\t,\u0016\u0017\r,\u0016\u0017)+\u000b\u001a\tCDJ%\u000e\u0016\u0017\u0014/%\u0005\u000f\u0010\u0006\b\u0002C\u000b\u001a)+\r>\r/\u000f\u0010U4\u0002\n\u000b7\u0016\u0017\u000b\u000e\r\b)\u0003\u000f\u001b\u0014\n\u0014\n)+*\u001a\u000b\u000e\r.Dq\u0016\u0017\r\b%)4*\u0007\u0006302\u0002\u001b\u000f\u0010\r\b*\u001a\u0006\b\u0002>8+\u0002\f\u0014\n\r\b)+\u00063\u0019\u001a\u0002\n\t,\u0016\u0017B+\u000bS9\n5.SUMMAR YANDFUTURE WORK\u0001\u0003\u00027%?\u000f\n8+\u0002Y\u0004\u001a\u0006\b\u0002\n\t\b\u0002\n\u000b\u000e\r\b\u0002\f\u0019^)4*\u0007\u0006N|'B\u00193\u000e}\b[\t\b\u0004&\u0002\f\u0014\n\r\b\u0006/\u000f\u001d\u001c.\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u001aB[02\u0006/\u000f\u0010#H\u0002\u00135DG)4\u0006\bUY\r\b)\u0003\u0004&\u0002\n\u0006,02)4\u0006\b# \u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u0019E#1*\u001a\t,\u0016\u0017\u0014O\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001c=)4\u000b!\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014\u0019?\u000f\u0010\r/\u000fYDG\u0016\u0017\r\b%W\u000f\u0010\u0014\n)4*\u0007\t\b\r,\u0016\u0017\u0014\u0018A\u000e*\u0007\u0002\n\u0006,\u0016\u0017\u0002\f\t\n9m3%\u001a\u0002'02\u0006/\u000f\u0010#-\u0002\fDG)+\u0006\bU\u0005\u0016\u0017\u000b\u000e8+)\u0010\u001c\u001784\u0002\n\tO#.* \u001cI5\r,\u0016\u0017\u0004\u000e\u001c\u0017\u0002Y#-)\u000e\u0019\u001a* \u001c\u0017\u0002\f\t7\r\b%?\u000f\u0010\ru\u0014\u001b\u000f\u0010\u000bE\u001f&\u0002O\u0016\u0017#-\u0004\u000e\u001c\u0017\u0002\n#-\u0002\n\u000b\u000e\r\b\u0002\f\u0019\u0016\u0017\u000bW\u0019 \u0016Itn\u0002\n\u0006\b\u0002\n\u000b\u000e\rOD3\u000f\n\"\u000e\t\n9r\t\b\u0002\n\r>)\u001003\u0002\f(\u000e\u0004&\u0002\n\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\t.%?\u000f\u001d84\u0002C\u001f&\u0002\n\u0002\n\u000bY\u0014\f)+\u000b\u001a\u0019\u0007*\u001a\u0014\n\r\b\u0002\n\u0019Y\r\b)u\t\b\r\b*\u0007\u0019\u001a\"\u0003\r\b%\u0007\u0002-\u0002/0h502\u0002\f\u0014\n\r\b\tF)\u00100S84\u000f\u001b\u0006,\u0016\u0017)+*\u001a\tG\u0019\u0007\u0002\f\t,\u0016\u0017B+\u000b-\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002\n\t\n9qX«(\u000e\u0004&\u0002\f\u0006,\u0016\u0017#-\u0002\n\u000b\u000e\r\b\ts%&\u000f\n8+\u00023\t\b%\u001a)\u001bDJ\u000bH\r\b%&\u000f\u001b\r\u0004\u001a\u0006\b)+\u0004&\u0002\f\u00061\u0014/%\u0007)\u001b\u0016\u0017\u0014\n\u0002C)\u00100p\r\b%\u0007\u0002\f\t\b\u0002-#-)\u000e\u0019\u001a* \u001c\u0017\u0002\n\t.\u0014\u001b\u000f\u001b\u000b7\u001c\u0017\u0002\u001b\u000f\u0010\u0019\u0003\r\b)O\u000f\u001b\u000b7\u0002/\u0012C\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\r.\t\b\"\u000e\t,5\r\b\u0002\n#ªD3% \u0016\u0017\u0014/%\u0003\u0019\u0007\u0002\n\r\b\u0002\f\u0014\n\r\b\t>#1*\u001a\t,\u0016\u0017\u0014-\u0014\n)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r>\t,\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"YD3%\u000e\u0016I\u001c\u0017\u0002@\r\b)\u001b\u001c\u0017\u0002\n\u0006/\u000f\u0010\r,\u0016\u0017\u000b\u001aB\r\b\u0002\n#H\u0004&)O\u0014\u0013%?\u000f\u0010\u000b\u001aB+\u0002\f\t\nlF\t\b)+#-\u0002C\u0004&\u0002\f\u0006,02)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\f\u0002@\t\b\r6\"+\u001c\u0017\u0002C\u0014\u0013%?\u000f\u0010\u000b\u001aB+\u0002\n\tH\u000f\u0010\u000b\u001a\u0019Y\u000b\u001a)\u0010\u0016\u0017\t\b\u0002+l\u000f\u001b\t1\u001c\u0017)+\u000b\u001aBY\u000f\u001b\t-\r\b%\u001a\u0002@\u0014/%?\u000f\u0010\u000b\u001aB+\u0002\n\tu\u000f\u0010\u0006\b\u0002C\u000b\u001a)+\r-\r\b)\u000e)O#.*\u001a\u0014\u0013%!\u000f\u0010\u000b\u001a\u0019\u0005\r\b%\u0007\u0002C\u0019\u000e\u0016ItS\u0002\n\u0006\b\u0002\n\u000b\u000e\r\u0004&\u0002\f\u0006,02)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\f\u0002\n\t>\u000f\u001b\u0006\b\u0002>\u001f&\u000f\u001b\t\b\u0002\n\u0019O)+\u000bO\r\b%\u001a\u0002>\t/\u000f\u0010#-\u0002>\t\b\u0014\f)+\u0006\b\u0002+9\u0007*\u001a\u0006\b\r\b%\u001a\u0002\n\u0006>\t\b\r\b*\u0007\u0019\u001a\"'\u0016\u0017\t>\u0019\u001a\u0002\n\t,\u0016\u0017\u0006\b\u0002\n\u0019Y\r\b)-Z?\u000b\u0007\u0019\u0003D3\u000f\n\"\u000e\t>\r\b)u\u000f\u0010*\u001a\r\b)+#C\u000f\u001b\r\b\u0002-\t\b\u0002\u0013\u001c\u0017\u0002\n\u0014\n\r,\u0016\u0017)4\u000b)\u001b0s\u0014\n\u0002\n\u0006\b\r/\u000f\u001d\u0016\u0017\u000bO\u0004?\u000f\u0010\u0006/\u000f\u001b#-\u0002\n\r\b\u0002\n\u0006\b\t;\r\b)-)+\u0004\u001a\r,\u0016\u0017#1\u0016\u0017\n\u0002.\u0004&\u0002\n\u0006,0)+\u0006\b#C\u000f\u001b\u000b\u0007\u0014\n\u000249Xs\u000f\u0010\u0014/%^)\u001b0.\r\b%\u0007\u0002\u0003\r\b%\u001a\u0006\b\u0002\n\u0002Y\u0004\u001a%&\u000f\u001b\t\b\u0002\n\tO)\u00100.\r\b%\u001a\u0002Y\u000f\n\u001c\u0017B+)4\u0006,\u0016\u0017\r\b%\u0007#$#C\u000f\n\"E\u000b\u001a\u0002\n\u0002\n\u0019[02*\u001a\u0006,5\r\b%\u001a\u0002\n\u0006=\u0006\b\u0002/Z&\u000b\u001a\u0002\n#-\u0002\n\u000b\u000e\r\n9:<\u0002\n\r\b\r\b\u0002\n\u0006;\t,\u0016\u0017B+\u000b?\u000f\u001d\u001cp\u0004\u0007\u0006\b)\u000e\u0014\n\u0002\f\t\b\t,\u0016\u0017\u000b\u0007Bu\r\b\u0002\n\u0014\u0013%\u001a\u000b\u000e\u0016\u0017A\u000e*\u0007\u0002\n\t.\u0014\u001b\u000f\u001b\u000b7\u001f&\u0002*\u001a\t\b\u0002\n\u0019-\u0016\u0017\u000bC\u0004\u0007%?\u000f\u0010\t\b\u0002.¦3\r\b)>B+\u0002\n\u000b\u001a\u0002\n\u0006/\u000f\u0010\r\b\u0002\u000f=#-)+\u0006\b\u00023#-\u0002\u001d\u000f\u0010\u000b\u000e\u0016\u0017\u000b\u0007B\u001b02* \u001c¢\t\b\u0002\nA\u000e*\u001a\u0002\n\u000b\u0007\u0014\f\u0002=\u0006\b\u0002\n\u0004\u000e5\u0006\b\u0002\n\t\b\u0002\f\u000b\u000e\r/\u000f\u0010\r,\u0016\u0017)+\u000bSK=\u0016\u0017#-\u0004\u001a\u0006\b)\u001b8+\u0002\n\u0019\u0005\u0016\u0017\u000b\u0007\u0019\u001a\u0002\n(+\u0016\u0017\u000b\u001aB[\r\b\u0002\n\u0014/%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002\n\t@\u0016\u0017\u000b!\u0004\u0007%?\u000f\u0010\t\b\u0002\u0018¤O#C\u000f\n\"02*\u001a\u0006\b\r\b%\u001a\u0002\n\u0006-\u0006\b\u0002\n\u0019\u001a*\u0007\u0014\f\u0002@06\u000f\n\u001c\u0017\t\b\u0002'%\u000e\u0016\u0017\r\b\t'\u000f\u001b\u000b\u0007\u0019\u0005\t\b\u0004&\u0002\f\u0002\n\u0019\u0005*\u0007\u0004\u0005\r\b%\u001a\u0002@\u001c\u0017)\u000e)+U\u000e*\u001a\u0004[\u0004\u001a\u0006\b)\u000e\u0014\n\u0002\n\t\b\t\nK#-)4\u0006\b\u0002-\u0002\u0013\u001c\u001e\u000f\u001b\u001f&)+\u0006/\u000f\u001b\r\b\u0002'#C\u000f\u001b\r\b\u0014\u0013%\u000e\u0016\u0017\u000b\u0007BO#H\u0002\n\r\b%\u0007)\u000e\u0019\u001a\t>\u0016\u0017\u000b7\u0004\u001a%&\u000f\u001b\t\b\u0002u@\u0014\n)4* \u001c\u0017\u00197\u001c\u0017\u0002\u001d\u000f\u0010\u0019Y\r\b)#-)4\u0006\b\u0002;\u000f\u0010\u0014\n\u0014\n*\u001a\u0006/\u000f\u0010\r\b\u0002;% \u0016\u0017B4% 52\u001c\u0017\u0002\n84\u0002\u0013\u001cF\t,\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"C\u0002\n\t\b\r,\u0016\u0017#\u0015\u000f\u0010\r,\u0016\u0017)+\u000bC02\u0006\b)+#\u0011\u001c\u0017)\u001bDG52\u001c\u0017\u0002\n8+\u0002\u0013\u001c#C\u000f\u001b\r\b\u0014\u0013%\u001a\u0002\n\t\n9s\u0001\u0003\u0002.\u000f\u0010\u0006\b\u0002.\u000f\u001d\u001c\u0017\t\b)H\u0004 \u001c\u001e\u000f\u001b\u000b\u0007\u000b\u000e\u0016\u0017\u000b\u0007BO\r\b)H\u000f\u0010*\u001aB+#H\u0002\n\u000b\u000e\r=\r\b%\u001a\u0002>\u000f\n\u001c\u0017B+)+\u0006,\u0016\u0017\r\b%\u001a#\r\b)%?\u000f\u0010\u000b\u001a\u0019 \u001c\u0017\u0002H#-)+\u0006\b\u0002>)\u001b0s\r\b%\u0007\u0002>\r6\"\u000e\u0004&\u0002\u00135\u001c\u001f\u0011\u0014\u001b\u000f\u001b\t\b\u0002=\u0016\u0017\u000b\u0007\u0014\u0013\u001c\u0017*\u001a\u0019\u000e\u0016\u0017\u000b&BO\r\b\u0006/\u000f\u0010\u000b\u001a\t\b\u0004&)+\t,\u0016\u0017\r,\u0016\u0017)4\u000b\u0007\t\n9\n6.REFERENCES£g¦/§HXq9pJ\u001cI\u001c\u001e\u000f\u0010#\u0015\u000f\u0010\u000b\u001a\u0014\u0013%\u001a\u0002+l'\u000e9qJ\u0002\n\u0006\b\u0006\b\u00024l>T19qJ\u0002\u0013\u001cI\u001c\u0017#.*\u0007\r\b%SlH:J9;\u0007\u0006\u001c )4\u001f&\u000f\u000el>m>9>\u000f\u0010\t\b\r\b\u000b\u001a\u0002\n\u0006>\u000f\u001b\u000b\u0007\u0019\u0003|79)\u0019G\u0006\b\u0002\n#-\u0002\f\u0006\nlsP\u001e\u0019G)4\u000b\u000e\r\b\u0002\n\u000b\u000e\r,56\u001f&\u000f\u001b\t\b\u0002\n\u00197}6\u0019\u0007\u0002\f\u000b\u000e\r,\u0016IZ&\u0014\u001b\u000f\u001b\r,\u0016\u0017)+\u000b)\u001003;*\u0007\u0019\u000e\u0016\u0017)Y|O\u000f\u001b\r\b\u0002\n\u0006,\u0016\u001e\u000f\u001d\u001c3\t,\u0016\u0017\u000b\u0007B\u0018|Ov<XsJ5ÃOL)\u001bD S\u0002\n8+\u0002/\u001c;~J\u0002\n\t\b\u0014\f\u0006,\u0016\u0017\u0004 5\r,\u0016\u0017)+\u000b\u001aR\u0007l<\u0016\u0017\u000bé(\nk&c2d\u001ba/k&`\u000ec2eh\u000ek\u0007`+ )&S¡+ÀG?+f\ne\u001ej\u0007Àª\u000ek !Oj\u000ef\neh]w(\nk]\n\u000ea/À1`+bcÊe\u001e\u000ekpd\u0010c2a/ehd<|\u001b`+ Il\u000e¤4+ ¦49£I¤\n§C\u000e9SvM9:<\u0002\u0013\u001cI\u001c\u0017)\u001alG19|')4\u000b\u000e\r,\u00163\u000f\u001b\u000b\u0007\u0019\u0018|79\u001a\u000f\u0010\u000b\u001a\u0019 \u001c\u0017\u0002\n\u0006\nlP\bmF\u0002\n\u0014\u0013%\u001a\u000b\u000e\u0016\u0017A\u000e*\u0007\u0002\n\t>02)4\u0006=*\u001a\r\b)+#C\u000f\u001b\r,\u0016\u0017\u0014>|'*\u001a\t,\u0016\u0017\u0014=mF\u0006/\u000f\u0010\u000b\u001a\t\b\u0014\n\u0006,\u0016\u0017\u0004\u001a\r,\u0016\u0017)+\u000b\u001aR\u0007l&\u0016\u0017\u000b(\nk&c2d\u001ba\u0013k?`\u000ecÊe\u001e\u000ek\u0007`+ 9&S¡+À>b&+f\fehj\u001aÀ\u000ek'!Oj\u000ef\neh]\u0005(\nk]\n\u000ea/À.`\u000ecÊe\u001e\u000ekpd\u001bc2a\u0013ehd<|\u001b`+ Il¤++4 9£I\n§Hn9;:«\u001c\u001e\u000f\u001b\u0014\u0013U\u000e\u001f\u001a*\u001a\u0006\b\u000b\u000f\u001b\u000b\u0007\u0019Å~9;~J\u0002\u001doq)4*\u0007\u0006\b\u00024lOP\b mF)\u000e)\u001b\u001c>02)+\u0006\u0019G)+\u000b\u000e\r\b\u0002\f\u000b\u000e\r:G\u000f\u0010\t\b\u0002\n\u0019 =\u000f\n8+\u0016\u0017B\u000e\u000f\u001b\r,\u0016\u0017)+\u000bE)\u001b0.|'*\u001a\t,\u0016\u0017\u0014\nR\u0007l3\u0016\u0017\u000bpa,\u001b]\u001c,\"­$! !Nj  \u0017c2ehÀ.d\nb#\u000eeh`+l&¦\u001b4+\u000e9£ ¥\u001b§C\u000e9*\u0019=9n:<\u0006\b)\u001bDJ\u000bN\u000f\u0010\u000b\u001a\u0019\u0003:J9b&%?\u000f\u0010\u000b\u001aB\u0007lpP/|'*\u0007\t,\u0016\u0017\u0014\u001b\u000f\n\u001c3\u001a\u0006\b\u0002\nA\u000e*\u0007\u0002\f\u000b\u0007\u0014\n\"umF\u0006/\u000f\u0010\u0014/U+5\u0016\u0017\u000b\u0007BC*\u001a\t,\u0016\u0017\u000b\u0007Bu\r\b%\u0007\u0002H|'\u0002\n\r\b%\u001a)\u000e\u0019\u0007\t;)\u00100~\u0019G)+\u000b\u000e84\u0002\n\u000b\u000e\r,\u0016\u0017)+\u000b?\u000f\u001d\u001cq\u000f\u0010\u000b\u001a\u0019wx =\u000f\u001b\u0006\b\u0006\b)\u001bDG\u0002\n\u0019Sx=*\u001a\r\b)\u000e\u0014\n)+\u0006\b\u0006\b\u0002\u0013\u001c\u001e\u000f\u001b\r,\u0016\u0017)+\u000b\u001aR\u0007l/©\";]\u0013\u000ej\u000ef\nc5B&S\u001b]\u001c\u0005\"À->4 l.\u0004\u0007\u0004S9.¤4\u0010¥\u000e\n5¤++\u001b¥\u00079¦\u001b+\u000e¦+9£I\n§.\u0001^9s\u0019q%?\u000f\u001d\u0016\u0015\u000f\u0010\u000b\u001a\u0019¼:J9B\u001fs\u0002\n\u0006\b\u0014\n)\u000e\u00024lOP/|'\u0002\u0013\u001c\u0017)\u000e\u0019\u001a\"oG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cuT=\u000bWm3%\u0007\u0002\u00017\u0002\n\u001f\u001aR\u0007lpa,\u001b]\u001c!Oj\u000e \u0017c2ehÀ.d\u001e#\u000eeh`$F\u000eÀGj\u0007c2ehk\u000eÁ\u0005`\u000ek*#d\u0010c5F\u000ea\u001cÃ\u000eehk\u000eÁ+l¤++4¤ 9\n£I\n§C\u000e9s\u0007)\u000e)4\r\b\u0002+lHPQoMm==o=¨soG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n8+\u0016\u0017\u000b\u001aBET=\u0006\b\u0014/%\u001a\u0002\n\t\b\r\b\u0006/\u000f\u001d\u001c|'*\u0007\t,\u0016\u0017\u0014\u0003\u001f\u000e\"L)4\u000b\u0007B\u001b5,mF\u0002\f\u0006\b#\u000e\r\b\u0006\b*\u0007\u0014\f\r\b*\u0007\u0006\b\u0002\nR\u001alL\u0016\u0017\u000b (\nk&c2d\u001ba\u0013k?`\u000ecÊe\u001e\u000ek&`\u0010 *&n¡+ÀG&+f\fehj\u001aÀ\u000ek!Oj\u000ef\ne\u001e]-(\nk]\n\u000ea/À1`\u000ec2eh\u000ekGd\u001bc2a\u0013e\u001ed5|\u001b`+ Il¤+4+ 9£IÃ\n§.19H=%\u000e\u0016\u001e\u000f\u0010\t\nlY\u000e9\u0015L)+B\u000f\u0010\u000bSl\u0003~9\u0018\u0019G%&\u000f\u001b#1\u001f&\u0002\f\u0006,\u001cI\u0016\u0017\u000b\u000f\u001b\u000b\u0007\u0019 :J9C\u000e#1\u0016\u0017\r\b%SlP\u001eý=*\u001a\u0002\n\u0006\b\">:s\">J*\u001a#-#1\u0016\u0017\u000b\u001aB|'*\u001a\t,\u0016\u0017\u0014\u001b\u000f\u001d\u001c\u001a}6\u000b 0)+\u0006\b#C\u000f\u001b\r,\u0016\u0017)+\u000bCoG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001c\u000e\u0016\u0017\u000b\u000f\u0010\u000bu=*\u001a\u0019 \u0016\u0017)u~=\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002\nR\u001al&\u0016\u0017\u000b Ga6\u001b]M\"­$! !Nj  \u0017c2ehÀ.d\u001e#\u000ee\u001e`+l&¦\u001b4+ 9£I\n§H19;=\u000f\u001b*\u0007\tN\u000f\u0010\u000b\u001a\u0019\u0011XG9;vM)\u0010\u001cI\u001c\u001e\u000f\u001b\t\b\r\b\u0006,\u00162l\u0003PQ;\u000bE=*\u001a\u0019 \u0016\u0017)W\u001a\u0006\b)+\u000b\u000e\rOXF\u000b\u001a\u001902)4\u0006ý=*\u0007\u0002\f\u0006\b\"+5,\u001f\u000e\"+53*\u0007#-#.\u0016\u0017\u000b\u0007Bu\u000e\"\u000e\t\b\r\b\u0002\n#-\t\bR\u0007l\u001a\u0016\u0017\u000b(\nk&c2d\u001ba\u0013k&`\u000ec2eh\u000ek?`\u0010 \u0011&S¡+ÀG?+bf\ne\u001ej\u0007Àr\u000ek'!Oj\u000ef\neh]-(\nk?\n\u000ea\u0013À.`\u000ec2eh\u000ekpd\u001bc2a\u0013ehd<|\u001b`+ Il¤++\u000e¦+9£I\n§HvM9\n},\u000b\u001a\u0019\u001a\"\u000eU.\u000f\u0010\u000b\u001a\u0019.o94|')4\r,D3\u000f\u001b\u000b \u00162lLPQ=\u0004\u001a\u0004\u001a\u0006\b)\u0010(+\u0016\u0017#\u0015\u000f\u0010\r\b\u000233\u0002\u001b\u000f\u0010\u0006\b\u0002\n\t\b\r¢J\u0002\u0013\u0016\u0017B4% 5\u001f&)+\u0006\b\t\n¨¼mF)\u001bD3\u000f\u0010\u0006\b\u0019\u001a\tYoG\u0002\n#H)\u00108+\u0016\u0017\u000b\u001aBW\r\b%\u0007\u0002é\u0019G*\u0007\u0006\b\t\b\u0002\u0005)\u001b0O~G\u0016\u0017#-\u0002\n\u000b\u001a\t,\u0016\u0017)+\u000b?\u000f\u001d\u001cI5\u0016\u0017\r,\"\u000eR\u001al+\u0016\u0017\u000b¡pa,\u001b]\u001c£¢2¤\u000ec\u0017_&S¡4Àq?+f\fehj\u001aÀE\u000ek¦¥?_&d\u0013\u000ea/¡=m©$F\u000eÀGj\u0007c2ehkÁl¦\u001b4+ 9£\u0017¦\u001b\n§Ho=9\u000e4\u000f\u001d\u0016\u0017\u000bSl\u001ao=9/>\u000f\u001b\t\b\r\b*\u0007\u0006,\u0016n\u000f\u0010\u000b\u001a\u0019C:J9419 \u0014/%\u000e*\u001a\u000b\u0007\u0014/U&l*!O`\u000e]/_&ehk?d§¢egf\feh\u000ek\u0007l|'\u0014\u001b=\u0006/\u000f\nDG5\bG\u0016I\u001cI\u001c2lF¦\u001b4+ 9£\u0017¦+¦/§H92=)4\t\b*\u0007B\u001b\u00162lb¨19\u001aG\u0016\u0017\t\b% \u0016\u0017%?\u000f\u0010\u0006/\u000f\u000elSm>9&\u001a\u000f\u0010U4\u000f\u001b\r/\u000f lS|792¨p\u000f\u0010#C\u000f\u001b#1*\u001a\u0006\b)H\u000f\u0010\u000b\u001a\u001919{=*\u0007\t\b%\u000e\u0016\u0017#C\u000f lpP\bvF\u0006/\u000f\u0010\u0014\n\r,\u0016\u0017\u0014\u001b\u000f\n\u001c~ý=*\u0007\u0002\f\u0006\b\"+5\b:s\"+5\b3*\u0007#H#1\u0016\u0017\u000b\u0007Bu\u000e\"\u000e\t\b\r\b\u0002\n#02)+\u0006¢\u000f3M\u000f\u0010\u0006\bB4\u0002G|'*\u001a\t,\u0016\u0017\u0014;~=\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002\nR\u001al+\u0016\u0017\u000bq\"­$! !Oj\u000e \u0017c2ehÀ1d\r#\u000eeh`+l\u001d¤++4 9£\u0017¦\u001b¤\n§Ho=9L\u000e9?|'\u0014\u001b;\u000f\u0010\u001fSlSF9&19?\u000e#1\u0016\u0017\r\b%Sl&}\u00139?>9&\u0001w\u0016\u0017\r\b\r\b\u0002\f\u000bLl7\u00199?F9SJ\u0002\f\u000b\u0007\u0019\u001a\u0002\n\u0006,5\t\b)+\u000bN\u000f\u0010\u000b\u001a\u0019\u0003n9F\u000e9\u0014\u0019G*\u0007\u000b\u001a\u000b\u000e\u0016\u0017\u000b\u0007B4%&\u000f\u001b#'l3P\bmF)\u001bD3\u000f\u0010\u0006\b\u0019\u001a\t=\r\b%\u001a\u0002>\u0019 \u0016\u0017B\u001b\u0016\u0017\r/\u000f\u001d\u001cp#1*\u001a\t,\u0016\u0017\u0014\u001cI\u0016\u0017\u001f\u0007\u0006/\u000f\u001b\u0006\b\"&¨\u0003mF*\u0007\u000b\u001a\u0002'\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c=02\u0006\b)4#\u000f\u001b\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014V\u0016\u0017\u000b\u001a\u0004\u001a*\u0007\r\bR\u001alJ\u0016\u0017\u000b©Ga6\u0010]\u001c\"­$!«ª>eIÁ\u000eehc2`+ *z¢eh¾\u001ba,`\u000ea/ehd\nf\u0013l&¦\u001d+4 9£\u0017¦\u001b\n§HXq9?~>9&\u000e\u0014/%\u0007\u0002\u0013\u0016\u0017\u0006\b\u0002\f\u0006\nl\u0001!Oj\u000ef\neh]\nb\u001czFegf\fc2d\u0010k?ehk+ÁB&S¡\u001bf\nc2d\u001bÀ>f\u0013lv«%S9S~>9\u001a\u0019 \u0016\u0017\t\b\t\b\u0002\n\u0006,5\r/\u000f\u0010\r,\u0016\u0017)4\u000bLln|O\u000f\u0010\t\b\t/\u000f\u0010\u0014/%\u000e*\u001a\t\b\u0002\n\r\b\r\b\t>},\u000b\u001a\t\b\r,\u0016\u0017\r\b*\u0007\r\b\u0002.)\u00100smF\u0002\f\u0014\u0013%\u001a\u000b\u0007)\u001b\u001c\u0017)+B4\"+l¤+4+ 9£\u0017¦\n¥\u001b§.194&9\u000ems\u000f\u0010\u000b\u001aB+*\u000e\u0016\u001e\u000f\u0010\u000b\u001a\u0002+l\u0014\">a\u0013c2e ¬p]\u0010e\u001e`+ Epd\u0010a6]\u0013d,c2eh\u000ek.`\u000ek*#B!Oj\u000ef\feh]pd\u0013]//Á+bk&ehc2eh\u000ek\u0007l\u000e\u0004\u0007\u0006,\u0016\u0017\u000b\u001aB+\u0002\n\u0006,5\u001c\u001fs\u0002\n\u0006,\u001c\u001e\u000f\u0010B\u001al<¦\u001d+4 9£\u0017¦\u001b\n§H19Gm3\u001b\u000f\u001b\u000b\u0007\u0002\n\r/\u000f\u001bU+\u0016\u0017\t\u0003\u000f\u0010\u000b\u001a\u0019\u0011vM90\u0019q)\u000e)4U&lCP\b=*\u001a\u0019 \u0016\u0017)!},\u000b\u000e02)+\u0006\b#\u0015\u000f\u0010\r,\u0016\u0017)+\u000b¼oG\u0002\u00135\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cuy2=}oJ{CmF)\u000e)\u0010\u001c\u0017\t\bR\u001al\u0016\u0017\u000b²(\nk&c2d\u001ba\u0013k?`\u000ecÊe\u001e\u000ek&`\u0010 ,&n¡+ÀG&+f\fehj\u001aÀ \u000ek!Oj\u000ef\ne\u001e]-(\nk]\n\u000ea/À1`\u000ec2eh\u000ekGd\u001bc2a\u0013e\u001ed5|\u001b`+ Il¤+4+ 9£\u0017¦\u001b\n§H19Mm3\u001b\u000f\u001b\u000b\u0007\u0002\f\r/\u000f\u0010U+\u0016\u0017\t\nlp19FXF\t\b\t,\u001cq\u000f\u0010\u000b\u001a\u0019\u0005vM9\u0001\u0019G)\u000e)+U&lJPQ=*\u001a\r\b)+#\u0015\u000f\u0010\r,\u0016\u0017\u0014O|'*\u000e5\t,\u0016\u0017\u0014\u001b\u000f\u001d\u001c=\u0002\n\u000b\u001a\u0006\b\u0002\u0019<\u001c\u001e\u000f\u0010\t\b\t,\u0016IZ?\u0014\u001b\u000f\u0010\r,\u0016\u0017)4\u000b\u0011)\u00100=*\u001a\u0019 \u0016\u0017)!+\u0016\u0017B+\u000b?\u000f\u001d\u001c\u0017\t\bR\u001alJ\u0016\u0017\u000b(\nk&c2d\u001ba\bbk&`\u000ec2eh\u000ek&`+ ,&n¡+ÀG&+f\fehj\u001aÀ \u000ek«!Oj\u000ef\ne\u001e](\nk]\n\u000ea/À1`\u000ec2eh\u000ek­Gd\u001bc2a/ehd<|\u0010`+ Il¤+4 ¦+9£\u0017¦\u001bÃ\n§HXq9\u0018\u00017)\u001b\u001c\u0017\u0019Llm>9Ä:«\u001c\u0017*\u0007#ul~9®=\u0002/\u0016\u0017\t,\u001c\u001e\u000f\u0010\u0006ª\u000f\u0010\u000b\u001a\u0019$\u000e9Y\u0001[%\u001a\u0002\u001b\u000f\u001b\r\b)+\u000bSlP\u001e\u0019G)+\u000b\u000e\r\b\u0002\f\u000b\u000e\r,5\b:p\u000f\u0010\t\b\u0002\n\u0019y\u0019p\u001c\u001e\u000f\u001b\t\b\t,\u0016IZ&\u0014\u001d\u000f\u0010\r,\u0016\u0017)+\u000bSl.\u000e\u0002\u001b\u000f\u0010\u0006\b\u0014/%E\u000f\u001b\u000b\u0007\u0019\u0005\u0006\b\u0002\n\r\b\u0006,\u0016\u0017\u0002\f8+\u000f\n\u001c>)\u001b0\u000f\u0010*\u001a\u0019 \u0016\u0017)4R\u0007l&\u0016\u0017\u000b(\u0018¯°¯±¯é!Oj\u000e \u0017c2ehÀ1d\r#\u000eeh`+l\u001a\u000ey6+{\u0013l¦\u001b+4 9£\u0017¦\u001b\n§w\u0019=9¨p\u000f\u0010\u000b\u001aB\u0007l¢P/|'*\u0007\t,\u0016\u0017\u0014H~=\u000f\u001b\r/\u000f\u0010\u001f?\u000f\u0010\t\b\u0002Hoq\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cp:G\u000f\u001b\t\b\u0002\n\u0019')4\u000bO\u000e\u0004&\u0002\n\u0014\n\r\b\u0006/\u000f\n\u001c\u0016\u0017#.\u0016I\u001c\u001e\u000f\u0010\u0006,\u0016\u0017\r,\"\u000eR\u001alJ\u0016\u0017\u000bé(\nk&c2d\u001ba/k&`\u000ec2eh\u000ek&`\u0010 )&S¡+ÀG?+f\ne\u001ej\u0007Àª\u000ek !Oj\u000ef\neh]w(\nk\u0007b\n\u000ea/À1`\u000ec2eh\u000ekGd\u001bc2a\u0013e\u001ed5|\u001b`+ Il¤+4 ¦+9£\u0017¦\u001b\n§w\u0019=9\u001c¨G\u000f\u001b\u000b\u0007B\u001alP\u0013|'\u0005\u00193n¨|'*\u001a\t,\u0016\u0017\u0014J;*\u0007\u0019\u000e\u0016\u0017)q\u0019G%&\u000f\u001b\u0006/\u000f\u0010\u0014\n\r\b\u0002\f\u0006,\u0016\u0017\t\b\r,\u0016\u0017\u0014. \u0002\nA\u000e*\u001a\u0002\n\u000b\u001a\u0014\n\u0002},\u000b\u001a\u0019\u0007\u0002\f(+\u0016\u0017\u000b\u0007B70)+\u0006H\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\" oq\u0002\f\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001c\u0017R\u0007lG\u0016\u0017\u000bé(\u0018¯°¯±¯«²u\u000ea\u001cÃf_&\u0013\u000ek \"G\u000eS \u0017eh]/`\u000ecÊe\u001e\u000ek\u0007f!mu&SeIÁ\u000ek&`+ ³Ga6\u0010]/d\nf\u0013f\fehk\u000eÁc2û\">j\u0003#\u000eeh`\u000ek*#\"=]/\u000ej f\fc2eh]\nf\u0013l4¤4+ ¦49£ ¤+\n§w\u0019=9´¨p\u000f\u0010\u000b\u001aB\u0007lqP\u0013XM\u0012C\u0014\u0013\u0016\u0017\u0002\n\u000b\u000e\r>;\u0014\n)+*\u001a\t\b\r,\u0016\u0017\u0014-},\u000b\u001a\u0019\u001a\u0002\n('02)+\u0006.|'*\u001a\t,\u0016\u0017\u0014CoG\u0002\n\r\b\u0006,\u0016\u0017\u0002\n84\u000f\n\u001cDG\u0016\u0017\r\b%\u001fp\u000f\u0010\u0006,\u0016\u0017)4*\u0007\tp~J\u0002\nB4\u0006\b\u0002\n\u0002\n\ts)\u00100+\u0016\u0017#1\u0016I\u001c\u001e\u000f\u001b\u0006,\u0016\u0017\r,\"\u000eR&l\u0007\u0016\u0017\u000b³Ga6\u0010]\u001c\u000f\"­$! !Oj\u000e Ibc2ehÀ1d\r#\u000eeh`+l\u001a¤4+4¤ 9£ ¤ ¦/§w\u0019=9µ¨p\u000f\u0010\u000b\u001aB[\u000f\u001b\u000b\u0007\u0019Em>9qL)+\u001d\u000f\u0010\u000b\u001a)\u00105v·¶ \u0002\f\u0006\b\u0002\n+luP\b},#C\u000f\u001bB+\u0002\u0018~=\u000f\u0010\r/\u000f\u001b\u001f&\u000f\u001b\t\b\u0002\u0005oG\u0002\u00135\r\b\u0006,\u0016\u0017\u0002\n84\u000f\u001d\u001cuDG\u0016\u0017\r\b% |'*\u000e\u001c\u0017\r,\u0016\u0017\u0004 \u001c\u0017\u0002/5,}6\u000b\u0007\t\b\r/\u000f\u001b\u000b\u0007\u0014\u001b\u0002S\u0002\u001b\u000f\u0010\u0006\b\u000b\u000e\u0016\u0017\u000b\u001aBmF\u0002\f\u0014\u0013%\u001a\u000b \u0016\u0017A\u000e*\u001a\u0002\n\t\bR\u001alGa6\u001b]\u0005(\nk&c2d\u001ba\u0013k?`\u000ecÊe\u001e\u000ek&`+ ©$F\u000ek]\fd\u0010a6d\u001bk&]\u0013d\u0003\u000ekª`\u000ecÊ`­¯GkÁ e\u001ek&d\u0013d\u001ba/ehk\u000eÁ+l¤+4+ l?\u0004\u0007\u0004S9S¤4+\n5\b¤\u001b¥\u000e\u000e9"
    },
    {
        "title": "ISMIR 2002, 3rd International Conference on Music Information Retrieval, Paris, France, October 13-17, 2002, Proceedings",
        "author": [],
        "year": "2002",
        "doi": "10.5281/zenodo.6546714",
        "url": "https://doi.org/10.5281/zenodo.6546714",
        "ee": "https://zenodo.org/records/6546714/files/An Integrated Approach -Formatted Paper.pdf",
        "abstract": "EchoDB is an integrated search engine specifically to search songs, lyrics, albums, artists, writers, Record Labels, and all other music data across all major music platforms. The present generation which is Gen-Z has wide access to various information from around the globe right on their mobile phones. Entertainment being a day-to-day thing helped the industry boom within a few decades of usage of smartphones with the help of the internet. Music has become a daily chore and finding various genres is also now one of the drawbacks. EchoDB is an Integrated search engine that helps to access any song with suitable links provided along with the information about the song and the artist.",
        "zenodo_id": 6546714,
        "dblp_key": "conf/ismir/2002",
        "keywords": [
            "EchoDB",
            "integrated search engine",
            "searches songs",
            "lyrics",
            "albums",
            "artists",
            "writers",
            "Record Labels",
            "music data",
            "all major music platforms"
        ],
        "content": "HBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 1 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nAn Integrated Approach of Music Search Engine  in Cross \nPlatforms – EchoDB  \n \nDr R.Poorvadevi * \nAssistant Professor,  CSE Department,  SCSVMV University,  Tamilnadu, India . \n*Corresponding Author  \nE-mail Id: -rpoorvadevi@kanchiuniv.ac.in  \n \nABSTRACT  \nEchoDB  is an integrated search engine specifically to search songs, lyrics, albums, artists, \nwriters, Record Labels, and all other music data across all major music platforms.  The \npresent generation which is Gen -Z has wide access to various information from arou nd the \nglobe right on their mobile phones. Entertainment being a day -to-day thing helped the \nindustry boom within a few decades of usage of smartphones with the help of the internet. \nMusic has become a daily chore and finding various genres is also now one  of the \ndrawbacks. EchoDB is an Integrated search engine that helps to access any song with \nsuitable links provided along with the information about the song and the artist.  \n \nKeywords :- Echo DB , Songs Search , Integrated Music search engine , echodb  music search , \nechodb songs search  \n \nINTRODUCTION  \nOverview  \nIn the modern era of music industry, where \ntechnology has eased things for anyone \nwho wants to pursue their music career by \nintroducing multiple sources to compose, \nhost and release their songs. Platforms like \nSpotify, Apple Music, Am azon music etc. \nto release. Platforms like DistroKid, \nAmuse, Indefy etc. to host the music and \napplications like FLStudios, GarageBand \netc. to create the music.  \n \nThese various platforms are making things \neasy for anyone who can afford computers \nand the internet nowadays. Since, \nplatforms like DistroKid and Amuse \nexpect you to pay lumps of money to host \nyour music in  order to reach higher \nengagement in top grossing platforms like \nSpotify and Apple music, artists \nsometimes might not always pay, thus, \nusually opting for a budget one or \nsometimes free. EchoDB helps artists to \nengage their audience by providing every \nuser the information they require when \nthey have heard  any kind of music and EchoDB does even provide the user with \nthe availability of the song on available \nplatforms and helps the user to experience \nevery music, just from a few clicks away. \nHowever, EchoDB in its initial stages \ndoesn't have any sophisticate d algorithms \nto recommend music for the user in order \nto make the UI more interactive,  \n \nObjectives  \n1. To provide Music Data across all \nmajor streaming platforms and \noptimize web search (SEO)  \n2. Analytics using ML & AI to \nunderstand the Trends in Music over \nthe ages \n3. Design efficient Algorithms to enable \ncomputers to recognize Human \nLanguages (Natural Language \nProcessing) better than ever  \n \nExisting Solutions  \nIt is intended to be integrated search \nengines for music but they are mostly \nbusiness -oriented. The existin g solutions \nare Google’s Search Engine and Shazam.  \n   \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 2 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nIn fact, Google’s search engine is still \nunder development in the case of music \ndata. Shazam is more business -oriented \nthan open -source music, and upon \nsearching this will result in only a few \nsongs that are further redirected to that \nsong’s search query in Spotify's database, \nwhich results are obviously being cashed \nin many ways.  \n \nDrawbacks of Existing Solutions  \n- Having multiple platforms would \nreduce the required engagement for \nunderrated artists.  \n- Having multiple platforms would \neven make the user urge to subscribe to \nmore than one platform, in order to listen \nto their favorite music.  \n- Few platforms like Deezer, Napster \nare not available in all the regions of the \nworld.  \n \nLITERATURE SURVEY  \nLiterature Survey  \nOnly a few research papers/articles were \navailable in the domain of music search \nengines. Here is the abstract of one of \nthose few papers available.  \n \nIn paper [1]  - Categories and Subject \nDescriptors: H.3.3 [Information Systems]: \nInformat ion Storage and  Retrieval; H.5.5 \n[Information Systems]: Information \nInterfaces and Presentation – Sound and \nMusic Computing  General . \n  \nAn approach is presented to automatically \nbuild a search engine for large -scale music \ncollections that can be queried thr ough \nnatural language. While existing \napproaches depend on explicit manual \nannotations and meta -data assigned to the \nindividual audio pieces, we automatically \nderive descriptions by making use of \nmethods from Web Retrieval And Music \nInformation Retrieval. Based on the ID3 \ntags of a collection of mp3 files, we \nretrieve relevant Web pages via  Google \nqueries and use the contents of these pages to characterize the music pieces and \nrepresent them by term vectors.  By \nincorporating complementary information \nabout acoustic similarity we are able to \nboth reduce the dimensionality of the \nvector space and improve the performance \nof retrieval  i.e. the quality of the results. \nFurthermore, the usage of audio similarity \nallows us to also characterize audio pieces \nwhen ther e is no associated information \nfound on the Web . \n \nProblem Statement  \nDeveloping a one -stop solution for the \nexisting problems in the field of music \nplatforms, with simple solutions like \nproviding links to access the songs along \nwith the details of the artists and Music \nvideos of that song.  \n \nFuture Projects (with EchoDB as base \nproject)  \nGraphene (Lightweight PHP MVC \nFramework) [Used in the 1st version of \nEchoDB]  \nPitchGrid (App Builder) [Converts any \nwebsite into an android or iOS \nApplication]  \n \nPROPOSED W ORK  \nProposed Method & Advantages  \nEchoDB is an open -source database for \nmusic, where people can search for any \nsong, album, lyrics, artists, and anything \nrelated to music across all streaming \nplatforms.  \n \nThis project can be a base for many \npotential researc h works and products in \nthe near future.  \n• Integrated Music Search Engine  \n• Data across all major streaming \nplatforms  \n• SEO Friendly & Sharable search \nresults  \n• Preview Music Video  \n• URL/URI for any song across all \nmajor streaming platforms  \n   \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 3 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nSYSTEM REQUIREMENTS  \nSoftware Environment  \nThe software requirement specification is \nan essential role in the software \ndevelopment stages. It defines the features \nand behavior of a software application to \nmeet the overall requirement for the \nSVMS. Hence, the specifications of the \nsoftware requirement are mentioned \nbelow.  \n• Linux Server  \n• 300 GB SSD Storage  \n• 12 GB RAM  • 6 CPU Cores  \n• SSL \n• CDN  \n• NOSQL Database (MongoDB)  \n \nTechnologies Used  \nFor implementing the project the following \ntechnologies were used:  \nFront -end: HTML, CS S, ReactJS, & \nBootstrap  \nBack -end: ExpressJS, Redux, MongoDB, \n& Nodejs  \nHosting: Netlify Cloud  \n \n System Architecture  \n \nFig.1:-System Architecture  \n \nSystem Planning and Design  \nWhen a user opens the application, they \ncan see the main search bar for Songs \nSearch. They can also see the Lyrics \nSearch  button on the top Navigation bar.  \n \nIn the Songs Search screen, they can type \nin the query and press Enter or click the \nsearch button to search songs for that \nquery. Then the query is further processed \nand the system sends requests to Spotify \nAPI, Apple Music APi, etc,.. and other 3rd \nparty APIs. The fetched data will be in the \nJSON format and will be further \nprocessed and merged togethe r. Then the \nprocessed data will be displayed in the \nResults View .  Now the users can click on listen to open \nthe Details View. Now they will have \naccess to the details of that particular song. \nThey can also find the links for that song \nto respective stream ing platforms like \nSpotify, Apple Music, Amazon Music, Jio \nSavan, YouTube, erc,... and they can click \nthat link to listen to that song on their \nfavorite streaming platform.  \n \n• Get data from Spotify, Apple Music \nand Musixmatch and store it in the \ndatabase  \n• Run ML algorithms on server and \nrefine the data  \n• Using Data Structures like linked lists \nperform Searching and Sorting \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 4 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nalgorithms for displaying results on \nthe client’s device  \n• Store search data and analyze the data using AI & Data Sciences to \nunderstand the tr ends and make the \nUser Experience (UX) better.  \n \n \nFig.2:-Cross Platform Compatibility  \n \nModules Description  \nModule description provides a detailed \nexplanation of the functionalities involved \nin the application.  \n \nThe following are the modules involved in \nthis application An API was built to merge \nSpotify’s API, Apple Music’s API, \nMusixmatch’s  API, and other 3rd party \nAPIs to connect results with all music \nstreaming services. Since this current \nphase of the project is more like an MVP, \nJavascript XML  HttpRequest was used to \nfetch the data from all the above -\nmentioned APIs.  \n• Songs Search  \n• Lyrics Search  \n• Data merging API  \n• Detail View  \n• Redirection API  \n \nSongs Search  \nThe main screen of the application is \nSongs Search. In this module, users can \nsearch for any song, album, artist, movie, \nlyricist, songwriter, etc .  \n \nWhen the user clicks the search b utton, it sends a request to all the APIs like Spotify, \nApple Music, Jio Savan, Amazon Music, \netc. Merges all the data together, and \ndisplays them in the results. Now users can \neither watch the Music Video or know the \ndetails of that song 1or listen to tha t song \non their favorite streaming service.  \n \nLyrics Search  \nUsers can navigate to Lyrics Search from \nthe main screen of the application by \nclicking the Lyrics Search button on the \ntop Navbar . In this module, users can \nsearch for any lyrics. When the user c licks \nthe search button, it sends a request to \nMusixmatch API and displays the data in \nthe results. Now users can click on the \nlyrics button on a song in the results list to \nget the lyrics of that song.  \n \nData merging API & Redirection API  \nData Merging API Module is the main \nfeature developed for this application. This \nModule basically sends requests to all the \nrespective APIs and fetches the data.  \n \nRedirection API Module fetches the \nURL/URI of a particular song using Data \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 5 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nMerging API and wh en the user clicks the \nparticular URL/URI, it redirects the \napplication to the respective application.  \n \n3.6 Methodology    \nThe application for the integrated music \nsearch engine provides information about \nthe user interface and gives  a brief \nstatement abou t the app to give users an \ninstant understanding of the app. The \nsearch  allows a user to search for a song, \nalbum, movie, lyrics, music composer, \netc,...  and get the links and details of that \nsearch result.  \n \nFeatures Provided  \nThe verified MVP  includes features which \nplay some vital role in the whole model. \nBelow are the listed  features:  \nSearching for any song across all major \nstreaming platforms like Spot ify, Amazon Music, Apple Music, Jio Saavn, \nYouTube, etc. \n- Details of song Including \nTrackName, AlbumName, Artist, \nGenre, ReleaseDate, Language,  etc. \n- Providing Links to a particular \nsong to all major streaming platforms.  \n- Lyrics Search &  \n- Sharing a particular search result or \na song.  \n \nFuture Enhancements  \nIn further versions of this application, \nuser’s search data will also be collected  \n- To enhance and personalize the \nsearch results  \n- Understand and improvise Natural \nLanguage Processing  \n- Understand the Trends in Music \nover the ages  \n \nEXPERIMENTAL RESULTS  \n \n \nFig.3:-Home Screen on various devices  \n  \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 6 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n \n \n \nFig.4:-Home Screen/ Songs Search View  \n \n \n                    \n Fig.5:-Result View                  Fig.6:-Details View                         Fig.7:-Share View  \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 7 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n                         \n \n \n \n \n \nTelugu Old Songs Results  \n                     \n \nFig.11: - Telugu Old Song 1                  \n \n \n \n \n \nFig.8:-Lyrics Search  Fig.9:-Results View                            Fig.10: -Lyrics on \nMusixmatch  \n \nFig.12: - Telugu old Song 2               Fig.13: -Telugu Old Song 3    \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 8 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n \nTelugu Old Songs Details  \n                \nFig.14: -Telugu Old Song 1    Fig.15: -Telugu old Song 2      Fig.16: -Telugu Old Song 3  \n \n \nTamil Old Songs Results  \n                  \nFig.17: -Tamil Old Song 1       Fig.18: -Tamil old Song 2     Fig.19: - Tamil Old Song 3  \n \nCONCLUSION  \nUpon completion of all the preprocessed \nreviews, this project has been successfully \ndeveloped and presented as the verified \nMVP (Minimum Viable Product) and a \nfully working model for the project as of \n02-04-2022, and this has been approved \nfor further documentation which concludes \nthis report.  \n FUTURE ENHANCEMENT  \nThe verified MVP  includes features which \nplay some vital role in the whole model. \nBelow are the listed  features:  \n \n- Searching for any song across all \nmajor streaming platforms like Spotify, \nAmazon Music, Apple Music, Jio Saavn, \nYouTube, etc. \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 9 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n- Details of song Including \nTrackName, Album  Name, Artist, \nGenre, Release  Date, Language,  etc. \n- Providing Links to a particular \nsong to all major streaming platforms.  \n- Lyrics Search  \n- Sharing a particular search result or \na song.  \n \nREFERENCES  \n1. Knees, P., Pohle, T ., Schedl, M., & \nWidmer, G. (2007, July). A music \nsearch engine built upon audio -based \nand web -based similarity measures. \nIn Proceedings of the 30th annual \ninternational ACM SIGIR conference \non Research and development in \ninformation retrieval  (pp. 447-454).   \n2. Aucouturier, J. J. (2006).  Ten \nexperiments on the modeling of \npolyphonic timbre  (Doctoral \ndissertation, Université Pierre et Marie \nCurie (Paris 6)).  \n3. Aucouturier, J. J., & Pachet, F. (2002, \nOctober). Music similarity measures: \nWhat's the use?. In  Ismir  (pp. 13 -17). \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 4. Aucouturier, J. J., Pachet, F., & \nSandler, M. (2005). \" The way it \nSounds\": timbre models for analysis \nand retrieval of music signals.  IEEE \nTransactions on Multimedia , 7(6), \n1028 -1035.  \n5. Yates , R., B., Neto  B.,R. . Modern \nInformation Retrieval. Addison -\nWesley, Reading, Massachusetts, \n1999. Google ScholarDigital Library"
    }
]